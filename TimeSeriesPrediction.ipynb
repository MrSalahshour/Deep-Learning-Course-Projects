{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISKf-ajkZKU6"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "## Course Name: Deep Learning\n",
        "#### Lecturer: Dr. Beigy\n",
        "\n",
        "---\n",
        "\n",
        "#### Notebooks Supervised By: Zeinab Sadat Taghavi\n",
        "#### Notebooks Prepared By: Zahra Rahimi, Zahra Khoramnejad, Mehran Sarmadi\n",
        "\n",
        "**Contact**: Ask your questions in Quera\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Replace the placeholders (between `## COMPLETE THE FOLLOWING SECTION  ##` and `## THE END ##`) with the appropriate details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7FFJaRgaqC1"
      },
      "source": [
        "---\n",
        "---\n",
        "## 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q8G6YosbKrX"
      },
      "source": [
        "In this notebook you have to design and train models for a time series prediction task on the provided dataset using these three different architectures:\n",
        "\n",
        "- Simple RNN\n",
        "\n",
        "- GRU\n",
        "\n",
        "- LSTM\n",
        "\n",
        "You will compare and rank them at the end of the notebook and explain why they were ranked that way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FANbmLJ2d2dp"
      },
      "source": [
        "---\n",
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WdzOu4UGgDFQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWan6z7eKEp"
      },
      "source": [
        "---\n",
        "---\n",
        "## 2 Dataset\n",
        "Electric Production IP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rtouQ7MVomh0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Electric_Production.csv', index_col='Date', parse_dates=True, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-m3r2Dw_os75",
        "outputId": "2695836c-ad1f-4b12-8ebd-6bfea1c65603"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4ad25682-dd86-4df7-bf52-16a2619d79eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1985-01-01</th>\n",
              "      <td>72.505203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-02-01</th>\n",
              "      <td>70.671997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-03-01</th>\n",
              "      <td>62.450199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-04-01</th>\n",
              "      <td>57.471401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-05-01</th>\n",
              "      <td>55.315102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ad25682-dd86-4df7-bf52-16a2619d79eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4ad25682-dd86-4df7-bf52-16a2619d79eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4ad25682-dd86-4df7-bf52-16a2619d79eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0fa0ce20-48b2-4898-a684-af963ec840dd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0fa0ce20-48b2-4898-a684-af963ec840dd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0fa0ce20-48b2-4898-a684-af963ec840dd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                Value\n",
              "Date                 \n",
              "1985-01-01  72.505203\n",
              "1985-02-01  70.671997\n",
              "1985-03-01  62.450199\n",
              "1985-04-01  57.471401\n",
              "1985-05-01  55.315102"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc6T4bkqqVQX",
        "outputId": "40a0d14a-dd08-4d2e-fb18-ec239a52d3ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 397 entries, 1985-01-01 to 2018-01-01\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Value   397 non-null    float32\n",
            "dtypes: float32(1)\n",
            "memory usage: 4.7 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "o_WhChFOqYb4",
        "outputId": "097c4093-99dc-4f97-9e78-a7b3f7cdba7c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGwCAYAAACD0J42AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADS70lEQVR4nOy9eZxkZXk9fm7tvff0bD0DAwybA8q+IyrKhEUgEPmKKIoEBDVogvwUJQFUoqJEEwIS0WgQVBIlKi5JICMYQBx2RpAdWWaGYWaYpfeu/f7+uPW893mXW1W39+55zufDZ7q76q5V1HvqPOc5j+f7vg+BQCAQCASCGYTEdJ+AQCAQCAQCgQkhKAKBQCAQCGYchKAIBAKBQCCYcRCCIhAIBAKBYMZBCIpAIBAIBIIZByEoAoFAIBAIZhyEoAgEAoFAIJhxSE33CYwF1WoVGzZsQEdHBzzPm+7TEQgEAoFA0AR838fg4CCWLl2KRKK+RjIrCcqGDRuwbNmy6T4NgUAgEAgEY8C6deuw8847133OrCQoHR0dAIIL7OzsnOazEQgEAoFA0AwGBgawbNkytY7Xw6wkKFTW6ezsFIIiEAgEAsEsQzP2DDHJCgQCgUAgmHEQgiIQCAQCgWDGQQiKQCAQCASCGYdZ6UERCAQCgWAiUKlUUCqVpvs05gzS6TSSyeSE7EsIikAgEAh2OPi+j40bN6Kvr2+6T2XOobu7G729vePOKROCIhAIBIIdDkROFi1ahNbWVgn9nAD4vo+RkRFs3rwZALBkyZJx7U8IikAgEAh2KFQqFUVO5s+fP92nM6fQ0tICANi8eTMWLVo0rnKPmGQFAoFAsEOBPCetra3TfCZzE3Rfx+vtiU1Q7r33Xpx66qlYunQpPM/D7bffrj3+hS98AStWrEBbWxvmzZuHlStX4sEHH9Ses23bNpx99tno7OxEd3c3zj//fAwNDY3rQgQCgUAgiAMp60wOJuq+xiYow8PDOOCAA3DDDTc4H997773xzW9+E08++SR+97vfYbfddsPxxx+PN954Qz3n7LPPxlNPPYVVq1bh17/+Ne69915ceOGFY78KgUAgEAgEcwqe7/v+mDf2PPz85z/H6aefHvmcgYEBdHV14Te/+Q2OO+44PPPMM9h3333x8MMP49BDDwUA3HHHHXj3u9+N9evXY+nSpdY+CoUCCoWCts9ly5ahv79fou4FAoFAEAv5fB4vv/wyli9fjlwuN92nM+dQ7/4SJ2hm/Z5UD0qxWMR3vvMddHV14YADDgAArF69Gt3d3YqcAMDKlSuRSCSsUhDh6quvRldXl/pPJhkLBAKBQBAfxx57LC6++OLpPo2mMCkE5de//jXa29uRy+XwT//0T1i1ahUWLFgAIGjtWrRokfb8VCqFnp4ebNy40bm/yy67DP39/eq/devWTcZpCwQCgUAwY3HqqafixBNPdD523333wfM8PPHEE1N8VvGwsX+06edOCkF55zvfiTVr1uD3v/89TjzxRJx55pmqL3osyGazanKxTDAWCAQCwY6I888/H6tWrcL69eutx2666SYceuih2H///afhzJrHX/3osaafOykEpa2tDXvuuSeOPPJIfO9730MqlcL3vvc9AEBvb69FVsrlMrZt24be3t7JOB2BQCAQCOrC932MFMtT/l8cG+gpp5yChQsX4vvf/77296GhIdx22204/fTT8f73vx877bQTWltbsd9+++Hf//3f6+7T1Y3b3d2tHWPdunU488wz0d3djZ6eHpx22ml45ZVXmj5vjnK1+eudkqC2arWqTK5HHXUU+vr68Oijj+KQQw4BANx9992oVqs44ogjpuJ0BAKBQCDQMFqqYN8r75zy4z591QlozTS3FKdSKZxzzjn4/ve/j7/7u79T7by33XYbKpUKPvjBD+K2227DZz/7WXR2duK//uu/8KEPfQh77LEHDj/88DGdX6lUwgknnICjjjoK9913H1KpFL70pS/hxBNPxBNPPIFMJhNrf+VKtennxlZQhoaGsGbNGqxZswYA8PLLL2PNmjVYu3YthoeH8bd/+7d44IEH8Oqrr+LRRx/Feeedh9deew3vfe97AQD77LMPTjzxRFxwwQV46KGHcP/99+MTn/gEzjrrLGcHj0AgEAgEggDnnXce/vSnP+Gee+5Rf7vppptwxhlnYNddd8WnP/1pHHjggdh9993xyU9+EieeeCJ+8pOfjPl4P/7xj1GtVvHd734X++23H/bZZx/cdNNNWLt2Lf7v//4v9v4mVUF55JFH8M53vlP9fskllwAAPvzhD+PGG2/Es88+i5tvvhlbtmzB/Pnzcdhhh+G+++7Dm9/8ZrXNj370I3ziE5/Acccdh0QigTPOOAPXXXdd3FMRCAQCgWBC0JJO4umrTpiW48bBihUrcPTRR+Pf/u3fcOyxx+LFF1/Efffdh6uuugqVSgVf+cpX8JOf/ASvvfYaisUiCoXCuBJz//CHP+DFF19ER0eH9vd8Po8//elPsfdXrkwiQTn22GPr1sx+9rOfNdxHT08Pbr311riHFggEAoFgUuB5XtOllunG+eefj09+8pO44YYbcNNNN2GPPfbAO97xDnzta1/DP//zP+Paa6/Ffvvth7a2Nlx88cUoFouR+/I8z1rTeUT90NAQDjnkEPzoRz+ytl24cGHsc69Umy/xzI5XQyAQCAQCAQDgzDPPxN/8zd/g1ltvxS233IKPf/zj8DwP999/P0477TR88IMfBBD4P59//nnsu+++kftauHAhXn/9dfX7Cy+8gJGREfX7wQcfjB//+MdYtGjRhHTQlmIoKDIsUCAQCASCWYT29na8733vw2WXXYbXX38d5557LgBgr732wqpVq/D73/8ezzzzDD760Y9i06ZNdff1rne9C9/85jfx+OOP45FHHsHHPvYxpNNp9fjZZ5+NBQsW4LTTTsN9992Hl19+Gf/3f/+Hv/7rv3a2OzdCOYaCIgRFIBAIBIJZhvPPPx/bt2/HCSecoBpMLr/8chx88ME44YQTcOyxx6K3t7fuKBoA+MY3voFly5bhbW97Gz7wgQ/g05/+tOZZaW1txb333otddtkF73nPe7DPPvvg/PPPRz6fH5OiUm6en4xvFs90IU6Wv0AgEAgEHDKLZ3JR7/4u///+E6/843unfxaPQCAQCAQCARCE4VVitBkLQREIBAKBQDDpiGOQBYSgCAQCgUAgmALEMcgCQlAEAoFAsINiFlowZwWi7mucFFlACIpAIBAIdjBQGy3P+xBMHOi+8nZlIF6KLCBBbQKBQCDYwZBMJtHd3Y3NmzcDCFppafCeYOzwfR8jIyPYvHkzuru7kUzqMf5xBgUCQlAEAoFAsAOit7cXABRJEUwcuru71f3lKMUs8QhBEQgEAsEOB8/zsGTJEixatEibPSMYH9LptKWcEERBEQgEAoGgSSSTycgFVTCxEJOsQCAQCASCGYe4JlkhKAKBQCAQCCYdpZglHiEoAoFAIBAIJh1S4hEIBAKBQDDjENckKwRFIBAIBIJpwvfvfxlf+OVTO0SqrSgoAoFAIBDMEnxj1fP4/u9fwbpto9N9KpMOMckKBAKBQDBLkC9Vgn/LlWk+k8lHSYYFCgQCgUAw8+H7Pko1VaFYjrd4z0aIgiIQCAQCwSxAkZlGizENpLMRYpIVCAQCgWAWgKsmpR1AQYk7i0cIikAgEAgE04ASK3mUYpY/ZiMq4kERCAQCgWDmQ1NQdoAST1wSJgRFIBAIBIJpACclhR2gxCMmWYFAIBAIZgEKO5iCUpYSj0AgEAgEMx+clOwIBEVKPAKBQCAQzALsaB4UMckKBAKBQDALwEnJjhDUJgqKQCAQCASzAJyUFHeANmMxyQoEAoFAMAtQ3ME8KGKSFQgEAoFgFmCHS5IVBUUgEAgEgpmPHW0Wz6SbZO+9916ceuqpWLp0KTzPw+23364eK5VK+OxnP4v99tsPbW1tWLp0Kc455xxs2LBB28e2bdtw9tlno7OzE93d3Tj//PMxNDQU91QEAoFAIJi1KO1gBGXSFZTh4WEccMABuOGGG6zHRkZG8Nhjj+GKK67AY489hp/97Gd47rnn8Od//ufa884++2w89dRTWLVqFX7961/j3nvvxYUXXhj3VAQCgUAgmLXQSzzxTbK/WPMavn7nc/D92WGwjetBScU9wEknnYSTTjrJ+VhXVxdWrVql/e2b3/wmDj/8cKxduxa77LILnnnmGdxxxx14+OGHceihhwIArr/+erz73e/G17/+dSxdujTuKQkEAoFAMOtQ1IYFxldQ/uY/1gAADl/eg7fvvXCiTmvSMOO6ePr7++F5Hrq7uwEAq1evRnd3tyInALBy5UokEgk8+OCDzn0UCgUMDAxo/wkEAoFAMJsxUUFt67ePTsTpTDpmlEk2n8/js5/9LN7//vejs7MTALBx40YsWrRIe14qlUJPTw82btzo3M/VV1+Nrq4u9d+yZcsm87QFAoFAMAtRKFeQL1Wm+zSaxkQFtY0UyxNxOpOOGZMkWyqVcOaZZ8L3fXzrW98a174uu+wy9Pf3q//WrVs3QWcpEAgEgrmAStXHUVffjaO/ejfKs8Rwqge1xTtn7jsZLswOUlaqxlNQYntQmjqJGjl59dVXcffddyv1BAB6e3uxefNm7fnlchnbtm1Db2+vc3/ZbBbZbHYyTlUgEAgEMwi+78PzvNjbbRsuYttwEQDQN1rCgvaZv2aMZ1hgmS32I6XZoaDEJY4TrqAQOXnhhRfwm9/8BvPnz9ceP+qoo9DX14dHH31U/e3uu+9GtVrFEUccMdGnIxAIBIJZgmrVx/u+/QDO+beHYnemzKbSDkH3oMS7Xk5oRmaJghLXJBtbQRkaGsKLL76ofn/55ZexZs0a9PT0YMmSJfh//+//4bHHHsOvf/1rVCoV5Svp6elBJpPBPvvsgxNPPBEXXHABbrzxRpRKJXziE5/AWWedJR08AoFAsANj+0gRD72yDQBQKFeRSyeb3naY+TBmS2x8cRweFP784VniQZn0Es8jjzyCd77zner3Sy65BADw4Q9/GF/4whfwy1/+EgBw4IEHatv99re/xbHHHgsA+NGPfoRPfOITOO6445BIJHDGGWfguuuui3sqAoFAIJhDKLBFt1CKR1CG8oygjCFTZDowHg8Kf/5gfnYQlLgm2dgE5dhjj60rvTUjy/X09ODWW2+Ne2iBQCAQzGHwMk2+XEEX0k1vO1QIF+nZkso6njZjvm3/aGnCzmkyMaPajAUCgUAgaBamghIHnKDETSydLozHJMsX+/6R2UFQpt0kKxAIBALBWMAVlEI5nvFzVpZ4OEGJec5cQekbLU7YOU0myjE9KEJQBAKBQDAjwBWU/DgUlNlT4gkX7LjnzBWXvlmioEiJRyAQCASzEuNSUAo7VhePVg4rVzFanPmtxjMmSVYgEAgEgjgwF9040Eo8s4SglMZhkjWfH7fMM1Is4/X+qZ3hM+OGBQoEAoFgx0KpUkUlpt8AMLp4YgavzXYFZTxdPED8Ms9f3vQw3n7Nb7F5MB9ru/GgJAqKQCAQCKYLxXIVb7/mt/iLf7k/9rbjUlC4B2WWmGTHMyzQUlBiEpSXtwyjVPGndBLypCfJCgQCgUAQhec2DuL1/jxe78+jWvWRSDQ/V6cwQR6U2dJmPJ6oe5PQ9Mcs8VBHzXimKMeFmGQFAoFAMG3g5CCupD+uLp5Z6EHRTLKVaqz5Q2bXz/aYCgrdo6m8V2KSFQgEAsG0gXtP4kr6WhfPeDwos6TEY6oXcXJCzG1HYnbx0GszlQqKmGQFAoFAMG3gBCXut/MJ86DMFgXFuMY498ssl8S916R0TWmJRxQUgUAgEEwX+MIZlyhMVFDbVJd4fN8fU9eSeZ5xyIK5bSnGtr7vq9ep3mtUKFcmlMCIgiIQCASCaQMv04yrxBPDJOv7/rR6UM75t4fwZ/90T+zF3Hx+HEI3HvWFk6moc+4fLeGtX70b7/326qb3Ww++78eOupcuHoFAIBBMGEYZyYhd4imNrcRTKFe1xS9ut8h4cf+LW1D1gU0DeSzraW16O7tMM3aTbDHGts3cq589th5bhorYMjQxc37ikhNAFBSBQCAQOBCno4QjPw6Cki+PLaiNl3fGctzxoFL1QWtvXNXHJBlxyjTj86/w/BX3Of/+T1vVz1WDXPi+HztIbywlMCEoAoFAINBQLFdxwrX34oJbHom9rU5Q4i1KY1VQeHknOK697VChjP98dD36Ryd2sF5xjL4Zfm9StayYOCUe8xrLYzTYul4j3/fx+xe3hM8xzK0f++GjOOxLv8Ebg4UYx4xPGoWgCAQCgUDDa32jeH7TEO5+dnNsJYUv0uNRUGIRFEtBsc/52lXP49O3/QEfufnhWOfUCMUxdh5xMtKWTVn7inPcYH8xSjxG/oqJ5zcNYbjoJpq+7+POpzZhsFDGqqc3xTimKCgCgUAgGCeoVFGp+rFVkInyoIynxONa6Fc9EyymD7+yPdY5NQJf4OOUeHg5p71GUMZSpskkE/G3bWCSfWpDv/Y7JzRcgVrUkW36mOJBEQgEAkHT2D5cxKdv+wMeenmb9neugozGDADjxCLuTJyxKijmOboW64OWdauftw41X5poBJ2gxFdQkgkP2TSRjPgm2dZssrZt88dupKCYpSr+nLXbRtTPqWTzYwzIg5KOsY0QFIFAINhBseqZTfjPR9fjX+97Sfs7JxkjpbK5WV1wBSXuTBzNgxJDQTG/nbsW6/Zc2LRqErLxQCvxxPCg0HaZZGJMKgiRv7bMWNQX5kFxkCrTOMvLM5ygxDG+0nshzmwmISgCgUDQAP0jJbzjH36Lq//7mek+lQnFYM1capZT+O/jUVDG1cUTQ40wF0qX34H/7cEJJCilMZZ4SJVIJz2kawQllgeFFJRMsrZtfLLA98NRL6WWE5Q4ig+9RikhKAKBQDBxeGpDP17dOoI7nto43acyoRgtBgTFJBK8VBF3xgsvD8Qt8YxVQakaRl7Xosv/9tjaifOhjNkkSwpKKqnKHrG6eGrbt43Bv1LWunjq36vgOeHz141ZQQmem/SEoAgEAsGEoVD7wI6b/TDTQeTD/CasKSgxr5krLrFLPGXuX4mx4DZR4uHXOFyIV7aqh7HODyKSkE56yKTGUOKpEEGJ70EpNfDNmH+LUlDivL6ioAgEAsEkgBbLuPNhZjqIfJgZGoXxmGTLk9vFU6n6+ONr/Vrnjhkk5io98GuM21Hy1IZ+nHbD/bifZYOExxqb6kPZIilW4hkLyRiLB6VRkqw1ZTnCgxKnxEP7SApBEQgEgolDSFDmloJC5MPM0OAkI26JhxOa0gR38Tz66nYc8qVVOOX63+GSH69RfzdLDW4FhRGUmK3Tf/Mfa/CHdX04+7sPWo+NtcQTKgqhSTaOalQwSjxxclC4MdaVJGsNMaz9XqpUsaEvr/5eGYOCIgRFIBAIJhC0cBTKVevb+mxGWOKpo6DE7OLh5lYzgbQezMwV12L/X0+8jr6RIIdj/fZRbVsO10LPF/C4pafBfHT6rN7FE6PzqBKWPKjEE4fglMwST5xtYyoodKxtw0XtXsdSUGr3PCltxgKBQDBxGGvWxUwHERSzxKO1Gcct8WgKytiTUV1qFVdY+EJZMUyyrhIOP5e4CkpnLh352FjfG2rBTnjIpQOSMZY25TGVeLRZPK42Y3eJxzxGHJMsV4yahRAUgUAgaAB93srsKfPc8/wbeHJ9f+TjpI5YJllGBMbnQWl+ATPva7nq1/XG8MWSFr96eSJcNXE97vt+ZKx/V0s0QWlkOI1CWQWXJZCrBbXFeW+FCsr4clAadTzxfZvCk+uYmwfzOONbv8dtj6zT/l6WEo9AIBBMPDhBidvVMl3YNJDHuTc9hFO/+Tt81whiI4xGlHjGkySreVBilFJci7u5UHLywx+jNmNa6BuXeOzpvOf820M47Yb7napAJyMoJonRu3iav1cVZhrNpgIFJR8nR6Ws56CMpdzC96PtO4KgmKUxl1J1w90v4tFXt+Mz//mE9nfp4hEIBIJJAP/Ani0KytahImgt/fJ/P4PX+kat50R6ULhJNm6bcWlsJlm6r7TgBn+LVlB4mYZ+plKJ0yRbp8RTKFdx3wtb8MT6fry6ddjatpOl0A7ko+f+xCnR0GKfYlH3Y5mG3K5Msvq2o8UKbrr/ZazdOmJt2zAHxfKgBM8382ZcZC5KRaLjSJKsQCAQTCAKWolnZnlQHl+7HUdffRf+89H12t85yfB99/wZIhN2DsrYFZSoMkzD7ZgiQMFlpiJRiGhhpoWz3kwb/nxT2RlgJlhXCSLBwsW2GPdxvCWeVNJDrqagxFFg1P2KKPHc+tBafPFXT+Pt//Bba9tSXA9KlRSUxt1SHYzMcSgFRYLaBAKBYOIw3hLPQy9vcyoY44Xv+/iLf/k9NvTn8cVfPaU9Vi9sixCloIw16r5S9bVv8nFKPHTMbCqpSh6mItHIg0ILvduDEi6uvq9/+x8YDVWRRsmqW4eK+mNjLPGEXTwJpfzEU1DIJOvu4nlx81Dkeek5KI0JCv1uKk8uBaWDGYpHiuF9FQ+KQCAQTALG2koKAE+s78OZ316Nt3717ok+LW2mzPIFbdpjNkGxF5N8Ex6UOCUes/wVp8RD55tNM9NoXQXF7uJpqePHiGqdBfQ2YhdJKGkERVdQxpqDwhfssZhkVRcPKSgGWdh1fqv6+bFX+/RjN1BQzPcDnatZ4nHdZ2qZBnQypxSUpHTxCAQCwYShWOFD7OIRlAde2jrRp6PwE9Yp0dOW0R6rtyADgfpC5KNqKAqFBl0867eP4Jt3v4C+EV1NMNWlOCWeZhQUTh64YZMMp/UUlKhFFwiHJgJuFYQvxFuGi8ZjY/OgUMhZOhmaZMeUg8LajLmBl7/+ZgKu3sXjIHO1fefSeleUWeIxu6zM425l96o8FSbZe++9F6eeeiqWLl0Kz/Nw++23a4//7Gc/w/HHH4/58+fD8zysWbPG2kc+n8dFF12E+fPno729HWeccQY2bdoU91QEAoFgSqCVeIrxPChxuiviYmA0/OZvyu2WrG+cR7FSNUK33AutK6jtw//2EL7+v8/jUqNTw24Vju9ByaYSzDRaX0GhBblieVCaICjsd+5BcZEMvu2WQV1BKfD7FoO8lipjV1CqVV8t+K21oDazbMXP5T6LoHAFxT6mnbHiq+NyuLp4NILC1CYiZJNqkh0eHsYBBxyAG264IfLxY445Bl/72tci9/GpT30Kv/rVr3DbbbfhnnvuwYYNG/Ce97wn7qkIBALBlGA8OShx59HEAf/GXS8NFnB3eXDw7RtF3f/pjaDT5X+f1r9YmvcmzjRjIgz1FAXzd1V6qP3bUqeLxyRonDjqCkojVWBiSjw8uCxuUBt/LYlEAPo18X29uGlQ277hLJ4KGXD1+2kpKA4Cqvl1uIKiPDfWJpFw223r4KSTTsJJJ50U+fiHPvQhAMArr7zifLy/vx/f+973cOutt+Jd73oXAOCmm27CPvvsgwceeABHHnlk3FMSCASChugbKeID//og/vzApfjYO/aItW0xYvFuBpNJUPjiaCsopmKgP24SD/74WE2ypn8jzrWTCpJMeMjUunjqmXfp8XQyoRbOsM04UFc81jFiEjS+uOoelPqzaUyTbKMunrVbR3DVr5/GR9+xOw7brYcdP+ziyabcnpsoaAQlm9T+3gKb3NUzTDuD2gwFhcijqaC4TLK6gmJ7UJIzOUn20UcfRalUwsqVK9XfVqxYgV122QWrV692blMoFDAwMKD9JxAIBHHw+Lo+PP36AH722PrGTzagl3jiERQ9r2NiyYrWOmtle9T3g9Tzi/AFLU7X0ng8KHyYXDjdtz7posdViYd9PTe3tUs8zSsofF91u3gc9+rzv/wjfvPMJrz3Rn19o/fCWKLuecdOq6aguMtNZiovv/ZK1Y+cZWQOIrTbjOvnoPASz5R4UMaLjRs3IpPJoLu7W/v74sWLsXHjRuc2V199Nbq6utR/y5Ytm4IzFQgEcwn04R93tgxQ/9toIzSKFR8P+P4aKSiNSjxRYXRx7petcMSf1ZLwOEGpX+IpGd/saaE3t61UfZhf9vnj3MvjNsmOvcSzzTDVElTUfSK6aykKpUq42AeEzlac6r3+Zvt31PRiCs0jcmPNPIphkp2z04wvu+wy9Pf3q//WrVvXeCOBQCBgoIUnrgICTJwHxdXSCQDrto3g/O8/jIdY23Dc82rkQYlX4qmvGEVlbdXztTQCj0JPp2yCUjJMvfyc6WktLIU2Ki2V2mCju3jq+ypMotdoWGBvV079zLts6PySzHPT7HuLXncicorQMc9P3Q4oh2Gao2SUeOj+VYztnCZZbih2KCgzmqD09vaiWCyir69P+/umTZvQ29vr3CabzaKzs1P7TyAQzF5MdKmjGdDiMRYFhS9wsYfnsUUnSn358E0P4a5nN+OCWx6Jte/6HpT6ZIGHaJmPax6UUsWaP8ONmfy4+SbC4aKglXgS9RUBIkhq4awpAplkQj1WNMgNIZxdwxQUTlAadPGY2S7cCOxSXxZ3hgRli+bJCPaZ0rp4mjXJBsch5YQIik6WojugzP//rGA2amHO6l08dpuxy4MSHmer43pnNEE55JBDkE6ncdddd6m/Pffcc1i7di2OOuqoqT4dgUAwxfjTG0M48KpV+Mb/Pjelx6UP4dFSxTL7Ndx2HCbZ4WL9xQ8AXqp1xfSzUkMz4CUUc/FolIMSVY7xfV8jA2Y6LBDmYwDA9hHeqVFftakHKh/wEk9R60oJz7c9o8+fUdsm3OUhfp+o00f3oDQwyZb5fa5TOqnYfg7O7ficn9CTkWBdS829t+j1ydS2c12z+ZppKbx1IuurVV/drzaji8cMamvYZjw8Pg9K7C6eoaEhvPjii+r3l19+GWvWrEFPTw922WUXbNu2DWvXrsWGDRsABOQDCJST3t5edHV14fzzz8cll1yCnp4edHZ24pOf/CSOOuoo6eARCHYAfO1/nsVQoYzr734R/9/xb5qy45rGT/p22Az0Ek889WeowCfw2gvQcCEkMCt6O2Ltm1+TuXA2SpI1lSRahFwqz2ixohZR8zlbh4pY0J4NzqFB6aAeiDSmkp7q9OBm0LxalBNBmaZgl3iSXqC+FKEvutyQGpZ4IhSUBsmq5n00c0QK5YpmXOWk45WtIzi01smjzeJhUfdm95EL9H4kU7Cr68kaE1C274e5P0B/zVqNEk/sNuOhorqecHrzJHbxPPLIIzjooINw0EEHAQAuueQSHHTQQbjyyisBAL/85S9x0EEH4eSTTwYAnHXWWTjooINw4403qn380z/9E0455RScccYZePvb347e3l787Gc/i3sqAoFgFmIy227rgX8Ixy3zjGcWDycgLnLz+No+9fPCjmzM8+KyfdwSTwRBYedIX3bNa+Yqg6tTI+PwkDRCWTPJerW/8QWXkmYTlmLAyY3Lv0KLZjrpqW/wpQgFxaViRJWLzP0E5xm9+GsKCgtqyzJFqqC9T8v4t9+9jPXb9YnEJkFxXbOdIeMeE2Bux39uy+hqkxXU1mCkQJkFypWmQkE59thjrXokx7nnnotzzz237j5yuRxuuOGGyLA3gUAwd+GShacCjeLb62E8s3g4QXEpCg++HEbhW1NkK9W6s0v4/sz7aueg6L/bhtZgeyphJRMe2jJJDOTLGpkpVaqR0e9EKFozSRTL1XglHuZByTjajIncZVNJpAzFQCc3tfIQK8vQftKJkNzEazOuYzBtECbHX6NXtoZEQ0XdJ8JpxrQ9KSq//sPruOrXT+OqXz+NP1x5PLpa09oxiAi6rtn2oESTLP4Yv57WrF5Ka0pBcQwaTCcTs8ODIhAIdmzEWbQmEpqC4ohvr7utZiCNW+Kp70F55vUw14kf55FXtmG/L/wvbv79K5H71jwoEZ4DWsTMmStRCkqeKRXUFcPJjKmmaApKpXGiaxSqLKgtXHBt02cunbAITLgt1GN88aTzSKcSIbmpPe77fhNBbXqreNTMG36e6nf2mnMFpaQIWQLppKfUKk6AuXJy+S/+aB0zaxCUugpKnS4eVyt8OsnKYTEUFIuglXVyIwRFIBDMWLjSJ6cChXGUeMYaXAY0VlD4/vhid9nPnsRoqYLP//Ip537NgC1bQQn225HV00Bdxw221z0ouXRSEQ3+3Lxx7/RW0mBb2i6OB0UtYJ5nKST8vLKMZKh8DmOxN7ctaSUefdEdLla0jBRzgXUFmUXNMHJtz+/B5gE2m4ayTJIePE/3oRD4Ue974Q1rn5kmPChUUtGD2+p4UEidSSas/YZKFbTfo66X/x5e7wxOkhUIBDs2zJCoqcJ40mDHk4MyzEyyrvIQ/2bLj8MzPRqdE+DyoASPt+d0oyNh1GgzpvIAXV8ulWCLZnRwG28lNSPn46hlVWeJx77v2VTSaqtVBMVzlztUiScZEhgiN1w9AWyVyznXRxvKF61UmL+7wtKIQLji7vm9zjuILBmXU04FJXh+Z0u6tj0nbI09KOlUQpE5UnvCxN7a69tkiSd4bo2gNDAAcwhBEQgEU4rpKvHwb5Bc1WgE3/eNEk/zBKVYrtYN+QrOy+0N6MjVtwiaC0FUkqwK2zIejy7xhApK1vGt3lRetrhKPI6skUagpyYiou5DZSehFuSwi8cuD7kVFHvR5f6T4DiNw+bqvaZWiUdTNcLHzGRVV9y9TlDC0hIdI/SgeLVzse9XZ+191KyCUmAKijLflkkFqRGjtO3jce2L72/OJskKBIK5g+kyyY61E6dc9bUsizgeFJMIuTwofOHg59iRTdfdt5X+GbHoKAXFWDhGIko8RMAyqQRy9K3eCG7j4AoRqRIUhhanxMODy+p5KrKppFV60HNQaos1J34qedWzykOmgmK+vq5F2JVSG5ZSGi/+AIu6r12rUqvKbtWEb8/LMHwfvIxHzyEFRSfC0e3gat+phArMo/cGnXPWkcbrusbgWOJBEQgEswTTkSILjN2DYn4jjKOgDBkEpZGCUoxQUFydk+a+fF83MRLxac+6Szyml4TCyLgHxVXiqTfDZzwlHi2oLWV7KlSJJ22rIKrNOBHGxuvlFF7i0b/9m++FKAUlmQiNrK7xBe0OpcL8nQ/t49ksACvxaOU0N8FVZC1NHhSd0JUrVfVadOZqBKVkE0nzGvjPGdbOXTTMyOHr23yJR3Xx1K9cahCCIhAIphTT12Y89QRl2Fpg6ntQ+Dl25EIFZdBRknLN9eEqivKgZJss8RgKSi7Nh9hF+3fMBRgYWxePCluLaDNWxCmVtEoPvM2YFm0t94PNrlE5KKQKNJiYzLtanImttecToayXg8L3x2cPAVDltIJWTnOXjwoRCgoRCU7OuhwKij2V2FZXMknbjNyMgkLHpveO2aI8o6cZCwSCHRu8/l0vU2mioZtkm/egWFOAHbNpomCWeNwKirvEQ6UKAOgbtiPw6bktbIIv96HQQqfmqUSUeGghoce5GTX0RVSs7VznbLYZl6t+02MFeImHFjFttgxTUMzSA29RDgfvuTwonqWgNJ6YHK2+VFkQGZXkzFEIliejZCzYVOJxmGTN9yldE1c5AFiEjZMcRZwcKgkRDT1JNizxmcpMVREUt0LGO57aa/cjVFDCTqtmIQRFIBBMKfiH2lS2HBciuiMawVxgqr5dw48Cj7kHojwoeomHyA//dto3WrS2o0WjlXX76IoDlXhCssBBix+VAEpqwa59g04lVIBY3tFmTAufnhxaI038nJrs2tJMssaCG1xPuKiaigE3YCoFhXe9cJNsROssEcKoEk8mabcwcwI1vz0DABgYNU23bgJE/x+kDJNsPUMykRc6blbN4tHPi46RSnjqtXApXa7BidzfYpqRGykomofKeH+IgiIQCGY8SprBcHoUlDgEpeBQKpodGNicgsIVpXCx5eRt+4itoKjzykQoKKrEQwTEUFCKehuqKnlQwmkyXNxci2Z3q/4NOdhW7+IBmvehKBXEizDJKsWHJck6OkQUqXIoO7yLR0WwV/RSmGmSpetLswW75Cil0NRiPjyRb6+uo/beoftM1+L2oLhHDJghfKbSQcfIphJORSk0M9vqSoFdb9gdZCgo1MVTpxvIVG7CWTxCUAQCwQxFhX2oxenyGC/MGSfNgj5027IpUIRDsz4U0yTrip83VaSwZh8+t2/EVlC4TK+CsxxdL+ZEWgJdA3kUyCTLF2RaiFyLZndLRjtffnxO5pr1oZRZFHqYBhvemzy7XjMtNsxB4QqKq8Rj56AQ2VAmV2vKc23blGeVlviCTHOU+gwyabch11dQtFBAi6CQcmO2GeuKUmiiTSri45rFEyoo9pcGbpI11aZcRImnwIZhmp1cPIivWQhBEQgEUwr9w3DqCMpYFZRQTg8XRpdB1QVLQYmQ+13P4bdm+7CDoKg00aSlCnDi06GC2twmWUVQLDMjK/FwX4ShoGgR6qwEQOuQi4RuHy7iQ997EL9Y85r6GzfJOqPumTcmLNPoAWIJTUHR5wcBepsxbUtEpc2hJvDnpXkuiCI34X57WgPCxhUU/jq0GCbYsuHJyDnIIN1ret/RY42i7ukY2VTCSdjUzKSsXaZzdfGUjfvcSEEJJk4ntb/JLB6BQDDjwT+Ap5Kg1PtmWg/ah66ju6QeGrWw1ico4WOuEk+JnVfYbWGXHlwlHt/31eJHQV5U4gn3645fV94V6g7RFJTQ+Jk2IuU5vvzfz+C+F7bgb/5jjfobT5J1xdXzoLbIacYJt4JSdJZ4aterTJ0hQeEmaO5BMSchc78GETb+Wrnaxuk9oLp4kmZQm0OtaqU0WL2LJ5xmrJe8SGHhJR6ti6d2/m1OD0qozpglHirTRJlkFXFK2gbbsnG9zUAIikAgmDIUy1VNtp/KVNnxthlnkolw8F6TCoqV7hrRNppOekyK1z/QgYgSTyVcDJJG6YEfJyzxsHJJqarC5zqNEk+JKyh1vtWT8lJkCzpvnXWRDMKT6/utv5U1guKaZmxH3VszYiIUlLJSOniJx1BQsmHujCubhnfxKJMsI4nzagoKf63qddOYAW/m61+p+mr/tG8qc5ldPJYHhU1+dpd4dA+KlhnD2oxNBcVlktUGJ1bC8zI7hMrKgyJdPAKBYAaiXsjXZEPr4okTV1/h3yht82Y9WBOGzd/ZQpIxygeNTLLKK5JirbNV3YMQEB/7WzLvDqEuHqUosAXZHdQWPN7dEua0hB1AtUU36VnlEA4+pZfQ0CTLwslMkuHyoLiUuqDEo5MfZZLNuQkKT6E1F2xOXua1kYISEhR6POGFBMhUUMyoezpv/hrRvhuVeGwPSsLpbTG7eIoOQpZxELKqrxMUfh18P5r6YhiZpYtHIBDMSJhtk9PmQYkxi0evq+vfchshTFa1yw58P1n+jdNQBQC7M0Q7L66gqAUq9C6YplAgNAnz4/IUUiBYkEMFhZObYFsqO/BzpkUonUg4VRDCsEPBqjAVxCwtBOdQ6+JJJRXJKBoLZyrhqTwRXQVhPhJDbVKG0XRS+WZchlJXizJ/b3STgsIya/hQP6VklIx7ZUbd1x4nMu95LGzNKPGYJlmziyeTTFjHBViJJ2un7vJ9q9JhTSlRCgozQZcjCErGej/XyJoQFIFAMBNhds+UyvbiNRnwfb/pEo8ZwlZkkvdYSzxtDikd0NtBTQNuhS3s/aMOBYXJ6eaiy7s4XESBFr/WTJjKStN/+YLsykEZNcy1/JxLvEyTcJd4+DduvlZVGMmgc+IlQF1BMcybvMTj8HJowwKN+6FakFPuxbzE77PZZuwo8QwWyjZRcHhBSoZp1JxmTPe5JZ1UBlsrqC1ZPwclm05Y+/V9Xx27JW2H+LlMsnTNVaPEA0QQlKRN5kNCJgRFIBDMQJjEYKpKPK40WBfuef4NHPDF/8V/P/l6uK3jA7tZ5Ud9O8/aiyagEwmT/DStoKSSSLJvusFxQmUm5VAjRtjil7IUhbBM4xpgR9u2ZVNsQB6VLRqXeF7bPqp+XtCeVT8TIUtoUfeOEg8jZGY5TAtq00yhYTnMjm+vPcZSaLV0X+YVUWqUob5kkgl0taSVAkOtxlyNML0gdL1kJrZmBNWUqhbHTCQqFVolHiNJNptKWhH6FTb80qWguFqy6ZrNWUv8PvLzCgzlevnI7FpqBkJQBALBlMHMD5mqEo+peETloHz43x7CQL6Mv/rRY9a2mmzdtIISPC9SQWFEwtw37+LZOhRtkk0nPatjxqXMlF0EJWN7X/gClXWUeOg1bHGQKlqwU4nwuGZZ609bhtTPnIRVGnpQmEk2pSsZfNCgS/UpMTKQtoLaws6jMCwtasHWr5f7k5IJT/l5yCjLvSJ0L11qU3B8nXSNstfIJIpEdOn+p0yCqgW16cSI3/OWekmypoJSDhUUnbg0KPGYHhTJQREIBDMR1oC6KSIopmckX6o2PSOGt11m6nSmuGCaEaO6eHh8e0F9sw/Pb6RYsUgVX/zMLh7ufajXEdOaSYWTgc2SB1dQHF08LoJSZuoLLX6mWvXyG8PqZ15a4K3CKcNgGTyX+UisNmOobV0KSr2oexchc5pkU3YKLS9pAMA8o9VYV1D0Eo/ZZmwSJ17iMYkiL+9p25olHo2g6NcLMOLsKPFkUwnN0FpiCkoyET7Gy3CuEo/ZaSU5KAKBYEZiugiKS/Go18kzvy0TbsvaeU3jXyOEZsQoD0pIJNQHurGAEbYMuiPU3SbZ0INgLsiArqBEzZfRPShcQQnLUq7WWKA2A6ZGbszXfO22sINHy1BhPhJX3kyJl4+MUgtPoXUpKHqJx2id5aZgR4lHb7vVy0M8Fh6AMspuNxQUzaxqkrkoBUWRyKR1TWYXj62gsBKPQYw4oQjTXu3cl3TSg+fp7eLcJ+R6XxU1QuZWUJIxWIcQFIFAMGUwSzzFKTLJqth3NiPG9INwcyzFlgNuybvZEo+loFhBbTUFJc1Msg4FBQDeGCpov7vMmxWHxO/yc5Aaw1WQsIsnLHnkHIP36HlaWUqpEeG2dM1mazlP1+UZKuFEYrsrhf+sl1rIJBs8J+G5FRSNZJieG5b74g55Y6U0o52Xx8IDoYJCJR5n5HxJL7WQKmOSTCJ2Oc2DoitkYYnH7OIJCar5GhLR87zQS1LkhMw4L+6N4fN06HGtzZi/N6z3c0gim4UQFIFAMGWYbgUllw4VA1PN4J0yGkFhXS3mh24jlE0PikFs6ntQDIIyqBMU/u09aZhG9VbRYL9VP9wn/3ZulnjC5FQ2LNAR8pVKepbPhCsoFAJmlnhM5YoW6gorH6SZIqCmO7PSU8q4XtVmzHJfOBnmRle7i6dad1u9A0hXUEyiEIa1lbTHs8lEaFY1TaNU4jFIpqagUImHPChmDkpEm3ngfXErKOkEL8OE7zWuKNE9o3uofEJMQeFx93VzUCo68WkGQlAEAsGUYbw5KL7v45FXtjln09SDa7qr6QdZz7pLeO2dL2BxTbJKQcnqi0R4XlyKd3fxEPnYYigoWptxbTFQCgrr4uCGRpcB0yzxaEmyKuTNV/vmwwTVvBW1bUgEiNyYpNQK6zOv1ws7gOjY/N8UW1jNYYEJL8xuifKR1OvEMbNo+H3hBDXMQdFn5YQlnhpBqYQKWVQ5jMiFIl1V/TVqzaRY67ROjOi9HHqQ9Ndf7x4K1KpQIfOs1nbzemkfQE1BYe9J04/D95Nh7w2LkImCIhAIZiJGzRyUmARlzbo+/L8bV+PSnz4Ra7t6AVKE1/pCgqJ9o2SBWq5yST3Qt8ZIBcVhkg1Dz4J/eztzABwEhV2TUlCMEg9PA+XnPeLIQVEEhS3mvJ00b5Qm+Ddwq1MjGXpQzNfcJCjmPJ1kIlzw+eOqHTgZLo4lVeJhbcYuDwp7DaMUI05AneWhFPfzuEs83WaJpxQu2Lybxvf1xZ7ODYgq8eidOJZJNqkTNhfxpb/zziOzvMeviZSm8H5VNRLpNMlWov8/M03BzUAIikAgmDLYJZ54HpQNfXkAepZGM3B9YNdTUFzfoFOJsSgowfNaIyblKqUjbS/29MG/uDMoN5klHj7ELiwP1L5hs4WTExTaJ+/EMVuU9VyQcFta8NU37JRnqz5M5WiNUFDMEo8iN6p8YHSPOLwxSvUxFJSk4UGh8pA28M/MQWH7ra8ocNWA7nO4X7qfdGztcUO546oD7bNZkyyfkEzHNYdFuhRDOq9C2X7f8OvlRBAIBxEWK2HnWyrpNskWXF8EjPZmSZIVCAQzEuMt8dC3v7xhNiWsWdeHG+/5kzUDxx1Xr++Dkx7+ga0PwNNNko1AH8oUiFWp+nqwFe/isRSUYNverhgKiiPhNJnwVGJrqKDUTLKZlOXJ4Z6bBCNl+dqCz0stYUmEFqHQCBnVZpyPCOvjXR7JhKdCz8LzChdO00SryA1rjfZ9tm05VEm4v4WfczqCgGolHsOzYUfOG/fSkYNSKFc11SGlvB5mmzG9RrpJlpNn2idXOfhxqfzHI/wbqYm8nZv/WypXw7h6pqA4Z/Ekk1Ynltm11AyEoAgEOyhKlSp+9YcN2DyYj73txv48zvjW7/GLNa/F2i5vqBaxCYqRlGniA//6AL76P8/iH/73Oe3vruAqUwXhA+yiJO/YCkpt2xbWPaTPPWHdNhHlkt7OFgDAlqHoNmOzoyKyy0MtfqTsJK1Yee65AaBm24wWK9o3/4xDcSgzchPVxUMJqeZ1cB+J59kTjfWJxPo5V7mCYpQ0gn2E29olHlbCc5V42IJtmnNNv0Z4L/VSi5kky42lYYlHV3a4ysUHIPL3XiapE6MwfC48rsfuSaFU1d43ppEV0H1E/BhB1D3UY6bZmO/HOc2YvUbNQgiKQLCDYtXTm/DJf38c19zxXOMnG7j3hTfw6Kvbcdsj62NtZyobcUs89GFntisTqJzw7Xte0v6uzyZxG1a3MuMtJyh8wY49zdjo4uHXoJ2Xg6CUlYISlHjqmWRDP4C+cNLikmHfgoFw4F9rJizxmIsubcPD2sxv/uaCzo2QUTkotgfF9pHw45drpQXiRlqQm9GSnaj5V2gNNH0zrhIP96DQe8OloLim+5pE0CRVUUFtXHWgbcwFn/uEeJIs7TPhhduYLcqcdAHQJhrrZSf7/Vw2tuU+Fa6QuRSUZgi3dPEIBIKGIE+DufA1A/rgjyIKUTCzPZpVIgiFBgQlw749c9Mrr7tHTSR2qSaAXuKJUlB+8sg6nHzdfdjQp3tjaNtcOqHKLPy4BRZ6FvWBvrhmkjU9KHxxNIO6bBOl3rnCZ/GQx8Dl9QjOPcxw4eoPVxzMJNl00kNLjZSZnpOoLh7bNBqeV4kpDilDQeGJwKmErRjwY2jqC5V42IJslqz4vUzz8pCjlGaeM3/c9D7x9xe9L8yZSGqkQCbcNs8UEO4tMU2yZsIt/csVmKhcH0sVYqVHOm3eZlyKbDMOt+OTkKWLRyAQNEQ4PyUeSQAYQYnwgkTBPFZ8D0rtwzuC2PAE2P96YoP62WmSLbsXSvNnLcgrgqBc+p9P4KkNA/i7nz+p/Z0WomQiodJkB/NhicM5zdjoWlnSFZR4zLh7Lb494V44VZaFEWzGk2TNkgdfkAGEGRylqqaApZOMCDgUlLDEE56z7/uKsJDCojwobBYPXRedM1/QeWmCp5vybTmpoucF98HOUOEljboelJQ9CZlnxujnTCUeNs2YdeJw0ut5epIsPZZnGTlcxTJ9L3zbsvUa6ipIsaKXeFw5KPxeBfsIy0AVTUHRy2z8uKaCwr+XCEERCAQNYRr54oA+PE1PSSPQh1mrY0hZM+Dftl3b8muhjh/+93oD/1ydO8E529+go877jxsGtN/5QkThb9zzo5V4TONnbdvOlpQiCTzuXldQdFUg7C5J1v7V9z3KygdRnThmeSBfqjCDbLCwRpWl0smE0yQbdNYEP3e1pLVtq5aCwkoL7H7rpTZfKzFQ9cAc+sdLHqZ/hV+vu4vH3rZxicdUUMLpvgVmNuUttxZRZKSa56Bw0mNvq3/poPcrH6Og5+foKgfAX0P7daCXQS/xuBUUFeLHrte85kYQgiIQ7KAIWyHj+UCA8Zd4QoIyNg9K1LG18km5Yv3sGp5GcMWq83PkbaiFCIIS1QqcSnpY1GG3C7tm8ZhqRCqRwIL22rasHMdTSk0Pim2S1csHoQEzZeVzmBI/n8djhXglQxWEZ3ukEh5aHR4UXt4hghI1TE4r8XDvS8KYD8MICm1rKihlRdgS1r3QhgU6c1DCbc1SWYE9xv91elAcXTzcj2GaZPlrmGMKCFdWzG3pXnDVh59XsVLVzomTHKUKlfXXmPtUNAWldsx/f2gd1tXmK7nUmSJTjMxrbgQhKALBDgo1Mn4qFZTaB1xLRpf3m4VOUOxtNVLC56loH8q2ERIIuzUA04MSfiin2TfRKHBPBPdVLOoIvCSbBzhBYbN4TA8KUysWOsgN79QwPSjmN2hzdg1PkiUvQ9UPtjclft5BYj6mSJUj26PF0cVD5Z1MMoFcRn8dohSUIjNnknITqiBGiYcIijHksKhIpufo4gmvyaWu8ZEC5n1UpbQGCooW1FaqOP0YZqcVJz88MG8gH6TUcnJhBvXRezljkIxi2SjxsIycMA1YJ0+8bKWl/db+fs/zb+B9316t7UM34Prae0NKPAKBoCFMOTgO1EyQuAoKlXjSgR+j3kLvAic0poJSrepehXxZLy0AdvR31L75z1zyjprF05ELu3S4ObfCSh6LXCWeOrN4+CJGCsoWh4LCW2dNk2TWWDgtAyZrYaV7wqPf6TlAcD/Nx0Jvg93h4wpq48QoY5TLiGgkDA9KueJbZSfuBeGEkDwoWSN5VR80aAS18deX3hu8FbxEia4Jy8hqdjxF5qCk9anCFaOMAvB5OlXtuJmUQVBqM6M4ueCETTuvlPE6cYJihPjR/4vmLB6e/cOD2jjR2NCfV9dG++a+l4qhgDWL2ATl3nvvxamnnoqlS5fC8zzcfvvt2uO+7+PKK6/EkiVL0NLSgpUrV+KFF17QnrNt2zacffbZ6OzsRHd3N84//3wMDQ3FPRWBQDAOjEtBKY7PJNsyTg8K4DC5GvviCkrJ9cFpKSi64mKmkKYc0e6u83pu42C4T6a+LOokghJR4okIakslIwiKq83YUlDc3SXc+2KmxZr+CB4SZqkr7H6Y2R4tjmGB3PvSKAqdR/AXjeNyIuD6dm4qKHqSrK5U8HJLPQUlmGuk+1csD4rRtqsUMqN8RI+7FJSqH5BtbjhNsrLWQM1knWWkhV5/c1uXSZaTl2QiJBquidR8HzzqPghqs+mDS52psPPxvElOkh0eHsYBBxyAG264wfn4Nddcg+uuuw433ngjHnzwQbS1teGEE05APh9+azj77LPx1FNPYdWqVfj1r3+Ne++9FxdeeGHcUxEIBOPAuLp41DdT3aTYCJYHJca2QP0SjxnexskTz8GI6uIxfSXmlN1UwrPMpvQ4V2Oe2xQSFF6mqVvicSoo4SK2sD3oTuIEhX9bTSpVwPY+AHaJh5eWuNmVG2FDk6xd4jH9CcWyno6aTiaUB6XIFAOVYJtOaqUDuo9AqIJwhcVUUNJscaTtEx5UR4ypoKjtU56lVLiGAbq8Trm0PS8pqp2XSixaycPZxcOMrnyoY7VqkR8iXaSgZJN824S2bVSJr1CuWuqaec31DLZVdt4uJURTZxjxJRUtjnoCAKnGT9Fx0kkn4aSTTnI+5vs+rr32Wlx++eU47bTTAAC33HILFi9ejNtvvx1nnXUWnnnmGdxxxx14+OGHceihhwIArr/+erz73e/G17/+dSxdutTab6FQQKEQ/o85MDAQ97QFAoGBYmUcBIWRgUK5ombNNAItCoqgxFRvOIkwI9QLFeN37kHhi5DjWzI3hxJKlSrSyYT2jZI+X/m25nms3coSadkHOpV4Nrm6eNJ6F48eTJao70FJJtSCHZkky0oTvBRGZYdsKoFiuaqVY2ifGU0l0YkC/2bOlYyEp6fnjhTL6MilQ3NuJloxom/YtHgXK8wXk9DVFSAkEFyNyDIFxffDb/CpRDgWgNQGviBnDXNt8HOooESFvEUN7dOi7tlkaFMR4tcW7N+3XsNsOonBQjks8ThMsnQfrVIcIyHm/KB00sNoibe3G6+xQ61KJjwMs/bxnbpb1HPo3HgJiohpHP8JMMEelJdffhkbN27EypUr1d+6urpwxBFHYPXqwESzevVqdHd3K3ICACtXrkQikcCDDz7o3O/VV1+Nrq4u9d+yZcsm8rQFgh0SYVbDWAhKhf3c/Pb0wUnyf1xyxEmH6UGpp6AQEeJpofy6K1UfviHm0LdgXvJwzeIZMab18g9uXragEs8bXEEphYsfJwKm8TMs8bjbjMNZPLYqQM+hv2uzXOjbeW1h5hktFODG71fY4aGnvfISTzD7JchIoURXKu2MagFxuuGYJ8UGxw8fNxdNviDT+y/BItSVWlGy4/lNpaLRsECuoNTr0gn2oUfHu7xPADBcsBdsrqaUmSoXvkbBv2SS5fvj25YYuXG+/sY5Z1JhubVa1Tux+D6KlSqqfkhQ/rQ5tGXQ/eaEjL9Go0pBiUc5JpSgbNy4EQCwePFi7e+LFy9Wj23cuBGLFi3SHk+lUujp6VHPMXHZZZehv79f/bdu3bqJPG2BYIeEOackDnSC0rwPRU33TddvM35u4yD+5j8ex8tbhrW/6yZZo8RjEC3Ng8K+NbrC1lzdROobJZUHEnrrJMFMRuW/8wFpi2qJsIOFsnqOq8RjRqGnEh4WdOgelHIlDL/i7a9KQYn4Zl+q+Np9oXtB/w4VQoKScigoZocHP2e6T7Toep7daswVlKyhoPBBdMF18TZjo2TBFrpRh4KiPChm6SnladuWK74W1JZN6edE1xbco1BBUV06Vtu1TmBcSbL8fqQdbcbBedmJsUQi+x0Kiq6+hCRUtRmz19AkVRlGqngqrLltqaynwZIxVrsfquSV1EqHdL3TqqBMFrLZLDo7O7X/BALB+GB+qMRBvo6SQfi/5zbjpH++D398rV/9zRyeF9Vm/J5/uR+/WLMBf/Wjx7S/c2JgHte8Dk1BccSZFzSFJVzE6Iu4K5/DnLUC2LNmuILCs0w6smHgGnXyaF0PWlutrqAsJAWlVuLh9y1QUPTzMttMeXoqXXeSDXxTBIUrKIYRtlCuhApKykFQqiGRI5hGWT5fxryXtDZaQW1a63PwtwQzjRZcJR6loOiKUdpUUJi/JcpAzctw5sThqBKPK8gtlQyVriGHguJ5oWG1zIylyoNSu6a+kVLt97CElmATq8tG2Qow2owrIYng++f3AghfRz2oLfx/4c1Lw3XY5bkBQp/MWD0oE0pQent7AQCbNm3S/r5p0yb1WG9vLzZv3qw9Xi6XsW3bNvUcgWA2Im5o2XRD5U/4iGV0BZor8Zx708N45vUBfOLWkGTQN7Rcun4Xz3DtA+2lN/TuvnpBbZbp1aGgaB6UiLbiFuPceNulS32pNwxPkZta2UMZZQdJCXEvjmZbJikow7W4e06oMimuoOjf7K3uEv4NmnkE6PWg0gOPYNcUFLW46eUfLQadEQCz1ZiXeMxgOiprmaWFUpkHm7G23Nrj9RWUivYeS9USULmXqMxKGubryxflXCqJjDG3yDTJ8qnCVVamUQs2KQp0n41UVbq+Qik8rhraWLsmF0EJ9hU8L1+qqHKl5UFxlHi4gdZsFefP42bnVMLDv5x9MN75poW167UJGf+XhlNOq4KyfPly9Pb24q677lJ/GxgYwIMPPoijjjoKAHDUUUehr68Pjz76qHrO3XffjWq1iiOOOGIiT0cgmDLc/ewmvOXzd+LfH1o73afSNLQFOqaKohGUBq3GQwVe8jC6eBqUl3i+CGCUeMySjqmglLiCYpMMl4mWE4XQz8E8Cg5yY5Z4RhwlHuocUVkoAwXNvGmad82W3bZMUou754bgFGsVpRKM6UHg5RTewUOgezJYWzh5PobmQYko8RRZfgr3GRDZG7VKPCntmzlPoU2YBKXC/S28JGIQFKcHRS8PeZ4e0a+VtJIJizTx95SmoFglHJ0IAgEZt30kNSIYoSjQNXEVLmNs2zcS+JBaDIJC7zFu2rbMzGWXByVUsooGmQv2wbqp2Gu06/w2XH7Kvtp9iNr3lCkoQ0NDWLNmDdasWQMgMMauWbMGa9euhed5uPjii/GlL30Jv/zlL/Hkk0/inHPOwdKlS3H66acDAPbZZx+ceOKJuOCCC/DQQw/h/vvvxyc+8QmcddZZzg4egWA2YM3aPpSrPh55Zft0n0rTaOTBqAdODhopR23Z8IPUajMu11du2rMGQeHSu6WgBI911kiNHlcelnhcJKPEvgmbLbn8WyP/Vk+gxYQ+e+nDmHfiEIGgePehQkkr46T5IEJDSqdFVXXyDBW0hcDz2FwUY1icShJlC7YrKp1ICJV4+Dd7jYSYJZ6kfc58EWpRCkpZuzdcQbGGyXn2whi+fvZ50fXwb+fheVWsFmV+vcMFvpjbSbL8vZ1NJSwPitmSzVWpUsW35uZQZD2ZkU3TKO2fG69Nk2zfaEn7PdxWJwP8uK42Y7sF3TY682sqVnibse4FKld9jXCbQX5EUJPJSSYojzzyCA466CAcdNBBAIBLLrkEBx10EK688koAwKWXXopPfvKTuPDCC3HYYYdhaGgId9xxB3K5nNrHj370I6xYsQLHHXcc3v3ud+OYY47Bd77znbinIhDMGNCCbXZ0NIN120ZwwS2P4JFXtk30adWFRlDGoaCY3TMmeAsyKQrNBrW1mwpKHWJEj3XWSEAwmE5fsAMPSjh4jcC/ZZtZJ7yLx5U0Sh++82s+EfoGyztxaPEIFzi9rTmd8rTzckWhq3k8gyFByar90kLh/iabZSUPbvokEIEZKtgppdyDohawhC7/F8p2DD4QElG6J/SatRpJsto8HUeCqelB4efobDN2eFDSjmviSkUqYXfxFAwiaL43ohZ7ICCxpsJC8f50n+0SD70OwTUlvPCas4bh2FRQiDRwRS9lvE5FZqDNGiSCe1C0GUGONnPlE2IlL9MXBYSvERHBuF08sXNQjj32WPU/vQue5+Gqq67CVVddFfmcnp4e3HrrrXEPLRDMWBQME2Ac/NeTr2PV05vQkUvh0N16JvrUIhE1vbcRfN+P1cXTxvIwzBKPS7nhny/1FBS7iyc4j66WNNZvH639rYpcOqn5SKhLxJV1YUZ0A9BKF/xbve/78DxPveYL2rN4Y7CgSGrZ8JEExw8XA64emcqOS42Y3xYQlG3DxTDnorYNN1fS/gG3gsI7hwjkbxhyeCOy2uKmKwbcs+E651ZjHo8KasskQ/JodC2FCoq9cGrR8LWf6f3H24z5t37TMMrvx3BBNwWrHJSKrqCQ8qGIYMVQDNjrkPACXxcnA2bYGilVpieD7h15VDKO14jAc2aCcyP1JZx3ZPqIOGkicuHKSNHvs172Alyvka/9v0nvi7ba/7/UGj2tJlmBYEcFLZZjUVDoA2WqTbZjVVBKFV+T5F05Kvw+tDGSQSbZlnR0DsoA6yRpz6b1c9bajN2m2M5cuE3YSk1JovpU2fCawm/ZioSUHSbZWueD74dkgK51QS3tNV8zOJo+EgDaN3De0skNmkEyrcP4qbImKo5v7hQg5g5qy7kUFO5BSeulB5faUChVw7ZpR3eIGZEenLP+rZ9/++f5Gq6Bf7SAlhnJ4N/ArfJBwkGqyiERTDvUlyHDFMwXa98PW7KJuPCIfd7J5fTGFEOzKr1vWjIGETQJSm3/5FFxqVwEyySboBIPvYZuwsZbgQEjbM9ZDqtdD/v/Talc7PXQSku17clDtr3mm5mTbcYCwUwHGUV5TbtZkPrSqFQy0RirgmImp7qI1VYWKMY/lGyTrK3GbhsuWn8jcFJhmnPpetqySeUHoXurCEgioXWeEHjyphlnzmVtfTx98Lgq8bRl1GOjJX14Xjjkzi7x0Dddvm+XqVCX6SvafpPKCxD4BPgAPMBQUFg4HEGVPBwmWeccF4f5sq6C4irxOLYNrsUkc/x6bCJA7wPXvSqwbA+XB8VUjPhrUKxU1b6JHIZJsbqhlKtRaYP8AOH9z6X1xziZ49uOFhsrKCZBodea3o/csMsJW6HiJre8SyvlUlA4QfF0EgnoHWBkdCYFtH/EXdJqBCEoAsEEIF/SJeyxbDuWRNfxoJFJdt22ETz0su2LMc2pLoLCSQapAb7v2yZZx3H5tubj9WfxUHeKPjkWgCZdm50afL98wiuRFr448m+ltM1I7bjdrRmVoTJSLFvR77QPui76Zq8WR7ZYEdFNcsWAh62ZCkoiVFBcXoAcux/OEk9aX7C1cgjzxoQqielBCYkAX4TCQYOGgqLloNQv8USVaRRBoSRZR9S9Zux1lHhGCnpgGr8nxbJN5pQZuepr/x/oxKmmgjCCYrYKRyooKielom0H2KZYy4NimGSdJNPZxRP+v+hSqsxuKSAkkfx59J7l95AUFDL2JqczSVYg2FFBC8ZYPCj0AWvmeEw2GpV4Trvhfpz57dWWedckBma7L6CTDHo+X7DrmWS3D9tx7gC0mn+wX3cXTzaZ0Abc8eOkU/pUWYJrYJzZ/pqqBZuZ83joG2tbNqm11VaYiuE5fBWmeZMHeVFuRKQqYMxacQV88W2y7H6Yra/8Z1d3iUtBoXOm6y2zBZtva+aKqDbjdLSCombxsFKbq3wUZmzYbcau9mdX2UIRMsPUSdvmDTLnGoDHJwLz4ww5FAVlko3yoCiSUdaOC9iKSUtGX76VSbZkkxtNfSvrj3MFRQ3V1BQhu305DNNjCopD9aESLbVGiwdFIJgGmN8QY21bniYFRSvxRJdabn1wrfZ3s7TiLPFoBCV4nJc8WtUsnvolHk5gysa8nKgunmw6YSkoZbagu+LquZmRdyZwUpUykjVpG1pMWjMpdV0jxbCrRV+8QqXDle0RllpsX4VGbiK6R8rVqtYCHSoDXEFxlXgMBcVBXgqOacbcqDmQr0+qABbUxoYFBp4muzzEiaKZJ8Mfzzs8KPRYwUGqgvOyg+kAPaG2WAkVFCIH3HOhFuSIMg3dS5cZ2eX1CX4n9UVvTw7OQX+uWfIxW5SjjK5W2ivr4jGHMvLnjRZtkyxvcR92GHupC48+D9JS4hEIph6kEgwXy3W73NzbTr1JtsKGggH1TbKPrtWzXczzdCXJbhsusMdrKgYzhVJtulL1rf1tG3ETFCvKPmIWTzaVjFZQkm4FJQw2YwMBm0jWBHTjJ09ODZUXWy4vshJPRiun0GJQ0Y5pHtdMME06SjzpJPvmzhUUVQqzFz/lQYnwvpjdNBmmKA26MlSSOlGkf3OGgsIDwMx7VapGeVBqXTwOQ3HGSars+zzk8twoz4VdDtMUAweJ4L8PORZsUj3INGp14hiLfT0FJWdtW6fEk7TfO1mD3EYZnc12bs9zv06u6+2o/X9O6bfdLaFPqxkIQREIJgD0wev78ab78m2nUkExF/t6JtlXt45oj1slngYKiqliAEB3a1r5NfgEXcAo8VSiSVRU1H0wOdbwoJRDfwRfcIlMuks8vt5pY3hF6MOcyERrhhOUspafQnCVeFxli2HHeHp9W31xpGNUqvYkWyBKQbEfdy3Y3INilqU8z1Oq0aBqJWXbGlNuC6z8oE3JJSOy57jeCPOm2cWje1B4WcpedMMclDpE0KGg8NcjVCqiVBAiGSGRINWDvhuYbfR0jq5yidVmHGGSpf8v+LZ1pxmzvBlSqjKO9+yoKuFFdB45fDNmjtG8Nr0rrxGEoAgEEwBumItrlFUm2Sns4jFNsSY5MlWgJ9b3q5/NLh6Xd2bbkMODUjum5wUfevTtiqazErZqHhS7DKP2a3bxsIXXVlDCEg8tGL7vmjqbCOfWVNzD06IUlNZsiiWnVrQZL2ofrMTj/mavt+WmXIsua51VBIUlepoD7ABDQakT1GaaYPl+CjzIy9FK7FZQ9HKYCglLG7OHHMF0epKsTbrsqHv1UMT8IJt01e1achiKPS9sB6frzaZMgkKKgk0UTMWkLWMQFMpBKdrbNt9m3KRJljwoDVrFaT+u14jvJyzxhOdlErDuVlFQBIIpR14jKPFKNdNhkm2koJjekDXr+tTPzZV4mIJCHhRj2i0lvlKIE2G75kGpp6BElXhsBcVV4gF4GJutoPDoby5rh9+wa74jap1lJZ7RYsWdyskWbFfmRDi7hBQUu6RRrLBWUaN9uVx1DwPkXS3mN2i6ZxyuJFl9cbNbiQcdYVy81AJAOzfX9GZXmaZU8Z3DAsPSg915oi3IdYLaXN00/L2TNxQUfr1UtjBLPIqg5B0qiEEq+BgIgCkoLvXFNMlaXTwGuXG9ho7wOM3rQ6qf43UgcJWLX6/TJGsqKK2ioAgEUw6uQAyPVUGZyhJPnfZd1+MjrGXSJigNTLJl3SRLH6QUqDZgKChRHpR6wwD54xkWxhZMdtVzQfgHNy2cPCGVEwFFIlwko6Zi0L1pzSRVAB1XUNxlGrfSYZpkXQsyJwpppaBwZcYmPrqCEt1mTHApKEBIxvg37BZTQXF18ZCCwl4jPhmYTLL8XtF+eJmGm3fDWTxU4gnPn/uMnETQIBHNKihAaPDuG3UbP01FwdUtRTAVhrSpoDiUKkKjqHueUaK8QKxV3Bzop6lNKfteEezOI6OkxZ7fmRMFRSCYdoxHQZmJHhTzcX5uZinKdd59I7oKUqmGfg76IO1SCopO6HjJp14rdFTUfTal56BwFSaVTFidGnzfeg5K1SJVgF224Nke9I14pFhGxelBCVtnlQfF2THh6OJx+QhIQWElHtMESfeE7oe7i8etApiPuUoiLRmdoCRdfo5yLZmVHZsWTV7iSXiOe8W+2acdZM85i4fnoDjKQyrqvhhdliqUK04Fha63P0pBqRGDIYeiYJV4LA+Kp52X7kHRj2OWfMwWZRfp0rJZLA8Kn0jN33c6IYks8TjKUmYS9DwhKALB1INngYzETJOlD9hKNTSpTTYaKSZmuYn/3kybsWl8LZTtqbKdLW4PCr9//Lysc4xqMzY8KJx8qdk0xsBAPerelrydRMHI9gjajMMSj/PDnrcDuxZOwyTbSEFxpYG6TLJ6F0/oAwmPaxou3d+gXUFuoYISvI5ph2/GJIpmO7crhZbuM/fruLwRyoPSMLvFJjCkVMVTUGoEZdQerBhcg+HJ4K+DcZ+jTLIjzjbjcFvPc5FKXX1xdTwNOsLjXIqhq4uHYIatmQqKlHgEghmEspGEGbfEo8e3TxNBifE7ERL6cHUSlIJ+D/IluxMjqsTDv+W52oxpgTCJklbi0RSUcB+qPdYoPYRGyrDLJypiPWspKHaJZ7hhm3Hoq9C7LfThelGeDDpfWqRas1RaKkeUjuonyZrfxvlizmP4XQpKq6mgJOzjci8IHTtUKtwmWW1uUR3zLqkcmkmWGTvp/enOm3HMrWHvjXxJv89ASMjIg2J38RglHof6Qmg1fidyN9wgqK0lnVThfwR6n9ULauMIxxW41CabKBKMy7WuV+viEZOsQDC9MEnFWLt4AFsVmCxYHpQYJR4KbOqufRtylVrs7XlXCykotknW932N4LlMskRsShVdceKx5FlNQQn24XnhAsgH4PF9ByWeUI2oF1xVKFdQrfrq+lsy3CQbRt3zRTXlKPHUVVAc7Z4u42c7C75zx9WHJSD6hu0KagvP0ygnGIsQvx+UxzGQr7PYl6vae5u3GZcq4bBA/uVcNxRTiYd/s695LlwlHka4qJvGFT4Xts7aj2kKisskW/Og2DkonnbcenH1donHWOwdnVjBz/rrFVyDqaBEG115Ro6eUGy/J6MUIvucXSUeUVAEgmmFqSDE8aD4vh5UNhYfitkF0wwsD0pZ79oxz8M1pE8RFEPJGGLlHaV2sEm49MGpPChMQcmXqtqkZB4oR4tFT1tGfRi/MRQGwhWYqpBzKCjphD1+njpxOFkgbwT3oLhIRLFc1VquWzNJvc3YRW4cJZ76HhR7odAm0taez7tBKAQsqjxA97ueedNclMLwMXsRajVKPC7Vh3eP0OLI72PVqTaxNmNj+GHwc7g9oPtX9LlGdvhcVHmEP1aMyIxpVe3xjdqMa/eZE4sGJR46D/p/IOo1NA2yQEhmRx0lHpqmTOD3p81Fqh2qH8H2oET7ZpIJT1OJ6P/5ZiEERSAYJ8zFPI4HJYj5jt5XI/z22c3Y/wv/i+vueiHWdqZiYppkzfNwlXjow8Y0zdI3+LZMUnU8cCWDvnGRw39gNCQ0rvIYnRsv8ew8rwUA8MqWkfCcWUBV1uFBcS1CTg+KI+qefyjTQlEoVzUymkuxoLZS/S6eYsUPh9i5ungcOShpx8JJZCrF2qe3D5N5022CJUKbrfON3AzjMksifAEjUkbn5E5lDVU1cxJy1Q+35YcNg9pCpUzzoBjEQFeqwoRbV1nKUowc/pWgzTh4HbQ2Y1XicSsoaUNR4F0tZvqrqaCYi7+2rdZybC/ddG+INLs6nly/c1LtGqxozhoyz9Eq8UR0KnXmUpYy1whCUASCccJUUOJ4UGwfRbwSz6U/fQIA8I+rno+1XSOTrKtEQ6CSBsVWm9dPXoT2XEotbPlSxcpYcJV4XJ4AM+Qrk0pg1/ltAIBXtw5b5xypoDg+sAsG+cmk9CTZcGF0kxs1WyadRCLhqeAtPiyQfyjzEo8azOZQZ5weFM2TYftMaCFQCgo750QibK/uVwpKnS6eiAVNLX7cJGssuq5Si0v1aWUhZS71RZv8XLEVFFPlSRieDDNyXiOCpufG0VYbqaAYOShRHhRnDkojk6xRPuGlpXqEEggVoorjfVWPoLRlw9b4ksNzFfweTVCsEo+xLU00ntcWz38CCEERzEG8MVho/KQJxHhKPFaWR8w0WbMDplk0MsXaXTxV6zEiGOY10ILQnk2pb3o8jyJVp8RDH3JdbGaHiklnC9zyBQFBeZkRFLWYpHUPCuWVRMW3A3rSrDaLxwiXC/Yf7nukFBpkAf5ttOwu4bASjzNALBVub22bCrstio5vurTQUEieuSjRIjvgKE1YC7aloBhmTq3E426V5dfram/mfh86J1cwnT7ELlpBsdQH6tRxtd2aC7CrRbniVlBaiIQ6zKjB73oLe9ahVgCBWmQqIVGZKtZ+nCWeaBJiqiAZB+EaLpSdhDw4L7YvkwgaJR4r66XmGYtrkAWEoAjmGP7jobU47Mu/wU8eWTdlxzRNonFMss1kitRDvSF/dberGC26MUyyRDSoTXi0FoZGCBWUtPpg56UW0yTLSRbdu/ZsUlug+DllkgnsOr8VAPAqL/GwxykzosDSYKOSNfm/PAclKuGUd8TwDBQA7mGBjrJEqeIr349rRsyIU0GpeWPK9sC34J7pCor5LZi+jdPC6spJUedZZ7EDdMJGA/Bcj/FyScFQUICQVA0oBcU+B54kqysdDfI5DN9M2qHsEJzR/g0UFPMazPM29wfohKQtm7I7cepsy2P2TdXKvIbgPKJ/17uw7Anc1jVwgmIqKGZbtfFeoZEWcQ2ygBAUwRzDsxsHAQDPvD4wZcc01YY4HpR6eSMTge/e9xI+cetjltJimmJLpqJSx5NCH2KkgHAPARCaAzu1Ek9VS3MNHreD2pR/JZvSPAiAXobZrVbieUVTUKjjIqEWYz7N1rkIUZIs809oAWH1/CvliirFUGlHN8na2R70c9SMGL44mtumuYLSVInHraCo39m3cPObvNXFU8dU2pKJ9lHwNmM+zJFA943eA/zbOR9w5wpqaxTBbia6pusQMtfAxmK5ftQ9wTR+RpXHAF35MOfwAC7lSt8XnYd5/oDtG6pHMlwm2UD1s0tp5jVYHhTmJTL3DYTvy7ghbYAQFMEcA32wu7I5Ju2YhgoSy4NiTQYemyIShS/91zP49ROv45Tr79O7hRq0GdM10Wc+bxGlhZUIBhB6JoCw9h6UeEhtsNuMeYmHFJgRtuCbWSW6B6WmoGwdUdu6gtoK5WpEiUcnAtxIyxfGepNw3QpK6EGpZ5LVg9qiu0sSjvJQVCQ5dfKEJllzcYsmHeaisvvCNu13c1+uqHu137S92ANujwKdM5X53PcqTMd1BbUR7BKPQVAiyiXm77yN3JUZY6oX3YYyUK88xomOOYcHqK+gBNvXUVAM/4qt5Lg9R9SVVPX5bKLofVkEJUK5IlBYm3mfmoEQFMGcAn2QjcaMmx8PJtSDEkNB4Z035geuiXXbRvFfT7yufm8YdV/7neRZLXK+tmi3pJNqsRlh1zHgIiiszVgFtdVKRDybI1RQklYbKScgO89rRTLhYbRUwebBghajzoPa9C4evggltX2WHCWeUtl3Dk/LsmviIW38Xx5178qjKEXMzLGIgLMDKCw9pLXFXvegWCUe41u3WT7geNteC4xto7+Nm4rCgvascztX260q8TgICle81HTniDIFoJM5wDbJZhyvA6GHfbvnBDRM3Y1WUCyCUodkuEpyHGaZxiYoNa+To4unnn/FPHaUqhMaf6PLQ1FdPFHnvMfCdgDA3os7rHNuBPsOCQSzGLSwjk6lgmIFtcUhKG7lohlwM7DrG1W1qpdxNg3m1c/NJsd25NIYyJcND0rYFdOSSaI4WsVo0S7TtOdSSk3KlyvIIThH+kBrSSeRSngoV30M5Etoy6bCwXvZlNa1ws8pm0oik0pgp+4WrN02gle2DGuLRDaVZNHu3IPiKtPY/hZOBCqOsDVe4qHXWplk06xls0IKSviBTYSjFDUjxiARSUeAWKnia+dLoAXP5TEB6isoHLvNb9U6bACXgsJKPIaCsqgjJCj8/Mib5DpnIrVRWSbNpKM2bI2uc595hwnvxKIvDPzemfemu0UvXZglnqxBBHPpBPKlquVdAXT/DmCTDOoCcga11em8AfT7ZZZs6JzIC1RPfYkaFkgw7+2Fb98dx75pId40BoIiCopgTqFY+0AZneBSST3kjQVhuBCnxBPdLdMImwZCwuEiNua+uKpEC1xOtYGaQW3Bc6lF0OVBySQ9zRRKoBJPRy6tKxmGGuF5XthqXOviGFYlnqRWagnOUS9r9NQWlf7RknZ+QYknNIQWHWZU7jPg1x/koDhKPFoXj8skG9wnWnQKZfcwQKXORJR4mlFQeHy7y3BqPj+8L8m6vxPesfdC62/1TLQmOV7UkVM/88GMg462W3r/kIISpZCMOnJhmm0zJp5er8QznxGU1mzY1ZJn6cTmORO6YigoQEguXATFKlMZRESVeOokyarzqKN6ma+n8gKp18Hc1vZRqeM0uN5kwsM+SzothasZCEERzCkoD8o0lHhowYyloIzDJLtpIFRQzE4afl6EYW0Inz5Pp2gc14yV5+dVZIt2i4ug1AhaB28zLrlTWbuMTp5hzSSrt2uaqkELIyF0rTREjXtB6ga1lXWTLG99DVqjo7fNlypKOaLwLr54UbaHKym2VPHrRt0TolpDXamd5oJntRmbCorx+5mH7oyd57Xgb1buDROuqHSCpaB0ZrXf6fUigsIXR7OLh5OMZMJTYWthiad5D0q9czavnSsonHS7FRSjxGOYZM3uIpNI0f1ylXgalmnSdRSUGMRhl55W7TEiZfT/oV3iib7v9TJWxgsp8QjmFJQHZQwlnu3DRfz0sfU47cCdsLAj23iDGogUzWvN4PX+/JS1GW9mJRsgKGfwb7PmPRhxzLhpz6awZaiozbwBeInH9qDwBFQ+vZdAC3MQ1FZTUMoVtFQo9TT88ONtyAAjKJmwi8eVgxJsG5KfcA5PEGff6sgjiZpYG9yP8Dkt7JpKzjbjsDxkmmSD4wO+H6pCKce3zwozfsb1oAT3yS55tBumS+ubu6Wg6I9/7Yz9Adh+FNdzXcMC6Xx7jG6NbDqJ4WIl9KA4fBjhoEF7YSywkQL11Ca7hBWt+piPaQpKJiRNJea3IrQYuS9mvkdUe3d4nqSgjMUk23yJp17n1b5LOrXHSEEhD0o9k+xei9r141rR9xNHUERBEcwp0Id+3IF9APD/3fYHfOm/nsFHbn441nYkA89vnwAFJUZpamO/TlBMQmIn3NolnjaHCRYIyQCVYArlqlJoysz8SSFdIxpBCU2yWc0LYpdLTKKgSjy8zbhitxkDISngCgotPLoXxJWDEpZpgmPQvkNyM1qqoOKIWHfloNA2nudZs2miwsXUzJQ67a9JreQR/uxKKW3P1i81mKqBuZB4nuckJ+ZxzH3zxXJhR9Y2qxoKil7iobk2ttrkOkf9m7z+XLONtd625vW4PCibmULJ1SlTQaGRDa7juM6DiIOrxBPloyG0qBKPvXTXUz0A/Zr3XaoTFP7/kmtffNu37NRV9ziNDPtxIAqKYE6BZPOxtOve/exmAMAf1vfH2i5ftks81arfVM3VNQm4WfASD2ATFEtBYd6YgkFQombxdLAP32KlimwqqXWf8ORUgirx5FLqW3u+VHGXS1jSKMBLPHU8KLVt+L7DvIrgMVpEeNhW1MA//m8mmWTkpsySZO3yAM9B4YtWSyaF4WLFqQpoQ+wo4bROtgdfsDwviKuPykExv5HPN6LFc0abaRQZccFWI8Jt+bXzDh4CnWNY4rFVH3rPmD6SeuFj5sJvKhl1SzzGY7xMQ+WOLbVBlFnW2QXo19uetefLWCZZg0zQ/zPOHJQGCsqZhy5D30gJx+2z2Nq2UZvx1qGi+nmvxboKYp5LvfLZW3bqNJ5bv7Q2HghBEcwpjKfEM1aQ6sG/wY2WKs5vSCZMlSMOsTLD18zWajvh1lZQlAclos2Y18mL5YCg8NJEq/HNCzAICjOUhl0t9oJMnpjQJBt28YQkQjeG5riCorwCtgHRZcA0PSjhvJ5QQcmXQnLDFyE156dUVe3VPKxMGT8dHhT+s3PqrOVBsRcs/lq5OmIIlBWj9s0Wys6YU2XrKSi8rEit49pxqd3XoaA0GpZnLpQu9Y1gekHqtUabSkXKQUCInHYYCgm/XlepxfSgZI1roPePU0ExtjVJ5nH7LHaSE9e2JnF4YfNQeE4G4TRVIXNfr20fVT8vX6CTG/M1ct2TsUJKPII5hfHkoJgfjs2CFsfu1rQKNms2rK1gdfE0f94mqbDJTrQHxSQgVomnFLYZh+dmh5q5TLJhiSfN2n0rrDQUXeIZ0XJQIkyyRFBSjEiQgpLSvSBASORcLarmvgPSxYfY2TNxsiwEbtTIQeE/hx6U8LjJhKfOa8RR4mnUOltPxjcXPBqoSOALxwE76zJ9I5iLveYjSumKggm6JiJseoqp/nxTdbQISp0cFDOPpF64nBfRzuw6J/O+8veHq5xhKhlW5kqNdCxot5NV+bZdLWn13GZgHTeGF6RRB9hzmwbVz1HDAgljibSPghAUwZxCgSkoZldLI5iLQbMgItCSTir/QbMEKV9n5k0jmJ03VonHOAfdgxJ6PYDooLZsKpzuS+fGp/DWazPWTLKsi8fVKkoEgdSX1gz3oOglnnAeSUh+wqFuwd+4FyTsTLCVCiI2vHyUS4fkRvlIIrp4TA8K/9mloHiep7IuaFtXPguhUWdKVPBXRy5lLRT8uQftMg9xYBku2WLISQUntOY5uzwo5sJoeVDqKDc2QWm+xMNhlsZMNcEkXVGdVeo41muk7++S4/fG3757BU54c6+9LTvH5QvaYpXhLAXFOI93rVgEAPjIMcutbc1WcfNe0TWbRmTAVozGEmkfBSnxCOYU9Km71VhyYyrhYSxzkPlAsdZs4D8YbnIeDy2sHdkUBgvlmAQlOucECJWdlnQSo6WK5kGhcourSyf4PSynZGtlBdXxwsoeYTtvsO9CuRKm0Ob4NGMWde9QMui6R5hJVnlQHGFqgOFBUXN4bC+IIigpV2mJVCFf/d3zPLSkkxgpVlSAmJaD4hoWqCWN6p0pLum9WAlVNte+CXFaR/liv9t8e3Hj/y8cPA6Ckkx4kf4qp4JSO2cin3qbsWEK9ppXjKwSTwMFxbx3BDN4zSRN9Uq1LpWikSdjj4XtKl3VBCc/uy9ocz4nCo1Msv945gG4/8Wt+LN97RJRm1niMdSYfz3nEFz938/iG2ce4DguV6bilw/rQRQUwZwCVxXilnlMqbJZFNS396TW3trctka3TAzvjFniMbuH6PpVd1GJKyjBtku6ckh4wPaREu55/o3wvBjp4qZQftx00tO6ZYBwUQYCqdwVde8yK9I+NZNsUn8sqotHN8naSkbYIeIgGaUgLbZS1TNarAAxZ5sxGxbIFjE6r3CuifubravEE1dBiTLJmjkXQBiBDwAHLItX4uHHOWy3aHJjdrQAIWkccky7NRf/OCUekxyYQ/vqRftzmKSKlweBcNyDC5z0Rp1XHNMov97lMQlKoxJPd2sGJ++/xHk+Jkkz7/u7VizGqkvegf137raPa+QajbVU7oIQFMGcAl+0R2IaZaMk4EZQaZPpUFEYbrrEEzyvi7XzNgtasOnzwPKg1B4nox2fskzbLu7M4dyjA8n3itv/aHe18BJPyWjJTdo5KHSMlnRSRWjTdZaaajO2c1BI3TBzUGjh423GOUdKaf+ovTCqa6pUNTKpIuvNMg0vDzEPCp1vi6PEQzCNrnT9oe8lusQTp+2WL7SuHB9uqjYXpEZY0duJZMLD4ct78O0PHmo9ftJbepFMeDjr8F2sx+iciQS6phkT6plXAb201EgxaHQv1TkYKo7nedp51VNQXKrMHgvbVXmtLZOMlanEz3H3CJUlclt2PxKebZKuB/MexPks5M+dyPIOMEkEZXBwEBdffDF23XVXtLS04Oijj8bDD4fZEr7v48orr8SSJUvQ0tKClStX4oUXXpiMUxHsYOClitgKCvvwq1Sb96/wGTFtasFuTkGhhTUkKM2fMy3YtK2Vg1LU2595maag1IwELjl+b3RkU1i7bQTP18xwRU1BSartK1Vfiw43PSh0DvT3rKPNWPsWzAhKuRKOt+clnjCOXi/xkHqTL1WZB4WXeHQVxJUkWmA+kmTCU4sa5buQIqSTiGC/vh8u+i4PCsFOB40mHVZMeJ1v456nL2i8zOQyYH7sHbtj+YI2fPU9+1mPNcIBy7rx2BV/hh9feKQV7Q4AN3zgYDzx+eOxtLvFeqze5GBTvbAzVPRFl2/rmjCtb9tcicdFQLRWYocqFHUMIMhUuf9z78LP/+po3PmptzvLXlFITZCCsuei9qa6CAktDcpc9cDv61gmFtfDpBCUj3zkI1i1ahV+8IMf4Mknn8Txxx+PlStX4rXXXgMAXHPNNbjuuutw44034sEHH0RbWxtOOOEE5PP5BnsWCKJRrlTBeYWpKDQClyab7cIBoEWW0+jyZj0oYSCaPfOm4XFrzyVzoEnIiCz0tIXf4Og5XCFpz6ZUUBURpAIjXdxQys20qaSnPthIrSI1Iky8DNWGMOreJiiFclVTvFozbJpx7Zik4JhJsqOlijpfVyT5gMMky8tHoTE3qXwbJrnhH/78GERgWlm6qKlOWK2zdbpL7Hk50YtsJqlnmfCfF3fmYGLPRR347aePdaoczaCrJR1p2kwkvMhFzWr35SpXQw9K+NyOXDrS++I6drMR7GYbsbm/eiWeqC6b1kwKB+0yDzvPa17FAMIyGADstiDetpz4u0ox9cA9KJlkItIj4wJ/jWa8gjI6Ooqf/vSnuOaaa/D2t78de+65J77whS9gzz33xLe+9S34vo9rr70Wl19+OU477TTsv//+uOWWW7BhwwbcfvvtE306gh0I1nC8mASFd/0M5ZsnKGEKaUJ1jsT2oNS6H+LkoKh5OVEKSu33zpZwrg0RL2WCrX24mGZVTmC454ITFC0HpbZfM7hMHxboGLzHunhI8aFvylTfL5kKCnlQlL/FTpINHq+pIAVSQXiZJvSgkO+Ff9O1OnHqeB+A+iUes0xRr3XYXETfttcC7fd6agsAfPDIXfCWnTpxyv5LrcemC812HgH1c1BcRIJg3nPzOOa+AOCvj9sLXS1pfO7EfaxtuRrlIj9f+Yv9sPfidvzdyfa248FurCwTtwzHy1/7x2wj58d6U29HLN8M/39johWUCe/iKZfLqFQqyOV0Bt/S0oLf/e53ePnll7Fx40asXLlSPdbV1YUjjjgCq1evxllnnWXts1AooFAI+ysGBgYm+rQFcwCNuloagbf8DsWYSMy9BPSNsNm4e1IsOsdQ4lEKSguRG7eC0pJOojWTQv9oSZ0X71oBwoVPxcozMsBLLXxmj54kW9H+pQUjx6Pu60zvLZarmtGVUlODc9JJU1YpKKEHpZ6CEp6v25xLapcrzpzOyWwVzqYSGiHWk2RjduIYOSmENy/txJIuvWRSLxYfAL50evzyzWSjHkHJphJIJjxVUq1HUDodLcwEVyorP24q4Vnvh0v+bG9cfNxeTlWGezJcJZoPHLELPnDE2JSoeth1fhtuv+itsXwrBE4U9tspLkEJr9dMim2EzGxSUDo6OnDUUUfh7//+77FhwwZUKhX88Ic/xOrVq/H6669j48aNAIDFi/VWp8WLF6vHTFx99dXo6upS/y1btmyiT1swB2B2tcRVUPgCPxihoGwfLuKBl7Zqags3jbbFNMnSOaupwTEUFPKRKA9KRJJsLh16Y0aUgqIv9qbfo6AUiYTWVkskI5nwkEx4Vu6LIkWGghKoL9FtxsVK1UqDDcswxiyeZFJ7XpQHxSYo9nELpQobUKi3KHM0GsSmtRkbre2Np+y6P4bPPmJX6298MVg0hkVsOlCvxMMHOwKue8UzVuooKI7Be/w4K/dZ7IwciCoZcUUhjodkInDgsm7s5PDyNAL3ze2zJB7J4ITszUvjkRtOwCcypA2YJA/KD37wA/i+j5122gnZbBbXXXcd3v/+9yORGNvhLrvsMvT396v/1q1bN8FnLJgLMBWUOB4U3/edce0mTrn+dzjrOw/gjj+GZJqrEUpBaVKBIULSVfOgNHvOvh9OwyWCYnYt8QA5IgykFnDfDJ074DCkGiUe+jt9KNEHuTLJGrkgnESUGLkh8KA2sxMnXeecgn2HHpR6JlmCM6iNdeJoCkq6vgrC81Zy6YS20JnSvLkw2p0q+u9f/ou34IK3LceZh+4ME3zBXtJl+0xmIiwvSDI6CM1KKWVrRr18DZeCwvG+w+N9qdUUlDrEaCZh3yWdOP3Apbj0xDfFjpvnE5rNYYCNwFW9rglWUCblzu+xxx645557MDw8jIGBASxZsgTve9/7sPvuu6O3N0jP27RpE5YsWaK22bRpEw488EDn/rLZLLLZ2fFtQTB9MD0ocaYKFytV8ODZKA/Ka33BTIpVz2zCSfst0Y6bTiaUWTKugkJG12bPmZdaiKDkI3JQcumEWnxNBUW17ColQzfJZpLuEo/qpDGUmREV/Z5SxzbPR++mCdUZM8uEJ8nyrJKMUeLhOSj827qpoERF3btKPFaZxpyvoh0nelYLAOvbcKNpty7lxPXc3tlCUAxCYhIWTlDMwLBmSzwuDwovN7x9r4XNnWwNfMGO09EynUgkPFx71kFj2pb/f7qityPWtnqJZ4Z7UDja2trQ1taG7du3484778Q111yD5cuXo7e3F3fddZciJAMDA3jwwQfx8Y9/fDJPRzDHYfo34nhQTHPqUKEU8cwA3NlfYmoEffNqts1YKSitodHV9/2GEde8nBXZZsxKJmY7sElQzMh51cWTTmpqg/KRUDuuMSxwJKLEA4RmVS0HhXXTKKMrlXhSIUHh6phtko0q8ZhlGnercN9oEGDWVsfomjbUX02pMb6tmpkSZmhasyUeF/hzZ4uCYk7zNUs+H3nbcvz44XV4y05dVodRsyUeF4k4eo/5uOq0N+PgXebFDg9r5EGZa1i+oA0fP3YPLO1uia2+pCYxB2VS7vydd94J3/fxpje9CS+++CI+85nPYMWKFfjLv/xLeJ6Hiy++GF/60pew1157Yfny5bjiiiuwdOlSnH766ZNxOoIdBJZJNkaJx0xwdXlQuO+kzUFQsqn4QW1Fw0cCNBfRz681iqCocksmqSTwkWI5KA8ZmSJcJeHlo0xS96DYJZ7gsVLFR6kSduLQ39NJDwkPqPphSqzbJFtR50/f5igDo1g2CEpSV1CKlTByvh5xiJoavL2WsOoyyRLqKSjz2vRvjfzbd0s6aWWSRCXLNgP+XNNAO1PRKFn1fYftgvcd5jacagpKnRLP4ct7rL95nodzjtotxpmGmE4PynTA8zx89sQVY9p2MnNQJuXO9/f347LLLsP69evR09ODM844A1/+8peRTgcnf+mll2J4eBgXXngh+vr6cMwxx+COO+6wOn8EgjgYjwfFXNxdHhROOnhdmk/CbVNdPPGmGfNR8SPFirbQvrp1GAOjZezHWgfpmKlE2Dlkz+KplUxSSS2fhZeHlILCVBLzcddjaaPEQ+dtzqbxPA+52lybYZeConXxkAdFL/EUK1UUKuG1EcHhBIRUkPpdPLZyAwDbhgO1TC/x1G9/5QTFLOHw4+7S02qpYaZiEie1MzMLFZRm80hc0Es89nL1P3/zNjz8yja879CJbZzgatps8aBMF7i6OCsUlDPPPBNnnnlm5OOe5+Gqq67CVVddNRmHF+ygsLp4xlPicSgo29ksEwqUqlZ9NQSPz6ZpNqiNzrklk0QmlUCxXLXI0lnfeQCv9+fxi4veigOWdQfbsRJNC/NiaNfEFBSez8LvU9Ys8VSqWqksG5GDwktD1CY6WqxYJR7ax0ixEpZ4IoLaCsqDUjPJMg8Kv15a8DlJ6BsJSEaO56AYBIV/u0skPKSTHkoVH9uGgwgDbrQ0yY3pf+Clq5269RIO33aZYyaOqZiY5aN64Iv7rPGg1GkzbgT+XnF5UPZZ0hm7Y6UZ8PfOjqCgjAf882LGtxkLBBOBuCmwwPhKPObxXAoKH7ZGC3Wpqnsj6Ft4M+SoWvWVIpFNJRXRMLd9vT9IWL7+7hfV38jMmknZWSQEXjIhlWW4WNHuk6uLxyynhLHwVea3CRYOz/M08mOWeILjJ9W++TEBI6jNbDNm5MVsiwYCkkG/9xNBqeNB2WuRno5JJGPbCCko0R6UXYy5JtxXsbTbyHzSCIpdhkkzNabeZGAXeClythCUem3GjcAVo3oelIlGWx3jrkDHrvPbsKA9g13nt1pfCsYLISiCGYfbHlmHt3z+Ttz5lDsXJwrjISjmcwcdBGX7SEhQzGwOQE9WbSYqnysZmZQ9eA8ISAzhN89sUsSJd9koYhPRZhzkoITEic6Zskzo+HQ9fFpxQAJCr4fKMmHf+jlBUiUeRg5MP40rj4SbZOn5yjdTCNubzcWOnkuvVzaixNPTlsH8dr0TkI7t9qCEP3ueXcbh57HzPLPEE27ryrPg1x93QOUWRpLjJo1OFw6sqX6EbLL5RaxZD8pEQw2NTCfHPOV8R0EmlcDvPvsu3HXJOyZ833LnBTMOq1/ainLVx0Mvb4u1nRV1H6vEYygorhIPJyhlmuzL/BpJ3s7b+Ng8lC2bchMNs2x11zObtOPTLB0gUBG4kVdLkiUFpVDWDLD83Gm/auaNGYNfqqrYeV6m4J08I+yY/No4uFLhTpJNaM8bijhn8zhAdIlnT0M94efVyCTb25mzSFazJR5XIii/d3E6eABg21Cx8ZNmGHad34bDdpunfh+7B2XqCAqR49nSYjzdyE0SkROCIphxoFIKL6k0g/GVeMw2Y1eJJ2w9ViUe1tWSSHAPSmMFhYyfNJU2x0ol6jnGeW2tLVCcoOw6vw3ZVAKDhTJe3jJsbcsVlJFiRSsPEbiSEcbGU6uww4PCvvm3sH2PqhwURlCMxb0jGy40LpMsLf7tLLvFbIsm5NLRv/NzMMs7fF+DziTZ+j4SXpUxSzyckPU6hvbxEk9cguKj+SnbMwkfPDLMdolHUJprM55o0LFoiKdgeiB3XzDjQN9ot8YkKIVxmGTNDJVGJlmVbmr4KugbV6EchIvVy18osHAxHvnN1RzzvFTQGmsTzqQS2G+nLjzy6nY8vrYPuy9sR6UathK3pJOaOlMs64FntB+6noLhBeFtxqWq3sUDMAWlWLai7oEwGZbAFxpVPnIEtfHOo2iCkoz8nU8YdhEUW9lxKyhmjgmgvzfNibae5+G8ty7H6/2jOGw3u/11PCWez5/6Znzsh4/i4pV7xdpuunHK/kvx8Cvb0NOWjZVJkmAdUFNZ4jl413k467BleOueCxo/WTBpEIIimHHYqhSUQoNn6qBFrD2bwlChHK/NuEZm5rdlsHW4iJGSQ0EZsU2yRcM0yhe2kWIZHXVkaTOLpIWVSghm2WrYCFqjRfagXboDgrJuO844ZGft2nPppPJmjBYr1nEB0wui75v+zZcqqsSTchAU7kHhUfEmieBtm6p8pM3iCf7WXlNmipWquu5GBIV7UPQSj52OWS/RlJMbF0HZwkotrlC9K0/d1/obgZd4etriJWTvs6QT93zmnbG2mQlIJrwxDTLk7/+pVFDSyQS+esb+U3Y8gRtS4hHMOJBSEbfeTos25SWYZZt6oAV9Xu3b8IijTVhTUIwJu5lUWA6hL4iNfChKQUmHhjxzO5Nk0YwfU1E4aJegxv/42j4AOsnh/pZ82a1G8FILKSj0N67sOEs87Lx5OByBl11y6YQzj6RYDkPeQgUl3Af5fxp6UNjvnHAsX9gGE+Y04KiJxKYJFgC2DsUjzxy8xHPK/kvqPFPA38dxy2GC2Q95xQUzCvlSRX1b3jpc1EyfjaAICs2mKcfwoNS2JbneRS62OUo85oLteZ5WtuC4+9lN+OgPHkH/aOBliVRQik0oKBWToHQDAJ7dOIiRYlmbw5Ng/hbexeNUULQ8kmCbHFNIeKw/gXcf0YKieVBSnDToihInSeQFUcMC2RwgIocNPSiGSfazJ67ApSe+ydlNY5Kd9ogSz2KHj+TYNwWzXVylo0bYOJBXP//FQTvF3n5HwljiBgRzB1LiEcwocBJQKAcR5s066embP7n9TYNpPdAH4fwaQRktVVCt+lpGBYWBAbZJNm2oEaOliuYfyZcqOO/7jwAAjli+Hucds1xlWlBZwhW4FuVBKRgkY0lXC7pa0ugfLWHdtlHQ2kskg8hPEFfvMMnymThlMqvWFBRGbswk2WDfoUnW3WYcnQbKfSADo3aWSXs2hW3loiqvZQzVgysdCc/2dHz82D0QBXNGTCtPkmXnsMdCm4Rcfsq+2HdJJ949BgWEd/YsdRAnQYg4PjLB3IMQFEEkhgplPPbqdhy9x/wpywIwO3e2DRebJiihglIr8cRQUOibPzc8jpZ0crTN0WZsEgWAd72EBOm2R9apn4nzRCkoXL0xSdZQwT3sDwgW1f7REorlKsgWQYoC/aspKFElHqP0RHkbo6zEwxM+SW0YKpTUvqM8KGZsOL9vAzVjsh62lsS2YaCv1kFlqh5cMcmlkw2HLEYd2zznRMLDD88/AvlSxRmI1plL49y3Lm/6WBwXvn0PVH3gA8ZgPIGNw5f34AcPvDrdpyGYJghBEUTi2lXP47u/exn/9L4D8BcH7Twlx3QRFFebpwu04JOCEkcepkW5uzUNzwum3A4Xy4qg+L6veVBISXApCnyxJ9x0/yvWtlEelHomWduD4sgUYXNrlDqTqRldyxUVMuciVYEHhWb42MSp5DDYEkHhnS1m1D3BjA1PJDykEh7KVZ8pKPbziRxaQW2OxNpmwduf2zJJK9H1mL0mp4OjqyU95sFsOxpO2X8J0kkPb9mpq/GTBXMOQlAEkdjQPxr825dv8MyJAw9DA+JloVgelFIwmbeZb9V5HmqWTmKYGT6BgCiUWaorkSFXcJlJUEqVKl5i+SR0LCICtOi6ou7NEo/lQXEpN6WqSsvIKfNukwoKS3TNGuWhoAPIVeKpEZSaqdnzdCLBiYOrEyOTSqBcrGAgX9KOC4TkJ8qDwlWPODNezOe3SiDXjITneTjxLWIk3lEhJllBJOgbvvktvlnEMbgSthqdO3GyUMwuHsBOYo0Cj1nn+RsEs9SiclAogt3RmUL3jZQBAikkpHRkDaXCpaCEnTLuLh7+c4EN/AsVlNCDQo/p3TShsdckTkQCipWq8s1oJZ7a40QmW41Si+4psduu6TiDVOJhqlCboaCYZZl31MyqgDu7ph4ydZQdgUAw/RCCIogELVSFMTjpf/74ehzwxf/F6j9tjbWdXeJpvp2Tgtp49kizrcajTDUIo9vDBc/0s9gm2XBBJgWASERfBEEpGHkjzi6ekt5dNGx4ULKNfCS1v3GSQEQgatuQrOnnBUCpHHqJJ1jcqfW2JRNthI1SUDhcJZ4oBeUYFqTlmp9UD/waXFknAoFgeiEERRAJ+qY9FgXlvue3YCBfxgMvxSQoI2NXUGhRbsumlEm0WXJFRKYlnVQLLldQTD9LVJIswCb01shLv0FQ6Fhmq3CrU0GhfJaAdFlJshFpsOZkYJ7mSufjVF80BcXOdqFtXSUeGmRHfhd1P5oo8XC4JhJvH7HPGQhKAN/+0CEAgFMPWGrtux64svW3794n1rYCgWDyIbqmIBJKQYnRDUOgab6umTb1QOFsizuz2DRQiBXWxife5lJJjJYqTSsoXDVodXTTmPsxk2SjMkWAYIif61ihymGYZB3Hndca5rNUq74zy4QrN6WKrqCkkgmkkx5KldCMyrel9twiLw/VtvW8YMbQcLGCgdFy7fm2STbs4NE/VnKNFBSzM0crCenGV9cclxPe3It7PnOsczBfPZx64FL879ObcNm7V+BNvXbSrEAgmF4IQRFEghbQOImsBFIf4voCSEHZa1EHNg0ULNNsPRRZAmoubWeR1EOeqQYhQQnP3Rw8qLp4HCZZPlwPAPpG9WvIKw+KTjJyzi4ed/tzweVBYd6XkKDoXS6lSlm182Yc5+yaiQMEZZvhYkUpKCltWGB0Nom5H5cHxcw20Qb+ZaPbkjl2nW8nxTbCO9+0CE98/nire0cgEMwMSIlnDiNfquD6u17AWd9ZjbVbR2JvH5Z4pk5B6asRkuULggVnLF08AUGh0LPmyBX3dLgVFP0eEEEoOVp2GykooQdFN7KqvBFHkmxXS1qVrYYjpvuGJKPCOnHsbpoBV4mnZpItORSU4Nxq29bxoBDM+PnYHhQteTZV97njhZATgWDmQgjKHMb5Nz+Mb6x6Hg+8tA2/fW5z7O1Dk2x8BYVm2cQlKHTMBe2BXD/smIkTBe7LUAPumiRXXH1pU8mozCRb0hftZko8BcMkS91FREAKZlBbnRyUXDoZnlchVIaiBv6ZPhK+f5ePRJ/FYxtwiaCE29pBbQRzum+9oDZA74Ayn9+WiSY7AoFgbkP+b5/DeHJ9v/q50eA6F2ihipPIShirgkIlk572TOztlacjyRWU5s6dL8quRFdSYihjRREUVeIJF2yzzZgWdUokzRvEzwxT05NkQ2JERCBKQeEm2YIxGZj/rFQQh0m2XPXZHB87BI3UF17SMhWTBe3RBKXD0c5rqiL18kkmWkERCAQzF/J/+xwGDxYzPRTNwDRzxgEtsnE9KBQE1lMzhQ7mS/WebmzLFJTaotjsufMFn7I3RhyBaaSCmMMCtS6etLvEQ0Pn8lbYGgWihZHy4XFDBaWdnZeri4cfN1+KVlCIZLjajIHwnvPyEJEjekulE/ZjBNOsykkSbwFX522cBy+7WCUemWgrEOwwkP/b5zDKlZCgjCXLJOziiU9QSPmIq6AUDVPoUKHcdOBbkZU1cjFLPJqCYoSiAWFZhhbYctXXu2kcfg6zzbi3RlCsHBRjWGCxXEWlxgT4ebXWOlqGC2V3DgopNxE+kqwiKGXt+ebPYU6KnehK4IqR6UGh8pw6bqp+iYffu5yhkJjHFQVFINhxIP+3z2GUqyGxiKuglCvhIhl35HmpUlULaBwFJNi2pqDUCErVb/7cOVnIOkyyvu/joh89ho/94FFUq75z22wqibYaERhx5KBQiQcICIhrNo1pku0zSzwRXTx8MVYkpsxLPGE+i6vNWHlfShGdOGmdOPHFnntKVKIrUz7M8DWuGOXSCfBpAqaCwn2ojUyy5jydyTbJCgSCmQv5v32Oolr1wdfguGPLuWoSV0HhpZE4CggQLp5dLWm1sDVbJtK6eEhBYeRmsFDGfz35Ou54aiNe2DzkPG4mlVCL8TA3yZYppTZcMEuVqnM2jdlmbHpQorp4uNpBrxcv1bRxD4orqM1pkrU9KAR+zp7nqe3DEhAnN/W35T4UU0Hhz23L1M9BMQmKOck6a7QkCwSCuQvJQZmjKFV1UhFXQdEJSrxteWmk6geLrJmN4UKl6ivVhrwgg/kyBgtlLGrmnNmiTQsdv45hVm56duOACufialE2lVBEwNVm3Mk8FKVKRInHVFBG9BJPwUySTeqBaEHAnKGgpBPKMDpSiDDJalH3dpuxaWa1zKnJBIrlqoqM17t4TAVFb89tSSfV/VpgKCi7LWjDBW9bjp62LJKOtl7djKufk0lozJKPQCCYuxAFZY6iYpQw4pZpOCmJG9Q2bPhOBgvNlXmoXAIEC2CHGtrXWEHxfT1dNetQUPh+/vha2OHEBwpmInJQiOC1ZpJI1RZZntjqUlCK5WCacv8opePm1PHKFTYvxzG9l47NZ+qECoq7xEPqQhB1XyvxGEFtHCZBqRc5bxLMtGFW5a+d2cUDAH938r74+LF7WH8HgJPe0qt+fn6Trmxxz8r+O3fhsN16nPsQCARzD0JQ5ihKFZ2gxFZQSmNXUMzskqZLNAZRoMWpme3NbZWCwks8bD9PcoLCVJZMMvR6jDiG9uXSCbU4lzQPCmszZjkoo6WKei2oxAMEJSNXhoqZJsvzTFpZPoszSbaBgmIRlGR9gqIpKOn6BGWY3au4ZZij91yA0w8M5uis3EfXynraMvjgkbvgrMOW4d8vOFI8KALBDgQp8cxRlCtGiWecHhTf9+F5zaVucu8G0HwnT4kdM51IKIOkOaX254+vx7OvD+LgXefh+H0Xw/M8jWRka1H3QOgdAXTi9NRrA6hWfSQSnrrWZMJDKplwRt2rWT2pJNJJD6Ol4L7UT3StqvJOKuGp1mnaHy/fEFqMY3OTbJvq4oloM1b5K2EUfj0FxQw9swlKPQVFfy+Yil1cfOPMA7Fy38U4cFm39diXTt9vXPsWCASzE0JQ5ijsEk+8Mg1XTXw/UCia/WY8Mk4FJZ30kEh4aK/5Pfj2Q4UyPn3bE+r6fnHRW3HAsm5LBXEpKEOs1DRYKOPVbSNYvqDNKpe4FJRwmGCyNjumXDPJ2iUeblYlg2x3axqJhIdcOoF8qYpRVqbhSapEjvKmgpJOaAqKs82YHdcVdd/Ig2KqIjkHcSLMb4s3mK8RkgkPp+wfbxqxQCCY2xC9dI6iNE4Piklo4nTymAqKqYBEoVTWO2LIg8IVmKF8WSNfmwcLAGxyE3pQwvMeMojTi7VOngKLuQeYD6TAFZSaIpFJqnIOL/FoBCUZGnQHVMx9QLZC825FIx8EVeIpGmmzqaTqIBrMl9U94cP3eImHt02H+65PSMyST1QOSks6iV16WiEQCASTCSEocxRWiWccJlkgXpqs6UFpxuQKAMWKThTaHQTFJFp0XaYKoqLu2XWY50G/my25FIg2UqqoFulRVeJJOImAq8RTKFdDIlEjFy2MgITn7DLJ6iWeXDqhSM6mgbwiaZ0toQjKS0thAm20CtLIJKurL+Fx9u7tkCF7AoFg0iEEZY6iXJ04kywQT4EZGaMHpWgoKO1MMVDnYRAnsxRCi6wr6t48DyrhmCSDSim+HyonvMRD5xcEtdWfZkzHpHZZboJ1KSgtab3Ek3coKOu3jwIIyiK8bMPNueFwQ6agGCW6RimtvHzEyc2KxR0wcXitu+Z9hy6zHhMIBIKxQDwocxRls4tnHCZZ1+/1YCoog016UMxU1lBBCb0jZukpbyootUXVFXVvE5QIBYUt+iPFMloyybBtlxOUiDbjkKBULAWFDzF0tQrzica+72smWUqxpbJWZy6lGZf1YYGOoLaMTmZ2m9+m3Q/Tz8L3zcnMiiU2QbnxQ4fgN89swin7L7EeEwgEgrFAFJQ5Clo4KbOjUK5a8e71YJV4YrQaj1lBMTpTFEHJR5d4LDNpTSXIGkoEYJd4bAUl2CaRCFNVw8nDYamFHmsY1FapqmPStVAi60ixEpaNHHkjI8VKMO+n9pJlU0ktJA6wB++5FBQtBI2d4z5LOu0SD293Nh7jSg0F3HH0tGVw5qHLrEA3gUAgGCsmnKBUKhVcccUVWL58OVpaWrDHHnvg7//+77W4c9/3ceWVV2LJkiVoaWnBypUr8cILL0z0qezQII8Cj2ZvdnAeYCsmcbqAyCTrxYyqpzZjamFVOSj1PCg1M2mkgsJLPHk3QXFlitD2YeR88G9LOqkWcr2LJ1QblAelVFXHJIJChGEwX1KvUYthQAX0EhAQlIG43wSA9TuRs3ypokp8UWWaA3buggl9MrJe/kkwNWVFb6e1rUAgEEw0JpygfO1rX8O3vvUtfPOb38QzzzyDr33ta7jmmmtw/fXXq+dcc801uO6663DjjTfiwQcfRFtbG0444QTk8/mJPp0dFjQokM8yiUMyzOnHcRQUKvHQTJZmTbKFCAVF86CYJZ6ye/CeK+qeiM782iBCUnpcLbstRrvvqNZmXL/Ek2UKCnUOtWV1k+y24aJ6vtYhkyETbUV7DTLJhK2gZN0KCr9feox8+PN+O9kEhT9uZqTsvrANS7pyOGTXeWqQo0AgEEwmJlyP/f3vf4/TTjsNJ598MgBgt912w7//+7/joYceAhCoJ9deey0uv/xynHbaaQCAW265BYsXL8btt9+Os846a6JPaYeEMm/Wuk6KtVTTZtGMB+X+F7egM5fGfsa3cSIkizuzeGOwEKPNWF/sSUEZdgSmEUYjjK6uHBTaz8KOLLYOF8MSTyX0eRByEWbVIEm2FnVfqbrzSJJh5Dzdiw7Dg7JtpFi7Vk8jNxpBYR6VRMJDayaJZMJzdvDwczBTdcPzCn8+wBGIxks3ZqhbLp3E/33mWKQTUhUWCARTgwn/tDn66KNx11134fnnnwcA/OEPf8Dvfvc7nHTSSQCAl19+GRs3bsTKlSvVNl1dXTjiiCOwevVq5z4LhQIGBga0/2Yb1m8fwU8fXW+1/04WyCSbTiRYa+s4CIpBDN4YLODs7z6IU7/5O4s00MK/uCOIdo8f1GbkoNTxoJCyY2aZuGbx0H4W1WbijBrzblxmVbOLJ5syTbKOacZpu82Y5ugogjJU1I5jHtfV5eN5HjpZyc5UVOzgNU8bzkdJtACwx8J2mDhieTjnxsxIoWuX9mKBQDBVmHAF5XOf+xwGBgawYsUKJJNJVCoVfPnLX8bZZ58NANi4cSMAYPHixdp2ixcvVo+ZuPrqq/HFL35xok91SnHSP9+HwXwZ20eK+Mjbdp/041GJJ5UMWlH7R0uxWoVtIqATli1DBfXz6pe24p1vCmeocKUCsE2zUaByCZGL5jwouoKSNRSUvKPEs5BKT1TiqehEIPjZ7KYJntOSSTKTbNUZOU9Ep1L1MZAv1a4lXds+eIxKPKaptFVTUOxW4c6WNLbX4vMtk6yZBGu0Fe+5qANf/PM3Y5f5rc6pwm9hZZ+1W4etxwUCgWAqMeEKyk9+8hP86Ec/wq233orHHnsMN998M77+9a/j5ptvHvM+L7vsMvT396v/1q1bN4FnPDUgX8CqpzdNyfFIQUklvLBsMJ4Sj+H94L6Su5/ZrD1GUfeLagSl+Vk8Rg6Ky4MSYd61CQoZVe0240WdRJzcIW9A2G2TN8yqOWaS5UFtrjZjICQi7TX1gkgDlXhas3YpBQheKyJfPGyNqyZWicdIijV/B4APH72bRiY5+DUMx2xLFwgEgonGhBOUz3zmM/jc5z6Hs846C/vttx8+9KEP4VOf+hSuvvpqAEBvbzBafdMmfaHetGmTesxENptFZ2en9t9sxcgUffCHCkpCLdrxSjxGO6/xOycddz+7WevSMhUUMxcl8pgROSgFZkYNSy21a6IcFEPJUB0tjmGBRJxG63XxOIgCoCfJkjoC6EZXN0EhBUU3yZphabzEQ++VNqaycFLSSEGJO1UYAI7eY37sbQQCgWAyMOEEZWRkBAnDSJdMJlGtLZjLly9Hb28v7rrrLvX4wMAAHnzwQRx11FETfTozDuacmskCtZmOWUExZ/EYv3OC8lrfKP70RlgSIBKxwCilNIIyyRplGiAkEaSYzKtNBraC2pK6glKp+ihVggwYizhZQW12u2+hVFHkLFWbdkxKA00q5s+n51EFZWuNiJD/w/SgtKbNEk/w+2ixolQqrrJoCkpO3zaVTIBXblwKSiNce9aBWLnPYtx83uGxtxUIBIKJxIR7UE499VR8+ctfxi677II3v/nNePzxx/GP//iPOO+88wAERr+LL74YX/rSl7DXXnth+fLluOKKK7B06VKcfvrpE306Mw7mpN/JgirxJD0kE3ZoWSPYXTz15+u8MVjAnosC4yUpDgs6wjZj3/e1ZFIXzFZhrggUShW0Z1PqGrpb09g4kI+MuufkJl+qwPM8kMizqMMwyRoGW779aKnCOniS2vOIoLRmdPOo5wVBb/lSWAKiLh418K92/8z5OORRiVRQGEExFRQ6Nx6PHxeLOnL47ocPjb2dQCAQTDQmnKBcf/31uOKKK/BXf/VX2Lx5M5YuXYqPfvSjuPLKK9VzLr30UgwPD+PCCy9EX18fjjnmGNxxxx3I5XITfTozDs0aRseLMEk2odJk4xGU4LnppIdSxbfyR8z4erou3/cxYigoVT8gEGbrqnXOimQE55tIeMgkEyhWwuh2Oq+uWuw7nZc1i4eRjUK5qlpzEx5UjgeRLFercI518ZiprEpBGXUbXYGAXPF7RjkoZueNWeKhY4wUK0rh4c/hJR7Tg2Ie18wyEQgEgtmECScoHR0duPbaa3HttddGPsfzPFx11VW46qqrJvrwMx5xh/aNFbQgp5Ne2JEyhjbjrpY0tgwVHQqK25NSKFeVUsEDvYYK5YYExWwzBoIyRbFStfJIqMRjTzMOjkEqRrEW+07X055NqXKLta1GUEIlIwxpI2UnIFDbhwMFpS1rX1c2nQQYiSM/TYdRljEVFCI7+WJFqW08bE8v8dgKCj9ubgwlHoFAIJgpkE+wKUap0vw8nHEdp0ZQ+MTb0VhJssFzaRE0Sz58gB8QEhau0rRmkurbfzOlLbPEA4RlitCDEuxnXlta+12pIGxR5nH3PHKefB80S8dlkuVThU0FpbVGGN6otVpHKSgcVKahgX+EeiZZt4JSn6C47p1AIBDMRghBmQV4akM/vvo/zzYdGQ9ABcKlkglt0WsWpJh0tuhEgDBkEA4q8dAxKCG1LWtnmURBtRk7lIxQQSEPiqGg1NJgtYF3jGTQvWvLpjTVYrRYYSUeOxY+2FZXMkgFoSyYtoxDQWHX0MY8KnaJx62olKu+8rhoCkqDEg8/rpR4BALBbIaMHp0C8BZcIPi2b6Z+1sP7v/MABvJl9I0U8dUz9m9qG1XiSXjWIt8MSFUggmIrKO6JxWF2R7DQtmWSeAPNeW9cJEMN3jO6eLrpvIwcFJfRtVCuKsNpay1ojbw1I6Wyc1ueJEtqUacyugbHppfVLNOY+4oiGPw4rt+3OlqROcFpzzqUG3bcnea1WI8LBALBbIF8xZoCFI14+76RYsQz3RiolSd+/vhrTW9DpaQki7ofS5IsLcrmtqRIdLemtd9J0aBjRikoX/r107jhty/q51wO5wcRrBJPTdkhD0qxEhhgCw6jazhVmMfGJ7XzG+GJrUmHB6VYsSYSmz6SNkeJh59HO3u+2Xljlnh4PP1WpdBwgpOu/S2JlDOOPvzbgY55OwKBQDBbIARlClA01IftI6WIZ9aHa2BfFMrKcOohx+LT4x6rR03+NUo8tUWb5u1Q8igRGVIVaHHlptrNA3l893cv4x/ufA6bBsIJ1i4Piqn+0DV0tYYLfb5Uqaug5MshCckZxGmkUHFG3fNtqSVYERRDuTDTYM3z4EpHWyapZZWYBMXzPLTWjk0lJL7/ZfNakfCAXee3Wcc0cdCyeQ2fIxAIBDMVQlCmACax2B5TQeHfivmCXg8qqC0ZmmRHIhSUz//ijzjrO6u1Th06Z0pd7R/VSZUZG68UlGJtZo0iAsG/PKyNqyn3PP+G+rnISBXBVFDo325mFh3lBMVJbqpW6y0RqJFi2TksMMc6n5SCYpR4CC4FhXtLOEHxPE8zurY4tiVCubUW5sb339uVwy8/cQxu+svDrO0A4MXNQ+rnZT1S4hEIBLMXQlCmAKaCEqfE4/u+8pMAwGOvbm9qOxV1n0ioBdXMLgGA/3tuM25e/SoeeGkbnt4QTommGTYUahZFUBbXJgNbJZ6M3vHCDb5cjbnnOUZQjCRZINok25JJahOLnQP/FLnhg/eCx2nRHym6t1VlMTaRuCOixONSUN69Xzi2IWEE1PHtXQZbUlVcHhQgGOpH990En6HTKBhPIBAIZjKEoEwBbAWl+RLPcLGi1BAAeGxtkwSFDQsktaHfIEaVqo+v/Pcz6vcBRmDonBd2uhWUYUVQ9Hk7ZIalBb6dEQEC//m+F95Q5ahSjDbjXDqpddqoVuEk78QJFRQzzj5UUCpWhgrtHwjySGwFpbEH5T0H76x+Xr99RHuMG11dBlvTONvmMMNGge7dkbv3NL2NQCAQzEQIQZkCmCFncUo8JjFYu20k4pk6whJPQvk1+ox9vbxlGM9vCksCpOz4vm+VeAZGS6gyojRoKig1YpI3TLKkLgxpCkr480C+jKdqyk2pYvtIssZUYhU7n0pqnTYuD0qWE5iSrpK08hKPq4unFjmve1DIoJqCV8dHAgRhc/927qHoyKbwNyv30h7jBMWVoWIG2rn2H4UffuQInHrAUlz//oOb3kYgEAhmIqTNeApgl3iaV1DMchBNwW0EbpJVCopBUPg0XiAgIYDedUQlnqoPDBXL6MylUaqEhIAIitVmXFtU25UZNSQlpln3jcHADOr0kTAFxfd91cWTSye0tFfnROJUSGCUSTalm3dHtBwUW7lxeVASCQ/tmZQiLi6SAQDvWrEYT3zheKvUwluNXeTD/FscBeXw5T04fLmoJwKBYPZDFJQpgFniieNBMUnF1iYJCk+SJQWl31BBzOA3Ik78fDtbUmrh7q89zrcjhYWSYimtVikoGWozdpd4+DUWKxTP71ZQipUwRj9rlHiKNPDPtW05HPhHf+MlHreCEu7b9KAAho/E4UEhuHwgHQ1KPCbhiaOgCAQCwVyBEJQpQMGImB+OaPf96v88i4tufUwLdiNVg/JIqLOjESpssafBer6vG2VNgkJEgZ9vhm1Pj9M+sqnwsagclPZsWEohmKFtiqA0CFvjw/cCBSVUOYqO8pBLQQlNsrRtue404yCoTVdQAJ1kRCkoUdBLPDb5WNKlG2BdHheBQCCY6xCCMgWghFTCiCP2vVyp4sZ7/oT/euJ1PP162E1DqsbuC9sBBIt5yQh+c6Gkung8ZFPhTByawAvYcfXkUSEfSTaV0NpiqSREfpOOXEqVcIaL5aAEUwoTW4N/7aC2KAWl5BoWyDp1yIeS8ALixDtt3BOJ2baGSbY1Gyo7rm1p38VKVZ0fbxdub9CJUw9aiSdtkw8zAdbVJSQQCARzHUJQpgDNKCi8dJNkSV60OO42v1UZM5sx2ZYrYYkHgKWCAHUUFGPBpm1JzSFPRls2pRb6qh+oDVbUfYM2YyAkPqFJlueghFH3yiCbTsLzWIR/hI+Eqy/mNVGJZjBfcvtXWMsx+X7aI0o8rTE8IsG29Us8S7tDgpJKeNbgQYFAINgRIJ98UwAz6t41l2bzQEH9TJHvQKhqdLdm0FOLd3eVeX777GbcsvoV9buaxZPUSQY36A4ZCan9yoNSU1BqC7xJbvh2razjZKhQtnJQ2rKh14NgDi20Sjyudt9ShRlk9VbhfNmdJMuj7s2JxFRm4cRQGxbomATcEVHiia2g1PaTSnjOmUw7MYLSmklKnolAINghIQRlCkAKSgeLVzfBE2LzrC25XxGUtIqdNzt5CuUK/vL7D+PKXzyFP70RtA2X1DRjT20P6K3GRDSWduthbFEKiklQ2rIpJBKeWqCHC2WloNSbxWNmqAyYJZ5IBYU6cRK1f0MPilsF4VH3+jVR2WpdrW3b83QTbMJBHtomSEGhY7vUEwDYmZV4smkp7wgEgh0TQlCmAKRIzKsRjGGXgjIYKii8JESqRldLSFDMTp5HXwnD26j8onJQaiWe7pZMbX/htkQU6Bs7+VMKrJRCxwZCgjJsKC+tzIdiDQt0BLURientatH2W3C0GWcdJlk6rxzrxKHrjYq6J/8KdfGQikG5Mp25NBIJXanggWm5dELzxnAy0xqTRJB6E9Wds7A9q34ecqT/CgQCwY4AIShTAFp459VUDJeCsnmQKSglt4Iyv72moAwVtG3vfWGL+pnIT0hQEmp7vj+AKygmUdA7XmgxDwmKboRVRtlCxcpBaXMGtQXPWdKpKzeNTLKjJb30RASCX5O7A8g2yZKKoWb7sOGD4fZ84J/+uK6gxCMob+rtQHdrGocvn+98nBMlsxwmEAgEOwqkf3EKoAgKU1B839e8BZuYB4XnkNDi29WSxvy24Ju1qaDc90I4z4bIT9ko8bg8KEpBqZUUgqF60eWQ/lG9lZjUETUQsGArKB21hb1Y67TJpBJqaOESo7RULzBNK/HUiAMRoy2MsPEU1pDc2CbZTmPgHx8+SOAKihlvTx6UsZhYe9oyePBvjxPzq0AgENSBfELGBM8oaRa0OJLJterb4W1vRCgoVHbpask4SzxbhwoqKh5gCkpFV1BccfekhCzuyKlun4HRkqU2RJV4SDmgVuLhYtlqM+YhZqSiUJs15X30j5ZQqfqgDLm0s0xTsWL0iTS83p9Xz007ykO8RVmZZFt0wtFVe204ONlpz5oEJaWucywm1mxKzK8CgUBQD0JQYuD6u17A4V+5SxkrmwUpA12sjGC2+HIFRTPJMg9KWOIJCcq67aPafqh8oqYZJ3UPiquLpyOXUmWcvtGS5dcwCQodwy7xhAoKLe6pZEI9bzCvb08eFB6GBujTjLmCEnXcDX2jtevQVZAwqM2hChnP7XIoKPUICv0eJ4Y+DpYvaJuU/QoEAsFsgRCUGPjGqufxxmAB3/jf52JtR56OlnSSDanTvQXcg8JNsqqckk0yBSUkMybRod/Lqs1Y7+IZcOSgtGdT6K4pCP2jJeQjungGTQUloy/SQ8yD0uJY3CmBlq5pUUdWZbvwMo3L6Foos31naKpwcF7UAWWWYcKoe5YkW/tbNpXQjuMq8ey9uD28BmPfi2r+GXpNJhr/cvbBWNHbgW9/6JBJ2b9AIBDMdOxwBOW3z23G5bc/qZVR4sIkF43AMzp4OYRQqfrYwlQRUlAqVR+lWqkmm0o6PShDFkEJti2poLaaSZY8KKN2F09bNqV8Jn0jTEGhEo9hsB1RHhQ9zn4ob+egADwUrVbiKYbEiLphKAfG80JSxc8hX6oyf0vNg5ILQ+KC40QpKNVwmnHtb0FCbkg6XCbZj75jD/Xzq1uHtccO2LkLV532Znz5L/aztpsI7LOkE3dc/Hac8ObeSdm/QCAQzHTscATl2lXP44cPrMXqP20d8z7M4LVG4J6O0FAakpytwwUVrAaECgqfgszn3gyMRs+1od8rtRJPuuYt6awT1NaWTWkTj+3U1ZqCktc9JNReTI8PFUpuBUU9TgQlLNUQ+Vm/PSibtWdTmjeDDwscVdvRcXVVo9P4Pce2pWvinTm8zOMq8eyxsF2pKEfvsUB7zPM8nHPUbjhwWbe1nUAgEAjGjx2ui2dbLQfEnBIcB8XyWAlKqKBwYsFTZIFQQSkwL0o2ldCm8xLMeTpEApRJNql7Lkg18X1fRe63Z1Oaz0Sdb+14lPNRrFRRrlTVdqYXZNtwSZWWOEHpZLHyvu8zghIcdx1Gsb7mpTG9ITyufqSoqzMdEcZVc9uRUkURQJ4W29FSn6AAwE8/fjRue2Q9Tj1gqfNxgUAgEEwOZjVB4Qt1syAVwCyNNALv3mlmWB9HkU3L5amr5jkRSEEhopBMeEglw+m93KNielBoEadhgdSdQx03I6UKqlUfhXJVLdpt2aQ2M4fuK5VIeLlmhCkZbYaS8QZv983wDJEwTbZYCY/bkkkqQrKupqB0GkSBJ8kSqVOTks3W36x7W65O8RZmrrh0O7p4gmtL47xjljsfEwgEAsHkYVaXeJ7fOBjr+b7vKzJgLuyNwNuCx6WgsFAzglmmIYIQ+iYo2j34t1ipolpbdKncQkSErqtS0U2yRCZ8P1BoOEFry6RC4lQsh8dlhlLa/0ihovwzpKBQiWdzzaxK04YJ3CQ7yvw7rZmkUi7WbyMFxa2CAKHqZR5XHafOtgSNoDAy5PKgCAQCgWD6MKsJyqtb47X7jhRDqT8uQeHGWDPDpBFUAFk6qYgAJyWmmpNXCoqe6MrnstA5DKksk8BAS+ShZCTJ5tIJ1TEzXKiwTpxkME9HU1BsQymVeUaKZRUGR9u42n25j4R7WOg+ZpJBZgkRFIqcN0kHJxQ0xZmIR2s6CR4lElXiIZASReDlJFcXj0AgEAimD7OaoLyyZbjxkxh4KcX0bjRCvZJMI/AZM2EXD1dQKsbzK9p2RBRybLGm59B5Lay1vQ5HJMmaJIMbZIN/Q/OuSYyAsMwzUgwVFPobqR4Dtfsyr9WtbAwVSmGZprYttelurKkvZoBaOhmqN9uHdQUlkfDQnnFPGAYCQsI7gnLG8D9+rC5RUAQCgWBGYXYTlK1xCYqdAdIseFtw30ixzjNt8AwOIgIj7PjmuVgKSq3UkmKLNT1nqHZei2oKyog1iydcoHl5yRz451ZQwrcHPT5SdHlQjNh4w8/RydqMzbA1M0fENMny86AW6VZHC7P5c7gti743FJVGXTwCgUAgmD7MaoLycswSzwAjKEOOicL1wD0jw8VKLB+KKvE0UFCo/BPlQQFCFYByXIjoEEEJFRTyoDCSwcpLw0WDoLB8loIxlA8Ijal9I0VFfsh4a3o/TD+HMskygkIKCqXjEkyTLL/+7bUWaS3hlR3b9K8Ez7UHD5rHakknNSIjEAgEgunHrCYor24djjUbZyAfrVo0gvn8OCoKb9t1eVCILPTUFuu80cWjDcBjbbfBeVEqa07bV9no4gGgkaMh5SOhmTmhuuJWUILn8UC5VmMmDmGeoaBwD8qopaBktee6SUatzbl2Xq0RZR1TyQmuwR4eaB5LDLICgUAw8zCrCUq+VFXehWbginlvFmanzfaR5nNUeIqpq4uHzoUWa1JHXF4QU0EhL8mizlqJhxQUKvEwDwYvL9klnrD92UxdBcJ4eYqkz6QSynBqEhRLQaESTyFUblrTwd/mmyWeOgoKgZd4+IwcV4lHV1B0lWRBe1b7VyAQCAQzB7M6BwUAXn5jGEtqQ+caYTwmWfP522MoKJQ8y3NQOOEhUkGLNSkYpkkW0IPL+H6oxFOsVJEvVUDCUjoRLtBcQRk2TbKZ0GPSmrGJEaklRFDaMlyZSCKTTKjrtBWUMKiNXgMyqFolnjoKiuv39pxbTeHnFm6nE50jlvfg4pV7WSmxAoFAIJh+zGoFBQD+FKOTZ3AcJR5TQemLpaCEC75TQSlS9wuVeKI9KBlLQdFLPICuFCVdCoqziycMUzOTZIHQb/LGYEBQeJkF0NULs4ung+2b7hspJc2YZE1FhisonQ1Msr1d4X0xFZRUMoGLV+6Nw5f3WNsJBAKBYHox4QRlt912g+d51n8XXXQRACCfz+Oiiy7C/Pnz0d7ejjPOOAObNm0a8/Fer2VvNIOBcXTxmFklcTwoDRWUmi+D1AQiCHmjiwewFRS6jq6WtCIvPMbfqaA4u3jCEg+RH65UEClQCkpWX+w5Oeiq40GhTpzuluA52VRSK9O4SjzzDZ9KnBLP3os71M/Z9Kzn4wKBQLDDYMI/sR9++GG8/vrr6r9Vq1YBAN773vcCAD71qU/hV7/6FW677Tbcc8892LBhA97znveM+Xj5UvPdNLzNOG7U/YhV4olWULhxl08kzqWSiiTw4w8pD0qUgmIbPfO1+TI04bctG4bAcYKieVAyDgUloyso5aqvHtNMssqDEhCMFktBCYlFVA5KpepjU3/gGeJtvbzM41JQTJVFK/HU4u0zyYSzE2dFLyMoKSEoAoFAMFsw4R6UhQsXar9/9atfxR577IF3vOMd6O/vx/e+9z3ceuuteNe73gUAuOmmm7DPPvvggQcewJFHHuncZ6FQQKEQznkZGBgIH4sxj4eXeArlYPAdTxath2YUFN/38eGbHsbWoQJ+cdFbkUomFNkAgoWVvv1zYkXkp8fyoDhMsmnavqKpMG3ZFNqyKWwfKekEJSIHxeziaWWL/tbhYu243CRbU1AGbQ8KoCsZpgelLRMkvvo+1FBAXrbpacuoVGAzqI0eJyQ8/X6QauJSTwBDQZFWYoFAIJg1mNSvlMViET/84Q9x3nnnwfM8PProoyiVSli5cqV6zooVK7DLLrtg9erVkfu5+uqr0dXVpf5btmyZeiyOgjJgTDAejmGUNQfVuaYhr9s2inuffwNPbRjA6zWlgMfiZ1MJtdCPlmwPCplki+WqGuhH2/F90H7p/JMJD9lUQqkcdG7JhKdFzms5KEaJJxhGGOybBCDNJFvbdlBF5Ed7UEzPiOd56jhEUDQFhREQTnTU40xhaUkntWtqb0BQ9ljUpn6m8pJAIBAIZj4mlaDcfvvt6Ovrw7nnngsA2LhxIzKZDLq7u7XnLV68GBs3bozcz2WXXYb+/n7137p169RjY1VQgHhhbUQGltRMl9zPQnjk1W3WeZGCkkkmkEh4iuCMOoLa5rGFulCuMrOq3cWTL4WR84FC4SkjKxEUrp4AZg6KbpIN9qMv8ppJ1njM9qDwEo89GZhKN9QWzqPlyWPSlkk6FS2uoJilJdqvy7sC6KrJK1viBfsJBAKBYPowqW3G3/ve93DSSSdh6dKl49pPNptFNuvOqojnQdEJSRyjLJGB3q4cXtoyjIFRe9tHXt3O9q0TFFrsiWCMlirwfR+e56nzmK8RlIrW/UMglSNQUNxpsFEEhUjFKFdQmPLQlk2p8k5wXNskG/U7KRjppGc9BgQk4zVmaObD+SigLopk9DDCY+776D3nY+U+i3HqAUuc23K8FsNQLRAIBILpxaQpKK+++ip+85vf4CMf+Yj6W29vL4rFIvr6+rTnbtq0Cb29vWM6ThwFxVQ94gz9G1EKSotzXwDw6CuMoNQITd4wurawBZZ8MKSUdObS2qwdd4mn1sVTslUQIirUymuqEfW6ePh+wmNFKyhRJZ7u1oxWgiHwdl/AXeJxGWSBkMAAYYmN0JlL47sfPhSnHbiTc1sA+PvT3wIA+If/t3/kcwQCgUAwszBpBOWmm27CokWLcPLJJ6u/HXLIIUin07jrrrvU35577jmsXbsWRx111JiOUxiDgkIqRBwFhcgAlXhMctM/UsLzmwfV71TCIQJFx+QTdUeLFW0mT1s2pR4vlCvOqHtdQanFxteIxby2YIHfVCujpC2Cwrt49IF/wc/64u/yoEQ9lwiK2cFDWGoQFD5QkJJcowb28RKPGbbWDD505K74w5XH472HLmv8ZIFAIBDMCExKiadareKmm27Chz/8YaRSLB+jqwvnn38+LrnkEvT09KCzsxOf/OQncdRRR0V28DRCvkkFpcLaZ5d2teClLcOxCAqZZJd01zwohkn22Y0D4GOBiHiQgkIkI5VMqNTV0VIFJDakkx4yqQRy6SSGi5WagmKXeEhB4V087Vl9MvCLm4cA2GTBlSQbpaAkE56mwFglHkNtoXZfc5IxYUm3nvbLA9beuWIRTt5/Cd5zkFsF4Z6WQowhjRxdMm9HIBAIZhUmhaD85je/wdq1a3HeeedZj/3TP/0TEokEzjjjDBQKBZxwwgn4l3/5lzEfi7fx1gPvmlnYkcVLW4ZjZaEMOUyy5CEJftf3RVOG84aCQj8TQSFSQ+SB55y4clBoP/lS1coyoYX85Vq6rjljhjwog/mSlp9iPs7Pg2CWdHaepxOOw5f3YEF7FsfvuxguLGEKSkc2pZGfrpY0bvjAwc7tAF0JGm3y9RYIBALB7MakEJTjjz8+cspwLpfDDTfcgBtuuGFCjtXsN+oiex4pDWNSUGoelFLFR75UVZ6SoYLRwkwlHufgvSQGapN9K7WhflQy4Umxrsh55UEp2yoItePSoEBzzg2RDIqrB6K7eHabH7bnBtvqCgrPFwGAPRe14+G/O87pPwGgzUuKMsM2g2YJqUAgEAhmN2Z9tGazCxYRlFQizOTg/o96qFZ91Qq8oD2rjKzcKDsUoaCYHhQgNHpqrcK1c+KzdtxBbaGCQj6Y0P+hE5IoBaXGX5Cq5aeEj4cE5Yjd9fk0rUxdSSY8i8AAiCQngK6gjIegjDT5mgkEAoFgdmPWE5RmFZQSn4fDhtfFPUZrJqkIAfehmCWe0INSIyiOicSjpYrqDiJPh1NB0aLuQwWFCAq1Cpsza+a3uRUUQls2pQe5MRJyxPL5kdv2duYUkWoWiztDglKtutW1ZiAKikAgEOwYmPUEpdkFixb7TCoRKihNE5TwGNlUQrXDDtSZ7TNithlzBYXSZIt62BrtP9iu4pxmnGUKCh2fQtJ6jJLO/PboIXuAndrKM2XMCb+8vdf0nzQDTmhGSvHmIHF0tbhNuAKBQCCYW5gDBKUa6XfhIAUlnRy7gkKdLTQvhoe1UYmHyAS1AKsSD/egcAWlprSQQsHVlULdacYVq8TT02oSFP33dDKhEQUzDXY7my9kDuhLstC3ncZAUDhGx1CmufWCI7Dvkk786zmHjOvYAoFAIJgdmPUEBQCKlcZlHvKgZJIJ1ZbbtIJiKBn1FBQqZdgKip3Kmi+FRlciC0Q2BvNlK+SNn0O+VFWkiBSUlkxSUzpMDwqgqyamgvLRt++B3s4cvvjnb466FQCAnbvHR1DG4iM5eo8F+O+/eRsO2mXeuI4tEAgEgtmBOUFQmom7Lzo8KM0OCzTNqiFBCQkOqRmLOwNSMGJ6UBwqyGixos6BzonCyvpHS/WnGZcrGKx1DnUwosGVjwXtdjlkt/mt6mczOfZNvR144G+Pw4eP3s1xF4BlPQExOfWAsY0u+NxJKwAAX3/vAWPaXiAQCAQ7DiZ1Fs9kg/ydwUJevzOkxBSUsZZ4SMkISzxcQQl+XtShKyguo2tY4qmq55EHhQjKwGgpwiRbS5ItVVWLMp/ky2femB4UAHjz0i48trYPgHtycD38+hNvwxtDBey5qD3WdoSPvWMPnHXYssgwN4FAIBAICLOaoGRTCRTRXNx9gTwoKW/MJlnygnTUKfEsqiko5rDAnMsky9qMyYPiJCgRHhTiR3ySMCko2VTCiqMHgH2XdqqfTQWlEbpa0+NOZBVyIhAIBIJmMKsJSi6VQLHaXCdP0aGgjNuDMmqXeHojPCh8no6Wg6JKPKTOBPveOlxUCkkuwoNCxMlUUIDAf+LKJdl3SUhQ4iooAoFAIBBMFWb1CpVJJYFic1koPAeFTLLjLvE4gtrIJKtyUFQXT5QHxa2g8LRXl4LCz73dQVDMDh7Cm3rDBNg405wFAoFAIJhKzGqTbJiq2ryCkk4m1GC74WJFa1GuVn38+OG1eHbjgLZtpEmWeVAGjRKPSpJ1dPHwEg+ZadsNk+xmRlAySZugEDwPaM84CEqbm6Dw7dduG3Y+RyAQCASC6casJihhqmrzCko2lVDllErV17b9v+c347M/fRInXnuftq3pBaEyDHXxFMoVRYBUm3GpgmrVrxt1r3tQaupMjfxsGQoISiaZQIJlkJhD/NozKe3xI5b3IJtK4Ji9Fkbei3etWAQAOOuwXSKfIxAIBALBdGJWl3iCxboaW0HhQ/GGCmWlKqzfPqr+PpgvKfOpOfAvzCoJFBTerrywI1BQfD8o77ii7pUHpRhG3Zttxvo1Rv/O/ScAcOhuPfjjF0/QJgCbuOEDB+PZjQM4cFl35HMEAoFAIJhOzG4FRZlNm+jiYVH3iYSnFAtulOXzZp5c38+21Us8ZhcQ+U9aM0mt3DJcqLg7cViJZ6igKygWQUnrL1EqmUCKKSbtOZtj1iMnQFBiOmiXeXWH+wkEAoFAMJ2Y1QSFYtv5rJwolCqB14T8HK4sFOq8AYDH1/Wpn0OTrL4tKScUmNaeTWnkZ6RYrqugBB6UstoWCBQRzhtMzwmgz9ThLcYCgUAgEMwVzGqCkmOD8xpBlXgsFSQkN/znNU6CEhAD8rAMF8vwfd+aiUNKzEix4oy6b+FdPDSLp3Y+iYSntf8umxcmvxL2Whx24pglHoFAIBAI5gJmNUHJJpvv4lFtxkpBsUs8XEFZs65PdfgUSnpQGxEI3w9ICJV42mtqBu1bU1C0oLbg58F8WREnHqrGyzy7L2yzruUtLGxNFBSBQCAQzEXMboKSbr6Lh8/iAaCMsrzEwxWUNwYL6BspafunEk9LOgmygQwXymofNBOHFBTNg8JKPFS22TocthJz/4tOUOxY+Tfv1KV+lrA1gUAgEMxFzGqCMpYcFFJQXHH3XEEBgjRXwC7xeJ6nERzKQKF9tjEDrlNBMcy96aSniBMQthoDwO4LXApKSFCkxCMQCASCuYhZTVAyMXJQLAXFYZIlPwhh+wgRFHuqMDfKhiWelPbvQD6cp5NzBLWZ+yJw0uEq8ey1OFRV+kdK1uMCgUAgEMx2zGqCkkvFV1DSSXcnDhCmvxK2kYJSsluF21hc/hDr4gGAebWBeJsGWFx9ylZQ1L4yOkEZZkrOzg6TLG8jHmni2gUCgUAgmG2Y1QQlTJK1F+lyparF2BfLuoISzuMJFYjhYgRBcfhIeIloyOji6a5N/N04kFfP5wqK2TrcaigqW4eK6udkwp1V8venvwVLu3L4xDv3dD4uEAgEAsFsxqw2MGTTweJdMNqM86UKTvvm/cikEvjFRW9FIuGxLp5gm7DEwxSUWolnQXsGW4aKjKDUKfEUy1abMSkoG/sDgpJMeJrqkU0l4HlBFxAQthgTFrRnAQzWvfYPHbkrPnTkrnWfIxAIBALBbMWcUFDyhoLyn4+ux3ObBvHka/3YUuuUsRUU2yRLP+9UK6tsNxWUtE1QdJNsoJzMqykor9cISs6Ip/c8TxvmR2oO4e9PfwuO2XMBbv3IEY1vgkAgEAgEcxCznKDUkmSZguL7Pm7+/Svq9039NYJSifKg8C6egOgsm9cCwPag5BqUeNpViYc8KAFByTrSYHdfEBpdWw0PyvIFbfjhR47A0XsuiL54gUAgEAjmMGY1QSEvB1dQHlu7HS9sHlK/E0kwFRRnF0/tZzKmbjO7eJwm2YqVg0IlHiI4poICBCRE7StjExiBQCAQCHZkzGqCQm3Go6w9mE8kBkKjarFi5qCEcfVAoLwoBaUnUFCoxJMv2SZZrsAQQWk3TLIEs60Y0NuHTQ+KQCAQCAQ7OmY1QXHN0xnI6504m2sEhUyyaSNJlrYtVqooVwPXKs2/2VrHJNueCQkKmWTpfEyCsrS7xTp3UVAEAoFAIIjGrCYoPIuEMDCqB5dtNEo82YhpxiOM5Ow0T1dQXG3GfHtqVTa7eAjLeuwsEx5hb3pQBAKBQCDY0THLCUqwsA/mQ1IyUPu5s0YWNtbC0kqVQB2xpxkHBIVKPbl0otbmGyTL5kuViC6egKz0j5ZUCaij1sXTmkmqUhIA7OIgKPxvQ0ZAnEAgEAgEOzpmNUHpZCoGhbJRuWWvxR0AwhKPOYuHyM1IsYJqNfSftGVS6MylkKoFpG0fKYbTjB05KJR1EvwtnNXDyzwugsJn72xigW4CgUAgEAhmOUFpq6kkVR8YrZEIKvHstSgooVCJpxCRgwIE6gkpKa3ZJDzPw7y2sBOnXomH9t+STiLFVBNe5nERFAB4y06dAICT91sS57IFAoFAIJjzmNXmh5Z0EgkvICiD+TJaMyllkt2zRlD6RkrIlyqhSbZGInLphNp2uFBRZlkyz85vy+CNwQLeGCwo86xmklXlJb2Dh8DLQS4PCgDcesGReGHTIA7eZd447oJAIBAIBHMPs1pB8TzPIgrkR1nW06rUks0DhdAkW/ub53ma0ZU8KDQXx4yrBwwPimFs7TBahYdYN1FXi97VQ+jMpXHIrj3wPPe8HYFAIBAIdlRMCkF57bXX8MEPfhDz589HS0sL9ttvPzzyyCPqcd/3ceWVV2LJkiVoaWnBypUr8cILL4zpWB25YPEnoymVeDpzafR25gAEZRhTQQF0o+xIjaAQaemplXheZwQl49hW/W4oKGa7s0AgEAgEguYx4QRl+/bteOtb34p0Oo3/+Z//wdNPP41vfOMbmDcvLGNcc801uO6663DjjTfiwQcfRFtbG0444QTk8/HNotTaO6QUlODfzpYU5rcHJGPrUFimyTjKNMOFsirxkIJCBIUUlFTC0zwmbcb8nA6zxONIjxUIBAKBQNAcJtyD8rWvfQ3Lli3DTTfdpP62fPly9bPv+7j22mtx+eWX47TTTgMA3HLLLVi8eDFuv/12nHXWWbGO167KNIFyErYZp1VpZctQQT0/4+jEGeIKSq10QybZDf1BMq1JONpMBcX4/R/PPACf+PfH8flT9411PQKBQCAQCCZBQfnlL3+JQw89FO9973uxaNEiHHTQQfjXf/1X9fjLL7+MjRs3YuXKlepvXV1dOOKII7B69WrnPguFAgYGBrT/CFRaGciXUSxXVSYJJyhvDBXV89PJ0O+hFJQiU1BqykhPrU2YFBRz4F82lcCCdj6RWPeZHLH7fDz8dytxyv5LI+6UQCAQCASCKEw4QXnppZfwrW99C3vttRfuvPNOfPzjH8df//Vf4+abbwYAbNy4EQCwePFibbvFixerx0xcffXV6OrqUv8tW7ZMPaYUlHxZC2xrz6XQmXMoKI4yDR/4pzwotbA2RVAMBcXzPBy4rFv9bpZ4BAKBQCAQjB0TTlCq1SoOPvhgfOUrX8FBBx2ECy+8EBdccAFuvPHGMe/zsssuQ39/v/pv3bp16jHlQSmUlTG1PZtCMuGFCspgQFDSSU/rmOED//pGAnLT3RKoIj21Lp7BGnFxeUo4QTFLPAKBQCAQCMaOCScoS5Yswb776r6LffbZB2vXrgUA9Pb2AgA2bdqkPWfTpk3qMRPZbBadnZ3af4R25iMJO3iCv5keFK6e8G2HC2X017albea16SUbHtJGOHBZaPw1u3gEAoFAIBCMHRNOUN761rfiueee0/72/PPPY9dddwUQGGZ7e3tx1113qccHBgbw4IMP4qijjop9PGozHsyXWQdPuvZvQBqIoKQjjK6c3BBBmd+W1Z5LAwQ59tu5S/1MOSsCgUAgEAjGjwn/2v+pT30KRx99NL7yla/gzDPPxEMPPYTvfOc7+M53vgMg8G5cfPHF+NKXvoS99toLy5cvxxVXXIGlS5fi9NNPj328djYwkDp4OkwFZTAwydZTUPpGg+fQDB0+SwcAdl/QZh2bB7DVRgEJBAKBQCCYAEw4QTnssMPw85//HJdddhmuuuoqLF++HNdeey3OPvts9ZxLL70Uw8PDuPDCC9HX14djjjkGd9xxB3K5XOzjtedcJZ609i/N6cmYCkot82S4ULFKPLl0Em2ZJIZrQwR3X9juPP517z8IP3tsPT501K6xz10gEAgEAoEbk2KcOOWUU3DKKadEPu55Hq666ipcddVV4z5Wh9bFE5R4SEHpNCLmTQWFl3hMggIAPe0ZDG8LclB2X2grKADw5wcsxZ8fIK3EAoFAIBBMJGZ93KmmoFBIW41kmDNwTAWFSjzbhosqP6WLlXb4vJ0ogiIQCAQCgWDiMesJCjfJbh8JfCRdyiSrE5R0hIKyoS9QSRIe0M5IySCbp7OwXTfNCgQCgUAgmDzMeoLCTbKbB4JunUUdAZnoyKbABwUv7dY9LkRQtg4HxKazJY1EItyACA8AmTgsEAgEAsEUYtYTFB7UtqkWyLawIyAiiYSnPCoAsNeiDm1bM1yt21Bc5rVmIBAIBAKBYOox6wkKlXOqPvCnzUMAgMWdYTmGe0r2Wqx34pgTiU3PynXvPwgrejvwo48cMaHnLBAIBAKBoD5mffxpLp3Ewo4s3hgsqHk6izrDUk7Qahx4TPZcpBMUU0ExPSuH7DoPd1z89kk4a4FAIBAIBPUw6xUUAFhmpLxyQ2upEia87mFkmXTk0silw1vQLSUdgUAgEAhmBOYGQelpVT/3tGW0duLNg+Ek41xaL+kkEx4O2Llb/d7VMusFJYFAIBAI5gTmBkGZFxIU6uAh0JTiKByyazjwz/SgCAQCgUAgmB7MDYLSE5Z4uP8EAN5/+DIAwKkRaa+coHS3SIlHIBAIBIKZgDlR09iZKSiLDQXl8pP3xTF7LsSxb1ro3PagXUKC4kMm/gkEAoFAMBMwJwiKVuLp1AlKWzaFk/dfErltT1uomrRm5sTtEAgEAoFg1mNOrMhLunNIeEEWyqKO+BORbz7vcPzm6U044+CdJ+HsBAKBQCAQxMWcICjpZAJLulrwWt+oFtLWLN6x90K8Y293CUggEAgEAsHUY06YZAHgzEOXYY+FbThst57pPhWBQCAQCATjhOf7/qxzhg4MDKCrqwv9/f3o7Oyc7tMRCAQCgUDQBOKs33NGQREIBAKBQDB3IARFIBAIBALBjIMQFIFAIBAIBDMOQlAEAoFAIBDMOAhBEQgEAoFAMOMgBEUgEAgEAsGMgxAUgUAgEAgEMw5CUAQCgUAgEMw4CEERCAQCgUAw4yAERSAQCAQCwYyDEBSBQCAQCAQzDkJQBAKBQCAQzDgIQREIBAKBQDDjIARFIBAIBALBjENquk9gLPB9H0AwtlkgEAgEAsHsAK3btI7Xw6wkKIODgwCAZcuWTfOZCAQCgUAgiIvBwUF0dXXVfY7nN0NjZhiq1Sr23ntvPProo/A8L/b2hx12GB5++OExHXs6th0YGMCyZcuwbt06dHZ2TtlxZ+O2471X4zn2bNxW3lvNQ+5V85B71Tx2tHvl+z4OOeQQPP/880gk6rtMZqWCkkgkkMlkGrKvKCSTyTEvXtO1LQB0dnaOafvZeL3Tda/Ge+zZuC0g7604kHvVPOReNY8d6V5lMpmG5ASYxSbZiy66aIfadjyYjdc7XfdqvMeejduOB7PxeuVeTc2248FsvF65VxO/7aws8exoGBgYQFdXF/r7+8fF0HcEyL2KB7lfzUPuVfOQe9U85F5FY9YqKDsSstksPv/5zyObzU73qcx4yL2KB7lfzUPuVfOQe9U85F5FQxQUgUAgEAgEMw6ioAgEAoFAIJhxEIIiEAgEAoFgxkEIikAgEAgEghkHISgCgUAgEAhmHISgTBHuvfdenHrqqVi6dCk8z8Ptt9+uPb5p0yace+65WLp0KVpbW3HiiSfihRde0J6zceNGfOhDH0Jvby/a2tpw8MEH46c//an2nMceewx/9md/hu7ubsyfPx8XXnghhoaGJvvyJhQTca/+9Kc/4S/+4i+wcOFCdHZ24swzz8SmTZucxysUCjjwwAPheR7WrFkzSVc1OZiqezUX3ldXX301DjvsMHR0dGDRokU4/fTT8dxzz2nPyefzuOiiizB//ny0t7fjjDPOsO7F2rVrcfLJJ6O1tRWLFi3CZz7zGZTLZecx77//fqRSKRx44IGTdVmTgqm8VzfccAP22WcftLS04E1vehNuueWWSb++icZE3a+//uu/xiGHHIJsNtvwPfPiiy+io6MD3d3dE3w1MwdCUKYIw8PDOOCAA3DDDTdYj/m+j9NPPx0vvfQSfvGLX+Dxxx/HrrvuipUrV2J4eFg975xzzsFzzz2HX/7yl3jyySfxnve8B2eeeSYef/xxAMCGDRuwcuVK7LnnnnjwwQdxxx134KmnnsK55547VZc5IRjvvRoeHsbxxx8Pz/Nw99134/7770exWMSpp56KarVq7fPSSy/F0qVLJ/26JgNTca/myvvqnnvuwUUXXYQHHngAq1atQqlUwvHHH6/9P/apT30Kv/rVr3DbbbfhnnvuwYYNG/Ce97xHPV6pVHDyySejWCzi97//PW6++WZ8//vfx5VXXmkdr6+vD+eccw6OO+64Kbm+icRU3atvfetbuOyyy/CFL3wBTz31FL74xS/ioosuwq9+9aspvd7xYiLuF+G8887D+973vrrHK5VKeP/734+3ve1tE34tMwq+YMoBwP/5z3+ufn/uued8AP4f//hH9bdKpeIvXLjQ/9d//Vf1t7a2Nv+WW27R9tXT06Oe8+1vf9tftGiRX6lU1ONPPPGED8B/4YUXJulqJhdjuVd33nmnn0gk/P7+fvWcvr4+3/M8f9WqVdr+//u//9tfsWKF/9RTT/kA/Mcff3xSr2cyMVn3ai6+r3zf9zdv3uwD8O+55x7f94PrTqfT/m233aae88wzz/gA/NWrV/u+H7xfEomEv3HjRvWcb33rW35nZ6dfKBS0/b/vfe/zL7/8cv/zn/+8f8ABB0z+BU0iJuteHXXUUf6nP/1p7ViXXHKJ/9a3vnWyL2lSMZb7xdHoPXPppZf6H/zgB/2bbrrJ7+rqmujTnzEQBWUGoFAoAAByuZz6WyKRQDabxe9+9zv1t6OPPho//vGPsW3bNlSrVfzHf/wH8vk8jj32WLUfc8ZBS0sLAGj7mc1o5l4VCgV4nqcFH+VyOSQSCe0+bNq0CRdccAF+8IMfoLW1dYquYOowUfdqrr6v+vv7AQA9PT0AgEcffRSlUgkrV65Uz1mxYgV22WUXrF69GgCwevVq7Lfffli8eLF6zgknnICBgQE89dRT6m833XQTXnrpJXz+85+fikuZdEzWvSoUCtr7EwjeWw899BBKpdKkXtNkYiz3q1ncfffduO2225yq6VyDEJQZAHqjXnbZZdi+fTuKxSK+9rWvYf369Xj99dfV837yk5+gVCph/vz5yGaz+OhHP4qf//zn2HPPPQEA73rXu7Bx40b8wz/8A4rFIrZv347Pfe5zAKDtZzajmXt15JFHoq2tDZ/97GcxMjKC4eFhfPrTn0alUlHP8X0f5557Lj72sY/h0EMPnc5LmjRM1L2ai++rarWKiy++GG9961vxlre8BUDg8cpkMlZNf/Hixdi4caN6Dl9w6XF6DABeeOEFfO5zn8MPf/hDpFKzch6rhsm8VyeccAK++93v4tFHH4Xv+3jkkUfw3e9+F6VSCVu2bJnkK5scjPV+NYOtW7fi3HPPxfe///0dIhZfCMoMQDqdxs9+9jM8//zz6OnpQWtrK37729/ipJNO0r61XnHFFejr68NvfvMbPPLII7jkkktw5pln4sknnwQAvPnNb8bNN9+Mb3zjG2htbUVvby+WL1+OxYsXNzU5cjagmXu1cOFC3HbbbfjVr36F9vZ2dHV1oa+vDwcffLB6zvXXX4/BwUFcdtll03k5k4qJuldz8X110UUX4Y9//CP+4z/+Y0L3W6lU8IEPfABf/OIXsffee0/ovqcLk3WvgOAz7aSTTsKRRx6JdDqN0047DR/+8IcBQN5bDlxwwQX4wAc+gLe//e0Tvu8ZiemuMe2IgOEV4Ojr6/M3b97s+77vH3744f5f/dVf+b7v+y+++KLlJ/B93z/uuOP8j370o9Z+Nm7c6A8ODvpDQ0N+IpHwf/KTn0zsRUwRxnKvON544w1/+/btvu/7/uLFi/1rrrnG933fP+200/xEIuEnk0n1HwA/mUz655xzzqRcy2Rjsu4Vx1x4X1100UX+zjvv7L/00kva3++66y4fgLoHhF122cX/x3/8R9/3ff+KK66wvAEvvfSSD8B/7LHH/O3bt6v3Ef3neZ7621133TWZlzbhmMx7xVEsFv1169b55XLZ/5d/+Re/o6ND8zzNFoznfnFEeVC6urq091YikVDvre9973sTeSkzAkJQpgH1FhLC888/7ycSCf/OO+/0fT80JT799NPa844//nj/ggsuiNzP9773Pb+1tdX6H2O2YCz3yoW77rrL9zzPf/bZZ33f9/1XX33Vf/LJJ9V/d955pw/A/8///E9/3bp1E3kJU4bJulcuzMb3VbVa9S+66CJ/6f/f3v2FNNn2cQD/zr0azrJamaVlFqUnptlBoWBRVBgZlRAW0TI6aJOgP0pRdNBBNYomqR3UiRoShEjoQbiD3HZQiNRYuFo6DZcny4qwGuZ02+85iGc8e9SXF9rmre/3Azu5r2v3ff9+3IMvN9fFMjLE7XZPGf97IWNbW1v4WF9f37QLP0dGRsJzHj58KKmpqTI+Pi7BYDDiuXI6nWIwGCQ3N1ecTqf4fL7YFxoF8ejVTLZv3y7Hjh2LYjWxF41+/dNMAcXlckU8Wzdu3JBFixaJ0+mUb9++RbUmJWBAiZOfP3+Kw+EQh8MhAKS2tlYcDod8/PhRRERaW1vFarXKhw8fpL29XdauXSvl5eXh709MTMiGDRukpKREenp6ZHBwUO7evSsqlUqePXsWntfQ0CB2u136+/vl/v37kpycLHV1dXGv90/8aa9ERBobG6W7u1sGBwelpaVFtFqtXLx4ccZrDg0NzcldPPHq1Xx4rgwGgyxevFhsNpt4vd7wZ2xsLDxHr9dLVlaWWCwWef36tRQVFUlRUVF4PBAISF5enuzdu1fevHkjZrNZ0tLS5MqVKzNedy7u4olXr/r7+6WlpUXcbrf09PRIRUWFaLVaGRoaime5fywa/RIRGRgYEIfDIWfOnJGcnJzwb/vfO8T+Nt938TCgxInVahUAUz4nT54UEZG6ujpZvXq1JCYmSlZWlly7dm3KQ+l2u6W8vFxWrFghGo1G8vPzp2w7PnHihGi1WklKSpp2fC6IRq8uX74s6enpkpiYKBs3bhSTySShUGjGa87VgBKvXs2H52q6PgGQpqam8Jxfv35JVVWVLF26VDQajRw+fFi8Xm/EeTwej+zbt0+Sk5Nl+fLlUl1dLZOTkzNedy4GlHj1yuVyyebNmyU5OVlSU1Pl4MGD//XNnVJFq187duyY9jwzBbb5HlBUIiJ/vpKFiIiIKHrm5jJpIiIimtcYUIiIiEhxGFCIiIhIcRhQiIiISHEYUIiIiEhxGFCIiIhIcRhQiIiISHEYUIiIiEhxGFCIiIhIcRhQiCgmKisroVKpoFKpkJiYiPT0dOzZsweNjY0IhUL/83mam5uxZMmS2N0oESkSAwoRxUxpaSm8Xi88Hg86Ozuxc+dOnDt3DmVlZQgEArN9e0SkYAwoRBQzCxYswMqVK5GZmYktW7bg6tWr6OjoQGdnJ5qbmwEAtbW12LRpE1JSUrBmzRpUVVXB5/MBAGw2G06dOoXv37+H38Zcv34dAOD3+1FTU4PMzEykpKRg27ZtsNlss1MoEUUdAwoRxdWuXbtQUFCAp0+fAgASEhJQX1+Pd+/e4dGjR7BYLLh06RIAoLi4GPfu3UNqaiq8Xi+8Xi9qamoAAGfPnkV3dzeePHmC3t5eHDlyBKWlpRgYGJi12ogoevhvxkQUE5WVlRgdHUV7e/uUsaNHj6K3txcul2vKWFtbG/R6Pb5+/Qrg9xqU8+fPY3R0NDxneHgY69evx/DwMDIyMsLHd+/eja1bt+LWrVtRr4eI4us/s30DRPT/R0SgUqkAAM+fP4fRaERfXx9+/PiBQCCA8fFxjI2NQaPRTPt9p9OJYDCInJyciON+vx/Lli2L+f0TUewxoBBR3L1//x7r1q2Dx+NBWVkZDAYDbt68Ca1WixcvXuD06dOYmJiYMaD4fD6o1WrY7Xao1eqIsYULF8ajBCKKMQYUIoori8UCp9OJCxcuwG63IxQKwWQyISHh95K41tbWiPlJSUkIBoMRxwoLCxEMBvH582eUlJTE7d6JKH4YUIgoZvx+Pz59+oRgMIiRkRGYzWYYjUaUlZVBp9Ph7du3mJycRENDAw4cOICXL1/iwYMHEefIzs6Gz+dDV1cXCgoKoNFokJOTg+PHj0On08FkMqGwsBBfvnxBV1cX8vPzsX///lmqmIiihbt4iChmzGYzVq1ahezsbJSWlsJqtaK+vh4dHR1Qq9UoKChAbW0tbt++jby8PDx+/BhGozHiHMXFxdDr9aioqEBaWhru3LkDAGhqaoJOp0N1dTVyc3Nx6NAhvHr1CllZWbNRKhFFGXfxEBERkeLwDQoREREpDgMKERERKQ4DChERESkOAwoREREpDgMKERERKQ4DChERESkOAwoREREpDgMKERERKQ4DChERESkOAwoREREpDgMKERERKc5f64zSvctvCBAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXj1Z6_vgSIZ"
      },
      "source": [
        "---\n",
        "### 2.1 Load and prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yd1P3PV4nZ2X"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset, look_back):\n",
        "    \"\"\"Transform a time series data into a prediction dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        look_back: Size of window for prediction\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    dataset = np.array(dataset)\n",
        "    data_length = len(dataset)\n",
        "    for i in range(look_back, data_length):\n",
        "        input = dataset[i-look_back: i]\n",
        "        output = dataset[i]\n",
        "        X.append(input)\n",
        "        y.append(output)\n",
        "\n",
        "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSFy2eNF5e-",
        "outputId": "a004fe95-1000-494b-bfe9-9c1a397fb488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH2FMeIhh169",
        "outputId": "94ac16f8-1948-48bd-a154-10f71f3d38f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train data -> torch.Size([237, 60, 1]) \n",
            "Shape of y_train data -> torch.Size([237, 1]) \n",
            "Shape of X_val data -> torch.Size([40, 60, 1]) \n",
            "Shape of y_val data -> torch.Size([40, 1]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "look_back = 60\n",
        "\n",
        "data_length = len(df)\n",
        "\n",
        "train_data_size = int(data_length * 0.75)\n",
        "validaion_data_size = int(data_length * 0.25)\n",
        "\n",
        "\n",
        "train_data = df[: train_data_size]\n",
        "validation_data = df[train_data_size: ]\n",
        "\n",
        "\n",
        "X_train, y_train = prepare_dataset(train_data, look_back)\n",
        "X_val, y_val = prepare_dataset(validation_data, look_back)\n",
        "\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "y_val = y_val.to(device)\n",
        "\n",
        "\n",
        "print(  f\"Shape of X_train data -> {X_train.shape} \\n\"\n",
        "        f\"Shape of y_train data -> {y_train.shape} \\n\"\n",
        "        f\"Shape of X_val data -> {X_val.shape} \\n\"\n",
        "        f\"Shape of y_val data -> {y_val.shape} \\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78V0kKvjp8f"
      },
      "source": [
        "---\n",
        "---\n",
        "## 3 Trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oEIg5tjsjnEi"
      },
      "outputs": [],
      "source": [
        "def trainer(model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs):\n",
        "    early_stopping_patience = 150\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    valid_loss_min=np.inf\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        # Forward and loss\n",
        "        predictions  = model(X_train)\n",
        "        train_loss = criterion(predictions.view(-1), y_train.view(-1))\n",
        "        train_losses.append(train_loss.item())\n",
        "        # Backward and optimization\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output_val = model(X_val)\n",
        "            valid_loss = criterion(output_val, y_val)\n",
        "            val_losses.append(valid_loss.item())\n",
        "\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                best_model = best_model = copy.deepcopy(model)\n",
        "                print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).')\n",
        "                valid_loss_min = valid_loss\n",
        "                early_stopping_counter = 0    # Reset counter if validation loss decreases\n",
        "            else:\n",
        "                print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
        "                early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter > early_stopping_patience:\n",
        "                print('Early stopped at epoch :', epoch)\n",
        "                break\n",
        "\n",
        "            print(f'\\t Train_Loss: {train_loss:.4f} Val_Loss: {valid_loss:.4f}  BEST VAL Loss: {valid_loss_min:.4f}\\n')\n",
        "\n",
        "    return best_model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIfT0qVkOTzG"
      },
      "source": [
        "---\n",
        "---\n",
        "## 4 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwmm5DhEBl8d"
      },
      "source": [
        "---\n",
        "### 4.1 Define single RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5YeW2H2cKsH7"
      },
      "outputs": [],
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
        "        super(RNNCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.Wxh = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "        self.bias = bias\n",
        "        self.bias_Wxh = nn.Parameter(torch.Tensor(hidden_size)) if bias else self.register_parameter('bias_Wxh', None)\n",
        "        self.bias_Whh = nn.Parameter(torch.Tensor(hidden_size)) if bias else self.register_parameter('bias_Whh', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        if hx is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "        if self.bias:\n",
        "          hy = torch.tanh(self.Wxh(input) + self.Whh(hx) + self.bias_Wxh + self.bias_Whh)\n",
        "        else:\n",
        "          hy = torch.tanh(self.Wxh(input) + self.Whh(hx))\n",
        "        return hy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ay_sL8ZBq1R"
      },
      "source": [
        "---\n",
        "### 4.2 RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KAcO44hi5HUf"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn_cell_list = nn.ModuleList()\n",
        "        self.rnn_cell_list.append(RNNCell(input_size, hidden_size, bias))\n",
        "        self.bias = bias\n",
        "        for l in range(1, num_layers):\n",
        "            self.rnn_cell_list.append(RNNCell(hidden_size, hidden_size, bias))\n",
        "        self.fc = nn.Linear(hidden_size, output_size, bias=bias)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        outs = []\n",
        "        h0 = [None] * len(self.rnn_cell_list) if hx is None else list(hx)\n",
        "        X = list(input.permute(1, 0, 2))\n",
        "        for j, cell in enumerate(self.rnn_cell_list):\n",
        "            hx_minus_one = h0[j]\n",
        "            for i in range(input.shape[1]):\n",
        "                hx = cell(X[i], hx_minus_one)\n",
        "                hx_minus_one = hx\n",
        "                X[i] = hx\n",
        "            outs.append(hx)\n",
        "        out = outs[-1].squeeze()\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msF4j7_fB6S5"
      },
      "source": [
        "---\n",
        "### 4.3 Train RNN model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwHvK1bI5GBz",
        "outputId": "2d6489a3-4545-4720-a525-2cfc541b3ea6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleRNN(\n",
              "  (rnn_cell_list): ModuleList(\n",
              "    (0): RNNCell(\n",
              "      (Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate model\n",
        "SimpleRNN_model = SimpleRNN(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "SimpleRNN_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "431lZ_uOlpsQ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(SimpleRNN_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWMAFQBKQ5mq",
        "outputId": "7622e653-a3f0-495a-9775-527e435ea623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\t Train_Loss: 195.5847 Val_Loss: 578.4521  BEST VAL Loss: 578.4521\n",
            "\n",
            "Epoch 334: Validation loss decreased (578.452087 --> 575.035095).\n",
            "\t Train_Loss: 194.3929 Val_Loss: 575.0351  BEST VAL Loss: 575.0351\n",
            "\n",
            "Epoch 335: Validation loss decreased (575.035095 --> 571.658081).\n",
            "\t Train_Loss: 193.2238 Val_Loss: 571.6581  BEST VAL Loss: 571.6581\n",
            "\n",
            "Epoch 336: Validation loss decreased (571.658081 --> 568.323181).\n",
            "\t Train_Loss: 192.0768 Val_Loss: 568.3232  BEST VAL Loss: 568.3232\n",
            "\n",
            "Epoch 337: Validation loss decreased (568.323181 --> 565.029297).\n",
            "\t Train_Loss: 190.9518 Val_Loss: 565.0293  BEST VAL Loss: 565.0293\n",
            "\n",
            "Epoch 338: Validation loss decreased (565.029297 --> 561.775330).\n",
            "\t Train_Loss: 189.8482 Val_Loss: 561.7753  BEST VAL Loss: 561.7753\n",
            "\n",
            "Epoch 339: Validation loss decreased (561.775330 --> 558.560730).\n",
            "\t Train_Loss: 188.7658 Val_Loss: 558.5607  BEST VAL Loss: 558.5607\n",
            "\n",
            "Epoch 340: Validation loss decreased (558.560730 --> 555.385376).\n",
            "\t Train_Loss: 187.7044 Val_Loss: 555.3854  BEST VAL Loss: 555.3854\n",
            "\n",
            "Epoch 341: Validation loss decreased (555.385376 --> 552.249023).\n",
            "\t Train_Loss: 186.6630 Val_Loss: 552.2490  BEST VAL Loss: 552.2490\n",
            "\n",
            "Epoch 342: Validation loss decreased (552.249023 --> 549.151245).\n",
            "\t Train_Loss: 185.6419 Val_Loss: 549.1512  BEST VAL Loss: 549.1512\n",
            "\n",
            "Epoch 343: Validation loss decreased (549.151245 --> 546.090637).\n",
            "\t Train_Loss: 184.6407 Val_Loss: 546.0906  BEST VAL Loss: 546.0906\n",
            "\n",
            "Epoch 344: Validation loss decreased (546.090637 --> 543.067505).\n",
            "\t Train_Loss: 183.6586 Val_Loss: 543.0675  BEST VAL Loss: 543.0675\n",
            "\n",
            "Epoch 345: Validation loss decreased (543.067505 --> 540.081665).\n",
            "\t Train_Loss: 182.6957 Val_Loss: 540.0817  BEST VAL Loss: 540.0817\n",
            "\n",
            "Epoch 346: Validation loss decreased (540.081665 --> 537.132324).\n",
            "\t Train_Loss: 181.7516 Val_Loss: 537.1323  BEST VAL Loss: 537.1323\n",
            "\n",
            "Epoch 347: Validation loss decreased (537.132324 --> 534.218750).\n",
            "\t Train_Loss: 180.8258 Val_Loss: 534.2188  BEST VAL Loss: 534.2188\n",
            "\n",
            "Epoch 348: Validation loss decreased (534.218750 --> 531.341003).\n",
            "\t Train_Loss: 179.9181 Val_Loss: 531.3410  BEST VAL Loss: 531.3410\n",
            "\n",
            "Epoch 349: Validation loss decreased (531.341003 --> 528.498535).\n",
            "\t Train_Loss: 179.0282 Val_Loss: 528.4985  BEST VAL Loss: 528.4985\n",
            "\n",
            "Epoch 350: Validation loss decreased (528.498535 --> 525.691284).\n",
            "\t Train_Loss: 178.1558 Val_Loss: 525.6913  BEST VAL Loss: 525.6913\n",
            "\n",
            "Epoch 351: Validation loss decreased (525.691284 --> 522.917908).\n",
            "\t Train_Loss: 177.3006 Val_Loss: 522.9179  BEST VAL Loss: 522.9179\n",
            "\n",
            "Epoch 352: Validation loss decreased (522.917908 --> 520.177979).\n",
            "\t Train_Loss: 176.4621 Val_Loss: 520.1780  BEST VAL Loss: 520.1780\n",
            "\n",
            "Epoch 353: Validation loss decreased (520.177979 --> 517.472839).\n",
            "\t Train_Loss: 175.6403 Val_Loss: 517.4728  BEST VAL Loss: 517.4728\n",
            "\n",
            "Epoch 354: Validation loss decreased (517.472839 --> 514.800354).\n",
            "\t Train_Loss: 174.8347 Val_Loss: 514.8004  BEST VAL Loss: 514.8004\n",
            "\n",
            "Epoch 355: Validation loss decreased (514.800354 --> 512.160828).\n",
            "\t Train_Loss: 174.0452 Val_Loss: 512.1608  BEST VAL Loss: 512.1608\n",
            "\n",
            "Epoch 356: Validation loss decreased (512.160828 --> 509.553528).\n",
            "\t Train_Loss: 173.2713 Val_Loss: 509.5535  BEST VAL Loss: 509.5535\n",
            "\n",
            "Epoch 357: Validation loss decreased (509.553528 --> 506.977844).\n",
            "\t Train_Loss: 172.5128 Val_Loss: 506.9778  BEST VAL Loss: 506.9778\n",
            "\n",
            "Epoch 358: Validation loss decreased (506.977844 --> 504.434174).\n",
            "\t Train_Loss: 171.7695 Val_Loss: 504.4342  BEST VAL Loss: 504.4342\n",
            "\n",
            "Epoch 359: Validation loss decreased (504.434174 --> 501.921844).\n",
            "\t Train_Loss: 171.0410 Val_Loss: 501.9218  BEST VAL Loss: 501.9218\n",
            "\n",
            "Epoch 360: Validation loss decreased (501.921844 --> 499.440582).\n",
            "\t Train_Loss: 170.3273 Val_Loss: 499.4406  BEST VAL Loss: 499.4406\n",
            "\n",
            "Epoch 361: Validation loss decreased (499.440582 --> 496.989502).\n",
            "\t Train_Loss: 169.6278 Val_Loss: 496.9895  BEST VAL Loss: 496.9895\n",
            "\n",
            "Epoch 362: Validation loss decreased (496.989502 --> 494.568451).\n",
            "\t Train_Loss: 168.9424 Val_Loss: 494.5685  BEST VAL Loss: 494.5685\n",
            "\n",
            "Epoch 363: Validation loss decreased (494.568451 --> 492.177643).\n",
            "\t Train_Loss: 168.2709 Val_Loss: 492.1776  BEST VAL Loss: 492.1776\n",
            "\n",
            "Epoch 364: Validation loss decreased (492.177643 --> 489.815826).\n",
            "\t Train_Loss: 167.6129 Val_Loss: 489.8158  BEST VAL Loss: 489.8158\n",
            "\n",
            "Epoch 365: Validation loss decreased (489.815826 --> 487.483002).\n",
            "\t Train_Loss: 166.9682 Val_Loss: 487.4830  BEST VAL Loss: 487.4830\n",
            "\n",
            "Epoch 366: Validation loss decreased (487.483002 --> 485.179596).\n",
            "\t Train_Loss: 166.3367 Val_Loss: 485.1796  BEST VAL Loss: 485.1796\n",
            "\n",
            "Epoch 367: Validation loss decreased (485.179596 --> 482.904297).\n",
            "\t Train_Loss: 165.7180 Val_Loss: 482.9043  BEST VAL Loss: 482.9043\n",
            "\n",
            "Epoch 368: Validation loss decreased (482.904297 --> 480.656494).\n",
            "\t Train_Loss: 165.1120 Val_Loss: 480.6565  BEST VAL Loss: 480.6565\n",
            "\n",
            "Epoch 369: Validation loss decreased (480.656494 --> 478.436920).\n",
            "\t Train_Loss: 164.5182 Val_Loss: 478.4369  BEST VAL Loss: 478.4369\n",
            "\n",
            "Epoch 370: Validation loss decreased (478.436920 --> 476.244843).\n",
            "\t Train_Loss: 163.9368 Val_Loss: 476.2448  BEST VAL Loss: 476.2448\n",
            "\n",
            "Epoch 371: Validation loss decreased (476.244843 --> 474.079102).\n",
            "\t Train_Loss: 163.3673 Val_Loss: 474.0791  BEST VAL Loss: 474.0791\n",
            "\n",
            "Epoch 372: Validation loss decreased (474.079102 --> 471.940430).\n",
            "\t Train_Loss: 162.8093 Val_Loss: 471.9404  BEST VAL Loss: 471.9404\n",
            "\n",
            "Epoch 373: Validation loss decreased (471.940430 --> 469.828613).\n",
            "\t Train_Loss: 162.2629 Val_Loss: 469.8286  BEST VAL Loss: 469.8286\n",
            "\n",
            "Epoch 374: Validation loss decreased (469.828613 --> 467.742340).\n",
            "\t Train_Loss: 161.7278 Val_Loss: 467.7423  BEST VAL Loss: 467.7423\n",
            "\n",
            "Epoch 375: Validation loss decreased (467.742340 --> 465.681305).\n",
            "\t Train_Loss: 161.2038 Val_Loss: 465.6813  BEST VAL Loss: 465.6813\n",
            "\n",
            "Epoch 376: Validation loss decreased (465.681305 --> 463.646729).\n",
            "\t Train_Loss: 160.6907 Val_Loss: 463.6467  BEST VAL Loss: 463.6467\n",
            "\n",
            "Epoch 377: Validation loss decreased (463.646729 --> 461.636871).\n",
            "\t Train_Loss: 160.1882 Val_Loss: 461.6369  BEST VAL Loss: 461.6369\n",
            "\n",
            "Epoch 378: Validation loss decreased (461.636871 --> 459.652435).\n",
            "\t Train_Loss: 159.6962 Val_Loss: 459.6524  BEST VAL Loss: 459.6524\n",
            "\n",
            "Epoch 379: Validation loss decreased (459.652435 --> 457.692047).\n",
            "\t Train_Loss: 159.2145 Val_Loss: 457.6920  BEST VAL Loss: 457.6920\n",
            "\n",
            "Epoch 380: Validation loss decreased (457.692047 --> 455.755463).\n",
            "\t Train_Loss: 158.7428 Val_Loss: 455.7555  BEST VAL Loss: 455.7555\n",
            "\n",
            "Epoch 381: Validation loss decreased (455.755463 --> 453.843842).\n",
            "\t Train_Loss: 158.2811 Val_Loss: 453.8438  BEST VAL Loss: 453.8438\n",
            "\n",
            "Epoch 382: Validation loss decreased (453.843842 --> 451.955078).\n",
            "\t Train_Loss: 157.8289 Val_Loss: 451.9551  BEST VAL Loss: 451.9551\n",
            "\n",
            "Epoch 383: Validation loss decreased (451.955078 --> 450.090546).\n",
            "\t Train_Loss: 157.3864 Val_Loss: 450.0905  BEST VAL Loss: 450.0905\n",
            "\n",
            "Epoch 384: Validation loss decreased (450.090546 --> 448.248749).\n",
            "\t Train_Loss: 156.9531 Val_Loss: 448.2487  BEST VAL Loss: 448.2487\n",
            "\n",
            "Epoch 385: Validation loss decreased (448.248749 --> 446.429413).\n",
            "\t Train_Loss: 156.5291 Val_Loss: 446.4294  BEST VAL Loss: 446.4294\n",
            "\n",
            "Epoch 386: Validation loss decreased (446.429413 --> 444.632629).\n",
            "\t Train_Loss: 156.1139 Val_Loss: 444.6326  BEST VAL Loss: 444.6326\n",
            "\n",
            "Epoch 387: Validation loss decreased (444.632629 --> 442.858612).\n",
            "\t Train_Loss: 155.7076 Val_Loss: 442.8586  BEST VAL Loss: 442.8586\n",
            "\n",
            "Epoch 388: Validation loss decreased (442.858612 --> 441.106537).\n",
            "\t Train_Loss: 155.3100 Val_Loss: 441.1065  BEST VAL Loss: 441.1065\n",
            "\n",
            "Epoch 389: Validation loss decreased (441.106537 --> 439.375793).\n",
            "\t Train_Loss: 154.9207 Val_Loss: 439.3758  BEST VAL Loss: 439.3758\n",
            "\n",
            "Epoch 390: Validation loss decreased (439.375793 --> 437.666565).\n",
            "\t Train_Loss: 154.5399 Val_Loss: 437.6666  BEST VAL Loss: 437.6666\n",
            "\n",
            "Epoch 391: Validation loss decreased (437.666565 --> 435.978607).\n",
            "\t Train_Loss: 154.1671 Val_Loss: 435.9786  BEST VAL Loss: 435.9786\n",
            "\n",
            "Epoch 392: Validation loss decreased (435.978607 --> 434.312256).\n",
            "\t Train_Loss: 153.8023 Val_Loss: 434.3123  BEST VAL Loss: 434.3123\n",
            "\n",
            "Epoch 393: Validation loss decreased (434.312256 --> 432.665985).\n",
            "\t Train_Loss: 153.4454 Val_Loss: 432.6660  BEST VAL Loss: 432.6660\n",
            "\n",
            "Epoch 394: Validation loss decreased (432.665985 --> 431.040253).\n",
            "\t Train_Loss: 153.0962 Val_Loss: 431.0403  BEST VAL Loss: 431.0403\n",
            "\n",
            "Epoch 395: Validation loss decreased (431.040253 --> 429.434479).\n",
            "\t Train_Loss: 152.7543 Val_Loss: 429.4345  BEST VAL Loss: 429.4345\n",
            "\n",
            "Epoch 396: Validation loss decreased (429.434479 --> 427.849579).\n",
            "\t Train_Loss: 152.4200 Val_Loss: 427.8496  BEST VAL Loss: 427.8496\n",
            "\n",
            "Epoch 397: Validation loss decreased (427.849579 --> 426.283783).\n",
            "\t Train_Loss: 152.0930 Val_Loss: 426.2838  BEST VAL Loss: 426.2838\n",
            "\n",
            "Epoch 398: Validation loss decreased (426.283783 --> 424.737213).\n",
            "\t Train_Loss: 151.7729 Val_Loss: 424.7372  BEST VAL Loss: 424.7372\n",
            "\n",
            "Epoch 399: Validation loss decreased (424.737213 --> 423.209961).\n",
            "\t Train_Loss: 151.4598 Val_Loss: 423.2100  BEST VAL Loss: 423.2100\n",
            "\n",
            "Epoch 400: Validation loss decreased (423.209961 --> 421.702148).\n",
            "\t Train_Loss: 151.1535 Val_Loss: 421.7021  BEST VAL Loss: 421.7021\n",
            "\n",
            "Epoch 401: Validation loss decreased (421.702148 --> 420.213135).\n",
            "\t Train_Loss: 150.8539 Val_Loss: 420.2131  BEST VAL Loss: 420.2131\n",
            "\n",
            "Epoch 402: Validation loss decreased (420.213135 --> 418.742767).\n",
            "\t Train_Loss: 150.5609 Val_Loss: 418.7428  BEST VAL Loss: 418.7428\n",
            "\n",
            "Epoch 403: Validation loss decreased (418.742767 --> 417.290741).\n",
            "\t Train_Loss: 150.2744 Val_Loss: 417.2907  BEST VAL Loss: 417.2907\n",
            "\n",
            "Epoch 404: Validation loss decreased (417.290741 --> 415.855865).\n",
            "\t Train_Loss: 149.9940 Val_Loss: 415.8559  BEST VAL Loss: 415.8559\n",
            "\n",
            "Epoch 405: Validation loss decreased (415.855865 --> 414.440247).\n",
            "\t Train_Loss: 149.7198 Val_Loss: 414.4402  BEST VAL Loss: 414.4402\n",
            "\n",
            "Epoch 406: Validation loss decreased (414.440247 --> 413.041656).\n",
            "\t Train_Loss: 149.4517 Val_Loss: 413.0417  BEST VAL Loss: 413.0417\n",
            "\n",
            "Epoch 407: Validation loss decreased (413.041656 --> 411.660309).\n",
            "\t Train_Loss: 149.1894 Val_Loss: 411.6603  BEST VAL Loss: 411.6603\n",
            "\n",
            "Epoch 408: Validation loss decreased (411.660309 --> 410.296539).\n",
            "\t Train_Loss: 148.9331 Val_Loss: 410.2965  BEST VAL Loss: 410.2965\n",
            "\n",
            "Epoch 409: Validation loss decreased (410.296539 --> 408.949951).\n",
            "\t Train_Loss: 148.6823 Val_Loss: 408.9500  BEST VAL Loss: 408.9500\n",
            "\n",
            "Epoch 410: Validation loss decreased (408.949951 --> 407.620178).\n",
            "\t Train_Loss: 148.4372 Val_Loss: 407.6202  BEST VAL Loss: 407.6202\n",
            "\n",
            "Epoch 411: Validation loss decreased (407.620178 --> 406.306549).\n",
            "\t Train_Loss: 148.1974 Val_Loss: 406.3065  BEST VAL Loss: 406.3065\n",
            "\n",
            "Epoch 412: Validation loss decreased (406.306549 --> 405.010101).\n",
            "\t Train_Loss: 147.9630 Val_Loss: 405.0101  BEST VAL Loss: 405.0101\n",
            "\n",
            "Epoch 413: Validation loss decreased (405.010101 --> 403.729340).\n",
            "\t Train_Loss: 147.7339 Val_Loss: 403.7293  BEST VAL Loss: 403.7293\n",
            "\n",
            "Epoch 414: Validation loss decreased (403.729340 --> 402.464813).\n",
            "\t Train_Loss: 147.5098 Val_Loss: 402.4648  BEST VAL Loss: 402.4648\n",
            "\n",
            "Epoch 415: Validation loss decreased (402.464813 --> 401.215851).\n",
            "\t Train_Loss: 147.2908 Val_Loss: 401.2159  BEST VAL Loss: 401.2159\n",
            "\n",
            "Epoch 416: Validation loss decreased (401.215851 --> 399.982910).\n",
            "\t Train_Loss: 147.0768 Val_Loss: 399.9829  BEST VAL Loss: 399.9829\n",
            "\n",
            "Epoch 417: Validation loss decreased (399.982910 --> 398.765594).\n",
            "\t Train_Loss: 146.8675 Val_Loss: 398.7656  BEST VAL Loss: 398.7656\n",
            "\n",
            "Epoch 418: Validation loss decreased (398.765594 --> 397.563110).\n",
            "\t Train_Loss: 146.6629 Val_Loss: 397.5631  BEST VAL Loss: 397.5631\n",
            "\n",
            "Epoch 419: Validation loss decreased (397.563110 --> 396.375885).\n",
            "\t Train_Loss: 146.4630 Val_Loss: 396.3759  BEST VAL Loss: 396.3759\n",
            "\n",
            "Epoch 420: Validation loss decreased (396.375885 --> 395.203827).\n",
            "\t Train_Loss: 146.2676 Val_Loss: 395.2038  BEST VAL Loss: 395.2038\n",
            "\n",
            "Epoch 421: Validation loss decreased (395.203827 --> 394.046356).\n",
            "\t Train_Loss: 146.0766 Val_Loss: 394.0464  BEST VAL Loss: 394.0464\n",
            "\n",
            "Epoch 422: Validation loss decreased (394.046356 --> 392.902832).\n",
            "\t Train_Loss: 145.8899 Val_Loss: 392.9028  BEST VAL Loss: 392.9028\n",
            "\n",
            "Epoch 423: Validation loss decreased (392.902832 --> 391.774292).\n",
            "\t Train_Loss: 145.7074 Val_Loss: 391.7743  BEST VAL Loss: 391.7743\n",
            "\n",
            "Epoch 424: Validation loss decreased (391.774292 --> 390.659668).\n",
            "\t Train_Loss: 145.5292 Val_Loss: 390.6597  BEST VAL Loss: 390.6597\n",
            "\n",
            "Epoch 425: Validation loss decreased (390.659668 --> 389.559174).\n",
            "\t Train_Loss: 145.3550 Val_Loss: 389.5592  BEST VAL Loss: 389.5592\n",
            "\n",
            "Epoch 426: Validation loss decreased (389.559174 --> 388.472809).\n",
            "\t Train_Loss: 145.1849 Val_Loss: 388.4728  BEST VAL Loss: 388.4728\n",
            "\n",
            "Epoch 427: Validation loss decreased (388.472809 --> 387.399902).\n",
            "\t Train_Loss: 145.0185 Val_Loss: 387.3999  BEST VAL Loss: 387.3999\n",
            "\n",
            "Epoch 428: Validation loss decreased (387.399902 --> 386.340546).\n",
            "\t Train_Loss: 144.8560 Val_Loss: 386.3405  BEST VAL Loss: 386.3405\n",
            "\n",
            "Epoch 429: Validation loss decreased (386.340546 --> 385.294403).\n",
            "\t Train_Loss: 144.6973 Val_Loss: 385.2944  BEST VAL Loss: 385.2944\n",
            "\n",
            "Epoch 430: Validation loss decreased (385.294403 --> 384.262360).\n",
            "\t Train_Loss: 144.5422 Val_Loss: 384.2624  BEST VAL Loss: 384.2624\n",
            "\n",
            "Epoch 431: Validation loss decreased (384.262360 --> 383.242188).\n",
            "\t Train_Loss: 144.3906 Val_Loss: 383.2422  BEST VAL Loss: 383.2422\n",
            "\n",
            "Epoch 432: Validation loss decreased (383.242188 --> 382.235352).\n",
            "\t Train_Loss: 144.2427 Val_Loss: 382.2354  BEST VAL Loss: 382.2354\n",
            "\n",
            "Epoch 433: Validation loss decreased (382.235352 --> 381.241364).\n",
            "\t Train_Loss: 144.0981 Val_Loss: 381.2414  BEST VAL Loss: 381.2414\n",
            "\n",
            "Epoch 434: Validation loss decreased (381.241364 --> 380.259796).\n",
            "\t Train_Loss: 143.9568 Val_Loss: 380.2598  BEST VAL Loss: 380.2598\n",
            "\n",
            "Epoch 435: Validation loss decreased (380.259796 --> 379.290253).\n",
            "\t Train_Loss: 143.8189 Val_Loss: 379.2903  BEST VAL Loss: 379.2903\n",
            "\n",
            "Epoch 436: Validation loss decreased (379.290253 --> 378.333496).\n",
            "\t Train_Loss: 143.6842 Val_Loss: 378.3335  BEST VAL Loss: 378.3335\n",
            "\n",
            "Epoch 437: Validation loss decreased (378.333496 --> 377.389069).\n",
            "\t Train_Loss: 143.5527 Val_Loss: 377.3891  BEST VAL Loss: 377.3891\n",
            "\n",
            "Epoch 438: Validation loss decreased (377.389069 --> 376.455627).\n",
            "\t Train_Loss: 143.4242 Val_Loss: 376.4556  BEST VAL Loss: 376.4556\n",
            "\n",
            "Epoch 439: Validation loss decreased (376.455627 --> 375.535248).\n",
            "\t Train_Loss: 143.2987 Val_Loss: 375.5352  BEST VAL Loss: 375.5352\n",
            "\n",
            "Epoch 440: Validation loss decreased (375.535248 --> 374.625824).\n",
            "\t Train_Loss: 143.1761 Val_Loss: 374.6258  BEST VAL Loss: 374.6258\n",
            "\n",
            "Epoch 441: Validation loss decreased (374.625824 --> 373.728119).\n",
            "\t Train_Loss: 143.0565 Val_Loss: 373.7281  BEST VAL Loss: 373.7281\n",
            "\n",
            "Epoch 442: Validation loss decreased (373.728119 --> 372.841614).\n",
            "\t Train_Loss: 142.9396 Val_Loss: 372.8416  BEST VAL Loss: 372.8416\n",
            "\n",
            "Epoch 443: Validation loss decreased (372.841614 --> 371.966797).\n",
            "\t Train_Loss: 142.8255 Val_Loss: 371.9668  BEST VAL Loss: 371.9668\n",
            "\n",
            "Epoch 444: Validation loss decreased (371.966797 --> 371.102753).\n",
            "\t Train_Loss: 142.7141 Val_Loss: 371.1028  BEST VAL Loss: 371.1028\n",
            "\n",
            "Epoch 445: Validation loss decreased (371.102753 --> 370.250092).\n",
            "\t Train_Loss: 142.6053 Val_Loss: 370.2501  BEST VAL Loss: 370.2501\n",
            "\n",
            "Epoch 446: Validation loss decreased (370.250092 --> 369.407776).\n",
            "\t Train_Loss: 142.4992 Val_Loss: 369.4078  BEST VAL Loss: 369.4078\n",
            "\n",
            "Epoch 447: Validation loss decreased (369.407776 --> 368.576263).\n",
            "\t Train_Loss: 142.3955 Val_Loss: 368.5763  BEST VAL Loss: 368.5763\n",
            "\n",
            "Epoch 448: Validation loss decreased (368.576263 --> 367.755798).\n",
            "\t Train_Loss: 142.2943 Val_Loss: 367.7558  BEST VAL Loss: 367.7558\n",
            "\n",
            "Epoch 449: Validation loss decreased (367.755798 --> 366.945465).\n",
            "\t Train_Loss: 142.1956 Val_Loss: 366.9455  BEST VAL Loss: 366.9455\n",
            "\n",
            "Epoch 450: Validation loss decreased (366.945465 --> 366.145355).\n",
            "\t Train_Loss: 142.0991 Val_Loss: 366.1454  BEST VAL Loss: 366.1454\n",
            "\n",
            "Epoch 451: Validation loss decreased (366.145355 --> 365.355927).\n",
            "\t Train_Loss: 142.0051 Val_Loss: 365.3559  BEST VAL Loss: 365.3559\n",
            "\n",
            "Epoch 452: Validation loss decreased (365.355927 --> 364.575592).\n",
            "\t Train_Loss: 141.9132 Val_Loss: 364.5756  BEST VAL Loss: 364.5756\n",
            "\n",
            "Epoch 453: Validation loss decreased (364.575592 --> 363.806030).\n",
            "\t Train_Loss: 141.8236 Val_Loss: 363.8060  BEST VAL Loss: 363.8060\n",
            "\n",
            "Epoch 454: Validation loss decreased (363.806030 --> 363.046539).\n",
            "\t Train_Loss: 141.7360 Val_Loss: 363.0465  BEST VAL Loss: 363.0465\n",
            "\n",
            "Epoch 455: Validation loss decreased (363.046539 --> 362.296356).\n",
            "\t Train_Loss: 141.6506 Val_Loss: 362.2964  BEST VAL Loss: 362.2964\n",
            "\n",
            "Epoch 456: Validation loss decreased (362.296356 --> 361.555634).\n",
            "\t Train_Loss: 141.5673 Val_Loss: 361.5556  BEST VAL Loss: 361.5556\n",
            "\n",
            "Epoch 457: Validation loss decreased (361.555634 --> 360.824890).\n",
            "\t Train_Loss: 141.4861 Val_Loss: 360.8249  BEST VAL Loss: 360.8249\n",
            "\n",
            "Epoch 458: Validation loss decreased (360.824890 --> 360.102875).\n",
            "\t Train_Loss: 141.4067 Val_Loss: 360.1029  BEST VAL Loss: 360.1029\n",
            "\n",
            "Epoch 459: Validation loss decreased (360.102875 --> 359.390625).\n",
            "\t Train_Loss: 141.3293 Val_Loss: 359.3906  BEST VAL Loss: 359.3906\n",
            "\n",
            "Epoch 460: Validation loss decreased (359.390625 --> 358.687225).\n",
            "\t Train_Loss: 141.2538 Val_Loss: 358.6872  BEST VAL Loss: 358.6872\n",
            "\n",
            "Epoch 461: Validation loss decreased (358.687225 --> 357.993378).\n",
            "\t Train_Loss: 141.1801 Val_Loss: 357.9934  BEST VAL Loss: 357.9934\n",
            "\n",
            "Epoch 462: Validation loss decreased (357.993378 --> 357.308197).\n",
            "\t Train_Loss: 141.1082 Val_Loss: 357.3082  BEST VAL Loss: 357.3082\n",
            "\n",
            "Epoch 463: Validation loss decreased (357.308197 --> 356.631287).\n",
            "\t Train_Loss: 141.0381 Val_Loss: 356.6313  BEST VAL Loss: 356.6313\n",
            "\n",
            "Epoch 464: Validation loss decreased (356.631287 --> 355.963593).\n",
            "\t Train_Loss: 140.9697 Val_Loss: 355.9636  BEST VAL Loss: 355.9636\n",
            "\n",
            "Epoch 465: Validation loss decreased (355.963593 --> 355.304291).\n",
            "\t Train_Loss: 140.9030 Val_Loss: 355.3043  BEST VAL Loss: 355.3043\n",
            "\n",
            "Epoch 466: Validation loss decreased (355.304291 --> 354.653961).\n",
            "\t Train_Loss: 140.8378 Val_Loss: 354.6540  BEST VAL Loss: 354.6540\n",
            "\n",
            "Epoch 467: Validation loss decreased (354.653961 --> 354.011047).\n",
            "\t Train_Loss: 140.7744 Val_Loss: 354.0110  BEST VAL Loss: 354.0110\n",
            "\n",
            "Epoch 468: Validation loss decreased (354.011047 --> 353.377533).\n",
            "\t Train_Loss: 140.7124 Val_Loss: 353.3775  BEST VAL Loss: 353.3775\n",
            "\n",
            "Epoch 469: Validation loss decreased (353.377533 --> 352.751862).\n",
            "\t Train_Loss: 140.6520 Val_Loss: 352.7519  BEST VAL Loss: 352.7519\n",
            "\n",
            "Epoch 470: Validation loss decreased (352.751862 --> 352.133759).\n",
            "\t Train_Loss: 140.5931 Val_Loss: 352.1338  BEST VAL Loss: 352.1338\n",
            "\n",
            "Epoch 471: Validation loss decreased (352.133759 --> 351.524506).\n",
            "\t Train_Loss: 140.5357 Val_Loss: 351.5245  BEST VAL Loss: 351.5245\n",
            "\n",
            "Epoch 472: Validation loss decreased (351.524506 --> 350.922333).\n",
            "\t Train_Loss: 140.4797 Val_Loss: 350.9223  BEST VAL Loss: 350.9223\n",
            "\n",
            "Epoch 473: Validation loss decreased (350.922333 --> 350.328705).\n",
            "\t Train_Loss: 140.4250 Val_Loss: 350.3287  BEST VAL Loss: 350.3287\n",
            "\n",
            "Epoch 474: Validation loss decreased (350.328705 --> 349.741821).\n",
            "\t Train_Loss: 140.3718 Val_Loss: 349.7418  BEST VAL Loss: 349.7418\n",
            "\n",
            "Epoch 475: Validation loss decreased (349.741821 --> 349.163788).\n",
            "\t Train_Loss: 140.3198 Val_Loss: 349.1638  BEST VAL Loss: 349.1638\n",
            "\n",
            "Epoch 476: Validation loss decreased (349.163788 --> 348.592163).\n",
            "\t Train_Loss: 140.2692 Val_Loss: 348.5922  BEST VAL Loss: 348.5922\n",
            "\n",
            "Epoch 477: Validation loss decreased (348.592163 --> 348.028809).\n",
            "\t Train_Loss: 140.2198 Val_Loss: 348.0288  BEST VAL Loss: 348.0288\n",
            "\n",
            "Epoch 478: Validation loss decreased (348.028809 --> 347.472382).\n",
            "\t Train_Loss: 140.1717 Val_Loss: 347.4724  BEST VAL Loss: 347.4724\n",
            "\n",
            "Epoch 479: Validation loss decreased (347.472382 --> 346.923401).\n",
            "\t Train_Loss: 140.1248 Val_Loss: 346.9234  BEST VAL Loss: 346.9234\n",
            "\n",
            "Epoch 480: Validation loss decreased (346.923401 --> 346.381500).\n",
            "\t Train_Loss: 140.0790 Val_Loss: 346.3815  BEST VAL Loss: 346.3815\n",
            "\n",
            "Epoch 481: Validation loss decreased (346.381500 --> 345.846222).\n",
            "\t Train_Loss: 140.0344 Val_Loss: 345.8462  BEST VAL Loss: 345.8462\n",
            "\n",
            "Epoch 482: Validation loss decreased (345.846222 --> 345.318237).\n",
            "\t Train_Loss: 139.9909 Val_Loss: 345.3182  BEST VAL Loss: 345.3182\n",
            "\n",
            "Epoch 483: Validation loss decreased (345.318237 --> 344.797089).\n",
            "\t Train_Loss: 139.9486 Val_Loss: 344.7971  BEST VAL Loss: 344.7971\n",
            "\n",
            "Epoch 484: Validation loss decreased (344.797089 --> 344.283020).\n",
            "\t Train_Loss: 139.9073 Val_Loss: 344.2830  BEST VAL Loss: 344.2830\n",
            "\n",
            "Epoch 485: Validation loss decreased (344.283020 --> 343.775848).\n",
            "\t Train_Loss: 139.8671 Val_Loss: 343.7758  BEST VAL Loss: 343.7758\n",
            "\n",
            "Epoch 486: Validation loss decreased (343.775848 --> 343.274872).\n",
            "\t Train_Loss: 139.8279 Val_Loss: 343.2749  BEST VAL Loss: 343.2749\n",
            "\n",
            "Epoch 487: Validation loss decreased (343.274872 --> 342.780243).\n",
            "\t Train_Loss: 139.7896 Val_Loss: 342.7802  BEST VAL Loss: 342.7802\n",
            "\n",
            "Epoch 488: Validation loss decreased (342.780243 --> 342.292938).\n",
            "\t Train_Loss: 139.7524 Val_Loss: 342.2929  BEST VAL Loss: 342.2929\n",
            "\n",
            "Epoch 489: Validation loss decreased (342.292938 --> 341.811371).\n",
            "\t Train_Loss: 139.7160 Val_Loss: 341.8114  BEST VAL Loss: 341.8114\n",
            "\n",
            "Epoch 490: Validation loss decreased (341.811371 --> 341.336334).\n",
            "\t Train_Loss: 139.6807 Val_Loss: 341.3363  BEST VAL Loss: 341.3363\n",
            "\n",
            "Epoch 491: Validation loss decreased (341.336334 --> 340.867004).\n",
            "\t Train_Loss: 139.6462 Val_Loss: 340.8670  BEST VAL Loss: 340.8670\n",
            "\n",
            "Epoch 492: Validation loss decreased (340.867004 --> 340.404694).\n",
            "\t Train_Loss: 139.6127 Val_Loss: 340.4047  BEST VAL Loss: 340.4047\n",
            "\n",
            "Epoch 493: Validation loss decreased (340.404694 --> 339.947998).\n",
            "\t Train_Loss: 139.5800 Val_Loss: 339.9480  BEST VAL Loss: 339.9480\n",
            "\n",
            "Epoch 494: Validation loss decreased (339.947998 --> 339.497162).\n",
            "\t Train_Loss: 139.5481 Val_Loss: 339.4972  BEST VAL Loss: 339.4972\n",
            "\n",
            "Epoch 495: Validation loss decreased (339.497162 --> 339.052643).\n",
            "\t Train_Loss: 139.5170 Val_Loss: 339.0526  BEST VAL Loss: 339.0526\n",
            "\n",
            "Epoch 496: Validation loss decreased (339.052643 --> 338.614319).\n",
            "\t Train_Loss: 139.4868 Val_Loss: 338.6143  BEST VAL Loss: 338.6143\n",
            "\n",
            "Epoch 497: Validation loss decreased (338.614319 --> 338.181305).\n",
            "\t Train_Loss: 139.4574 Val_Loss: 338.1813  BEST VAL Loss: 338.1813\n",
            "\n",
            "Epoch 498: Validation loss decreased (338.181305 --> 337.753876).\n",
            "\t Train_Loss: 139.4286 Val_Loss: 337.7539  BEST VAL Loss: 337.7539\n",
            "\n",
            "Epoch 499: Validation loss decreased (337.753876 --> 337.332611).\n",
            "\t Train_Loss: 139.4007 Val_Loss: 337.3326  BEST VAL Loss: 337.3326\n",
            "\n",
            "Epoch 500: Validation loss decreased (337.332611 --> 336.916718).\n",
            "\t Train_Loss: 139.3735 Val_Loss: 336.9167  BEST VAL Loss: 336.9167\n",
            "\n",
            "Epoch 501: Validation loss decreased (336.916718 --> 336.506470).\n",
            "\t Train_Loss: 139.3470 Val_Loss: 336.5065  BEST VAL Loss: 336.5065\n",
            "\n",
            "Epoch 502: Validation loss decreased (336.506470 --> 336.101349).\n",
            "\t Train_Loss: 139.3212 Val_Loss: 336.1013  BEST VAL Loss: 336.1013\n",
            "\n",
            "Epoch 503: Validation loss decreased (336.101349 --> 335.701752).\n",
            "\t Train_Loss: 139.2959 Val_Loss: 335.7018  BEST VAL Loss: 335.7018\n",
            "\n",
            "Epoch 504: Validation loss decreased (335.701752 --> 335.307281).\n",
            "\t Train_Loss: 139.2715 Val_Loss: 335.3073  BEST VAL Loss: 335.3073\n",
            "\n",
            "Epoch 505: Validation loss decreased (335.307281 --> 334.918213).\n",
            "\t Train_Loss: 139.2477 Val_Loss: 334.9182  BEST VAL Loss: 334.9182\n",
            "\n",
            "Epoch 506: Validation loss decreased (334.918213 --> 334.534149).\n",
            "\t Train_Loss: 139.2244 Val_Loss: 334.5341  BEST VAL Loss: 334.5341\n",
            "\n",
            "Epoch 507: Validation loss decreased (334.534149 --> 334.155914).\n",
            "\t Train_Loss: 139.2018 Val_Loss: 334.1559  BEST VAL Loss: 334.1559\n",
            "\n",
            "Epoch 508: Validation loss decreased (334.155914 --> 333.781952).\n",
            "\t Train_Loss: 139.1797 Val_Loss: 333.7820  BEST VAL Loss: 333.7820\n",
            "\n",
            "Epoch 509: Validation loss decreased (333.781952 --> 333.413605).\n",
            "\t Train_Loss: 139.1583 Val_Loss: 333.4136  BEST VAL Loss: 333.4136\n",
            "\n",
            "Epoch 510: Validation loss decreased (333.413605 --> 333.049683).\n",
            "\t Train_Loss: 139.1375 Val_Loss: 333.0497  BEST VAL Loss: 333.0497\n",
            "\n",
            "Epoch 511: Validation loss decreased (333.049683 --> 332.690979).\n",
            "\t Train_Loss: 139.1171 Val_Loss: 332.6910  BEST VAL Loss: 332.6910\n",
            "\n",
            "Epoch 512: Validation loss decreased (332.690979 --> 332.337128).\n",
            "\t Train_Loss: 139.0974 Val_Loss: 332.3371  BEST VAL Loss: 332.3371\n",
            "\n",
            "Epoch 513: Validation loss decreased (332.337128 --> 331.987518).\n",
            "\t Train_Loss: 139.0781 Val_Loss: 331.9875  BEST VAL Loss: 331.9875\n",
            "\n",
            "Epoch 514: Validation loss decreased (331.987518 --> 331.642822).\n",
            "\t Train_Loss: 139.0593 Val_Loss: 331.6428  BEST VAL Loss: 331.6428\n",
            "\n",
            "Epoch 515: Validation loss decreased (331.642822 --> 331.303070).\n",
            "\t Train_Loss: 139.0411 Val_Loss: 331.3031  BEST VAL Loss: 331.3031\n",
            "\n",
            "Epoch 516: Validation loss decreased (331.303070 --> 330.967987).\n",
            "\t Train_Loss: 139.0234 Val_Loss: 330.9680  BEST VAL Loss: 330.9680\n",
            "\n",
            "Epoch 517: Validation loss decreased (330.967987 --> 330.636810).\n",
            "\t Train_Loss: 139.0061 Val_Loss: 330.6368  BEST VAL Loss: 330.6368\n",
            "\n",
            "Epoch 518: Validation loss decreased (330.636810 --> 330.310760).\n",
            "\t Train_Loss: 138.9893 Val_Loss: 330.3108  BEST VAL Loss: 330.3108\n",
            "\n",
            "Epoch 519: Validation loss decreased (330.310760 --> 329.988617).\n",
            "\t Train_Loss: 138.9729 Val_Loss: 329.9886  BEST VAL Loss: 329.9886\n",
            "\n",
            "Epoch 520: Validation loss decreased (329.988617 --> 329.671051).\n",
            "\t Train_Loss: 138.9570 Val_Loss: 329.6711  BEST VAL Loss: 329.6711\n",
            "\n",
            "Epoch 521: Validation loss decreased (329.671051 --> 329.357391).\n",
            "\t Train_Loss: 138.9415 Val_Loss: 329.3574  BEST VAL Loss: 329.3574\n",
            "\n",
            "Epoch 522: Validation loss decreased (329.357391 --> 329.048553).\n",
            "\t Train_Loss: 138.9265 Val_Loss: 329.0486  BEST VAL Loss: 329.0486\n",
            "\n",
            "Epoch 523: Validation loss decreased (329.048553 --> 328.743530).\n",
            "\t Train_Loss: 138.9117 Val_Loss: 328.7435  BEST VAL Loss: 328.7435\n",
            "\n",
            "Epoch 524: Validation loss decreased (328.743530 --> 328.442719).\n",
            "\t Train_Loss: 138.8974 Val_Loss: 328.4427  BEST VAL Loss: 328.4427\n",
            "\n",
            "Epoch 525: Validation loss decreased (328.442719 --> 328.145996).\n",
            "\t Train_Loss: 138.8835 Val_Loss: 328.1460  BEST VAL Loss: 328.1460\n",
            "\n",
            "Epoch 526: Validation loss decreased (328.145996 --> 327.853607).\n",
            "\t Train_Loss: 138.8701 Val_Loss: 327.8536  BEST VAL Loss: 327.8536\n",
            "\n",
            "Epoch 527: Validation loss decreased (327.853607 --> 327.564758).\n",
            "\t Train_Loss: 138.8569 Val_Loss: 327.5648  BEST VAL Loss: 327.5648\n",
            "\n",
            "Epoch 528: Validation loss decreased (327.564758 --> 327.279938).\n",
            "\t Train_Loss: 138.8441 Val_Loss: 327.2799  BEST VAL Loss: 327.2799\n",
            "\n",
            "Epoch 529: Validation loss decreased (327.279938 --> 326.999237).\n",
            "\t Train_Loss: 138.8317 Val_Loss: 326.9992  BEST VAL Loss: 326.9992\n",
            "\n",
            "Epoch 530: Validation loss decreased (326.999237 --> 326.722076).\n",
            "\t Train_Loss: 138.8196 Val_Loss: 326.7221  BEST VAL Loss: 326.7221\n",
            "\n",
            "Epoch 531: Validation loss decreased (326.722076 --> 326.448975).\n",
            "\t Train_Loss: 138.8078 Val_Loss: 326.4490  BEST VAL Loss: 326.4490\n",
            "\n",
            "Epoch 532: Validation loss decreased (326.448975 --> 326.179749).\n",
            "\t Train_Loss: 138.7964 Val_Loss: 326.1797  BEST VAL Loss: 326.1797\n",
            "\n",
            "Epoch 533: Validation loss decreased (326.179749 --> 325.913513).\n",
            "\t Train_Loss: 138.7852 Val_Loss: 325.9135  BEST VAL Loss: 325.9135\n",
            "\n",
            "Epoch 534: Validation loss decreased (325.913513 --> 325.651154).\n",
            "\t Train_Loss: 138.7744 Val_Loss: 325.6512  BEST VAL Loss: 325.6512\n",
            "\n",
            "Epoch 535: Validation loss decreased (325.651154 --> 325.392578).\n",
            "\t Train_Loss: 138.7638 Val_Loss: 325.3926  BEST VAL Loss: 325.3926\n",
            "\n",
            "Epoch 536: Validation loss decreased (325.392578 --> 325.137543).\n",
            "\t Train_Loss: 138.7536 Val_Loss: 325.1375  BEST VAL Loss: 325.1375\n",
            "\n",
            "Epoch 537: Validation loss decreased (325.137543 --> 324.886536).\n",
            "\t Train_Loss: 138.7437 Val_Loss: 324.8865  BEST VAL Loss: 324.8865\n",
            "\n",
            "Epoch 538: Validation loss decreased (324.886536 --> 324.638397).\n",
            "\t Train_Loss: 138.7340 Val_Loss: 324.6384  BEST VAL Loss: 324.6384\n",
            "\n",
            "Epoch 539: Validation loss decreased (324.638397 --> 324.393799).\n",
            "\t Train_Loss: 138.7246 Val_Loss: 324.3938  BEST VAL Loss: 324.3938\n",
            "\n",
            "Epoch 540: Validation loss decreased (324.393799 --> 324.152710).\n",
            "\t Train_Loss: 138.7154 Val_Loss: 324.1527  BEST VAL Loss: 324.1527\n",
            "\n",
            "Epoch 541: Validation loss decreased (324.152710 --> 323.914734).\n",
            "\t Train_Loss: 138.7065 Val_Loss: 323.9147  BEST VAL Loss: 323.9147\n",
            "\n",
            "Epoch 542: Validation loss decreased (323.914734 --> 323.680115).\n",
            "\t Train_Loss: 138.6978 Val_Loss: 323.6801  BEST VAL Loss: 323.6801\n",
            "\n",
            "Epoch 543: Validation loss decreased (323.680115 --> 323.448883).\n",
            "\t Train_Loss: 138.6895 Val_Loss: 323.4489  BEST VAL Loss: 323.4489\n",
            "\n",
            "Epoch 544: Validation loss decreased (323.448883 --> 323.221008).\n",
            "\t Train_Loss: 138.6813 Val_Loss: 323.2210  BEST VAL Loss: 323.2210\n",
            "\n",
            "Epoch 545: Validation loss decreased (323.221008 --> 322.995514).\n",
            "\t Train_Loss: 138.6733 Val_Loss: 322.9955  BEST VAL Loss: 322.9955\n",
            "\n",
            "Epoch 546: Validation loss decreased (322.995514 --> 322.774170).\n",
            "\t Train_Loss: 138.6656 Val_Loss: 322.7742  BEST VAL Loss: 322.7742\n",
            "\n",
            "Epoch 547: Validation loss decreased (322.774170 --> 322.555237).\n",
            "\t Train_Loss: 138.6581 Val_Loss: 322.5552  BEST VAL Loss: 322.5552\n",
            "\n",
            "Epoch 548: Validation loss decreased (322.555237 --> 322.339569).\n",
            "\t Train_Loss: 138.6508 Val_Loss: 322.3396  BEST VAL Loss: 322.3396\n",
            "\n",
            "Epoch 549: Validation loss decreased (322.339569 --> 322.127380).\n",
            "\t Train_Loss: 138.6436 Val_Loss: 322.1274  BEST VAL Loss: 322.1274\n",
            "\n",
            "Epoch 550: Validation loss decreased (322.127380 --> 321.917755).\n",
            "\t Train_Loss: 138.6368 Val_Loss: 321.9178  BEST VAL Loss: 321.9178\n",
            "\n",
            "Epoch 551: Validation loss decreased (321.917755 --> 321.710754).\n",
            "\t Train_Loss: 138.6300 Val_Loss: 321.7108  BEST VAL Loss: 321.7108\n",
            "\n",
            "Epoch 552: Validation loss decreased (321.710754 --> 321.506989).\n",
            "\t Train_Loss: 138.6236 Val_Loss: 321.5070  BEST VAL Loss: 321.5070\n",
            "\n",
            "Epoch 553: Validation loss decreased (321.506989 --> 321.305542).\n",
            "\t Train_Loss: 138.6173 Val_Loss: 321.3055  BEST VAL Loss: 321.3055\n",
            "\n",
            "Epoch 554: Validation loss decreased (321.305542 --> 321.107422).\n",
            "\t Train_Loss: 138.6111 Val_Loss: 321.1074  BEST VAL Loss: 321.1074\n",
            "\n",
            "Epoch 555: Validation loss decreased (321.107422 --> 320.912445).\n",
            "\t Train_Loss: 138.6051 Val_Loss: 320.9124  BEST VAL Loss: 320.9124\n",
            "\n",
            "Epoch 556: Validation loss decreased (320.912445 --> 320.719208).\n",
            "\t Train_Loss: 138.5993 Val_Loss: 320.7192  BEST VAL Loss: 320.7192\n",
            "\n",
            "Epoch 557: Validation loss decreased (320.719208 --> 320.529785).\n",
            "\t Train_Loss: 138.5937 Val_Loss: 320.5298  BEST VAL Loss: 320.5298\n",
            "\n",
            "Epoch 558: Validation loss decreased (320.529785 --> 320.342346).\n",
            "\t Train_Loss: 138.5882 Val_Loss: 320.3423  BEST VAL Loss: 320.3423\n",
            "\n",
            "Epoch 559: Validation loss decreased (320.342346 --> 320.157623).\n",
            "\t Train_Loss: 138.5829 Val_Loss: 320.1576  BEST VAL Loss: 320.1576\n",
            "\n",
            "Epoch 560: Validation loss decreased (320.157623 --> 319.975494).\n",
            "\t Train_Loss: 138.5777 Val_Loss: 319.9755  BEST VAL Loss: 319.9755\n",
            "\n",
            "Epoch 561: Validation loss decreased (319.975494 --> 319.796265).\n",
            "\t Train_Loss: 138.5726 Val_Loss: 319.7963  BEST VAL Loss: 319.7963\n",
            "\n",
            "Epoch 562: Validation loss decreased (319.796265 --> 319.619446).\n",
            "\t Train_Loss: 138.5678 Val_Loss: 319.6194  BEST VAL Loss: 319.6194\n",
            "\n",
            "Epoch 563: Validation loss decreased (319.619446 --> 319.444916).\n",
            "\t Train_Loss: 138.5630 Val_Loss: 319.4449  BEST VAL Loss: 319.4449\n",
            "\n",
            "Epoch 564: Validation loss decreased (319.444916 --> 319.272919).\n",
            "\t Train_Loss: 138.5584 Val_Loss: 319.2729  BEST VAL Loss: 319.2729\n",
            "\n",
            "Epoch 565: Validation loss decreased (319.272919 --> 319.103455).\n",
            "\t Train_Loss: 138.5539 Val_Loss: 319.1035  BEST VAL Loss: 319.1035\n",
            "\n",
            "Epoch 566: Validation loss decreased (319.103455 --> 318.936401).\n",
            "\t Train_Loss: 138.5497 Val_Loss: 318.9364  BEST VAL Loss: 318.9364\n",
            "\n",
            "Epoch 567: Validation loss decreased (318.936401 --> 318.771790).\n",
            "\t Train_Loss: 138.5454 Val_Loss: 318.7718  BEST VAL Loss: 318.7718\n",
            "\n",
            "Epoch 568: Validation loss decreased (318.771790 --> 318.609619).\n",
            "\t Train_Loss: 138.5413 Val_Loss: 318.6096  BEST VAL Loss: 318.6096\n",
            "\n",
            "Epoch 569: Validation loss decreased (318.609619 --> 318.449615).\n",
            "\t Train_Loss: 138.5373 Val_Loss: 318.4496  BEST VAL Loss: 318.4496\n",
            "\n",
            "Epoch 570: Validation loss decreased (318.449615 --> 318.291809).\n",
            "\t Train_Loss: 138.5335 Val_Loss: 318.2918  BEST VAL Loss: 318.2918\n",
            "\n",
            "Epoch 571: Validation loss decreased (318.291809 --> 318.136200).\n",
            "\t Train_Loss: 138.5297 Val_Loss: 318.1362  BEST VAL Loss: 318.1362\n",
            "\n",
            "Epoch 572: Validation loss decreased (318.136200 --> 317.983246).\n",
            "\t Train_Loss: 138.5260 Val_Loss: 317.9832  BEST VAL Loss: 317.9832\n",
            "\n",
            "Epoch 573: Validation loss decreased (317.983246 --> 317.832397).\n",
            "\t Train_Loss: 138.5225 Val_Loss: 317.8324  BEST VAL Loss: 317.8324\n",
            "\n",
            "Epoch 574: Validation loss decreased (317.832397 --> 317.683136).\n",
            "\t Train_Loss: 138.5191 Val_Loss: 317.6831  BEST VAL Loss: 317.6831\n",
            "\n",
            "Epoch 575: Validation loss decreased (317.683136 --> 317.536682).\n",
            "\t Train_Loss: 138.5158 Val_Loss: 317.5367  BEST VAL Loss: 317.5367\n",
            "\n",
            "Epoch 576: Validation loss decreased (317.536682 --> 317.391815).\n",
            "\t Train_Loss: 138.5126 Val_Loss: 317.3918  BEST VAL Loss: 317.3918\n",
            "\n",
            "Epoch 577: Validation loss decreased (317.391815 --> 317.249359).\n",
            "\t Train_Loss: 138.5094 Val_Loss: 317.2494  BEST VAL Loss: 317.2494\n",
            "\n",
            "Epoch 578: Validation loss decreased (317.249359 --> 317.108612).\n",
            "\t Train_Loss: 138.5064 Val_Loss: 317.1086  BEST VAL Loss: 317.1086\n",
            "\n",
            "Epoch 579: Validation loss decreased (317.108612 --> 316.970306).\n",
            "\t Train_Loss: 138.5034 Val_Loss: 316.9703  BEST VAL Loss: 316.9703\n",
            "\n",
            "Epoch 580: Validation loss decreased (316.970306 --> 316.834076).\n",
            "\t Train_Loss: 138.5006 Val_Loss: 316.8341  BEST VAL Loss: 316.8341\n",
            "\n",
            "Epoch 581: Validation loss decreased (316.834076 --> 316.699677).\n",
            "\t Train_Loss: 138.4977 Val_Loss: 316.6997  BEST VAL Loss: 316.6997\n",
            "\n",
            "Epoch 582: Validation loss decreased (316.699677 --> 316.567108).\n",
            "\t Train_Loss: 138.4951 Val_Loss: 316.5671  BEST VAL Loss: 316.5671\n",
            "\n",
            "Epoch 583: Validation loss decreased (316.567108 --> 316.436523).\n",
            "\t Train_Loss: 138.4924 Val_Loss: 316.4365  BEST VAL Loss: 316.4365\n",
            "\n",
            "Epoch 584: Validation loss decreased (316.436523 --> 316.307831).\n",
            "\t Train_Loss: 138.4899 Val_Loss: 316.3078  BEST VAL Loss: 316.3078\n",
            "\n",
            "Epoch 585: Validation loss decreased (316.307831 --> 316.181183).\n",
            "\t Train_Loss: 138.4873 Val_Loss: 316.1812  BEST VAL Loss: 316.1812\n",
            "\n",
            "Epoch 586: Validation loss decreased (316.181183 --> 316.056122).\n",
            "\t Train_Loss: 138.4850 Val_Loss: 316.0561  BEST VAL Loss: 316.0561\n",
            "\n",
            "Epoch 587: Validation loss decreased (316.056122 --> 315.932953).\n",
            "\t Train_Loss: 138.4827 Val_Loss: 315.9330  BEST VAL Loss: 315.9330\n",
            "\n",
            "Epoch 588: Validation loss decreased (315.932953 --> 315.811859).\n",
            "\t Train_Loss: 138.4804 Val_Loss: 315.8119  BEST VAL Loss: 315.8119\n",
            "\n",
            "Epoch 589: Validation loss decreased (315.811859 --> 315.691986).\n",
            "\t Train_Loss: 138.4782 Val_Loss: 315.6920  BEST VAL Loss: 315.6920\n",
            "\n",
            "Epoch 590: Validation loss decreased (315.691986 --> 315.574280).\n",
            "\t Train_Loss: 138.4761 Val_Loss: 315.5743  BEST VAL Loss: 315.5743\n",
            "\n",
            "Epoch 591: Validation loss decreased (315.574280 --> 315.458588).\n",
            "\t Train_Loss: 138.4740 Val_Loss: 315.4586  BEST VAL Loss: 315.4586\n",
            "\n",
            "Epoch 592: Validation loss decreased (315.458588 --> 315.344391).\n",
            "\t Train_Loss: 138.4720 Val_Loss: 315.3444  BEST VAL Loss: 315.3444\n",
            "\n",
            "Epoch 593: Validation loss decreased (315.344391 --> 315.231354).\n",
            "\t Train_Loss: 138.4701 Val_Loss: 315.2314  BEST VAL Loss: 315.2314\n",
            "\n",
            "Epoch 594: Validation loss decreased (315.231354 --> 315.120087).\n",
            "\t Train_Loss: 138.4682 Val_Loss: 315.1201  BEST VAL Loss: 315.1201\n",
            "\n",
            "Epoch 595: Validation loss decreased (315.120087 --> 315.011078).\n",
            "\t Train_Loss: 138.4664 Val_Loss: 315.0111  BEST VAL Loss: 315.0111\n",
            "\n",
            "Epoch 596: Validation loss decreased (315.011078 --> 314.903656).\n",
            "\t Train_Loss: 138.4646 Val_Loss: 314.9037  BEST VAL Loss: 314.9037\n",
            "\n",
            "Epoch 597: Validation loss decreased (314.903656 --> 314.797272).\n",
            "\t Train_Loss: 138.4629 Val_Loss: 314.7973  BEST VAL Loss: 314.7973\n",
            "\n",
            "Epoch 598: Validation loss decreased (314.797272 --> 314.692719).\n",
            "\t Train_Loss: 138.4612 Val_Loss: 314.6927  BEST VAL Loss: 314.6927\n",
            "\n",
            "Epoch 599: Validation loss decreased (314.692719 --> 314.589172).\n",
            "\t Train_Loss: 138.4596 Val_Loss: 314.5892  BEST VAL Loss: 314.5892\n",
            "\n",
            "Epoch 600: Validation loss decreased (314.589172 --> 314.487854).\n",
            "\t Train_Loss: 138.4580 Val_Loss: 314.4879  BEST VAL Loss: 314.4879\n",
            "\n",
            "Epoch 601: Validation loss decreased (314.487854 --> 314.388031).\n",
            "\t Train_Loss: 138.4565 Val_Loss: 314.3880  BEST VAL Loss: 314.3880\n",
            "\n",
            "Epoch 602: Validation loss decreased (314.388031 --> 314.289398).\n",
            "\t Train_Loss: 138.4550 Val_Loss: 314.2894  BEST VAL Loss: 314.2894\n",
            "\n",
            "Epoch 603: Validation loss decreased (314.289398 --> 314.192352).\n",
            "\t Train_Loss: 138.4536 Val_Loss: 314.1924  BEST VAL Loss: 314.1924\n",
            "\n",
            "Epoch 604: Validation loss decreased (314.192352 --> 314.096893).\n",
            "\t Train_Loss: 138.4522 Val_Loss: 314.0969  BEST VAL Loss: 314.0969\n",
            "\n",
            "Epoch 605: Validation loss decreased (314.096893 --> 314.002625).\n",
            "\t Train_Loss: 138.4509 Val_Loss: 314.0026  BEST VAL Loss: 314.0026\n",
            "\n",
            "Epoch 606: Validation loss decreased (314.002625 --> 313.910034).\n",
            "\t Train_Loss: 138.4495 Val_Loss: 313.9100  BEST VAL Loss: 313.9100\n",
            "\n",
            "Epoch 607: Validation loss decreased (313.910034 --> 313.818512).\n",
            "\t Train_Loss: 138.4482 Val_Loss: 313.8185  BEST VAL Loss: 313.8185\n",
            "\n",
            "Epoch 608: Validation loss decreased (313.818512 --> 313.728699).\n",
            "\t Train_Loss: 138.4471 Val_Loss: 313.7287  BEST VAL Loss: 313.7287\n",
            "\n",
            "Epoch 609: Validation loss decreased (313.728699 --> 313.640198).\n",
            "\t Train_Loss: 138.4458 Val_Loss: 313.6402  BEST VAL Loss: 313.6402\n",
            "\n",
            "Epoch 610: Validation loss decreased (313.640198 --> 313.552612).\n",
            "\t Train_Loss: 138.4447 Val_Loss: 313.5526  BEST VAL Loss: 313.5526\n",
            "\n",
            "Epoch 611: Validation loss decreased (313.552612 --> 313.466980).\n",
            "\t Train_Loss: 138.4436 Val_Loss: 313.4670  BEST VAL Loss: 313.4670\n",
            "\n",
            "Epoch 612: Validation loss decreased (313.466980 --> 313.382141).\n",
            "\t Train_Loss: 138.4425 Val_Loss: 313.3821  BEST VAL Loss: 313.3821\n",
            "\n",
            "Epoch 613: Validation loss decreased (313.382141 --> 313.298492).\n",
            "\t Train_Loss: 138.4414 Val_Loss: 313.2985  BEST VAL Loss: 313.2985\n",
            "\n",
            "Epoch 614: Validation loss decreased (313.298492 --> 313.216309).\n",
            "\t Train_Loss: 138.4404 Val_Loss: 313.2163  BEST VAL Loss: 313.2163\n",
            "\n",
            "Epoch 615: Validation loss decreased (313.216309 --> 313.135590).\n",
            "\t Train_Loss: 138.4394 Val_Loss: 313.1356  BEST VAL Loss: 313.1356\n",
            "\n",
            "Epoch 616: Validation loss decreased (313.135590 --> 313.055786).\n",
            "\t Train_Loss: 138.4385 Val_Loss: 313.0558  BEST VAL Loss: 313.0558\n",
            "\n",
            "Epoch 617: Validation loss decreased (313.055786 --> 312.977051).\n",
            "\t Train_Loss: 138.4375 Val_Loss: 312.9771  BEST VAL Loss: 312.9771\n",
            "\n",
            "Epoch 618: Validation loss decreased (312.977051 --> 312.900116).\n",
            "\t Train_Loss: 138.4366 Val_Loss: 312.9001  BEST VAL Loss: 312.9001\n",
            "\n",
            "Epoch 619: Validation loss decreased (312.900116 --> 312.824036).\n",
            "\t Train_Loss: 138.4357 Val_Loss: 312.8240  BEST VAL Loss: 312.8240\n",
            "\n",
            "Epoch 620: Validation loss decreased (312.824036 --> 312.749084).\n",
            "\t Train_Loss: 138.4348 Val_Loss: 312.7491  BEST VAL Loss: 312.7491\n",
            "\n",
            "Epoch 621: Validation loss decreased (312.749084 --> 312.675293).\n",
            "\t Train_Loss: 138.4340 Val_Loss: 312.6753  BEST VAL Loss: 312.6753\n",
            "\n",
            "Epoch 622: Validation loss decreased (312.675293 --> 312.602509).\n",
            "\t Train_Loss: 138.4330 Val_Loss: 312.6025  BEST VAL Loss: 312.6025\n",
            "\n",
            "Epoch 623: Validation loss decreased (312.602509 --> 312.531219).\n",
            "\t Train_Loss: 138.4322 Val_Loss: 312.5312  BEST VAL Loss: 312.5312\n",
            "\n",
            "Epoch 624: Validation loss decreased (312.531219 --> 312.460632).\n",
            "\t Train_Loss: 138.4313 Val_Loss: 312.4606  BEST VAL Loss: 312.4606\n",
            "\n",
            "Epoch 625: Validation loss decreased (312.460632 --> 312.391174).\n",
            "\t Train_Loss: 138.4302 Val_Loss: 312.3912  BEST VAL Loss: 312.3912\n",
            "\n",
            "Epoch 626: Validation loss decreased (312.391174 --> 312.322906).\n",
            "\t Train_Loss: 138.4287 Val_Loss: 312.3229  BEST VAL Loss: 312.3229\n",
            "\n",
            "Epoch 627: Validation loss decreased (312.322906 --> 312.256561).\n",
            "\t Train_Loss: 138.4258 Val_Loss: 312.2566  BEST VAL Loss: 312.2566\n",
            "\n",
            "Epoch 628: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4124 Val_Loss: 312.2675  BEST VAL Loss: 312.2566\n",
            "\n",
            "Epoch 629: Validation loss did not decrease\n",
            "\t Train_Loss: 137.9202 Val_Loss: 327.5330  BEST VAL Loss: 312.2566\n",
            "\n",
            "Epoch 630: Validation loss did not decrease\n",
            "\t Train_Loss: 127.6696 Val_Loss: 338.7307  BEST VAL Loss: 312.2566\n",
            "\n",
            "Epoch 631: Validation loss decreased (312.256561 --> 312.040375).\n",
            "\t Train_Loss: 127.6150 Val_Loss: 312.0404  BEST VAL Loss: 312.0404\n",
            "\n",
            "Epoch 632: Validation loss decreased (312.040375 --> 311.625061).\n",
            "\t Train_Loss: 136.1423 Val_Loss: 311.6251  BEST VAL Loss: 311.6251\n",
            "\n",
            "Epoch 633: Validation loss decreased (311.625061 --> 311.431702).\n",
            "\t Train_Loss: 138.1046 Val_Loss: 311.4317  BEST VAL Loss: 311.4317\n",
            "\n",
            "Epoch 634: Validation loss decreased (311.431702 --> 311.265564).\n",
            "\t Train_Loss: 138.3595 Val_Loss: 311.2656  BEST VAL Loss: 311.2656\n",
            "\n",
            "Epoch 635: Validation loss decreased (311.265564 --> 311.113220).\n",
            "\t Train_Loss: 138.4059 Val_Loss: 311.1132  BEST VAL Loss: 311.1132\n",
            "\n",
            "Epoch 636: Validation loss decreased (311.113220 --> 310.972015).\n",
            "\t Train_Loss: 138.4156 Val_Loss: 310.9720  BEST VAL Loss: 310.9720\n",
            "\n",
            "Epoch 637: Validation loss decreased (310.972015 --> 310.840607).\n",
            "\t Train_Loss: 138.4174 Val_Loss: 310.8406  BEST VAL Loss: 310.8406\n",
            "\n",
            "Epoch 638: Validation loss decreased (310.840607 --> 310.718842).\n",
            "\t Train_Loss: 138.4174 Val_Loss: 310.7188  BEST VAL Loss: 310.7188\n",
            "\n",
            "Epoch 639: Validation loss decreased (310.718842 --> 310.605713).\n",
            "\t Train_Loss: 138.4169 Val_Loss: 310.6057  BEST VAL Loss: 310.6057\n",
            "\n",
            "Epoch 640: Validation loss decreased (310.605713 --> 310.500641).\n",
            "\t Train_Loss: 138.4164 Val_Loss: 310.5006  BEST VAL Loss: 310.5006\n",
            "\n",
            "Epoch 641: Validation loss decreased (310.500641 --> 310.402893).\n",
            "\t Train_Loss: 138.4159 Val_Loss: 310.4029  BEST VAL Loss: 310.4029\n",
            "\n",
            "Epoch 642: Validation loss decreased (310.402893 --> 310.311493).\n",
            "\t Train_Loss: 138.4154 Val_Loss: 310.3115  BEST VAL Loss: 310.3115\n",
            "\n",
            "Epoch 643: Validation loss decreased (310.311493 --> 310.225983).\n",
            "\t Train_Loss: 138.4149 Val_Loss: 310.2260  BEST VAL Loss: 310.2260\n",
            "\n",
            "Epoch 644: Validation loss decreased (310.225983 --> 310.146637).\n",
            "\t Train_Loss: 138.4145 Val_Loss: 310.1466  BEST VAL Loss: 310.1466\n",
            "\n",
            "Epoch 645: Validation loss decreased (310.146637 --> 310.072174).\n",
            "\t Train_Loss: 138.4141 Val_Loss: 310.0722  BEST VAL Loss: 310.0722\n",
            "\n",
            "Epoch 646: Validation loss decreased (310.072174 --> 310.002869).\n",
            "\t Train_Loss: 138.4138 Val_Loss: 310.0029  BEST VAL Loss: 310.0029\n",
            "\n",
            "Epoch 647: Validation loss decreased (310.002869 --> 309.937317).\n",
            "\t Train_Loss: 138.4134 Val_Loss: 309.9373  BEST VAL Loss: 309.9373\n",
            "\n",
            "Epoch 648: Validation loss decreased (309.937317 --> 309.876282).\n",
            "\t Train_Loss: 138.4132 Val_Loss: 309.8763  BEST VAL Loss: 309.8763\n",
            "\n",
            "Epoch 649: Validation loss decreased (309.876282 --> 309.819061).\n",
            "\t Train_Loss: 138.4129 Val_Loss: 309.8191  BEST VAL Loss: 309.8191\n",
            "\n",
            "Epoch 650: Validation loss decreased (309.819061 --> 309.765228).\n",
            "\t Train_Loss: 138.4127 Val_Loss: 309.7652  BEST VAL Loss: 309.7652\n",
            "\n",
            "Epoch 651: Validation loss decreased (309.765228 --> 309.714294).\n",
            "\t Train_Loss: 138.4125 Val_Loss: 309.7143  BEST VAL Loss: 309.7143\n",
            "\n",
            "Epoch 652: Validation loss decreased (309.714294 --> 309.666901).\n",
            "\t Train_Loss: 138.4122 Val_Loss: 309.6669  BEST VAL Loss: 309.6669\n",
            "\n",
            "Epoch 653: Validation loss decreased (309.666901 --> 309.621979).\n",
            "\t Train_Loss: 138.4121 Val_Loss: 309.6220  BEST VAL Loss: 309.6220\n",
            "\n",
            "Epoch 654: Validation loss decreased (309.621979 --> 309.579224).\n",
            "\t Train_Loss: 138.4119 Val_Loss: 309.5792  BEST VAL Loss: 309.5792\n",
            "\n",
            "Epoch 655: Validation loss decreased (309.579224 --> 309.539307).\n",
            "\t Train_Loss: 138.4117 Val_Loss: 309.5393  BEST VAL Loss: 309.5393\n",
            "\n",
            "Epoch 656: Validation loss decreased (309.539307 --> 309.500824).\n",
            "\t Train_Loss: 138.4116 Val_Loss: 309.5008  BEST VAL Loss: 309.5008\n",
            "\n",
            "Epoch 657: Validation loss decreased (309.500824 --> 309.464935).\n",
            "\t Train_Loss: 138.4115 Val_Loss: 309.4649  BEST VAL Loss: 309.4649\n",
            "\n",
            "Epoch 658: Validation loss decreased (309.464935 --> 309.430756).\n",
            "\t Train_Loss: 138.4113 Val_Loss: 309.4308  BEST VAL Loss: 309.4308\n",
            "\n",
            "Epoch 659: Validation loss decreased (309.430756 --> 309.398224).\n",
            "\t Train_Loss: 138.4111 Val_Loss: 309.3982  BEST VAL Loss: 309.3982\n",
            "\n",
            "Epoch 660: Validation loss decreased (309.398224 --> 309.367279).\n",
            "\t Train_Loss: 138.4111 Val_Loss: 309.3673  BEST VAL Loss: 309.3673\n",
            "\n",
            "Epoch 661: Validation loss decreased (309.367279 --> 309.337738).\n",
            "\t Train_Loss: 138.4110 Val_Loss: 309.3377  BEST VAL Loss: 309.3377\n",
            "\n",
            "Epoch 662: Validation loss decreased (309.337738 --> 309.309662).\n",
            "\t Train_Loss: 138.4108 Val_Loss: 309.3097  BEST VAL Loss: 309.3097\n",
            "\n",
            "Epoch 663: Validation loss decreased (309.309662 --> 309.282898).\n",
            "\t Train_Loss: 138.4107 Val_Loss: 309.2829  BEST VAL Loss: 309.2829\n",
            "\n",
            "Epoch 664: Validation loss decreased (309.282898 --> 309.257294).\n",
            "\t Train_Loss: 138.4106 Val_Loss: 309.2573  BEST VAL Loss: 309.2573\n",
            "\n",
            "Epoch 665: Validation loss decreased (309.257294 --> 309.232452).\n",
            "\t Train_Loss: 138.4105 Val_Loss: 309.2325  BEST VAL Loss: 309.2325\n",
            "\n",
            "Epoch 666: Validation loss decreased (309.232452 --> 309.208954).\n",
            "\t Train_Loss: 138.4103 Val_Loss: 309.2090  BEST VAL Loss: 309.2090\n",
            "\n",
            "Epoch 667: Validation loss decreased (309.208954 --> 309.186432).\n",
            "\t Train_Loss: 138.4102 Val_Loss: 309.1864  BEST VAL Loss: 309.1864\n",
            "\n",
            "Epoch 668: Validation loss decreased (309.186432 --> 309.164642).\n",
            "\t Train_Loss: 138.4101 Val_Loss: 309.1646  BEST VAL Loss: 309.1646\n",
            "\n",
            "Epoch 669: Validation loss decreased (309.164642 --> 309.143646).\n",
            "\t Train_Loss: 138.4100 Val_Loss: 309.1436  BEST VAL Loss: 309.1436\n",
            "\n",
            "Epoch 670: Validation loss decreased (309.143646 --> 309.123962).\n",
            "\t Train_Loss: 138.4098 Val_Loss: 309.1240  BEST VAL Loss: 309.1240\n",
            "\n",
            "Epoch 671: Validation loss decreased (309.123962 --> 309.104645).\n",
            "\t Train_Loss: 138.4097 Val_Loss: 309.1046  BEST VAL Loss: 309.1046\n",
            "\n",
            "Epoch 672: Validation loss decreased (309.104645 --> 309.085846).\n",
            "\t Train_Loss: 138.4096 Val_Loss: 309.0858  BEST VAL Loss: 309.0858\n",
            "\n",
            "Epoch 673: Validation loss decreased (309.085846 --> 309.068024).\n",
            "\t Train_Loss: 138.4093 Val_Loss: 309.0680  BEST VAL Loss: 309.0680\n",
            "\n",
            "Epoch 674: Validation loss decreased (309.068024 --> 309.050598).\n",
            "\t Train_Loss: 138.4092 Val_Loss: 309.0506  BEST VAL Loss: 309.0506\n",
            "\n",
            "Epoch 675: Validation loss decreased (309.050598 --> 309.033844).\n",
            "\t Train_Loss: 138.4090 Val_Loss: 309.0338  BEST VAL Loss: 309.0338\n",
            "\n",
            "Epoch 676: Validation loss decreased (309.033844 --> 309.017426).\n",
            "\t Train_Loss: 138.4087 Val_Loss: 309.0174  BEST VAL Loss: 309.0174\n",
            "\n",
            "Epoch 677: Validation loss decreased (309.017426 --> 309.001892).\n",
            "\t Train_Loss: 138.4084 Val_Loss: 309.0019  BEST VAL Loss: 309.0019\n",
            "\n",
            "Epoch 678: Validation loss decreased (309.001892 --> 308.986603).\n",
            "\t Train_Loss: 138.4081 Val_Loss: 308.9866  BEST VAL Loss: 308.9866\n",
            "\n",
            "Epoch 679: Validation loss decreased (308.986603 --> 308.971893).\n",
            "\t Train_Loss: 138.4077 Val_Loss: 308.9719  BEST VAL Loss: 308.9719\n",
            "\n",
            "Epoch 680: Validation loss decreased (308.971893 --> 308.957611).\n",
            "\t Train_Loss: 138.4072 Val_Loss: 308.9576  BEST VAL Loss: 308.9576\n",
            "\n",
            "Epoch 681: Validation loss decreased (308.957611 --> 308.943604).\n",
            "\t Train_Loss: 138.4065 Val_Loss: 308.9436  BEST VAL Loss: 308.9436\n",
            "\n",
            "Epoch 682: Validation loss decreased (308.943604 --> 308.930237).\n",
            "\t Train_Loss: 138.4057 Val_Loss: 308.9302  BEST VAL Loss: 308.9302\n",
            "\n",
            "Epoch 683: Validation loss decreased (308.930237 --> 308.917572).\n",
            "\t Train_Loss: 138.4045 Val_Loss: 308.9176  BEST VAL Loss: 308.9176\n",
            "\n",
            "Epoch 684: Validation loss decreased (308.917572 --> 308.904877).\n",
            "\t Train_Loss: 138.4029 Val_Loss: 308.9049  BEST VAL Loss: 308.9049\n",
            "\n",
            "Epoch 685: Validation loss decreased (308.904877 --> 308.893646).\n",
            "\t Train_Loss: 138.4004 Val_Loss: 308.8936  BEST VAL Loss: 308.8936\n",
            "\n",
            "Epoch 686: Validation loss decreased (308.893646 --> 308.883545).\n",
            "\t Train_Loss: 138.3965 Val_Loss: 308.8835  BEST VAL Loss: 308.8835\n",
            "\n",
            "Epoch 687: Validation loss decreased (308.883545 --> 308.877075).\n",
            "\t Train_Loss: 138.3895 Val_Loss: 308.8771  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 688: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3746 Val_Loss: 308.8810  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 689: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3343 Val_Loss: 308.9625  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 690: Validation loss did not decrease\n",
            "\t Train_Loss: 138.1503 Val_Loss: 312.0845  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 691: Validation loss did not decrease\n",
            "\t Train_Loss: 135.3498 Val_Loss: 417.2663  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 692: Validation loss did not decrease\n",
            "\t Train_Loss: 149.6812 Val_Loss: 414.5159  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 693: Validation loss did not decrease\n",
            "\t Train_Loss: 148.9815 Val_Loss: 330.8733  BEST VAL Loss: 308.8771\n",
            "\n",
            "Epoch 694: Validation loss decreased (308.877075 --> 308.353363).\n",
            "\t Train_Loss: 131.4107 Val_Loss: 308.3534  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 695: Validation loss did not decrease\n",
            "\t Train_Loss: 136.9935 Val_Loss: 318.0745  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 696: Validation loss did not decrease\n",
            "\t Train_Loss: 125.8081 Val_Loss: 426.3371  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 697: Validation loss did not decrease\n",
            "\t Train_Loss: 151.7029 Val_Loss: 427.5294  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 698: Validation loss did not decrease\n",
            "\t Train_Loss: 152.0240 Val_Loss: 426.8717  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 699: Validation loss did not decrease\n",
            "\t Train_Loss: 151.8923 Val_Loss: 426.0521  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 700: Validation loss did not decrease\n",
            "\t Train_Loss: 151.7257 Val_Loss: 425.1153  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 701: Validation loss did not decrease\n",
            "\t Train_Loss: 151.5359 Val_Loss: 424.0762  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 702: Validation loss did not decrease\n",
            "\t Train_Loss: 151.3267 Val_Loss: 422.9453  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 703: Validation loss did not decrease\n",
            "\t Train_Loss: 151.1006 Val_Loss: 421.7354  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 704: Validation loss did not decrease\n",
            "\t Train_Loss: 150.8604 Val_Loss: 420.4552  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 705: Validation loss did not decrease\n",
            "\t Train_Loss: 150.6084 Val_Loss: 419.1157  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 706: Validation loss did not decrease\n",
            "\t Train_Loss: 150.3467 Val_Loss: 417.7245  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 707: Validation loss did not decrease\n",
            "\t Train_Loss: 150.0774 Val_Loss: 416.2888  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 708: Validation loss did not decrease\n",
            "\t Train_Loss: 149.8022 Val_Loss: 414.8167  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 709: Validation loss did not decrease\n",
            "\t Train_Loss: 149.5227 Val_Loss: 413.3134  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 710: Validation loss did not decrease\n",
            "\t Train_Loss: 149.2402 Val_Loss: 411.7854  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 711: Validation loss did not decrease\n",
            "\t Train_Loss: 148.9561 Val_Loss: 410.2378  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 712: Validation loss did not decrease\n",
            "\t Train_Loss: 148.6714 Val_Loss: 408.6748  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 713: Validation loss did not decrease\n",
            "\t Train_Loss: 148.3874 Val_Loss: 407.1014  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 714: Validation loss did not decrease\n",
            "\t Train_Loss: 148.1045 Val_Loss: 405.5212  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 715: Validation loss did not decrease\n",
            "\t Train_Loss: 147.8238 Val_Loss: 403.9368  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 716: Validation loss did not decrease\n",
            "\t Train_Loss: 147.5460 Val_Loss: 402.3527  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 717: Validation loss did not decrease\n",
            "\t Train_Loss: 147.2715 Val_Loss: 400.7705  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 718: Validation loss did not decrease\n",
            "\t Train_Loss: 147.0008 Val_Loss: 399.1935  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 719: Validation loss did not decrease\n",
            "\t Train_Loss: 146.7344 Val_Loss: 397.6229  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 720: Validation loss did not decrease\n",
            "\t Train_Loss: 146.4727 Val_Loss: 396.0618  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 721: Validation loss did not decrease\n",
            "\t Train_Loss: 146.2157 Val_Loss: 394.5116  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 722: Validation loss did not decrease\n",
            "\t Train_Loss: 145.9632 Val_Loss: 392.9879  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 723: Validation loss did not decrease\n",
            "\t Train_Loss: 145.6551 Val_Loss: 395.4210  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 724: Validation loss did not decrease\n",
            "\t Train_Loss: 142.0017 Val_Loss: 511.1123  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 725: Validation loss did not decrease\n",
            "\t Train_Loss: 173.1119 Val_Loss: 408.8826  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 726: Validation loss did not decrease\n",
            "\t Train_Loss: 142.0200 Val_Loss: 386.5779  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 727: Validation loss did not decrease\n",
            "\t Train_Loss: 144.4548 Val_Loss: 384.8340  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 728: Validation loss did not decrease\n",
            "\t Train_Loss: 144.4700 Val_Loss: 383.1989  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 729: Validation loss did not decrease\n",
            "\t Train_Loss: 144.2363 Val_Loss: 381.6054  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 730: Validation loss did not decrease\n",
            "\t Train_Loss: 144.0083 Val_Loss: 380.0504  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 731: Validation loss did not decrease\n",
            "\t Train_Loss: 143.7896 Val_Loss: 378.5332  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 732: Validation loss did not decrease\n",
            "\t Train_Loss: 143.5799 Val_Loss: 377.0525  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 733: Validation loss did not decrease\n",
            "\t Train_Loss: 143.3787 Val_Loss: 375.6065  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 734: Validation loss did not decrease\n",
            "\t Train_Loss: 143.1855 Val_Loss: 374.1949  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 735: Validation loss did not decrease\n",
            "\t Train_Loss: 143.0001 Val_Loss: 372.8155  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 736: Validation loss did not decrease\n",
            "\t Train_Loss: 142.8221 Val_Loss: 371.4679  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 737: Validation loss did not decrease\n",
            "\t Train_Loss: 142.6511 Val_Loss: 370.1514  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 738: Validation loss did not decrease\n",
            "\t Train_Loss: 142.4869 Val_Loss: 368.8645  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 739: Validation loss did not decrease\n",
            "\t Train_Loss: 142.3292 Val_Loss: 367.6063  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 740: Validation loss did not decrease\n",
            "\t Train_Loss: 142.1777 Val_Loss: 366.3767  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 741: Validation loss did not decrease\n",
            "\t Train_Loss: 142.0321 Val_Loss: 365.1737  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 742: Validation loss did not decrease\n",
            "\t Train_Loss: 141.8922 Val_Loss: 363.9977  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 743: Validation loss did not decrease\n",
            "\t Train_Loss: 141.7576 Val_Loss: 362.8471  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 744: Validation loss did not decrease\n",
            "\t Train_Loss: 141.6284 Val_Loss: 361.7217  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 745: Validation loss did not decrease\n",
            "\t Train_Loss: 141.5042 Val_Loss: 360.6207  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 746: Validation loss did not decrease\n",
            "\t Train_Loss: 141.3847 Val_Loss: 359.5430  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 747: Validation loss did not decrease\n",
            "\t Train_Loss: 141.2698 Val_Loss: 358.4889  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 748: Validation loss did not decrease\n",
            "\t Train_Loss: 141.1594 Val_Loss: 357.4566  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 749: Validation loss did not decrease\n",
            "\t Train_Loss: 141.0532 Val_Loss: 356.4468  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 750: Validation loss did not decrease\n",
            "\t Train_Loss: 140.9511 Val_Loss: 355.4579  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 751: Validation loss did not decrease\n",
            "\t Train_Loss: 140.8528 Val_Loss: 354.4902  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 752: Validation loss did not decrease\n",
            "\t Train_Loss: 140.7584 Val_Loss: 353.5426  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 753: Validation loss did not decrease\n",
            "\t Train_Loss: 140.6676 Val_Loss: 352.6145  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 754: Validation loss did not decrease\n",
            "\t Train_Loss: 140.5802 Val_Loss: 351.7063  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 755: Validation loss did not decrease\n",
            "\t Train_Loss: 140.4962 Val_Loss: 350.8165  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 756: Validation loss did not decrease\n",
            "\t Train_Loss: 140.4154 Val_Loss: 349.9454  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 757: Validation loss did not decrease\n",
            "\t Train_Loss: 140.3376 Val_Loss: 349.0919  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 758: Validation loss did not decrease\n",
            "\t Train_Loss: 140.2629 Val_Loss: 348.2566  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 759: Validation loss did not decrease\n",
            "\t Train_Loss: 140.1909 Val_Loss: 347.4377  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 760: Validation loss did not decrease\n",
            "\t Train_Loss: 140.1218 Val_Loss: 346.6361  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 761: Validation loss did not decrease\n",
            "\t Train_Loss: 140.0553 Val_Loss: 345.8509  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 762: Validation loss did not decrease\n",
            "\t Train_Loss: 139.9913 Val_Loss: 345.0815  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 763: Validation loss did not decrease\n",
            "\t Train_Loss: 139.9297 Val_Loss: 344.3280  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 764: Validation loss did not decrease\n",
            "\t Train_Loss: 139.8704 Val_Loss: 343.5897  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 765: Validation loss did not decrease\n",
            "\t Train_Loss: 139.8136 Val_Loss: 342.8664  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 766: Validation loss did not decrease\n",
            "\t Train_Loss: 139.7587 Val_Loss: 342.1581  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 767: Validation loss did not decrease\n",
            "\t Train_Loss: 139.7061 Val_Loss: 341.4642  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 768: Validation loss did not decrease\n",
            "\t Train_Loss: 139.6554 Val_Loss: 340.7841  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 769: Validation loss did not decrease\n",
            "\t Train_Loss: 139.6066 Val_Loss: 340.1173  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 770: Validation loss did not decrease\n",
            "\t Train_Loss: 139.5598 Val_Loss: 339.4647  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 771: Validation loss did not decrease\n",
            "\t Train_Loss: 139.5147 Val_Loss: 338.8250  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 772: Validation loss did not decrease\n",
            "\t Train_Loss: 139.4713 Val_Loss: 338.1982  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 773: Validation loss did not decrease\n",
            "\t Train_Loss: 139.4296 Val_Loss: 337.5838  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 774: Validation loss did not decrease\n",
            "\t Train_Loss: 139.3895 Val_Loss: 336.9822  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 775: Validation loss did not decrease\n",
            "\t Train_Loss: 139.3510 Val_Loss: 336.3924  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 776: Validation loss did not decrease\n",
            "\t Train_Loss: 139.3139 Val_Loss: 335.8148  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 777: Validation loss did not decrease\n",
            "\t Train_Loss: 139.2782 Val_Loss: 335.2483  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 778: Validation loss did not decrease\n",
            "\t Train_Loss: 139.2440 Val_Loss: 334.6933  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 779: Validation loss did not decrease\n",
            "\t Train_Loss: 139.2110 Val_Loss: 334.1497  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 780: Validation loss did not decrease\n",
            "\t Train_Loss: 139.1793 Val_Loss: 333.6170  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 781: Validation loss did not decrease\n",
            "\t Train_Loss: 139.1488 Val_Loss: 333.0950  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 782: Validation loss did not decrease\n",
            "\t Train_Loss: 139.1195 Val_Loss: 332.5833  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 783: Validation loss did not decrease\n",
            "\t Train_Loss: 139.0913 Val_Loss: 332.0821  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 784: Validation loss did not decrease\n",
            "\t Train_Loss: 139.0643 Val_Loss: 331.5908  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 785: Validation loss did not decrease\n",
            "\t Train_Loss: 139.0382 Val_Loss: 331.1089  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 786: Validation loss did not decrease\n",
            "\t Train_Loss: 139.0132 Val_Loss: 330.6371  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 787: Validation loss did not decrease\n",
            "\t Train_Loss: 138.9891 Val_Loss: 330.1752  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 788: Validation loss did not decrease\n",
            "\t Train_Loss: 138.9660 Val_Loss: 329.7219  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 789: Validation loss did not decrease\n",
            "\t Train_Loss: 138.9437 Val_Loss: 329.2780  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 790: Validation loss did not decrease\n",
            "\t Train_Loss: 138.9223 Val_Loss: 328.8431  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 791: Validation loss did not decrease\n",
            "\t Train_Loss: 138.9017 Val_Loss: 328.4168  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 792: Validation loss did not decrease\n",
            "\t Train_Loss: 138.8818 Val_Loss: 327.9991  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 793: Validation loss did not decrease\n",
            "\t Train_Loss: 138.8624 Val_Loss: 327.5899  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 794: Validation loss did not decrease\n",
            "\t Train_Loss: 138.8424 Val_Loss: 327.1913  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 795: Validation loss did not decrease\n",
            "\t Train_Loss: 138.8033 Val_Loss: 328.2213  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 796: Validation loss did not decrease\n",
            "\t Train_Loss: 135.7961 Val_Loss: 419.6823  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 797: Validation loss did not decrease\n",
            "\t Train_Loss: 148.9659 Val_Loss: 334.8028  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 798: Validation loss did not decrease\n",
            "\t Train_Loss: 131.1533 Val_Loss: 326.6888  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 799: Validation loss did not decrease\n",
            "\t Train_Loss: 135.2450 Val_Loss: 326.1641  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 800: Validation loss did not decrease\n",
            "\t Train_Loss: 134.7468 Val_Loss: 331.4995  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 801: Validation loss did not decrease\n",
            "\t Train_Loss: 129.6075 Val_Loss: 376.5672  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 802: Validation loss did not decrease\n",
            "\t Train_Loss: 136.9172 Val_Loss: 339.1732  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 803: Validation loss did not decrease\n",
            "\t Train_Loss: 127.6618 Val_Loss: 324.6938  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 804: Validation loss did not decrease\n",
            "\t Train_Loss: 130.8010 Val_Loss: 323.2723  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 805: Validation loss did not decrease\n",
            "\t Train_Loss: 131.3567 Val_Loss: 325.5975  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 806: Validation loss did not decrease\n",
            "\t Train_Loss: 127.1366 Val_Loss: 343.1982  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 807: Validation loss did not decrease\n",
            "\t Train_Loss: 126.7238 Val_Loss: 346.5781  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 808: Validation loss did not decrease\n",
            "\t Train_Loss: 127.4596 Val_Loss: 328.1225  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 809: Validation loss did not decrease\n",
            "\t Train_Loss: 123.6773 Val_Loss: 321.3654  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 810: Validation loss did not decrease\n",
            "\t Train_Loss: 125.2985 Val_Loss: 320.6554  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 811: Validation loss did not decrease\n",
            "\t Train_Loss: 124.5347 Val_Loss: 324.5719  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 812: Validation loss did not decrease\n",
            "\t Train_Loss: 122.0060 Val_Loss: 333.0995  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 813: Validation loss did not decrease\n",
            "\t Train_Loss: 122.9424 Val_Loss: 331.1252  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 814: Validation loss did not decrease\n",
            "\t Train_Loss: 122.2923 Val_Loss: 321.7886  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 815: Validation loss did not decrease\n",
            "\t Train_Loss: 120.5326 Val_Loss: 317.0169  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 816: Validation loss did not decrease\n",
            "\t Train_Loss: 121.1412 Val_Loss: 316.0071  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 817: Validation loss did not decrease\n",
            "\t Train_Loss: 120.7479 Val_Loss: 317.7716  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 818: Validation loss did not decrease\n",
            "\t Train_Loss: 119.3784 Val_Loss: 321.6539  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 819: Validation loss did not decrease\n",
            "\t Train_Loss: 119.4843 Val_Loss: 321.8572  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 820: Validation loss did not decrease\n",
            "\t Train_Loss: 119.4933 Val_Loss: 317.0085  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 821: Validation loss did not decrease\n",
            "\t Train_Loss: 118.4379 Val_Loss: 312.6760  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 822: Validation loss did not decrease\n",
            "\t Train_Loss: 118.2872 Val_Loss: 310.7846  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 823: Validation loss did not decrease\n",
            "\t Train_Loss: 118.4089 Val_Loss: 310.6604  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 824: Validation loss did not decrease\n",
            "\t Train_Loss: 117.7853 Val_Loss: 312.0477  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 825: Validation loss did not decrease\n",
            "\t Train_Loss: 117.3271 Val_Loss: 313.3720  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 826: Validation loss did not decrease\n",
            "\t Train_Loss: 117.4530 Val_Loss: 312.0652  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 827: Validation loss did not decrease\n",
            "\t Train_Loss: 117.1784 Val_Loss: 308.9044  BEST VAL Loss: 308.3534\n",
            "\n",
            "Epoch 828: Validation loss decreased (308.353363 --> 306.362549).\n",
            "\t Train_Loss: 116.6849 Val_Loss: 306.3625  BEST VAL Loss: 306.3625\n",
            "\n",
            "Epoch 829: Validation loss decreased (306.362549 --> 305.096771).\n",
            "\t Train_Loss: 116.6392 Val_Loss: 305.0968  BEST VAL Loss: 305.0968\n",
            "\n",
            "Epoch 830: Validation loss decreased (305.096771 --> 304.884369).\n",
            "\t Train_Loss: 116.5916 Val_Loss: 304.8844  BEST VAL Loss: 304.8844\n",
            "\n",
            "Epoch 831: Validation loss did not decrease\n",
            "\t Train_Loss: 116.2313 Val_Loss: 305.4598  BEST VAL Loss: 304.8844\n",
            "\n",
            "Epoch 832: Validation loss did not decrease\n",
            "\t Train_Loss: 115.9947 Val_Loss: 305.8568  BEST VAL Loss: 304.8844\n",
            "\n",
            "Epoch 833: Validation loss decreased (304.884369 --> 304.879120).\n",
            "\t Train_Loss: 115.9935 Val_Loss: 304.8791  BEST VAL Loss: 304.8791\n",
            "\n",
            "Epoch 834: Validation loss decreased (304.879120 --> 302.857910).\n",
            "\t Train_Loss: 115.8223 Val_Loss: 302.8579  BEST VAL Loss: 302.8579\n",
            "\n",
            "Epoch 835: Validation loss decreased (302.857910 --> 301.014404).\n",
            "\t Train_Loss: 115.5438 Val_Loss: 301.0144  BEST VAL Loss: 301.0144\n",
            "\n",
            "Epoch 836: Validation loss decreased (301.014404 --> 299.884033).\n",
            "\t Train_Loss: 115.4512 Val_Loss: 299.8840  BEST VAL Loss: 299.8840\n",
            "\n",
            "Epoch 837: Validation loss decreased (299.884033 --> 299.443115).\n",
            "\t Train_Loss: 115.3958 Val_Loss: 299.4431  BEST VAL Loss: 299.4431\n",
            "\n",
            "Epoch 838: Validation loss did not decrease\n",
            "\t Train_Loss: 115.2006 Val_Loss: 299.5155  BEST VAL Loss: 299.4431\n",
            "\n",
            "Epoch 839: Validation loss did not decrease\n",
            "\t Train_Loss: 115.0184 Val_Loss: 299.6148  BEST VAL Loss: 299.4431\n",
            "\n",
            "Epoch 840: Validation loss decreased (299.443115 --> 299.068024).\n",
            "\t Train_Loss: 114.9571 Val_Loss: 299.0680  BEST VAL Loss: 299.0680\n",
            "\n",
            "Epoch 841: Validation loss decreased (299.068024 --> 297.793549).\n",
            "\t Train_Loss: 114.8644 Val_Loss: 297.7935  BEST VAL Loss: 297.7935\n",
            "\n",
            "Epoch 842: Validation loss decreased (297.793549 --> 296.368256).\n",
            "\t Train_Loss: 114.6913 Val_Loss: 296.3683  BEST VAL Loss: 296.3683\n",
            "\n",
            "Epoch 843: Validation loss decreased (296.368256 --> 295.278198).\n",
            "\t Train_Loss: 114.5708 Val_Loss: 295.2782  BEST VAL Loss: 295.2782\n",
            "\n",
            "Epoch 844: Validation loss decreased (295.278198 --> 294.649170).\n",
            "\t Train_Loss: 114.5082 Val_Loss: 294.6492  BEST VAL Loss: 294.6492\n",
            "\n",
            "Epoch 845: Validation loss decreased (294.649170 --> 294.414215).\n",
            "\t Train_Loss: 114.4005 Val_Loss: 294.4142  BEST VAL Loss: 294.4142\n",
            "\n",
            "Epoch 846: Validation loss decreased (294.414215 --> 294.347412).\n",
            "\t Train_Loss: 114.2596 Val_Loss: 294.3474  BEST VAL Loss: 294.3474\n",
            "\n",
            "Epoch 847: Validation loss decreased (294.347412 --> 294.070862).\n",
            "\t Train_Loss: 114.1636 Val_Loss: 294.0709  BEST VAL Loss: 294.0709\n",
            "\n",
            "Epoch 848: Validation loss decreased (294.070862 --> 293.341156).\n",
            "\t Train_Loss: 114.0904 Val_Loss: 293.3412  BEST VAL Loss: 293.3412\n",
            "\n",
            "Epoch 849: Validation loss decreased (293.341156 --> 292.317596).\n",
            "\t Train_Loss: 113.9731 Val_Loss: 292.3176  BEST VAL Loss: 292.3176\n",
            "\n",
            "Epoch 850: Validation loss decreased (292.317596 --> 291.816772).\n",
            "\t Train_Loss: 113.7991 Val_Loss: 291.8168  BEST VAL Loss: 291.8168\n",
            "\n",
            "Epoch 851: Validation loss did not decrease\n",
            "\t Train_Loss: 112.5497 Val_Loss: 364.0011  BEST VAL Loss: 291.8168\n",
            "\n",
            "Epoch 852: Validation loss decreased (291.816772 --> 290.847015).\n",
            "\t Train_Loss: 122.0648 Val_Loss: 290.8470  BEST VAL Loss: 290.8470\n",
            "\n",
            "Epoch 853: Validation loss decreased (290.847015 --> 288.407288).\n",
            "\t Train_Loss: 110.6211 Val_Loss: 288.4073  BEST VAL Loss: 288.4073\n",
            "\n",
            "Epoch 854: Validation loss decreased (288.407288 --> 288.280914).\n",
            "\t Train_Loss: 113.2914 Val_Loss: 288.2809  BEST VAL Loss: 288.2809\n",
            "\n",
            "Epoch 855: Validation loss did not decrease\n",
            "\t Train_Loss: 113.4080 Val_Loss: 288.8048  BEST VAL Loss: 288.2809\n",
            "\n",
            "Epoch 856: Validation loss did not decrease\n",
            "\t Train_Loss: 113.3361 Val_Loss: 289.2611  BEST VAL Loss: 288.2809\n",
            "\n",
            "Epoch 857: Validation loss did not decrease\n",
            "\t Train_Loss: 113.3922 Val_Loss: 288.7591  BEST VAL Loss: 288.2809\n",
            "\n",
            "Epoch 858: Validation loss decreased (288.280914 --> 287.319244).\n",
            "\t Train_Loss: 113.3246 Val_Loss: 287.3192  BEST VAL Loss: 287.3192\n",
            "\n",
            "Epoch 859: Validation loss decreased (287.319244 --> 285.754852).\n",
            "\t Train_Loss: 113.1219 Val_Loss: 285.7549  BEST VAL Loss: 285.7549\n",
            "\n",
            "Epoch 860: Validation loss decreased (285.754852 --> 284.638031).\n",
            "\t Train_Loss: 113.0185 Val_Loss: 284.6380  BEST VAL Loss: 284.6380\n",
            "\n",
            "Epoch 861: Validation loss decreased (284.638031 --> 284.071106).\n",
            "\t Train_Loss: 113.0201 Val_Loss: 284.0711  BEST VAL Loss: 284.0711\n",
            "\n",
            "Epoch 862: Validation loss decreased (284.071106 --> 284.003571).\n",
            "\t Train_Loss: 112.9531 Val_Loss: 284.0036  BEST VAL Loss: 284.0036\n",
            "\n",
            "Epoch 863: Validation loss did not decrease\n",
            "\t Train_Loss: 112.8029 Val_Loss: 284.2737  BEST VAL Loss: 284.0036\n",
            "\n",
            "Epoch 864: Validation loss did not decrease\n",
            "\t Train_Loss: 112.7022 Val_Loss: 284.4770  BEST VAL Loss: 284.0036\n",
            "\n",
            "Epoch 865: Validation loss did not decrease\n",
            "\t Train_Loss: 112.6772 Val_Loss: 284.1559  BEST VAL Loss: 284.0036\n",
            "\n",
            "Epoch 866: Validation loss decreased (284.003571 --> 283.260498).\n",
            "\t Train_Loss: 112.6190 Val_Loss: 283.2605  BEST VAL Loss: 283.2605\n",
            "\n",
            "Epoch 867: Validation loss decreased (283.260498 --> 282.167572).\n",
            "\t Train_Loss: 112.5009 Val_Loss: 282.1676  BEST VAL Loss: 282.1676\n",
            "\n",
            "Epoch 868: Validation loss decreased (282.167572 --> 281.262787).\n",
            "\t Train_Loss: 112.4072 Val_Loss: 281.2628  BEST VAL Loss: 281.2628\n",
            "\n",
            "Epoch 869: Validation loss decreased (281.262787 --> 280.712799).\n",
            "\t Train_Loss: 112.3640 Val_Loss: 280.7128  BEST VAL Loss: 280.7128\n",
            "\n",
            "Epoch 870: Validation loss decreased (280.712799 --> 280.521118).\n",
            "\t Train_Loss: 112.3096 Val_Loss: 280.5211  BEST VAL Loss: 280.5211\n",
            "\n",
            "Epoch 871: Validation loss did not decrease\n",
            "\t Train_Loss: 112.2158 Val_Loss: 280.5830  BEST VAL Loss: 280.5211\n",
            "\n",
            "Epoch 872: Validation loss did not decrease\n",
            "\t Train_Loss: 112.1254 Val_Loss: 280.6766  BEST VAL Loss: 280.5211\n",
            "\n",
            "Epoch 873: Validation loss did not decrease\n",
            "\t Train_Loss: 112.0686 Val_Loss: 280.5326  BEST VAL Loss: 280.5211\n",
            "\n",
            "Epoch 874: Validation loss decreased (280.521118 --> 280.033691).\n",
            "\t Train_Loss: 112.0158 Val_Loss: 280.0337  BEST VAL Loss: 280.0337\n",
            "\n",
            "Epoch 875: Validation loss decreased (280.033691 --> 279.299011).\n",
            "\t Train_Loss: 111.9377 Val_Loss: 279.2990  BEST VAL Loss: 279.2990\n",
            "\n",
            "Epoch 876: Validation loss decreased (279.299011 --> 278.560242).\n",
            "\t Train_Loss: 111.8533 Val_Loss: 278.5602  BEST VAL Loss: 278.5602\n",
            "\n",
            "Epoch 877: Validation loss decreased (278.560242 --> 277.995392).\n",
            "\t Train_Loss: 111.7885 Val_Loss: 277.9954  BEST VAL Loss: 277.9954\n",
            "\n",
            "Epoch 878: Validation loss decreased (277.995392 --> 277.669403).\n",
            "\t Train_Loss: 111.7333 Val_Loss: 277.6694  BEST VAL Loss: 277.6694\n",
            "\n",
            "Epoch 879: Validation loss decreased (277.669403 --> 277.553497).\n",
            "\t Train_Loss: 111.6656 Val_Loss: 277.5535  BEST VAL Loss: 277.5535\n",
            "\n",
            "Epoch 880: Validation loss decreased (277.553497 --> 277.540771).\n",
            "\t Train_Loss: 111.5880 Val_Loss: 277.5408  BEST VAL Loss: 277.5408\n",
            "\n",
            "Epoch 881: Validation loss decreased (277.540771 --> 277.477295).\n",
            "\t Train_Loss: 111.5185 Val_Loss: 277.4773  BEST VAL Loss: 277.4773\n",
            "\n",
            "Epoch 882: Validation loss decreased (277.477295 --> 277.238129).\n",
            "\t Train_Loss: 111.4595 Val_Loss: 277.2381  BEST VAL Loss: 277.2381\n",
            "\n",
            "Epoch 883: Validation loss decreased (277.238129 --> 276.802826).\n",
            "\t Train_Loss: 111.3974 Val_Loss: 276.8028  BEST VAL Loss: 276.8028\n",
            "\n",
            "Epoch 884: Validation loss decreased (276.802826 --> 276.258087).\n",
            "\t Train_Loss: 111.3266 Val_Loss: 276.2581  BEST VAL Loss: 276.2581\n",
            "\n",
            "Epoch 885: Validation loss decreased (276.258087 --> 275.731903).\n",
            "\t Train_Loss: 111.2565 Val_Loss: 275.7319  BEST VAL Loss: 275.7319\n",
            "\n",
            "Epoch 886: Validation loss decreased (275.731903 --> 275.318298).\n",
            "\t Train_Loss: 111.1940 Val_Loss: 275.3183  BEST VAL Loss: 275.3183\n",
            "\n",
            "Epoch 887: Validation loss decreased (275.318298 --> 275.051971).\n",
            "\t Train_Loss: 111.1337 Val_Loss: 275.0520  BEST VAL Loss: 275.0520\n",
            "\n",
            "Epoch 888: Validation loss decreased (275.051971 --> 274.907471).\n",
            "\t Train_Loss: 111.0686 Val_Loss: 274.9075  BEST VAL Loss: 274.9075\n",
            "\n",
            "Epoch 889: Validation loss decreased (274.907471 --> 274.814453).\n",
            "\t Train_Loss: 111.0008 Val_Loss: 274.8145  BEST VAL Loss: 274.8145\n",
            "\n",
            "Epoch 890: Validation loss decreased (274.814453 --> 274.684784).\n",
            "\t Train_Loss: 110.9364 Val_Loss: 274.6848  BEST VAL Loss: 274.6848\n",
            "\n",
            "Epoch 891: Validation loss decreased (274.684784 --> 274.454681).\n",
            "\t Train_Loss: 110.8758 Val_Loss: 274.4547  BEST VAL Loss: 274.4547\n",
            "\n",
            "Epoch 892: Validation loss decreased (274.454681 --> 274.112885).\n",
            "\t Train_Loss: 110.8144 Val_Loss: 274.1129  BEST VAL Loss: 274.1129\n",
            "\n",
            "Epoch 893: Validation loss decreased (274.112885 --> 273.704437).\n",
            "\t Train_Loss: 110.7502 Val_Loss: 273.7044  BEST VAL Loss: 273.7044\n",
            "\n",
            "Epoch 894: Validation loss decreased (273.704437 --> 273.297852).\n",
            "\t Train_Loss: 110.6862 Val_Loss: 273.2979  BEST VAL Loss: 273.2979\n",
            "\n",
            "Epoch 895: Validation loss decreased (273.297852 --> 272.949554).\n",
            "\t Train_Loss: 110.6251 Val_Loss: 272.9496  BEST VAL Loss: 272.9496\n",
            "\n",
            "Epoch 896: Validation loss decreased (272.949554 --> 272.686493).\n",
            "\t Train_Loss: 110.5655 Val_Loss: 272.6865  BEST VAL Loss: 272.6865\n",
            "\n",
            "Epoch 897: Validation loss decreased (272.686493 --> 272.499237).\n",
            "\t Train_Loss: 110.5046 Val_Loss: 272.4992  BEST VAL Loss: 272.4992\n",
            "\n",
            "Epoch 898: Validation loss decreased (272.499237 --> 272.351654).\n",
            "\t Train_Loss: 110.4427 Val_Loss: 272.3517  BEST VAL Loss: 272.3517\n",
            "\n",
            "Epoch 899: Validation loss decreased (272.351654 --> 272.195343).\n",
            "\t Train_Loss: 110.3816 Val_Loss: 272.1953  BEST VAL Loss: 272.1953\n",
            "\n",
            "Epoch 900: Validation loss decreased (272.195343 --> 271.990326).\n",
            "\t Train_Loss: 110.3221 Val_Loss: 271.9903  BEST VAL Loss: 271.9903\n",
            "\n",
            "Epoch 901: Validation loss decreased (271.990326 --> 271.722656).\n",
            "\t Train_Loss: 110.2625 Val_Loss: 271.7227  BEST VAL Loss: 271.7227\n",
            "\n",
            "Epoch 902: Validation loss decreased (271.722656 --> 271.407623).\n",
            "\t Train_Loss: 110.2009 Val_Loss: 271.4076  BEST VAL Loss: 271.4076\n",
            "\n",
            "Epoch 903: Validation loss decreased (271.407623 --> 271.078949).\n",
            "\t Train_Loss: 110.1364 Val_Loss: 271.0789  BEST VAL Loss: 271.0789\n",
            "\n",
            "Epoch 904: Validation loss decreased (271.078949 --> 270.778046).\n",
            "\t Train_Loss: 110.0648 Val_Loss: 270.7780  BEST VAL Loss: 270.7780\n",
            "\n",
            "Epoch 905: Validation loss decreased (270.778046 --> 270.599823).\n",
            "\t Train_Loss: 109.9611 Val_Loss: 270.5998  BEST VAL Loss: 270.5998\n",
            "\n",
            "Epoch 906: Validation loss did not decrease\n",
            "\t Train_Loss: 109.5575 Val_Loss: 289.5284  BEST VAL Loss: 270.5998\n",
            "\n",
            "Epoch 907: Validation loss decreased (270.599823 --> 269.977234).\n",
            "\t Train_Loss: 102.8073 Val_Loss: 269.9772  BEST VAL Loss: 269.9772\n",
            "\n",
            "Epoch 908: Validation loss decreased (269.977234 --> 269.496185).\n",
            "\t Train_Loss: 109.1584 Val_Loss: 269.4962  BEST VAL Loss: 269.4962\n",
            "\n",
            "Epoch 909: Validation loss decreased (269.496185 --> 269.274200).\n",
            "\t Train_Loss: 109.7518 Val_Loss: 269.2742  BEST VAL Loss: 269.2742\n",
            "\n",
            "Epoch 910: Validation loss decreased (269.274200 --> 269.138062).\n",
            "\t Train_Loss: 109.7553 Val_Loss: 269.1381  BEST VAL Loss: 269.1381\n",
            "\n",
            "Epoch 911: Validation loss decreased (269.138062 --> 269.021088).\n",
            "\t Train_Loss: 109.7060 Val_Loss: 269.0211  BEST VAL Loss: 269.0211\n",
            "\n",
            "Epoch 912: Validation loss decreased (269.021088 --> 268.859680).\n",
            "\t Train_Loss: 109.6538 Val_Loss: 268.8597  BEST VAL Loss: 268.8597\n",
            "\n",
            "Epoch 913: Validation loss decreased (268.859680 --> 268.617340).\n",
            "\t Train_Loss: 109.6030 Val_Loss: 268.6173  BEST VAL Loss: 268.6173\n",
            "\n",
            "Epoch 914: Validation loss decreased (268.617340 --> 268.295135).\n",
            "\t Train_Loss: 109.5509 Val_Loss: 268.2951  BEST VAL Loss: 268.2951\n",
            "\n",
            "Epoch 915: Validation loss decreased (268.295135 --> 267.929657).\n",
            "\t Train_Loss: 109.4972 Val_Loss: 267.9297  BEST VAL Loss: 267.9297\n",
            "\n",
            "Epoch 916: Validation loss decreased (267.929657 --> 267.571136).\n",
            "\t Train_Loss: 109.4442 Val_Loss: 267.5711  BEST VAL Loss: 267.5711\n",
            "\n",
            "Epoch 917: Validation loss decreased (267.571136 --> 267.261444).\n",
            "\t Train_Loss: 109.3934 Val_Loss: 267.2614  BEST VAL Loss: 267.2614\n",
            "\n",
            "Epoch 918: Validation loss decreased (267.261444 --> 267.020050).\n",
            "\t Train_Loss: 109.3440 Val_Loss: 267.0201  BEST VAL Loss: 267.0201\n",
            "\n",
            "Epoch 919: Validation loss decreased (267.020050 --> 266.842682).\n",
            "\t Train_Loss: 109.2940 Val_Loss: 266.8427  BEST VAL Loss: 266.8427\n",
            "\n",
            "Epoch 920: Validation loss decreased (266.842682 --> 266.703949).\n",
            "\t Train_Loss: 109.2431 Val_Loss: 266.7039  BEST VAL Loss: 266.7039\n",
            "\n",
            "Epoch 921: Validation loss decreased (266.703949 --> 266.569275).\n",
            "\t Train_Loss: 109.1929 Val_Loss: 266.5693  BEST VAL Loss: 266.5693\n",
            "\n",
            "Epoch 922: Validation loss decreased (266.569275 --> 266.406067).\n",
            "\t Train_Loss: 109.1441 Val_Loss: 266.4061  BEST VAL Loss: 266.4061\n",
            "\n",
            "Epoch 923: Validation loss decreased (266.406067 --> 266.196991).\n",
            "\t Train_Loss: 109.0961 Val_Loss: 266.1970  BEST VAL Loss: 266.1970\n",
            "\n",
            "Epoch 924: Validation loss decreased (266.196991 --> 265.945374).\n",
            "\t Train_Loss: 109.0478 Val_Loss: 265.9454  BEST VAL Loss: 265.9454\n",
            "\n",
            "Epoch 925: Validation loss decreased (265.945374 --> 265.671967).\n",
            "\t Train_Loss: 108.9992 Val_Loss: 265.6720  BEST VAL Loss: 265.6720\n",
            "\n",
            "Epoch 926: Validation loss decreased (265.671967 --> 265.403473).\n",
            "\t Train_Loss: 108.9509 Val_Loss: 265.4035  BEST VAL Loss: 265.4035\n",
            "\n",
            "Epoch 927: Validation loss decreased (265.403473 --> 265.163116).\n",
            "\t Train_Loss: 108.9036 Val_Loss: 265.1631  BEST VAL Loss: 265.1631\n",
            "\n",
            "Epoch 928: Validation loss decreased (265.163116 --> 264.961151).\n",
            "\t Train_Loss: 108.8568 Val_Loss: 264.9612  BEST VAL Loss: 264.9612\n",
            "\n",
            "Epoch 929: Validation loss decreased (264.961151 --> 264.796143).\n",
            "\t Train_Loss: 108.8099 Val_Loss: 264.7961  BEST VAL Loss: 264.7961\n",
            "\n",
            "Epoch 930: Validation loss decreased (264.796143 --> 264.653290).\n",
            "\t Train_Loss: 108.7628 Val_Loss: 264.6533  BEST VAL Loss: 264.6533\n",
            "\n",
            "Epoch 931: Validation loss decreased (264.653290 --> 264.513153).\n",
            "\t Train_Loss: 108.7160 Val_Loss: 264.5132  BEST VAL Loss: 264.5132\n",
            "\n",
            "Epoch 932: Validation loss decreased (264.513153 --> 264.357544).\n",
            "\t Train_Loss: 108.6698 Val_Loss: 264.3575  BEST VAL Loss: 264.3575\n",
            "\n",
            "Epoch 933: Validation loss decreased (264.357544 --> 264.178406).\n",
            "\t Train_Loss: 108.6239 Val_Loss: 264.1784  BEST VAL Loss: 264.1784\n",
            "\n",
            "Epoch 934: Validation loss decreased (264.178406 --> 263.974823).\n",
            "\t Train_Loss: 108.5781 Val_Loss: 263.9748  BEST VAL Loss: 263.9748\n",
            "\n",
            "Epoch 935: Validation loss decreased (263.974823 --> 263.758301).\n",
            "\t Train_Loss: 108.5323 Val_Loss: 263.7583  BEST VAL Loss: 263.7583\n",
            "\n",
            "Epoch 936: Validation loss decreased (263.758301 --> 263.542816).\n",
            "\t Train_Loss: 108.4866 Val_Loss: 263.5428  BEST VAL Loss: 263.5428\n",
            "\n",
            "Epoch 937: Validation loss decreased (263.542816 --> 263.342621).\n",
            "\t Train_Loss: 108.4413 Val_Loss: 263.3426  BEST VAL Loss: 263.3426\n",
            "\n",
            "Epoch 938: Validation loss decreased (263.342621 --> 263.164032).\n",
            "\t Train_Loss: 108.3964 Val_Loss: 263.1640  BEST VAL Loss: 263.1640\n",
            "\n",
            "Epoch 939: Validation loss decreased (263.164032 --> 263.007507).\n",
            "\t Train_Loss: 108.3514 Val_Loss: 263.0075  BEST VAL Loss: 263.0075\n",
            "\n",
            "Epoch 940: Validation loss decreased (263.007507 --> 262.865570).\n",
            "\t Train_Loss: 108.3066 Val_Loss: 262.8656  BEST VAL Loss: 262.8656\n",
            "\n",
            "Epoch 941: Validation loss decreased (262.865570 --> 262.727234).\n",
            "\t Train_Loss: 108.2618 Val_Loss: 262.7272  BEST VAL Loss: 262.7272\n",
            "\n",
            "Epoch 942: Validation loss decreased (262.727234 --> 262.583221).\n",
            "\t Train_Loss: 108.2174 Val_Loss: 262.5832  BEST VAL Loss: 262.5832\n",
            "\n",
            "Epoch 943: Validation loss decreased (262.583221 --> 262.426575).\n",
            "\t Train_Loss: 108.1731 Val_Loss: 262.4266  BEST VAL Loss: 262.4266\n",
            "\n",
            "Epoch 944: Validation loss decreased (262.426575 --> 262.256409).\n",
            "\t Train_Loss: 108.1290 Val_Loss: 262.2564  BEST VAL Loss: 262.2564\n",
            "\n",
            "Epoch 945: Validation loss decreased (262.256409 --> 262.077606).\n",
            "\t Train_Loss: 108.0850 Val_Loss: 262.0776  BEST VAL Loss: 262.0776\n",
            "\n",
            "Epoch 946: Validation loss decreased (262.077606 --> 261.898407).\n",
            "\t Train_Loss: 108.0411 Val_Loss: 261.8984  BEST VAL Loss: 261.8984\n",
            "\n",
            "Epoch 947: Validation loss decreased (261.898407 --> 261.726349).\n",
            "\t Train_Loss: 107.9974 Val_Loss: 261.7263  BEST VAL Loss: 261.7263\n",
            "\n",
            "Epoch 948: Validation loss decreased (261.726349 --> 261.565979).\n",
            "\t Train_Loss: 107.9539 Val_Loss: 261.5660  BEST VAL Loss: 261.5660\n",
            "\n",
            "Epoch 949: Validation loss decreased (261.565979 --> 261.419189).\n",
            "\t Train_Loss: 107.9104 Val_Loss: 261.4192  BEST VAL Loss: 261.4192\n",
            "\n",
            "Epoch 950: Validation loss decreased (261.419189 --> 261.282440).\n",
            "\t Train_Loss: 107.8671 Val_Loss: 261.2824  BEST VAL Loss: 261.2824\n",
            "\n",
            "Epoch 951: Validation loss decreased (261.282440 --> 261.150482).\n",
            "\t Train_Loss: 107.8240 Val_Loss: 261.1505  BEST VAL Loss: 261.1505\n",
            "\n",
            "Epoch 952: Validation loss decreased (261.150482 --> 261.017487).\n",
            "\t Train_Loss: 107.7809 Val_Loss: 261.0175  BEST VAL Loss: 261.0175\n",
            "\n",
            "Epoch 953: Validation loss decreased (261.017487 --> 260.878387).\n",
            "\t Train_Loss: 107.7380 Val_Loss: 260.8784  BEST VAL Loss: 260.8784\n",
            "\n",
            "Epoch 954: Validation loss decreased (260.878387 --> 260.732330).\n",
            "\t Train_Loss: 107.6953 Val_Loss: 260.7323  BEST VAL Loss: 260.7323\n",
            "\n",
            "Epoch 955: Validation loss decreased (260.732330 --> 260.580841).\n",
            "\t Train_Loss: 107.6526 Val_Loss: 260.5808  BEST VAL Loss: 260.5808\n",
            "\n",
            "Epoch 956: Validation loss decreased (260.580841 --> 260.427826).\n",
            "\t Train_Loss: 107.6101 Val_Loss: 260.4278  BEST VAL Loss: 260.4278\n",
            "\n",
            "Epoch 957: Validation loss decreased (260.427826 --> 260.277557).\n",
            "\t Train_Loss: 107.5677 Val_Loss: 260.2776  BEST VAL Loss: 260.2776\n",
            "\n",
            "Epoch 958: Validation loss decreased (260.277557 --> 260.133881).\n",
            "\t Train_Loss: 107.5254 Val_Loss: 260.1339  BEST VAL Loss: 260.1339\n",
            "\n",
            "Epoch 959: Validation loss decreased (260.133881 --> 259.997864).\n",
            "\t Train_Loss: 107.4832 Val_Loss: 259.9979  BEST VAL Loss: 259.9979\n",
            "\n",
            "Epoch 960: Validation loss decreased (259.997864 --> 259.869232).\n",
            "\t Train_Loss: 107.4411 Val_Loss: 259.8692  BEST VAL Loss: 259.8692\n",
            "\n",
            "Epoch 961: Validation loss decreased (259.869232 --> 259.745087).\n",
            "\t Train_Loss: 107.3992 Val_Loss: 259.7451  BEST VAL Loss: 259.7451\n",
            "\n",
            "Epoch 962: Validation loss decreased (259.745087 --> 259.622223).\n",
            "\t Train_Loss: 107.3573 Val_Loss: 259.6222  BEST VAL Loss: 259.6222\n",
            "\n",
            "Epoch 963: Validation loss decreased (259.622223 --> 259.497131).\n",
            "\t Train_Loss: 107.3156 Val_Loss: 259.4971  BEST VAL Loss: 259.4971\n",
            "\n",
            "Epoch 964: Validation loss decreased (259.497131 --> 259.368744).\n",
            "\t Train_Loss: 107.2739 Val_Loss: 259.3687  BEST VAL Loss: 259.3687\n",
            "\n",
            "Epoch 965: Validation loss decreased (259.368744 --> 259.237274).\n",
            "\t Train_Loss: 107.2323 Val_Loss: 259.2373  BEST VAL Loss: 259.2373\n",
            "\n",
            "Epoch 966: Validation loss decreased (259.237274 --> 259.104004).\n",
            "\t Train_Loss: 107.1909 Val_Loss: 259.1040  BEST VAL Loss: 259.1040\n",
            "\n",
            "Epoch 967: Validation loss decreased (259.104004 --> 258.971558).\n",
            "\t Train_Loss: 107.1495 Val_Loss: 258.9716  BEST VAL Loss: 258.9716\n",
            "\n",
            "Epoch 968: Validation loss decreased (258.971558 --> 258.842621).\n",
            "\t Train_Loss: 107.1082 Val_Loss: 258.8426  BEST VAL Loss: 258.8426\n",
            "\n",
            "Epoch 969: Validation loss decreased (258.842621 --> 258.717804).\n",
            "\t Train_Loss: 107.0670 Val_Loss: 258.7178  BEST VAL Loss: 258.7178\n",
            "\n",
            "Epoch 970: Validation loss decreased (258.717804 --> 258.598114).\n",
            "\t Train_Loss: 107.0260 Val_Loss: 258.5981  BEST VAL Loss: 258.5981\n",
            "\n",
            "Epoch 971: Validation loss decreased (258.598114 --> 258.481995).\n",
            "\t Train_Loss: 106.9849 Val_Loss: 258.4820  BEST VAL Loss: 258.4820\n",
            "\n",
            "Epoch 972: Validation loss decreased (258.481995 --> 258.368225).\n",
            "\t Train_Loss: 106.9440 Val_Loss: 258.3682  BEST VAL Loss: 258.3682\n",
            "\n",
            "Epoch 973: Validation loss decreased (258.368225 --> 258.254120).\n",
            "\t Train_Loss: 106.9032 Val_Loss: 258.2541  BEST VAL Loss: 258.2541\n",
            "\n",
            "Epoch 974: Validation loss decreased (258.254120 --> 258.139008).\n",
            "\t Train_Loss: 106.8624 Val_Loss: 258.1390  BEST VAL Loss: 258.1390\n",
            "\n",
            "Epoch 975: Validation loss decreased (258.139008 --> 258.021973).\n",
            "\t Train_Loss: 106.8217 Val_Loss: 258.0220  BEST VAL Loss: 258.0220\n",
            "\n",
            "Epoch 976: Validation loss decreased (258.021973 --> 257.903778).\n",
            "\t Train_Loss: 106.7811 Val_Loss: 257.9038  BEST VAL Loss: 257.9038\n",
            "\n",
            "Epoch 977: Validation loss decreased (257.903778 --> 257.785736).\n",
            "\t Train_Loss: 106.7406 Val_Loss: 257.7857  BEST VAL Loss: 257.7857\n",
            "\n",
            "Epoch 978: Validation loss decreased (257.785736 --> 257.669281).\n",
            "\t Train_Loss: 106.7002 Val_Loss: 257.6693  BEST VAL Loss: 257.6693\n",
            "\n",
            "Epoch 979: Validation loss decreased (257.669281 --> 257.555237).\n",
            "\t Train_Loss: 106.6598 Val_Loss: 257.5552  BEST VAL Loss: 257.5552\n",
            "\n",
            "Epoch 980: Validation loss decreased (257.555237 --> 257.444000).\n",
            "\t Train_Loss: 106.6195 Val_Loss: 257.4440  BEST VAL Loss: 257.4440\n",
            "\n",
            "Epoch 981: Validation loss decreased (257.444000 --> 257.335632).\n",
            "\t Train_Loss: 106.5793 Val_Loss: 257.3356  BEST VAL Loss: 257.3356\n",
            "\n",
            "Epoch 982: Validation loss decreased (257.335632 --> 257.229340).\n",
            "\t Train_Loss: 106.5391 Val_Loss: 257.2293  BEST VAL Loss: 257.2293\n",
            "\n",
            "Epoch 983: Validation loss decreased (257.229340 --> 257.123962).\n",
            "\t Train_Loss: 106.4990 Val_Loss: 257.1240  BEST VAL Loss: 257.1240\n",
            "\n",
            "Epoch 984: Validation loss decreased (257.123962 --> 257.018463).\n",
            "\t Train_Loss: 106.4590 Val_Loss: 257.0185  BEST VAL Loss: 257.0185\n",
            "\n",
            "Epoch 985: Validation loss decreased (257.018463 --> 256.912933).\n",
            "\t Train_Loss: 106.4191 Val_Loss: 256.9129  BEST VAL Loss: 256.9129\n",
            "\n",
            "Epoch 986: Validation loss decreased (256.912933 --> 256.805908).\n",
            "\t Train_Loss: 106.3792 Val_Loss: 256.8059  BEST VAL Loss: 256.8059\n",
            "\n",
            "Epoch 987: Validation loss decreased (256.805908 --> 256.699005).\n",
            "\t Train_Loss: 106.3393 Val_Loss: 256.6990  BEST VAL Loss: 256.6990\n",
            "\n",
            "Epoch 988: Validation loss decreased (256.699005 --> 256.593018).\n",
            "\t Train_Loss: 106.2995 Val_Loss: 256.5930  BEST VAL Loss: 256.5930\n",
            "\n",
            "Epoch 989: Validation loss decreased (256.593018 --> 256.487854).\n",
            "\t Train_Loss: 106.2599 Val_Loss: 256.4879  BEST VAL Loss: 256.4879\n",
            "\n",
            "Epoch 990: Validation loss decreased (256.487854 --> 256.384918).\n",
            "\t Train_Loss: 106.2202 Val_Loss: 256.3849  BEST VAL Loss: 256.3849\n",
            "\n",
            "Epoch 991: Validation loss decreased (256.384918 --> 256.283447).\n",
            "\t Train_Loss: 106.1806 Val_Loss: 256.2834  BEST VAL Loss: 256.2834\n",
            "\n",
            "Epoch 992: Validation loss decreased (256.283447 --> 256.183929).\n",
            "\t Train_Loss: 106.1411 Val_Loss: 256.1839  BEST VAL Loss: 256.1839\n",
            "\n",
            "Epoch 993: Validation loss decreased (256.183929 --> 256.085327).\n",
            "\t Train_Loss: 106.1016 Val_Loss: 256.0853  BEST VAL Loss: 256.0853\n",
            "\n",
            "Epoch 994: Validation loss decreased (256.085327 --> 255.987656).\n",
            "\t Train_Loss: 106.0622 Val_Loss: 255.9877  BEST VAL Loss: 255.9877\n",
            "\n",
            "Epoch 995: Validation loss decreased (255.987656 --> 255.889603).\n",
            "\t Train_Loss: 106.0229 Val_Loss: 255.8896  BEST VAL Loss: 255.8896\n",
            "\n",
            "Epoch 996: Validation loss decreased (255.889603 --> 255.791626).\n",
            "\t Train_Loss: 105.9836 Val_Loss: 255.7916  BEST VAL Loss: 255.7916\n",
            "\n",
            "Epoch 997: Validation loss decreased (255.791626 --> 255.693558).\n",
            "\t Train_Loss: 105.9443 Val_Loss: 255.6936  BEST VAL Loss: 255.6936\n",
            "\n",
            "Epoch 998: Validation loss decreased (255.693558 --> 255.595215).\n",
            "\t Train_Loss: 105.9052 Val_Loss: 255.5952  BEST VAL Loss: 255.5952\n",
            "\n",
            "Epoch 999: Validation loss decreased (255.595215 --> 255.498169).\n",
            "\t Train_Loss: 105.8660 Val_Loss: 255.4982  BEST VAL Loss: 255.4982\n",
            "\n",
            "Epoch 1000: Validation loss decreased (255.498169 --> 255.401932).\n",
            "\t Train_Loss: 105.8269 Val_Loss: 255.4019  BEST VAL Loss: 255.4019\n",
            "\n",
            "Epoch 1001: Validation loss decreased (255.401932 --> 255.307083).\n",
            "\t Train_Loss: 105.7879 Val_Loss: 255.3071  BEST VAL Loss: 255.3071\n",
            "\n",
            "Epoch 1002: Validation loss decreased (255.307083 --> 255.213181).\n",
            "\t Train_Loss: 105.7489 Val_Loss: 255.2132  BEST VAL Loss: 255.2132\n",
            "\n",
            "Epoch 1003: Validation loss decreased (255.213181 --> 255.120636).\n",
            "\t Train_Loss: 105.7099 Val_Loss: 255.1206  BEST VAL Loss: 255.1206\n",
            "\n",
            "Epoch 1004: Validation loss decreased (255.120636 --> 255.028397).\n",
            "\t Train_Loss: 105.6710 Val_Loss: 255.0284  BEST VAL Loss: 255.0284\n",
            "\n",
            "Epoch 1005: Validation loss decreased (255.028397 --> 254.936630).\n",
            "\t Train_Loss: 105.6322 Val_Loss: 254.9366  BEST VAL Loss: 254.9366\n",
            "\n",
            "Epoch 1006: Validation loss decreased (254.936630 --> 254.845169).\n",
            "\t Train_Loss: 105.5934 Val_Loss: 254.8452  BEST VAL Loss: 254.8452\n",
            "\n",
            "Epoch 1007: Validation loss decreased (254.845169 --> 254.753662).\n",
            "\t Train_Loss: 105.5546 Val_Loss: 254.7537  BEST VAL Loss: 254.7537\n",
            "\n",
            "Epoch 1008: Validation loss decreased (254.753662 --> 254.662506).\n",
            "\t Train_Loss: 105.5159 Val_Loss: 254.6625  BEST VAL Loss: 254.6625\n",
            "\n",
            "Epoch 1009: Validation loss decreased (254.662506 --> 254.572021).\n",
            "\t Train_Loss: 105.4772 Val_Loss: 254.5720  BEST VAL Loss: 254.5720\n",
            "\n",
            "Epoch 1010: Validation loss decreased (254.572021 --> 254.481155).\n",
            "\t Train_Loss: 105.4386 Val_Loss: 254.4812  BEST VAL Loss: 254.4812\n",
            "\n",
            "Epoch 1011: Validation loss decreased (254.481155 --> 254.391647).\n",
            "\t Train_Loss: 105.4000 Val_Loss: 254.3916  BEST VAL Loss: 254.3916\n",
            "\n",
            "Epoch 1012: Validation loss decreased (254.391647 --> 254.303085).\n",
            "\t Train_Loss: 105.3615 Val_Loss: 254.3031  BEST VAL Loss: 254.3031\n",
            "\n",
            "Epoch 1013: Validation loss decreased (254.303085 --> 254.215210).\n",
            "\t Train_Loss: 105.3230 Val_Loss: 254.2152  BEST VAL Loss: 254.2152\n",
            "\n",
            "Epoch 1014: Validation loss decreased (254.215210 --> 254.127838).\n",
            "\t Train_Loss: 105.2845 Val_Loss: 254.1278  BEST VAL Loss: 254.1278\n",
            "\n",
            "Epoch 1015: Validation loss decreased (254.127838 --> 254.041016).\n",
            "\t Train_Loss: 105.2461 Val_Loss: 254.0410  BEST VAL Loss: 254.0410\n",
            "\n",
            "Epoch 1016: Validation loss decreased (254.041016 --> 253.955185).\n",
            "\t Train_Loss: 105.2077 Val_Loss: 253.9552  BEST VAL Loss: 253.9552\n",
            "\n",
            "Epoch 1017: Validation loss decreased (253.955185 --> 253.868561).\n",
            "\t Train_Loss: 105.1694 Val_Loss: 253.8686  BEST VAL Loss: 253.8686\n",
            "\n",
            "Epoch 1018: Validation loss decreased (253.868561 --> 253.782822).\n",
            "\t Train_Loss: 105.1311 Val_Loss: 253.7828  BEST VAL Loss: 253.7828\n",
            "\n",
            "Epoch 1019: Validation loss decreased (253.782822 --> 253.696823).\n",
            "\t Train_Loss: 105.0928 Val_Loss: 253.6968  BEST VAL Loss: 253.6968\n",
            "\n",
            "Epoch 1020: Validation loss decreased (253.696823 --> 253.611740).\n",
            "\t Train_Loss: 105.0546 Val_Loss: 253.6117  BEST VAL Loss: 253.6117\n",
            "\n",
            "Epoch 1021: Validation loss decreased (253.611740 --> 253.526779).\n",
            "\t Train_Loss: 105.0164 Val_Loss: 253.5268  BEST VAL Loss: 253.5268\n",
            "\n",
            "Epoch 1022: Validation loss decreased (253.526779 --> 253.442413).\n",
            "\t Train_Loss: 104.9783 Val_Loss: 253.4424  BEST VAL Loss: 253.4424\n",
            "\n",
            "Epoch 1023: Validation loss decreased (253.442413 --> 253.358398).\n",
            "\t Train_Loss: 104.9401 Val_Loss: 253.3584  BEST VAL Loss: 253.3584\n",
            "\n",
            "Epoch 1024: Validation loss decreased (253.358398 --> 253.275681).\n",
            "\t Train_Loss: 104.9020 Val_Loss: 253.2757  BEST VAL Loss: 253.2757\n",
            "\n",
            "Epoch 1025: Validation loss decreased (253.275681 --> 253.192902).\n",
            "\t Train_Loss: 104.8640 Val_Loss: 253.1929  BEST VAL Loss: 253.1929\n",
            "\n",
            "Epoch 1026: Validation loss decreased (253.192902 --> 253.110504).\n",
            "\t Train_Loss: 104.8260 Val_Loss: 253.1105  BEST VAL Loss: 253.1105\n",
            "\n",
            "Epoch 1027: Validation loss decreased (253.110504 --> 253.028641).\n",
            "\t Train_Loss: 104.7880 Val_Loss: 253.0286  BEST VAL Loss: 253.0286\n",
            "\n",
            "Epoch 1028: Validation loss decreased (253.028641 --> 252.946930).\n",
            "\t Train_Loss: 104.7501 Val_Loss: 252.9469  BEST VAL Loss: 252.9469\n",
            "\n",
            "Epoch 1029: Validation loss decreased (252.946930 --> 252.865479).\n",
            "\t Train_Loss: 104.7121 Val_Loss: 252.8655  BEST VAL Loss: 252.8655\n",
            "\n",
            "Epoch 1030: Validation loss decreased (252.865479 --> 252.783936).\n",
            "\t Train_Loss: 104.6743 Val_Loss: 252.7839  BEST VAL Loss: 252.7839\n",
            "\n",
            "Epoch 1031: Validation loss decreased (252.783936 --> 252.703415).\n",
            "\t Train_Loss: 104.6364 Val_Loss: 252.7034  BEST VAL Loss: 252.7034\n",
            "\n",
            "Epoch 1032: Validation loss decreased (252.703415 --> 252.622711).\n",
            "\t Train_Loss: 104.5986 Val_Loss: 252.6227  BEST VAL Loss: 252.6227\n",
            "\n",
            "Epoch 1033: Validation loss decreased (252.622711 --> 252.542557).\n",
            "\t Train_Loss: 104.5608 Val_Loss: 252.5426  BEST VAL Loss: 252.5426\n",
            "\n",
            "Epoch 1034: Validation loss decreased (252.542557 --> 252.462799).\n",
            "\t Train_Loss: 104.5231 Val_Loss: 252.4628  BEST VAL Loss: 252.4628\n",
            "\n",
            "Epoch 1035: Validation loss decreased (252.462799 --> 252.383942).\n",
            "\t Train_Loss: 104.4854 Val_Loss: 252.3839  BEST VAL Loss: 252.3839\n",
            "\n",
            "Epoch 1036: Validation loss decreased (252.383942 --> 252.304977).\n",
            "\t Train_Loss: 104.4477 Val_Loss: 252.3050  BEST VAL Loss: 252.3050\n",
            "\n",
            "Epoch 1037: Validation loss decreased (252.304977 --> 252.226364).\n",
            "\t Train_Loss: 104.4100 Val_Loss: 252.2264  BEST VAL Loss: 252.2264\n",
            "\n",
            "Epoch 1038: Validation loss decreased (252.226364 --> 252.148239).\n",
            "\t Train_Loss: 104.3724 Val_Loss: 252.1482  BEST VAL Loss: 252.1482\n",
            "\n",
            "Epoch 1039: Validation loss decreased (252.148239 --> 252.070236).\n",
            "\t Train_Loss: 104.3348 Val_Loss: 252.0702  BEST VAL Loss: 252.0702\n",
            "\n",
            "Epoch 1040: Validation loss decreased (252.070236 --> 251.992218).\n",
            "\t Train_Loss: 104.2971 Val_Loss: 251.9922  BEST VAL Loss: 251.9922\n",
            "\n",
            "Epoch 1041: Validation loss decreased (251.992218 --> 251.914307).\n",
            "\t Train_Loss: 104.2596 Val_Loss: 251.9143  BEST VAL Loss: 251.9143\n",
            "\n",
            "Epoch 1042: Validation loss decreased (251.914307 --> 251.837204).\n",
            "\t Train_Loss: 104.2221 Val_Loss: 251.8372  BEST VAL Loss: 251.8372\n",
            "\n",
            "Epoch 1043: Validation loss decreased (251.837204 --> 251.760010).\n",
            "\t Train_Loss: 104.1846 Val_Loss: 251.7600  BEST VAL Loss: 251.7600\n",
            "\n",
            "Epoch 1044: Validation loss decreased (251.760010 --> 251.683380).\n",
            "\t Train_Loss: 104.1471 Val_Loss: 251.6834  BEST VAL Loss: 251.6834\n",
            "\n",
            "Epoch 1045: Validation loss decreased (251.683380 --> 251.606888).\n",
            "\t Train_Loss: 104.1097 Val_Loss: 251.6069  BEST VAL Loss: 251.6069\n",
            "\n",
            "Epoch 1046: Validation loss decreased (251.606888 --> 251.531036).\n",
            "\t Train_Loss: 104.0723 Val_Loss: 251.5310  BEST VAL Loss: 251.5310\n",
            "\n",
            "Epoch 1047: Validation loss decreased (251.531036 --> 251.455231).\n",
            "\t Train_Loss: 104.0349 Val_Loss: 251.4552  BEST VAL Loss: 251.4552\n",
            "\n",
            "Epoch 1048: Validation loss decreased (251.455231 --> 251.379761).\n",
            "\t Train_Loss: 103.9976 Val_Loss: 251.3798  BEST VAL Loss: 251.3798\n",
            "\n",
            "Epoch 1049: Validation loss decreased (251.379761 --> 251.304688).\n",
            "\t Train_Loss: 103.9602 Val_Loss: 251.3047  BEST VAL Loss: 251.3047\n",
            "\n",
            "Epoch 1050: Validation loss decreased (251.304688 --> 251.229370).\n",
            "\t Train_Loss: 103.9229 Val_Loss: 251.2294  BEST VAL Loss: 251.2294\n",
            "\n",
            "Epoch 1051: Validation loss decreased (251.229370 --> 251.154449).\n",
            "\t Train_Loss: 103.8856 Val_Loss: 251.1544  BEST VAL Loss: 251.1544\n",
            "\n",
            "Epoch 1052: Validation loss decreased (251.154449 --> 251.079590).\n",
            "\t Train_Loss: 103.8484 Val_Loss: 251.0796  BEST VAL Loss: 251.0796\n",
            "\n",
            "Epoch 1053: Validation loss decreased (251.079590 --> 251.005035).\n",
            "\t Train_Loss: 103.8111 Val_Loss: 251.0050  BEST VAL Loss: 251.0050\n",
            "\n",
            "Epoch 1054: Validation loss decreased (251.005035 --> 250.930939).\n",
            "\t Train_Loss: 103.7739 Val_Loss: 250.9309  BEST VAL Loss: 250.9309\n",
            "\n",
            "Epoch 1055: Validation loss decreased (250.930939 --> 250.857132).\n",
            "\t Train_Loss: 103.7367 Val_Loss: 250.8571  BEST VAL Loss: 250.8571\n",
            "\n",
            "Epoch 1056: Validation loss decreased (250.857132 --> 250.783112).\n",
            "\t Train_Loss: 103.6996 Val_Loss: 250.7831  BEST VAL Loss: 250.7831\n",
            "\n",
            "Epoch 1057: Validation loss decreased (250.783112 --> 250.710007).\n",
            "\t Train_Loss: 103.6625 Val_Loss: 250.7100  BEST VAL Loss: 250.7100\n",
            "\n",
            "Epoch 1058: Validation loss decreased (250.710007 --> 250.636337).\n",
            "\t Train_Loss: 103.6254 Val_Loss: 250.6363  BEST VAL Loss: 250.6363\n",
            "\n",
            "Epoch 1059: Validation loss decreased (250.636337 --> 250.563583).\n",
            "\t Train_Loss: 103.5883 Val_Loss: 250.5636  BEST VAL Loss: 250.5636\n",
            "\n",
            "Epoch 1060: Validation loss decreased (250.563583 --> 250.490768).\n",
            "\t Train_Loss: 103.5512 Val_Loss: 250.4908  BEST VAL Loss: 250.4908\n",
            "\n",
            "Epoch 1061: Validation loss decreased (250.490768 --> 250.417633).\n",
            "\t Train_Loss: 103.5142 Val_Loss: 250.4176  BEST VAL Loss: 250.4176\n",
            "\n",
            "Epoch 1062: Validation loss decreased (250.417633 --> 250.345322).\n",
            "\t Train_Loss: 103.4771 Val_Loss: 250.3453  BEST VAL Loss: 250.3453\n",
            "\n",
            "Epoch 1063: Validation loss decreased (250.345322 --> 250.272858).\n",
            "\t Train_Loss: 103.4401 Val_Loss: 250.2729  BEST VAL Loss: 250.2729\n",
            "\n",
            "Epoch 1064: Validation loss decreased (250.272858 --> 250.200836).\n",
            "\t Train_Loss: 103.4032 Val_Loss: 250.2008  BEST VAL Loss: 250.2008\n",
            "\n",
            "Epoch 1065: Validation loss decreased (250.200836 --> 250.128906).\n",
            "\t Train_Loss: 103.3662 Val_Loss: 250.1289  BEST VAL Loss: 250.1289\n",
            "\n",
            "Epoch 1066: Validation loss decreased (250.128906 --> 250.057037).\n",
            "\t Train_Loss: 103.3293 Val_Loss: 250.0570  BEST VAL Loss: 250.0570\n",
            "\n",
            "Epoch 1067: Validation loss decreased (250.057037 --> 249.985474).\n",
            "\t Train_Loss: 103.2924 Val_Loss: 249.9855  BEST VAL Loss: 249.9855\n",
            "\n",
            "Epoch 1068: Validation loss decreased (249.985474 --> 249.914062).\n",
            "\t Train_Loss: 103.2554 Val_Loss: 249.9141  BEST VAL Loss: 249.9141\n",
            "\n",
            "Epoch 1069: Validation loss decreased (249.914062 --> 249.842636).\n",
            "\t Train_Loss: 103.2186 Val_Loss: 249.8426  BEST VAL Loss: 249.8426\n",
            "\n",
            "Epoch 1070: Validation loss decreased (249.842636 --> 249.771591).\n",
            "\t Train_Loss: 103.1818 Val_Loss: 249.7716  BEST VAL Loss: 249.7716\n",
            "\n",
            "Epoch 1071: Validation loss decreased (249.771591 --> 249.701096).\n",
            "\t Train_Loss: 103.1449 Val_Loss: 249.7011  BEST VAL Loss: 249.7011\n",
            "\n",
            "Epoch 1072: Validation loss decreased (249.701096 --> 249.630035).\n",
            "\t Train_Loss: 103.1081 Val_Loss: 249.6300  BEST VAL Loss: 249.6300\n",
            "\n",
            "Epoch 1073: Validation loss decreased (249.630035 --> 249.558884).\n",
            "\t Train_Loss: 103.0713 Val_Loss: 249.5589  BEST VAL Loss: 249.5589\n",
            "\n",
            "Epoch 1074: Validation loss decreased (249.558884 --> 249.488815).\n",
            "\t Train_Loss: 103.0345 Val_Loss: 249.4888  BEST VAL Loss: 249.4888\n",
            "\n",
            "Epoch 1075: Validation loss decreased (249.488815 --> 249.418243).\n",
            "\t Train_Loss: 102.9978 Val_Loss: 249.4182  BEST VAL Loss: 249.4182\n",
            "\n",
            "Epoch 1076: Validation loss decreased (249.418243 --> 249.348099).\n",
            "\t Train_Loss: 102.9611 Val_Loss: 249.3481  BEST VAL Loss: 249.3481\n",
            "\n",
            "Epoch 1077: Validation loss decreased (249.348099 --> 249.278030).\n",
            "\t Train_Loss: 102.9244 Val_Loss: 249.2780  BEST VAL Loss: 249.2780\n",
            "\n",
            "Epoch 1078: Validation loss decreased (249.278030 --> 249.208282).\n",
            "\t Train_Loss: 102.8877 Val_Loss: 249.2083  BEST VAL Loss: 249.2083\n",
            "\n",
            "Epoch 1079: Validation loss decreased (249.208282 --> 249.138306).\n",
            "\t Train_Loss: 102.8510 Val_Loss: 249.1383  BEST VAL Loss: 249.1383\n",
            "\n",
            "Epoch 1080: Validation loss decreased (249.138306 --> 249.068771).\n",
            "\t Train_Loss: 102.8143 Val_Loss: 249.0688  BEST VAL Loss: 249.0688\n",
            "\n",
            "Epoch 1081: Validation loss decreased (249.068771 --> 248.999313).\n",
            "\t Train_Loss: 102.7777 Val_Loss: 248.9993  BEST VAL Loss: 248.9993\n",
            "\n",
            "Epoch 1082: Validation loss decreased (248.999313 --> 248.929916).\n",
            "\t Train_Loss: 102.7411 Val_Loss: 248.9299  BEST VAL Loss: 248.9299\n",
            "\n",
            "Epoch 1083: Validation loss decreased (248.929916 --> 248.860641).\n",
            "\t Train_Loss: 102.7045 Val_Loss: 248.8606  BEST VAL Loss: 248.8606\n",
            "\n",
            "Epoch 1084: Validation loss decreased (248.860641 --> 248.791748).\n",
            "\t Train_Loss: 102.6680 Val_Loss: 248.7917  BEST VAL Loss: 248.7917\n",
            "\n",
            "Epoch 1085: Validation loss decreased (248.791748 --> 248.722687).\n",
            "\t Train_Loss: 102.6314 Val_Loss: 248.7227  BEST VAL Loss: 248.7227\n",
            "\n",
            "Epoch 1086: Validation loss decreased (248.722687 --> 248.653763).\n",
            "\t Train_Loss: 102.5949 Val_Loss: 248.6538  BEST VAL Loss: 248.6538\n",
            "\n",
            "Epoch 1087: Validation loss decreased (248.653763 --> 248.585159).\n",
            "\t Train_Loss: 102.5584 Val_Loss: 248.5852  BEST VAL Loss: 248.5852\n",
            "\n",
            "Epoch 1088: Validation loss decreased (248.585159 --> 248.516556).\n",
            "\t Train_Loss: 102.5218 Val_Loss: 248.5166  BEST VAL Loss: 248.5166\n",
            "\n",
            "Epoch 1089: Validation loss decreased (248.516556 --> 248.448074).\n",
            "\t Train_Loss: 102.4854 Val_Loss: 248.4481  BEST VAL Loss: 248.4481\n",
            "\n",
            "Epoch 1090: Validation loss decreased (248.448074 --> 248.379639).\n",
            "\t Train_Loss: 102.4489 Val_Loss: 248.3796  BEST VAL Loss: 248.3796\n",
            "\n",
            "Epoch 1091: Validation loss decreased (248.379639 --> 248.311279).\n",
            "\t Train_Loss: 102.4124 Val_Loss: 248.3113  BEST VAL Loss: 248.3113\n",
            "\n",
            "Epoch 1092: Validation loss decreased (248.311279 --> 248.243118).\n",
            "\t Train_Loss: 102.3760 Val_Loss: 248.2431  BEST VAL Loss: 248.2431\n",
            "\n",
            "Epoch 1093: Validation loss decreased (248.243118 --> 248.174805).\n",
            "\t Train_Loss: 102.3396 Val_Loss: 248.1748  BEST VAL Loss: 248.1748\n",
            "\n",
            "Epoch 1094: Validation loss decreased (248.174805 --> 248.106888).\n",
            "\t Train_Loss: 102.3032 Val_Loss: 248.1069  BEST VAL Loss: 248.1069\n",
            "\n",
            "Epoch 1095: Validation loss decreased (248.106888 --> 248.039047).\n",
            "\t Train_Loss: 102.2668 Val_Loss: 248.0390  BEST VAL Loss: 248.0390\n",
            "\n",
            "Epoch 1096: Validation loss decreased (248.039047 --> 247.971115).\n",
            "\t Train_Loss: 102.2305 Val_Loss: 247.9711  BEST VAL Loss: 247.9711\n",
            "\n",
            "Epoch 1097: Validation loss decreased (247.971115 --> 247.903610).\n",
            "\t Train_Loss: 102.1941 Val_Loss: 247.9036  BEST VAL Loss: 247.9036\n",
            "\n",
            "Epoch 1098: Validation loss decreased (247.903610 --> 247.835846).\n",
            "\t Train_Loss: 102.1578 Val_Loss: 247.8358  BEST VAL Loss: 247.8358\n",
            "\n",
            "Epoch 1099: Validation loss decreased (247.835846 --> 247.768356).\n",
            "\t Train_Loss: 102.1214 Val_Loss: 247.7684  BEST VAL Loss: 247.7684\n",
            "\n",
            "Epoch 1100: Validation loss decreased (247.768356 --> 247.701218).\n",
            "\t Train_Loss: 102.0851 Val_Loss: 247.7012  BEST VAL Loss: 247.7012\n",
            "\n",
            "Epoch 1101: Validation loss decreased (247.701218 --> 247.633698).\n",
            "\t Train_Loss: 102.0489 Val_Loss: 247.6337  BEST VAL Loss: 247.6337\n",
            "\n",
            "Epoch 1102: Validation loss decreased (247.633698 --> 247.566437).\n",
            "\t Train_Loss: 102.0126 Val_Loss: 247.5664  BEST VAL Loss: 247.5664\n",
            "\n",
            "Epoch 1103: Validation loss decreased (247.566437 --> 247.499512).\n",
            "\t Train_Loss: 101.9763 Val_Loss: 247.4995  BEST VAL Loss: 247.4995\n",
            "\n",
            "Epoch 1104: Validation loss decreased (247.499512 --> 247.432159).\n",
            "\t Train_Loss: 101.9401 Val_Loss: 247.4322  BEST VAL Loss: 247.4322\n",
            "\n",
            "Epoch 1105: Validation loss decreased (247.432159 --> 247.365265).\n",
            "\t Train_Loss: 101.9039 Val_Loss: 247.3653  BEST VAL Loss: 247.3653\n",
            "\n",
            "Epoch 1106: Validation loss decreased (247.365265 --> 247.298218).\n",
            "\t Train_Loss: 101.8677 Val_Loss: 247.2982  BEST VAL Loss: 247.2982\n",
            "\n",
            "Epoch 1107: Validation loss decreased (247.298218 --> 247.231308).\n",
            "\t Train_Loss: 101.8315 Val_Loss: 247.2313  BEST VAL Loss: 247.2313\n",
            "\n",
            "Epoch 1108: Validation loss decreased (247.231308 --> 247.164871).\n",
            "\t Train_Loss: 101.7953 Val_Loss: 247.1649  BEST VAL Loss: 247.1649\n",
            "\n",
            "Epoch 1109: Validation loss decreased (247.164871 --> 247.097977).\n",
            "\t Train_Loss: 101.7592 Val_Loss: 247.0980  BEST VAL Loss: 247.0980\n",
            "\n",
            "Epoch 1110: Validation loss decreased (247.097977 --> 247.031418).\n",
            "\t Train_Loss: 101.7230 Val_Loss: 247.0314  BEST VAL Loss: 247.0314\n",
            "\n",
            "Epoch 1111: Validation loss decreased (247.031418 --> 246.964798).\n",
            "\t Train_Loss: 101.6869 Val_Loss: 246.9648  BEST VAL Loss: 246.9648\n",
            "\n",
            "Epoch 1112: Validation loss decreased (246.964798 --> 246.898300).\n",
            "\t Train_Loss: 101.6508 Val_Loss: 246.8983  BEST VAL Loss: 246.8983\n",
            "\n",
            "Epoch 1113: Validation loss decreased (246.898300 --> 246.832352).\n",
            "\t Train_Loss: 101.6147 Val_Loss: 246.8324  BEST VAL Loss: 246.8324\n",
            "\n",
            "Epoch 1114: Validation loss decreased (246.832352 --> 246.765656).\n",
            "\t Train_Loss: 101.5786 Val_Loss: 246.7657  BEST VAL Loss: 246.7657\n",
            "\n",
            "Epoch 1115: Validation loss decreased (246.765656 --> 246.699570).\n",
            "\t Train_Loss: 101.5425 Val_Loss: 246.6996  BEST VAL Loss: 246.6996\n",
            "\n",
            "Epoch 1116: Validation loss decreased (246.699570 --> 246.633453).\n",
            "\t Train_Loss: 101.5065 Val_Loss: 246.6335  BEST VAL Loss: 246.6335\n",
            "\n",
            "Epoch 1117: Validation loss decreased (246.633453 --> 246.566895).\n",
            "\t Train_Loss: 101.4705 Val_Loss: 246.5669  BEST VAL Loss: 246.5669\n",
            "\n",
            "Epoch 1118: Validation loss decreased (246.566895 --> 246.501129).\n",
            "\t Train_Loss: 101.4344 Val_Loss: 246.5011  BEST VAL Loss: 246.5011\n",
            "\n",
            "Epoch 1119: Validation loss decreased (246.501129 --> 246.435349).\n",
            "\t Train_Loss: 101.3984 Val_Loss: 246.4353  BEST VAL Loss: 246.4353\n",
            "\n",
            "Epoch 1120: Validation loss decreased (246.435349 --> 246.368851).\n",
            "\t Train_Loss: 101.3624 Val_Loss: 246.3689  BEST VAL Loss: 246.3689\n",
            "\n",
            "Epoch 1121: Validation loss decreased (246.368851 --> 246.303177).\n",
            "\t Train_Loss: 101.3264 Val_Loss: 246.3032  BEST VAL Loss: 246.3032\n",
            "\n",
            "Epoch 1122: Validation loss decreased (246.303177 --> 246.237549).\n",
            "\t Train_Loss: 101.2905 Val_Loss: 246.2375  BEST VAL Loss: 246.2375\n",
            "\n",
            "Epoch 1123: Validation loss decreased (246.237549 --> 246.171677).\n",
            "\t Train_Loss: 101.2545 Val_Loss: 246.1717  BEST VAL Loss: 246.1717\n",
            "\n",
            "Epoch 1124: Validation loss decreased (246.171677 --> 246.105621).\n",
            "\t Train_Loss: 101.2186 Val_Loss: 246.1056  BEST VAL Loss: 246.1056\n",
            "\n",
            "Epoch 1125: Validation loss decreased (246.105621 --> 246.040207).\n",
            "\t Train_Loss: 101.1826 Val_Loss: 246.0402  BEST VAL Loss: 246.0402\n",
            "\n",
            "Epoch 1126: Validation loss decreased (246.040207 --> 245.974609).\n",
            "\t Train_Loss: 101.1467 Val_Loss: 245.9746  BEST VAL Loss: 245.9746\n",
            "\n",
            "Epoch 1127: Validation loss decreased (245.974609 --> 245.909225).\n",
            "\t Train_Loss: 101.1108 Val_Loss: 245.9092  BEST VAL Loss: 245.9092\n",
            "\n",
            "Epoch 1128: Validation loss decreased (245.909225 --> 245.843750).\n",
            "\t Train_Loss: 101.0749 Val_Loss: 245.8438  BEST VAL Loss: 245.8438\n",
            "\n",
            "Epoch 1129: Validation loss decreased (245.843750 --> 245.777985).\n",
            "\t Train_Loss: 101.0391 Val_Loss: 245.7780  BEST VAL Loss: 245.7780\n",
            "\n",
            "Epoch 1130: Validation loss decreased (245.777985 --> 245.712479).\n",
            "\t Train_Loss: 101.0032 Val_Loss: 245.7125  BEST VAL Loss: 245.7125\n",
            "\n",
            "Epoch 1131: Validation loss decreased (245.712479 --> 245.647263).\n",
            "\t Train_Loss: 100.9674 Val_Loss: 245.6473  BEST VAL Loss: 245.6473\n",
            "\n",
            "Epoch 1132: Validation loss decreased (245.647263 --> 245.581894).\n",
            "\t Train_Loss: 100.9316 Val_Loss: 245.5819  BEST VAL Loss: 245.5819\n",
            "\n",
            "Epoch 1133: Validation loss decreased (245.581894 --> 245.516708).\n",
            "\t Train_Loss: 100.8957 Val_Loss: 245.5167  BEST VAL Loss: 245.5167\n",
            "\n",
            "Epoch 1134: Validation loss decreased (245.516708 --> 245.451569).\n",
            "\t Train_Loss: 100.8599 Val_Loss: 245.4516  BEST VAL Loss: 245.4516\n",
            "\n",
            "Epoch 1135: Validation loss decreased (245.451569 --> 245.386459).\n",
            "\t Train_Loss: 100.8241 Val_Loss: 245.3865  BEST VAL Loss: 245.3865\n",
            "\n",
            "Epoch 1136: Validation loss decreased (245.386459 --> 245.321442).\n",
            "\t Train_Loss: 100.7884 Val_Loss: 245.3214  BEST VAL Loss: 245.3214\n",
            "\n",
            "Epoch 1137: Validation loss decreased (245.321442 --> 245.256256).\n",
            "\t Train_Loss: 100.7526 Val_Loss: 245.2563  BEST VAL Loss: 245.2563\n",
            "\n",
            "Epoch 1138: Validation loss decreased (245.256256 --> 245.191116).\n",
            "\t Train_Loss: 100.7169 Val_Loss: 245.1911  BEST VAL Loss: 245.1911\n",
            "\n",
            "Epoch 1139: Validation loss decreased (245.191116 --> 245.126129).\n",
            "\t Train_Loss: 100.6811 Val_Loss: 245.1261  BEST VAL Loss: 245.1261\n",
            "\n",
            "Epoch 1140: Validation loss decreased (245.126129 --> 245.061035).\n",
            "\t Train_Loss: 100.6454 Val_Loss: 245.0610  BEST VAL Loss: 245.0610\n",
            "\n",
            "Epoch 1141: Validation loss decreased (245.061035 --> 244.996094).\n",
            "\t Train_Loss: 100.6096 Val_Loss: 244.9961  BEST VAL Loss: 244.9961\n",
            "\n",
            "Epoch 1142: Validation loss decreased (244.996094 --> 244.931351).\n",
            "\t Train_Loss: 100.5740 Val_Loss: 244.9314  BEST VAL Loss: 244.9314\n",
            "\n",
            "Epoch 1143: Validation loss decreased (244.931351 --> 244.866501).\n",
            "\t Train_Loss: 100.5383 Val_Loss: 244.8665  BEST VAL Loss: 244.8665\n",
            "\n",
            "Epoch 1144: Validation loss decreased (244.866501 --> 244.801559).\n",
            "\t Train_Loss: 100.5026 Val_Loss: 244.8016  BEST VAL Loss: 244.8016\n",
            "\n",
            "Epoch 1145: Validation loss decreased (244.801559 --> 244.736938).\n",
            "\t Train_Loss: 100.4669 Val_Loss: 244.7369  BEST VAL Loss: 244.7369\n",
            "\n",
            "Epoch 1146: Validation loss decreased (244.736938 --> 244.672195).\n",
            "\t Train_Loss: 100.4313 Val_Loss: 244.6722  BEST VAL Loss: 244.6722\n",
            "\n",
            "Epoch 1147: Validation loss decreased (244.672195 --> 244.607376).\n",
            "\t Train_Loss: 100.3956 Val_Loss: 244.6074  BEST VAL Loss: 244.6074\n",
            "\n",
            "Epoch 1148: Validation loss decreased (244.607376 --> 244.542847).\n",
            "\t Train_Loss: 100.3600 Val_Loss: 244.5428  BEST VAL Loss: 244.5428\n",
            "\n",
            "Epoch 1149: Validation loss decreased (244.542847 --> 244.478027).\n",
            "\t Train_Loss: 100.3244 Val_Loss: 244.4780  BEST VAL Loss: 244.4780\n",
            "\n",
            "Epoch 1150: Validation loss decreased (244.478027 --> 244.413437).\n",
            "\t Train_Loss: 100.2888 Val_Loss: 244.4134  BEST VAL Loss: 244.4134\n",
            "\n",
            "Epoch 1151: Validation loss decreased (244.413437 --> 244.348984).\n",
            "\t Train_Loss: 100.2533 Val_Loss: 244.3490  BEST VAL Loss: 244.3490\n",
            "\n",
            "Epoch 1152: Validation loss decreased (244.348984 --> 244.284409).\n",
            "\t Train_Loss: 100.2177 Val_Loss: 244.2844  BEST VAL Loss: 244.2844\n",
            "\n",
            "Epoch 1153: Validation loss decreased (244.284409 --> 244.219635).\n",
            "\t Train_Loss: 100.1821 Val_Loss: 244.2196  BEST VAL Loss: 244.2196\n",
            "\n",
            "Epoch 1154: Validation loss decreased (244.219635 --> 244.155182).\n",
            "\t Train_Loss: 100.1466 Val_Loss: 244.1552  BEST VAL Loss: 244.1552\n",
            "\n",
            "Epoch 1155: Validation loss decreased (244.155182 --> 244.090744).\n",
            "\t Train_Loss: 100.1110 Val_Loss: 244.0907  BEST VAL Loss: 244.0907\n",
            "\n",
            "Epoch 1156: Validation loss decreased (244.090744 --> 244.026657).\n",
            "\t Train_Loss: 100.0755 Val_Loss: 244.0267  BEST VAL Loss: 244.0267\n",
            "\n",
            "Epoch 1157: Validation loss decreased (244.026657 --> 243.962311).\n",
            "\t Train_Loss: 100.0400 Val_Loss: 243.9623  BEST VAL Loss: 243.9623\n",
            "\n",
            "Epoch 1158: Validation loss decreased (243.962311 --> 243.897736).\n",
            "\t Train_Loss: 100.0045 Val_Loss: 243.8977  BEST VAL Loss: 243.8977\n",
            "\n",
            "Epoch 1159: Validation loss decreased (243.897736 --> 243.833496).\n",
            "\t Train_Loss: 99.9690 Val_Loss: 243.8335  BEST VAL Loss: 243.8335\n",
            "\n",
            "Epoch 1160: Validation loss decreased (243.833496 --> 243.769089).\n",
            "\t Train_Loss: 99.9335 Val_Loss: 243.7691  BEST VAL Loss: 243.7691\n",
            "\n",
            "Epoch 1161: Validation loss decreased (243.769089 --> 243.704758).\n",
            "\t Train_Loss: 99.8981 Val_Loss: 243.7048  BEST VAL Loss: 243.7048\n",
            "\n",
            "Epoch 1162: Validation loss decreased (243.704758 --> 243.640488).\n",
            "\t Train_Loss: 99.8626 Val_Loss: 243.6405  BEST VAL Loss: 243.6405\n",
            "\n",
            "Epoch 1163: Validation loss decreased (243.640488 --> 243.576218).\n",
            "\t Train_Loss: 99.8271 Val_Loss: 243.5762  BEST VAL Loss: 243.5762\n",
            "\n",
            "Epoch 1164: Validation loss decreased (243.576218 --> 243.512360).\n",
            "\t Train_Loss: 99.7917 Val_Loss: 243.5124  BEST VAL Loss: 243.5124\n",
            "\n",
            "Epoch 1165: Validation loss decreased (243.512360 --> 243.447952).\n",
            "\t Train_Loss: 99.7563 Val_Loss: 243.4480  BEST VAL Loss: 243.4480\n",
            "\n",
            "Epoch 1166: Validation loss decreased (243.447952 --> 243.383835).\n",
            "\t Train_Loss: 99.7209 Val_Loss: 243.3838  BEST VAL Loss: 243.3838\n",
            "\n",
            "Epoch 1167: Validation loss decreased (243.383835 --> 243.319778).\n",
            "\t Train_Loss: 99.6855 Val_Loss: 243.3198  BEST VAL Loss: 243.3198\n",
            "\n",
            "Epoch 1168: Validation loss decreased (243.319778 --> 243.255478).\n",
            "\t Train_Loss: 99.6501 Val_Loss: 243.2555  BEST VAL Loss: 243.2555\n",
            "\n",
            "Epoch 1169: Validation loss decreased (243.255478 --> 243.191605).\n",
            "\t Train_Loss: 99.6147 Val_Loss: 243.1916  BEST VAL Loss: 243.1916\n",
            "\n",
            "Epoch 1170: Validation loss decreased (243.191605 --> 243.127731).\n",
            "\t Train_Loss: 99.5794 Val_Loss: 243.1277  BEST VAL Loss: 243.1277\n",
            "\n",
            "Epoch 1171: Validation loss decreased (243.127731 --> 243.063354).\n",
            "\t Train_Loss: 99.5440 Val_Loss: 243.0634  BEST VAL Loss: 243.0634\n",
            "\n",
            "Epoch 1172: Validation loss decreased (243.063354 --> 242.999588).\n",
            "\t Train_Loss: 99.5087 Val_Loss: 242.9996  BEST VAL Loss: 242.9996\n",
            "\n",
            "Epoch 1173: Validation loss decreased (242.999588 --> 242.935303).\n",
            "\t Train_Loss: 99.4734 Val_Loss: 242.9353  BEST VAL Loss: 242.9353\n",
            "\n",
            "Epoch 1174: Validation loss decreased (242.935303 --> 242.871460).\n",
            "\t Train_Loss: 99.4380 Val_Loss: 242.8715  BEST VAL Loss: 242.8715\n",
            "\n",
            "Epoch 1175: Validation loss decreased (242.871460 --> 242.807770).\n",
            "\t Train_Loss: 99.4027 Val_Loss: 242.8078  BEST VAL Loss: 242.8078\n",
            "\n",
            "Epoch 1176: Validation loss decreased (242.807770 --> 242.743607).\n",
            "\t Train_Loss: 99.3674 Val_Loss: 242.7436  BEST VAL Loss: 242.7436\n",
            "\n",
            "Epoch 1177: Validation loss decreased (242.743607 --> 242.679886).\n",
            "\t Train_Loss: 99.3322 Val_Loss: 242.6799  BEST VAL Loss: 242.6799\n",
            "\n",
            "Epoch 1178: Validation loss decreased (242.679886 --> 242.615799).\n",
            "\t Train_Loss: 99.2969 Val_Loss: 242.6158  BEST VAL Loss: 242.6158\n",
            "\n",
            "Epoch 1179: Validation loss decreased (242.615799 --> 242.552200).\n",
            "\t Train_Loss: 99.2616 Val_Loss: 242.5522  BEST VAL Loss: 242.5522\n",
            "\n",
            "Epoch 1180: Validation loss decreased (242.552200 --> 242.488281).\n",
            "\t Train_Loss: 99.2264 Val_Loss: 242.4883  BEST VAL Loss: 242.4883\n",
            "\n",
            "Epoch 1181: Validation loss decreased (242.488281 --> 242.424271).\n",
            "\t Train_Loss: 99.1911 Val_Loss: 242.4243  BEST VAL Loss: 242.4243\n",
            "\n",
            "Epoch 1182: Validation loss decreased (242.424271 --> 242.360428).\n",
            "\t Train_Loss: 99.1559 Val_Loss: 242.3604  BEST VAL Loss: 242.3604\n",
            "\n",
            "Epoch 1183: Validation loss decreased (242.360428 --> 242.296631).\n",
            "\t Train_Loss: 99.1207 Val_Loss: 242.2966  BEST VAL Loss: 242.2966\n",
            "\n",
            "Epoch 1184: Validation loss decreased (242.296631 --> 242.233109).\n",
            "\t Train_Loss: 99.0855 Val_Loss: 242.2331  BEST VAL Loss: 242.2331\n",
            "\n",
            "Epoch 1185: Validation loss decreased (242.233109 --> 242.169098).\n",
            "\t Train_Loss: 99.0503 Val_Loss: 242.1691  BEST VAL Loss: 242.1691\n",
            "\n",
            "Epoch 1186: Validation loss decreased (242.169098 --> 242.105377).\n",
            "\t Train_Loss: 99.0151 Val_Loss: 242.1054  BEST VAL Loss: 242.1054\n",
            "\n",
            "Epoch 1187: Validation loss decreased (242.105377 --> 242.041748).\n",
            "\t Train_Loss: 98.9800 Val_Loss: 242.0417  BEST VAL Loss: 242.0417\n",
            "\n",
            "Epoch 1188: Validation loss decreased (242.041748 --> 241.978195).\n",
            "\t Train_Loss: 98.9448 Val_Loss: 241.9782  BEST VAL Loss: 241.9782\n",
            "\n",
            "Epoch 1189: Validation loss decreased (241.978195 --> 241.914505).\n",
            "\t Train_Loss: 98.9096 Val_Loss: 241.9145  BEST VAL Loss: 241.9145\n",
            "\n",
            "Epoch 1190: Validation loss decreased (241.914505 --> 241.850662).\n",
            "\t Train_Loss: 98.8745 Val_Loss: 241.8507  BEST VAL Loss: 241.8507\n",
            "\n",
            "Epoch 1191: Validation loss decreased (241.850662 --> 241.787109).\n",
            "\t Train_Loss: 98.8394 Val_Loss: 241.7871  BEST VAL Loss: 241.7871\n",
            "\n",
            "Epoch 1192: Validation loss decreased (241.787109 --> 241.723541).\n",
            "\t Train_Loss: 98.8043 Val_Loss: 241.7235  BEST VAL Loss: 241.7235\n",
            "\n",
            "Epoch 1193: Validation loss decreased (241.723541 --> 241.659866).\n",
            "\t Train_Loss: 98.7692 Val_Loss: 241.6599  BEST VAL Loss: 241.6599\n",
            "\n",
            "Epoch 1194: Validation loss decreased (241.659866 --> 241.596146).\n",
            "\t Train_Loss: 98.7341 Val_Loss: 241.5961  BEST VAL Loss: 241.5961\n",
            "\n",
            "Epoch 1195: Validation loss decreased (241.596146 --> 241.532715).\n",
            "\t Train_Loss: 98.6990 Val_Loss: 241.5327  BEST VAL Loss: 241.5327\n",
            "\n",
            "Epoch 1196: Validation loss decreased (241.532715 --> 241.469269).\n",
            "\t Train_Loss: 98.6639 Val_Loss: 241.4693  BEST VAL Loss: 241.4693\n",
            "\n",
            "Epoch 1197: Validation loss decreased (241.469269 --> 241.405502).\n",
            "\t Train_Loss: 98.6288 Val_Loss: 241.4055  BEST VAL Loss: 241.4055\n",
            "\n",
            "Epoch 1198: Validation loss decreased (241.405502 --> 241.342163).\n",
            "\t Train_Loss: 98.5938 Val_Loss: 241.3422  BEST VAL Loss: 241.3422\n",
            "\n",
            "Epoch 1199: Validation loss decreased (241.342163 --> 241.278549).\n",
            "\t Train_Loss: 98.5587 Val_Loss: 241.2785  BEST VAL Loss: 241.2785\n",
            "\n",
            "Epoch 1200: Validation loss decreased (241.278549 --> 241.214996).\n",
            "\t Train_Loss: 98.5237 Val_Loss: 241.2150  BEST VAL Loss: 241.2150\n",
            "\n",
            "Epoch 1201: Validation loss decreased (241.214996 --> 241.151489).\n",
            "\t Train_Loss: 98.4887 Val_Loss: 241.1515  BEST VAL Loss: 241.1515\n",
            "\n",
            "Epoch 1202: Validation loss decreased (241.151489 --> 241.087936).\n",
            "\t Train_Loss: 98.4537 Val_Loss: 241.0879  BEST VAL Loss: 241.0879\n",
            "\n",
            "Epoch 1203: Validation loss decreased (241.087936 --> 241.024658).\n",
            "\t Train_Loss: 98.4187 Val_Loss: 241.0247  BEST VAL Loss: 241.0247\n",
            "\n",
            "Epoch 1204: Validation loss decreased (241.024658 --> 240.961044).\n",
            "\t Train_Loss: 98.3837 Val_Loss: 240.9610  BEST VAL Loss: 240.9610\n",
            "\n",
            "Epoch 1205: Validation loss decreased (240.961044 --> 240.897415).\n",
            "\t Train_Loss: 98.3487 Val_Loss: 240.8974  BEST VAL Loss: 240.8974\n",
            "\n",
            "Epoch 1206: Validation loss decreased (240.897415 --> 240.833984).\n",
            "\t Train_Loss: 98.3138 Val_Loss: 240.8340  BEST VAL Loss: 240.8340\n",
            "\n",
            "Epoch 1207: Validation loss decreased (240.833984 --> 240.770630).\n",
            "\t Train_Loss: 98.2788 Val_Loss: 240.7706  BEST VAL Loss: 240.7706\n",
            "\n",
            "Epoch 1208: Validation loss decreased (240.770630 --> 240.707275).\n",
            "\t Train_Loss: 98.2439 Val_Loss: 240.7073  BEST VAL Loss: 240.7073\n",
            "\n",
            "Epoch 1209: Validation loss decreased (240.707275 --> 240.643585).\n",
            "\t Train_Loss: 98.2089 Val_Loss: 240.6436  BEST VAL Loss: 240.6436\n",
            "\n",
            "Epoch 1210: Validation loss decreased (240.643585 --> 240.580521).\n",
            "\t Train_Loss: 98.1740 Val_Loss: 240.5805  BEST VAL Loss: 240.5805\n",
            "\n",
            "Epoch 1211: Validation loss decreased (240.580521 --> 240.517014).\n",
            "\t Train_Loss: 98.1391 Val_Loss: 240.5170  BEST VAL Loss: 240.5170\n",
            "\n",
            "Epoch 1212: Validation loss decreased (240.517014 --> 240.453537).\n",
            "\t Train_Loss: 98.1042 Val_Loss: 240.4535  BEST VAL Loss: 240.4535\n",
            "\n",
            "Epoch 1213: Validation loss decreased (240.453537 --> 240.390244).\n",
            "\t Train_Loss: 98.0693 Val_Loss: 240.3902  BEST VAL Loss: 240.3902\n",
            "\n",
            "Epoch 1214: Validation loss decreased (240.390244 --> 240.326981).\n",
            "\t Train_Loss: 98.0344 Val_Loss: 240.3270  BEST VAL Loss: 240.3270\n",
            "\n",
            "Epoch 1215: Validation loss decreased (240.326981 --> 240.263672).\n",
            "\t Train_Loss: 97.9995 Val_Loss: 240.2637  BEST VAL Loss: 240.2637\n",
            "\n",
            "Epoch 1216: Validation loss decreased (240.263672 --> 240.200119).\n",
            "\t Train_Loss: 97.9647 Val_Loss: 240.2001  BEST VAL Loss: 240.2001\n",
            "\n",
            "Epoch 1217: Validation loss decreased (240.200119 --> 240.136871).\n",
            "\t Train_Loss: 97.9298 Val_Loss: 240.1369  BEST VAL Loss: 240.1369\n",
            "\n",
            "Epoch 1218: Validation loss decreased (240.136871 --> 240.073639).\n",
            "\t Train_Loss: 97.8950 Val_Loss: 240.0736  BEST VAL Loss: 240.0736\n",
            "\n",
            "Epoch 1219: Validation loss decreased (240.073639 --> 240.010162).\n",
            "\t Train_Loss: 97.8602 Val_Loss: 240.0102  BEST VAL Loss: 240.0102\n",
            "\n",
            "Epoch 1220: Validation loss decreased (240.010162 --> 239.946930).\n",
            "\t Train_Loss: 97.8253 Val_Loss: 239.9469  BEST VAL Loss: 239.9469\n",
            "\n",
            "Epoch 1221: Validation loss decreased (239.946930 --> 239.883530).\n",
            "\t Train_Loss: 97.7905 Val_Loss: 239.8835  BEST VAL Loss: 239.8835\n",
            "\n",
            "Epoch 1222: Validation loss decreased (239.883530 --> 239.820358).\n",
            "\t Train_Loss: 97.7557 Val_Loss: 239.8204  BEST VAL Loss: 239.8204\n",
            "\n",
            "Epoch 1223: Validation loss decreased (239.820358 --> 239.756943).\n",
            "\t Train_Loss: 97.7210 Val_Loss: 239.7569  BEST VAL Loss: 239.7569\n",
            "\n",
            "Epoch 1224: Validation loss decreased (239.756943 --> 239.693558).\n",
            "\t Train_Loss: 97.6862 Val_Loss: 239.6936  BEST VAL Loss: 239.6936\n",
            "\n",
            "Epoch 1225: Validation loss decreased (239.693558 --> 239.630325).\n",
            "\t Train_Loss: 97.6514 Val_Loss: 239.6303  BEST VAL Loss: 239.6303\n",
            "\n",
            "Epoch 1226: Validation loss decreased (239.630325 --> 239.567169).\n",
            "\t Train_Loss: 97.6167 Val_Loss: 239.5672  BEST VAL Loss: 239.5672\n",
            "\n",
            "Epoch 1227: Validation loss decreased (239.567169 --> 239.504013).\n",
            "\t Train_Loss: 97.5819 Val_Loss: 239.5040  BEST VAL Loss: 239.5040\n",
            "\n",
            "Epoch 1228: Validation loss decreased (239.504013 --> 239.440826).\n",
            "\t Train_Loss: 97.5471 Val_Loss: 239.4408  BEST VAL Loss: 239.4408\n",
            "\n",
            "Epoch 1229: Validation loss decreased (239.440826 --> 239.377762).\n",
            "\t Train_Loss: 97.5124 Val_Loss: 239.3778  BEST VAL Loss: 239.3778\n",
            "\n",
            "Epoch 1230: Validation loss decreased (239.377762 --> 239.314316).\n",
            "\t Train_Loss: 97.4777 Val_Loss: 239.3143  BEST VAL Loss: 239.3143\n",
            "\n",
            "Epoch 1231: Validation loss decreased (239.314316 --> 239.251099).\n",
            "\t Train_Loss: 97.4430 Val_Loss: 239.2511  BEST VAL Loss: 239.2511\n",
            "\n",
            "Epoch 1232: Validation loss decreased (239.251099 --> 239.188156).\n",
            "\t Train_Loss: 97.4083 Val_Loss: 239.1882  BEST VAL Loss: 239.1882\n",
            "\n",
            "Epoch 1233: Validation loss decreased (239.188156 --> 239.124954).\n",
            "\t Train_Loss: 97.3736 Val_Loss: 239.1250  BEST VAL Loss: 239.1250\n",
            "\n",
            "Epoch 1234: Validation loss decreased (239.124954 --> 239.061691).\n",
            "\t Train_Loss: 97.3389 Val_Loss: 239.0617  BEST VAL Loss: 239.0617\n",
            "\n",
            "Epoch 1235: Validation loss decreased (239.061691 --> 238.998566).\n",
            "\t Train_Loss: 97.3043 Val_Loss: 238.9986  BEST VAL Loss: 238.9986\n",
            "\n",
            "Epoch 1236: Validation loss decreased (238.998566 --> 238.935547).\n",
            "\t Train_Loss: 97.2696 Val_Loss: 238.9355  BEST VAL Loss: 238.9355\n",
            "\n",
            "Epoch 1237: Validation loss decreased (238.935547 --> 238.872314).\n",
            "\t Train_Loss: 97.2350 Val_Loss: 238.8723  BEST VAL Loss: 238.8723\n",
            "\n",
            "Epoch 1238: Validation loss decreased (238.872314 --> 238.809036).\n",
            "\t Train_Loss: 97.2003 Val_Loss: 238.8090  BEST VAL Loss: 238.8090\n",
            "\n",
            "Epoch 1239: Validation loss decreased (238.809036 --> 238.746017).\n",
            "\t Train_Loss: 97.1657 Val_Loss: 238.7460  BEST VAL Loss: 238.7460\n",
            "\n",
            "Epoch 1240: Validation loss decreased (238.746017 --> 238.682861).\n",
            "\t Train_Loss: 97.1311 Val_Loss: 238.6829  BEST VAL Loss: 238.6829\n",
            "\n",
            "Epoch 1241: Validation loss decreased (238.682861 --> 238.619873).\n",
            "\t Train_Loss: 97.0965 Val_Loss: 238.6199  BEST VAL Loss: 238.6199\n",
            "\n",
            "Epoch 1242: Validation loss decreased (238.619873 --> 238.556808).\n",
            "\t Train_Loss: 97.0619 Val_Loss: 238.5568  BEST VAL Loss: 238.5568\n",
            "\n",
            "Epoch 1243: Validation loss decreased (238.556808 --> 238.493637).\n",
            "\t Train_Loss: 97.0274 Val_Loss: 238.4936  BEST VAL Loss: 238.4936\n",
            "\n",
            "Epoch 1244: Validation loss decreased (238.493637 --> 238.430786).\n",
            "\t Train_Loss: 96.9928 Val_Loss: 238.4308  BEST VAL Loss: 238.4308\n",
            "\n",
            "Epoch 1245: Validation loss decreased (238.430786 --> 238.367584).\n",
            "\t Train_Loss: 96.9582 Val_Loss: 238.3676  BEST VAL Loss: 238.3676\n",
            "\n",
            "Epoch 1246: Validation loss decreased (238.367584 --> 238.304794).\n",
            "\t Train_Loss: 96.9237 Val_Loss: 238.3048  BEST VAL Loss: 238.3048\n",
            "\n",
            "Epoch 1247: Validation loss decreased (238.304794 --> 238.241608).\n",
            "\t Train_Loss: 96.8891 Val_Loss: 238.2416  BEST VAL Loss: 238.2416\n",
            "\n",
            "Epoch 1248: Validation loss decreased (238.241608 --> 238.178818).\n",
            "\t Train_Loss: 96.8546 Val_Loss: 238.1788  BEST VAL Loss: 238.1788\n",
            "\n",
            "Epoch 1249: Validation loss decreased (238.178818 --> 238.115585).\n",
            "\t Train_Loss: 96.8201 Val_Loss: 238.1156  BEST VAL Loss: 238.1156\n",
            "\n",
            "Epoch 1250: Validation loss decreased (238.115585 --> 238.052597).\n",
            "\t Train_Loss: 96.7855 Val_Loss: 238.0526  BEST VAL Loss: 238.0526\n",
            "\n",
            "Epoch 1251: Validation loss decreased (238.052597 --> 237.989655).\n",
            "\t Train_Loss: 96.7511 Val_Loss: 237.9897  BEST VAL Loss: 237.9897\n",
            "\n",
            "Epoch 1252: Validation loss decreased (237.989655 --> 237.926712).\n",
            "\t Train_Loss: 96.7166 Val_Loss: 237.9267  BEST VAL Loss: 237.9267\n",
            "\n",
            "Epoch 1253: Validation loss decreased (237.926712 --> 237.863724).\n",
            "\t Train_Loss: 96.6821 Val_Loss: 237.8637  BEST VAL Loss: 237.8637\n",
            "\n",
            "Epoch 1254: Validation loss decreased (237.863724 --> 237.800690).\n",
            "\t Train_Loss: 96.6476 Val_Loss: 237.8007  BEST VAL Loss: 237.8007\n",
            "\n",
            "Epoch 1255: Validation loss decreased (237.800690 --> 237.737900).\n",
            "\t Train_Loss: 96.6131 Val_Loss: 237.7379  BEST VAL Loss: 237.7379\n",
            "\n",
            "Epoch 1256: Validation loss decreased (237.737900 --> 237.674927).\n",
            "\t Train_Loss: 96.5787 Val_Loss: 237.6749  BEST VAL Loss: 237.6749\n",
            "\n",
            "Epoch 1257: Validation loss decreased (237.674927 --> 237.611847).\n",
            "\t Train_Loss: 96.5442 Val_Loss: 237.6118  BEST VAL Loss: 237.6118\n",
            "\n",
            "Epoch 1258: Validation loss decreased (237.611847 --> 237.548996).\n",
            "\t Train_Loss: 96.5098 Val_Loss: 237.5490  BEST VAL Loss: 237.5490\n",
            "\n",
            "Epoch 1259: Validation loss decreased (237.548996 --> 237.486008).\n",
            "\t Train_Loss: 96.4754 Val_Loss: 237.4860  BEST VAL Loss: 237.4860\n",
            "\n",
            "Epoch 1260: Validation loss decreased (237.486008 --> 237.423340).\n",
            "\t Train_Loss: 96.4410 Val_Loss: 237.4233  BEST VAL Loss: 237.4233\n",
            "\n",
            "Epoch 1261: Validation loss decreased (237.423340 --> 237.360229).\n",
            "\t Train_Loss: 96.4066 Val_Loss: 237.3602  BEST VAL Loss: 237.3602\n",
            "\n",
            "Epoch 1262: Validation loss decreased (237.360229 --> 237.297592).\n",
            "\t Train_Loss: 96.3722 Val_Loss: 237.2976  BEST VAL Loss: 237.2976\n",
            "\n",
            "Epoch 1263: Validation loss decreased (237.297592 --> 237.234573).\n",
            "\t Train_Loss: 96.3378 Val_Loss: 237.2346  BEST VAL Loss: 237.2346\n",
            "\n",
            "Epoch 1264: Validation loss decreased (237.234573 --> 237.171631).\n",
            "\t Train_Loss: 96.3034 Val_Loss: 237.1716  BEST VAL Loss: 237.1716\n",
            "\n",
            "Epoch 1265: Validation loss decreased (237.171631 --> 237.108795).\n",
            "\t Train_Loss: 96.2691 Val_Loss: 237.1088  BEST VAL Loss: 237.1088\n",
            "\n",
            "Epoch 1266: Validation loss decreased (237.108795 --> 237.045975).\n",
            "\t Train_Loss: 96.2347 Val_Loss: 237.0460  BEST VAL Loss: 237.0460\n",
            "\n",
            "Epoch 1267: Validation loss decreased (237.045975 --> 236.983200).\n",
            "\t Train_Loss: 96.2003 Val_Loss: 236.9832  BEST VAL Loss: 236.9832\n",
            "\n",
            "Epoch 1268: Validation loss decreased (236.983200 --> 236.920273).\n",
            "\t Train_Loss: 96.1661 Val_Loss: 236.9203  BEST VAL Loss: 236.9203\n",
            "\n",
            "Epoch 1269: Validation loss decreased (236.920273 --> 236.857376).\n",
            "\t Train_Loss: 96.1317 Val_Loss: 236.8574  BEST VAL Loss: 236.8574\n",
            "\n",
            "Epoch 1270: Validation loss decreased (236.857376 --> 236.794601).\n",
            "\t Train_Loss: 96.0974 Val_Loss: 236.7946  BEST VAL Loss: 236.7946\n",
            "\n",
            "Epoch 1271: Validation loss decreased (236.794601 --> 236.731812).\n",
            "\t Train_Loss: 96.0631 Val_Loss: 236.7318  BEST VAL Loss: 236.7318\n",
            "\n",
            "Epoch 1272: Validation loss decreased (236.731812 --> 236.668747).\n",
            "\t Train_Loss: 96.0288 Val_Loss: 236.6687  BEST VAL Loss: 236.6687\n",
            "\n",
            "Epoch 1273: Validation loss decreased (236.668747 --> 236.605988).\n",
            "\t Train_Loss: 95.9946 Val_Loss: 236.6060  BEST VAL Loss: 236.6060\n",
            "\n",
            "Epoch 1274: Validation loss decreased (236.605988 --> 236.543320).\n",
            "\t Train_Loss: 95.9603 Val_Loss: 236.5433  BEST VAL Loss: 236.5433\n",
            "\n",
            "Epoch 1275: Validation loss decreased (236.543320 --> 236.480453).\n",
            "\t Train_Loss: 95.9260 Val_Loss: 236.4805  BEST VAL Loss: 236.4805\n",
            "\n",
            "Epoch 1276: Validation loss decreased (236.480453 --> 236.417557).\n",
            "\t Train_Loss: 95.8918 Val_Loss: 236.4176  BEST VAL Loss: 236.4176\n",
            "\n",
            "Epoch 1277: Validation loss decreased (236.417557 --> 236.354736).\n",
            "\t Train_Loss: 95.8575 Val_Loss: 236.3547  BEST VAL Loss: 236.3547\n",
            "\n",
            "Epoch 1278: Validation loss decreased (236.354736 --> 236.291946).\n",
            "\t Train_Loss: 95.8233 Val_Loss: 236.2919  BEST VAL Loss: 236.2919\n",
            "\n",
            "Epoch 1279: Validation loss decreased (236.291946 --> 236.229401).\n",
            "\t Train_Loss: 95.7891 Val_Loss: 236.2294  BEST VAL Loss: 236.2294\n",
            "\n",
            "Epoch 1280: Validation loss decreased (236.229401 --> 236.166534).\n",
            "\t Train_Loss: 95.7549 Val_Loss: 236.1665  BEST VAL Loss: 236.1665\n",
            "\n",
            "Epoch 1281: Validation loss decreased (236.166534 --> 236.103470).\n",
            "\t Train_Loss: 95.7207 Val_Loss: 236.1035  BEST VAL Loss: 236.1035\n",
            "\n",
            "Epoch 1282: Validation loss decreased (236.103470 --> 236.040848).\n",
            "\t Train_Loss: 95.6865 Val_Loss: 236.0408  BEST VAL Loss: 236.0408\n",
            "\n",
            "Epoch 1283: Validation loss decreased (236.040848 --> 235.978073).\n",
            "\t Train_Loss: 95.6523 Val_Loss: 235.9781  BEST VAL Loss: 235.9781\n",
            "\n",
            "Epoch 1284: Validation loss decreased (235.978073 --> 235.915436).\n",
            "\t Train_Loss: 95.6181 Val_Loss: 235.9154  BEST VAL Loss: 235.9154\n",
            "\n",
            "Epoch 1285: Validation loss decreased (235.915436 --> 235.852707).\n",
            "\t Train_Loss: 95.5840 Val_Loss: 235.8527  BEST VAL Loss: 235.8527\n",
            "\n",
            "Epoch 1286: Validation loss decreased (235.852707 --> 235.789795).\n",
            "\t Train_Loss: 95.5498 Val_Loss: 235.7898  BEST VAL Loss: 235.7898\n",
            "\n",
            "Epoch 1287: Validation loss decreased (235.789795 --> 235.727325).\n",
            "\t Train_Loss: 95.5157 Val_Loss: 235.7273  BEST VAL Loss: 235.7273\n",
            "\n",
            "Epoch 1288: Validation loss decreased (235.727325 --> 235.664597).\n",
            "\t Train_Loss: 95.4816 Val_Loss: 235.6646  BEST VAL Loss: 235.6646\n",
            "\n",
            "Epoch 1289: Validation loss decreased (235.664597 --> 235.601837).\n",
            "\t Train_Loss: 95.4474 Val_Loss: 235.6018  BEST VAL Loss: 235.6018\n",
            "\n",
            "Epoch 1290: Validation loss decreased (235.601837 --> 235.539185).\n",
            "\t Train_Loss: 95.4133 Val_Loss: 235.5392  BEST VAL Loss: 235.5392\n",
            "\n",
            "Epoch 1291: Validation loss decreased (235.539185 --> 235.476227).\n",
            "\t Train_Loss: 95.3792 Val_Loss: 235.4762  BEST VAL Loss: 235.4762\n",
            "\n",
            "Epoch 1292: Validation loss decreased (235.476227 --> 235.413651).\n",
            "\t Train_Loss: 95.3451 Val_Loss: 235.4137  BEST VAL Loss: 235.4137\n",
            "\n",
            "Epoch 1293: Validation loss decreased (235.413651 --> 235.351105).\n",
            "\t Train_Loss: 95.3111 Val_Loss: 235.3511  BEST VAL Loss: 235.3511\n",
            "\n",
            "Epoch 1294: Validation loss decreased (235.351105 --> 235.288528).\n",
            "\t Train_Loss: 95.2770 Val_Loss: 235.2885  BEST VAL Loss: 235.2885\n",
            "\n",
            "Epoch 1295: Validation loss decreased (235.288528 --> 235.225739).\n",
            "\t Train_Loss: 95.2429 Val_Loss: 235.2257  BEST VAL Loss: 235.2257\n",
            "\n",
            "Epoch 1296: Validation loss decreased (235.225739 --> 235.162796).\n",
            "\t Train_Loss: 95.2089 Val_Loss: 235.1628  BEST VAL Loss: 235.1628\n",
            "\n",
            "Epoch 1297: Validation loss decreased (235.162796 --> 235.100449).\n",
            "\t Train_Loss: 95.1748 Val_Loss: 235.1004  BEST VAL Loss: 235.1004\n",
            "\n",
            "Epoch 1298: Validation loss decreased (235.100449 --> 235.037796).\n",
            "\t Train_Loss: 95.1408 Val_Loss: 235.0378  BEST VAL Loss: 235.0378\n",
            "\n",
            "Epoch 1299: Validation loss decreased (235.037796 --> 234.975266).\n",
            "\t Train_Loss: 95.1068 Val_Loss: 234.9753  BEST VAL Loss: 234.9753\n",
            "\n",
            "Epoch 1300: Validation loss decreased (234.975266 --> 234.912354).\n",
            "\t Train_Loss: 95.0728 Val_Loss: 234.9124  BEST VAL Loss: 234.9124\n",
            "\n",
            "Epoch 1301: Validation loss decreased (234.912354 --> 234.849762).\n",
            "\t Train_Loss: 95.0387 Val_Loss: 234.8498  BEST VAL Loss: 234.8498\n",
            "\n",
            "Epoch 1302: Validation loss decreased (234.849762 --> 234.787064).\n",
            "\t Train_Loss: 95.0048 Val_Loss: 234.7871  BEST VAL Loss: 234.7871\n",
            "\n",
            "Epoch 1303: Validation loss decreased (234.787064 --> 234.724564).\n",
            "\t Train_Loss: 94.9708 Val_Loss: 234.7246  BEST VAL Loss: 234.7246\n",
            "\n",
            "Epoch 1304: Validation loss decreased (234.724564 --> 234.662155).\n",
            "\t Train_Loss: 94.9368 Val_Loss: 234.6622  BEST VAL Loss: 234.6622\n",
            "\n",
            "Epoch 1305: Validation loss decreased (234.662155 --> 234.599487).\n",
            "\t Train_Loss: 94.9029 Val_Loss: 234.5995  BEST VAL Loss: 234.5995\n",
            "\n",
            "Epoch 1306: Validation loss decreased (234.599487 --> 234.536606).\n",
            "\t Train_Loss: 94.8690 Val_Loss: 234.5366  BEST VAL Loss: 234.5366\n",
            "\n",
            "Epoch 1307: Validation loss decreased (234.536606 --> 234.474167).\n",
            "\t Train_Loss: 94.8350 Val_Loss: 234.4742  BEST VAL Loss: 234.4742\n",
            "\n",
            "Epoch 1308: Validation loss decreased (234.474167 --> 234.411285).\n",
            "\t Train_Loss: 94.8011 Val_Loss: 234.4113  BEST VAL Loss: 234.4113\n",
            "\n",
            "Epoch 1309: Validation loss decreased (234.411285 --> 234.349030).\n",
            "\t Train_Loss: 94.7672 Val_Loss: 234.3490  BEST VAL Loss: 234.3490\n",
            "\n",
            "Epoch 1310: Validation loss decreased (234.349030 --> 234.286423).\n",
            "\t Train_Loss: 94.7332 Val_Loss: 234.2864  BEST VAL Loss: 234.2864\n",
            "\n",
            "Epoch 1311: Validation loss decreased (234.286423 --> 234.223953).\n",
            "\t Train_Loss: 94.6993 Val_Loss: 234.2240  BEST VAL Loss: 234.2240\n",
            "\n",
            "Epoch 1312: Validation loss decreased (234.223953 --> 234.161407).\n",
            "\t Train_Loss: 94.6655 Val_Loss: 234.1614  BEST VAL Loss: 234.1614\n",
            "\n",
            "Epoch 1313: Validation loss decreased (234.161407 --> 234.098740).\n",
            "\t Train_Loss: 94.6316 Val_Loss: 234.0987  BEST VAL Loss: 234.0987\n",
            "\n",
            "Epoch 1314: Validation loss decreased (234.098740 --> 234.035995).\n",
            "\t Train_Loss: 94.5977 Val_Loss: 234.0360  BEST VAL Loss: 234.0360\n",
            "\n",
            "Epoch 1315: Validation loss decreased (234.035995 --> 233.973633).\n",
            "\t Train_Loss: 94.5639 Val_Loss: 233.9736  BEST VAL Loss: 233.9736\n",
            "\n",
            "Epoch 1316: Validation loss decreased (233.973633 --> 233.911331).\n",
            "\t Train_Loss: 94.5300 Val_Loss: 233.9113  BEST VAL Loss: 233.9113\n",
            "\n",
            "Epoch 1317: Validation loss decreased (233.911331 --> 233.848907).\n",
            "\t Train_Loss: 94.4962 Val_Loss: 233.8489  BEST VAL Loss: 233.8489\n",
            "\n",
            "Epoch 1318: Validation loss decreased (233.848907 --> 233.786331).\n",
            "\t Train_Loss: 94.4623 Val_Loss: 233.7863  BEST VAL Loss: 233.7863\n",
            "\n",
            "Epoch 1319: Validation loss decreased (233.786331 --> 233.723541).\n",
            "\t Train_Loss: 94.4285 Val_Loss: 233.7235  BEST VAL Loss: 233.7235\n",
            "\n",
            "Epoch 1320: Validation loss decreased (233.723541 --> 233.661041).\n",
            "\t Train_Loss: 94.3947 Val_Loss: 233.6610  BEST VAL Loss: 233.6610\n",
            "\n",
            "Epoch 1321: Validation loss decreased (233.661041 --> 233.598267).\n",
            "\t Train_Loss: 94.3609 Val_Loss: 233.5983  BEST VAL Loss: 233.5983\n",
            "\n",
            "Epoch 1322: Validation loss decreased (233.598267 --> 233.536041).\n",
            "\t Train_Loss: 94.3271 Val_Loss: 233.5360  BEST VAL Loss: 233.5360\n",
            "\n",
            "Epoch 1323: Validation loss decreased (233.536041 --> 233.473557).\n",
            "\t Train_Loss: 94.2934 Val_Loss: 233.4736  BEST VAL Loss: 233.4736\n",
            "\n",
            "Epoch 1324: Validation loss decreased (233.473557 --> 233.411133).\n",
            "\t Train_Loss: 94.2596 Val_Loss: 233.4111  BEST VAL Loss: 233.4111\n",
            "\n",
            "Epoch 1325: Validation loss decreased (233.411133 --> 233.348785).\n",
            "\t Train_Loss: 94.2259 Val_Loss: 233.3488  BEST VAL Loss: 233.3488\n",
            "\n",
            "Epoch 1326: Validation loss decreased (233.348785 --> 233.286331).\n",
            "\t Train_Loss: 94.1921 Val_Loss: 233.2863  BEST VAL Loss: 233.2863\n",
            "\n",
            "Epoch 1327: Validation loss decreased (233.286331 --> 233.223679).\n",
            "\t Train_Loss: 94.1584 Val_Loss: 233.2237  BEST VAL Loss: 233.2237\n",
            "\n",
            "Epoch 1328: Validation loss decreased (233.223679 --> 233.161240).\n",
            "\t Train_Loss: 94.1247 Val_Loss: 233.1612  BEST VAL Loss: 233.1612\n",
            "\n",
            "Epoch 1329: Validation loss decreased (233.161240 --> 233.098541).\n",
            "\t Train_Loss: 94.0909 Val_Loss: 233.0985  BEST VAL Loss: 233.0985\n",
            "\n",
            "Epoch 1330: Validation loss decreased (233.098541 --> 233.036423).\n",
            "\t Train_Loss: 94.0572 Val_Loss: 233.0364  BEST VAL Loss: 233.0364\n",
            "\n",
            "Epoch 1331: Validation loss decreased (233.036423 --> 232.973984).\n",
            "\t Train_Loss: 94.0235 Val_Loss: 232.9740  BEST VAL Loss: 232.9740\n",
            "\n",
            "Epoch 1332: Validation loss decreased (232.973984 --> 232.911377).\n",
            "\t Train_Loss: 93.9899 Val_Loss: 232.9114  BEST VAL Loss: 232.9114\n",
            "\n",
            "Epoch 1333: Validation loss decreased (232.911377 --> 232.849319).\n",
            "\t Train_Loss: 93.9562 Val_Loss: 232.8493  BEST VAL Loss: 232.8493\n",
            "\n",
            "Epoch 1334: Validation loss decreased (232.849319 --> 232.786911).\n",
            "\t Train_Loss: 93.9225 Val_Loss: 232.7869  BEST VAL Loss: 232.7869\n",
            "\n",
            "Epoch 1335: Validation loss decreased (232.786911 --> 232.724411).\n",
            "\t Train_Loss: 93.8889 Val_Loss: 232.7244  BEST VAL Loss: 232.7244\n",
            "\n",
            "Epoch 1336: Validation loss decreased (232.724411 --> 232.662064).\n",
            "\t Train_Loss: 93.8552 Val_Loss: 232.6621  BEST VAL Loss: 232.6621\n",
            "\n",
            "Epoch 1337: Validation loss decreased (232.662064 --> 232.599472).\n",
            "\t Train_Loss: 93.8216 Val_Loss: 232.5995  BEST VAL Loss: 232.5995\n",
            "\n",
            "Epoch 1338: Validation loss decreased (232.599472 --> 232.537354).\n",
            "\t Train_Loss: 93.7880 Val_Loss: 232.5374  BEST VAL Loss: 232.5374\n",
            "\n",
            "Epoch 1339: Validation loss decreased (232.537354 --> 232.474762).\n",
            "\t Train_Loss: 93.7544 Val_Loss: 232.4748  BEST VAL Loss: 232.4748\n",
            "\n",
            "Epoch 1340: Validation loss decreased (232.474762 --> 232.412399).\n",
            "\t Train_Loss: 93.7207 Val_Loss: 232.4124  BEST VAL Loss: 232.4124\n",
            "\n",
            "Epoch 1341: Validation loss decreased (232.412399 --> 232.350021).\n",
            "\t Train_Loss: 93.6872 Val_Loss: 232.3500  BEST VAL Loss: 232.3500\n",
            "\n",
            "Epoch 1342: Validation loss decreased (232.350021 --> 232.287506).\n",
            "\t Train_Loss: 93.6536 Val_Loss: 232.2875  BEST VAL Loss: 232.2875\n",
            "\n",
            "Epoch 1343: Validation loss decreased (232.287506 --> 232.225250).\n",
            "\t Train_Loss: 93.6200 Val_Loss: 232.2253  BEST VAL Loss: 232.2253\n",
            "\n",
            "Epoch 1344: Validation loss decreased (232.225250 --> 232.163208).\n",
            "\t Train_Loss: 93.5865 Val_Loss: 232.1632  BEST VAL Loss: 232.1632\n",
            "\n",
            "Epoch 1345: Validation loss decreased (232.163208 --> 232.100876).\n",
            "\t Train_Loss: 93.5529 Val_Loss: 232.1009  BEST VAL Loss: 232.1009\n",
            "\n",
            "Epoch 1346: Validation loss decreased (232.100876 --> 232.038559).\n",
            "\t Train_Loss: 93.5194 Val_Loss: 232.0386  BEST VAL Loss: 232.0386\n",
            "\n",
            "Epoch 1347: Validation loss decreased (232.038559 --> 231.976318).\n",
            "\t Train_Loss: 93.4858 Val_Loss: 231.9763  BEST VAL Loss: 231.9763\n",
            "\n",
            "Epoch 1348: Validation loss decreased (231.976318 --> 231.913818).\n",
            "\t Train_Loss: 93.4523 Val_Loss: 231.9138  BEST VAL Loss: 231.9138\n",
            "\n",
            "Epoch 1349: Validation loss decreased (231.913818 --> 231.851471).\n",
            "\t Train_Loss: 93.4188 Val_Loss: 231.8515  BEST VAL Loss: 231.8515\n",
            "\n",
            "Epoch 1350: Validation loss decreased (231.851471 --> 231.789261).\n",
            "\t Train_Loss: 93.3853 Val_Loss: 231.7893  BEST VAL Loss: 231.7893\n",
            "\n",
            "Epoch 1351: Validation loss decreased (231.789261 --> 231.726959).\n",
            "\t Train_Loss: 93.3518 Val_Loss: 231.7270  BEST VAL Loss: 231.7270\n",
            "\n",
            "Epoch 1352: Validation loss decreased (231.726959 --> 231.664703).\n",
            "\t Train_Loss: 93.3183 Val_Loss: 231.6647  BEST VAL Loss: 231.6647\n",
            "\n",
            "Epoch 1353: Validation loss decreased (231.664703 --> 231.602249).\n",
            "\t Train_Loss: 93.2849 Val_Loss: 231.6022  BEST VAL Loss: 231.6022\n",
            "\n",
            "Epoch 1354: Validation loss decreased (231.602249 --> 231.539917).\n",
            "\t Train_Loss: 93.2514 Val_Loss: 231.5399  BEST VAL Loss: 231.5399\n",
            "\n",
            "Epoch 1355: Validation loss decreased (231.539917 --> 231.477859).\n",
            "\t Train_Loss: 93.2180 Val_Loss: 231.4779  BEST VAL Loss: 231.4779\n",
            "\n",
            "Epoch 1356: Validation loss decreased (231.477859 --> 231.415390).\n",
            "\t Train_Loss: 93.1845 Val_Loss: 231.4154  BEST VAL Loss: 231.4154\n",
            "\n",
            "Epoch 1357: Validation loss decreased (231.415390 --> 231.353271).\n",
            "\t Train_Loss: 93.1511 Val_Loss: 231.3533  BEST VAL Loss: 231.3533\n",
            "\n",
            "Epoch 1358: Validation loss decreased (231.353271 --> 231.291122).\n",
            "\t Train_Loss: 93.1177 Val_Loss: 231.2911  BEST VAL Loss: 231.2911\n",
            "\n",
            "Epoch 1359: Validation loss decreased (231.291122 --> 231.228760).\n",
            "\t Train_Loss: 93.0843 Val_Loss: 231.2288  BEST VAL Loss: 231.2288\n",
            "\n",
            "Epoch 1360: Validation loss decreased (231.228760 --> 231.166428).\n",
            "\t Train_Loss: 93.0509 Val_Loss: 231.1664  BEST VAL Loss: 231.1664\n",
            "\n",
            "Epoch 1361: Validation loss decreased (231.166428 --> 231.104538).\n",
            "\t Train_Loss: 93.0175 Val_Loss: 231.1045  BEST VAL Loss: 231.1045\n",
            "\n",
            "Epoch 1362: Validation loss decreased (231.104538 --> 231.042267).\n",
            "\t Train_Loss: 92.9841 Val_Loss: 231.0423  BEST VAL Loss: 231.0423\n",
            "\n",
            "Epoch 1363: Validation loss decreased (231.042267 --> 230.980011).\n",
            "\t Train_Loss: 92.9508 Val_Loss: 230.9800  BEST VAL Loss: 230.9800\n",
            "\n",
            "Epoch 1364: Validation loss decreased (230.980011 --> 230.918015).\n",
            "\t Train_Loss: 92.9174 Val_Loss: 230.9180  BEST VAL Loss: 230.9180\n",
            "\n",
            "Epoch 1365: Validation loss decreased (230.918015 --> 230.855743).\n",
            "\t Train_Loss: 92.8841 Val_Loss: 230.8557  BEST VAL Loss: 230.8557\n",
            "\n",
            "Epoch 1366: Validation loss decreased (230.855743 --> 230.793503).\n",
            "\t Train_Loss: 92.8507 Val_Loss: 230.7935  BEST VAL Loss: 230.7935\n",
            "\n",
            "Epoch 1367: Validation loss decreased (230.793503 --> 230.731400).\n",
            "\t Train_Loss: 92.8174 Val_Loss: 230.7314  BEST VAL Loss: 230.7314\n",
            "\n",
            "Epoch 1368: Validation loss decreased (230.731400 --> 230.669144).\n",
            "\t Train_Loss: 92.7841 Val_Loss: 230.6691  BEST VAL Loss: 230.6691\n",
            "\n",
            "Epoch 1369: Validation loss decreased (230.669144 --> 230.606918).\n",
            "\t Train_Loss: 92.7508 Val_Loss: 230.6069  BEST VAL Loss: 230.6069\n",
            "\n",
            "Epoch 1370: Validation loss decreased (230.606918 --> 230.544907).\n",
            "\t Train_Loss: 92.7175 Val_Loss: 230.5449  BEST VAL Loss: 230.5449\n",
            "\n",
            "Epoch 1371: Validation loss decreased (230.544907 --> 230.482788).\n",
            "\t Train_Loss: 92.6843 Val_Loss: 230.4828  BEST VAL Loss: 230.4828\n",
            "\n",
            "Epoch 1372: Validation loss decreased (230.482788 --> 230.420776).\n",
            "\t Train_Loss: 92.6510 Val_Loss: 230.4208  BEST VAL Loss: 230.4208\n",
            "\n",
            "Epoch 1373: Validation loss decreased (230.420776 --> 230.358749).\n",
            "\t Train_Loss: 92.6177 Val_Loss: 230.3587  BEST VAL Loss: 230.3587\n",
            "\n",
            "Epoch 1374: Validation loss decreased (230.358749 --> 230.296661).\n",
            "\t Train_Loss: 92.5845 Val_Loss: 230.2967  BEST VAL Loss: 230.2967\n",
            "\n",
            "Epoch 1375: Validation loss decreased (230.296661 --> 230.234619).\n",
            "\t Train_Loss: 92.5513 Val_Loss: 230.2346  BEST VAL Loss: 230.2346\n",
            "\n",
            "Epoch 1376: Validation loss decreased (230.234619 --> 230.172409).\n",
            "\t Train_Loss: 92.5180 Val_Loss: 230.1724  BEST VAL Loss: 230.1724\n",
            "\n",
            "Epoch 1377: Validation loss decreased (230.172409 --> 230.110306).\n",
            "\t Train_Loss: 92.4848 Val_Loss: 230.1103  BEST VAL Loss: 230.1103\n",
            "\n",
            "Epoch 1378: Validation loss decreased (230.110306 --> 230.048248).\n",
            "\t Train_Loss: 92.4516 Val_Loss: 230.0482  BEST VAL Loss: 230.0482\n",
            "\n",
            "Epoch 1379: Validation loss decreased (230.048248 --> 229.986084).\n",
            "\t Train_Loss: 92.4184 Val_Loss: 229.9861  BEST VAL Loss: 229.9861\n",
            "\n",
            "Epoch 1380: Validation loss decreased (229.986084 --> 229.923950).\n",
            "\t Train_Loss: 92.3852 Val_Loss: 229.9240  BEST VAL Loss: 229.9240\n",
            "\n",
            "Epoch 1381: Validation loss decreased (229.923950 --> 229.861938).\n",
            "\t Train_Loss: 92.3520 Val_Loss: 229.8619  BEST VAL Loss: 229.8619\n",
            "\n",
            "Epoch 1382: Validation loss decreased (229.861938 --> 229.799911).\n",
            "\t Train_Loss: 92.3188 Val_Loss: 229.7999  BEST VAL Loss: 229.7999\n",
            "\n",
            "Epoch 1383: Validation loss decreased (229.799911 --> 229.737793).\n",
            "\t Train_Loss: 92.2857 Val_Loss: 229.7378  BEST VAL Loss: 229.7378\n",
            "\n",
            "Epoch 1384: Validation loss decreased (229.737793 --> 229.675934).\n",
            "\t Train_Loss: 92.2525 Val_Loss: 229.6759  BEST VAL Loss: 229.6759\n",
            "\n",
            "Epoch 1385: Validation loss decreased (229.675934 --> 229.613525).\n",
            "\t Train_Loss: 92.2194 Val_Loss: 229.6135  BEST VAL Loss: 229.6135\n",
            "\n",
            "Epoch 1386: Validation loss decreased (229.613525 --> 229.551224).\n",
            "\t Train_Loss: 92.1863 Val_Loss: 229.5512  BEST VAL Loss: 229.5512\n",
            "\n",
            "Epoch 1387: Validation loss decreased (229.551224 --> 229.489304).\n",
            "\t Train_Loss: 92.1532 Val_Loss: 229.4893  BEST VAL Loss: 229.4893\n",
            "\n",
            "Epoch 1388: Validation loss decreased (229.489304 --> 229.427322).\n",
            "\t Train_Loss: 92.1201 Val_Loss: 229.4273  BEST VAL Loss: 229.4273\n",
            "\n",
            "Epoch 1389: Validation loss decreased (229.427322 --> 229.365189).\n",
            "\t Train_Loss: 92.0870 Val_Loss: 229.3652  BEST VAL Loss: 229.3652\n",
            "\n",
            "Epoch 1390: Validation loss decreased (229.365189 --> 229.303177).\n",
            "\t Train_Loss: 92.0539 Val_Loss: 229.3032  BEST VAL Loss: 229.3032\n",
            "\n",
            "Epoch 1391: Validation loss decreased (229.303177 --> 229.241333).\n",
            "\t Train_Loss: 92.0208 Val_Loss: 229.2413  BEST VAL Loss: 229.2413\n",
            "\n",
            "Epoch 1392: Validation loss decreased (229.241333 --> 229.179321).\n",
            "\t Train_Loss: 91.9878 Val_Loss: 229.1793  BEST VAL Loss: 229.1793\n",
            "\n",
            "Epoch 1393: Validation loss decreased (229.179321 --> 229.117432).\n",
            "\t Train_Loss: 91.9548 Val_Loss: 229.1174  BEST VAL Loss: 229.1174\n",
            "\n",
            "Epoch 1394: Validation loss decreased (229.117432 --> 229.055786).\n",
            "\t Train_Loss: 91.9217 Val_Loss: 229.0558  BEST VAL Loss: 229.0558\n",
            "\n",
            "Epoch 1395: Validation loss decreased (229.055786 --> 228.993729).\n",
            "\t Train_Loss: 91.8887 Val_Loss: 228.9937  BEST VAL Loss: 228.9937\n",
            "\n",
            "Epoch 1396: Validation loss decreased (228.993729 --> 228.931625).\n",
            "\t Train_Loss: 91.8557 Val_Loss: 228.9316  BEST VAL Loss: 228.9316\n",
            "\n",
            "Epoch 1397: Validation loss decreased (228.931625 --> 228.869858).\n",
            "\t Train_Loss: 91.8226 Val_Loss: 228.8699  BEST VAL Loss: 228.8699\n",
            "\n",
            "Epoch 1398: Validation loss decreased (228.869858 --> 228.807907).\n",
            "\t Train_Loss: 91.7896 Val_Loss: 228.8079  BEST VAL Loss: 228.8079\n",
            "\n",
            "Epoch 1399: Validation loss decreased (228.807907 --> 228.745850).\n",
            "\t Train_Loss: 91.7567 Val_Loss: 228.7458  BEST VAL Loss: 228.7458\n",
            "\n",
            "Epoch 1400: Validation loss decreased (228.745850 --> 228.684189).\n",
            "\t Train_Loss: 91.7237 Val_Loss: 228.6842  BEST VAL Loss: 228.6842\n",
            "\n",
            "Epoch 1401: Validation loss decreased (228.684189 --> 228.622177).\n",
            "\t Train_Loss: 91.6907 Val_Loss: 228.6222  BEST VAL Loss: 228.6222\n",
            "\n",
            "Epoch 1402: Validation loss decreased (228.622177 --> 228.560410).\n",
            "\t Train_Loss: 91.6578 Val_Loss: 228.5604  BEST VAL Loss: 228.5604\n",
            "\n",
            "Epoch 1403: Validation loss decreased (228.560410 --> 228.498581).\n",
            "\t Train_Loss: 91.6248 Val_Loss: 228.4986  BEST VAL Loss: 228.4986\n",
            "\n",
            "Epoch 1404: Validation loss decreased (228.498581 --> 228.436523).\n",
            "\t Train_Loss: 91.5919 Val_Loss: 228.4365  BEST VAL Loss: 228.4365\n",
            "\n",
            "Epoch 1405: Validation loss decreased (228.436523 --> 228.374557).\n",
            "\t Train_Loss: 91.5590 Val_Loss: 228.3746  BEST VAL Loss: 228.3746\n",
            "\n",
            "Epoch 1406: Validation loss decreased (228.374557 --> 228.312668).\n",
            "\t Train_Loss: 91.5261 Val_Loss: 228.3127  BEST VAL Loss: 228.3127\n",
            "\n",
            "Epoch 1407: Validation loss decreased (228.312668 --> 228.250839).\n",
            "\t Train_Loss: 91.4932 Val_Loss: 228.2508  BEST VAL Loss: 228.2508\n",
            "\n",
            "Epoch 1408: Validation loss decreased (228.250839 --> 228.188950).\n",
            "\t Train_Loss: 91.4603 Val_Loss: 228.1889  BEST VAL Loss: 228.1889\n",
            "\n",
            "Epoch 1409: Validation loss decreased (228.188950 --> 228.127151).\n",
            "\t Train_Loss: 91.4274 Val_Loss: 228.1272  BEST VAL Loss: 228.1272\n",
            "\n",
            "Epoch 1410: Validation loss decreased (228.127151 --> 228.065460).\n",
            "\t Train_Loss: 91.3945 Val_Loss: 228.0655  BEST VAL Loss: 228.0655\n",
            "\n",
            "Epoch 1411: Validation loss decreased (228.065460 --> 228.003586).\n",
            "\t Train_Loss: 91.3617 Val_Loss: 228.0036  BEST VAL Loss: 228.0036\n",
            "\n",
            "Epoch 1412: Validation loss decreased (228.003586 --> 227.941849).\n",
            "\t Train_Loss: 91.3288 Val_Loss: 227.9418  BEST VAL Loss: 227.9418\n",
            "\n",
            "Epoch 1413: Validation loss decreased (227.941849 --> 227.880127).\n",
            "\t Train_Loss: 91.2960 Val_Loss: 227.8801  BEST VAL Loss: 227.8801\n",
            "\n",
            "Epoch 1414: Validation loss decreased (227.880127 --> 227.818192).\n",
            "\t Train_Loss: 91.2632 Val_Loss: 227.8182  BEST VAL Loss: 227.8182\n",
            "\n",
            "Epoch 1415: Validation loss decreased (227.818192 --> 227.756332).\n",
            "\t Train_Loss: 91.2303 Val_Loss: 227.7563  BEST VAL Loss: 227.7563\n",
            "\n",
            "Epoch 1416: Validation loss decreased (227.756332 --> 227.694626).\n",
            "\t Train_Loss: 91.1976 Val_Loss: 227.6946  BEST VAL Loss: 227.6946\n",
            "\n",
            "Epoch 1417: Validation loss decreased (227.694626 --> 227.632721).\n",
            "\t Train_Loss: 91.1647 Val_Loss: 227.6327  BEST VAL Loss: 227.6327\n",
            "\n",
            "Epoch 1418: Validation loss decreased (227.632721 --> 227.571075).\n",
            "\t Train_Loss: 91.1320 Val_Loss: 227.5711  BEST VAL Loss: 227.5711\n",
            "\n",
            "Epoch 1419: Validation loss decreased (227.571075 --> 227.509140).\n",
            "\t Train_Loss: 91.0992 Val_Loss: 227.5091  BEST VAL Loss: 227.5091\n",
            "\n",
            "Epoch 1420: Validation loss decreased (227.509140 --> 227.447556).\n",
            "\t Train_Loss: 91.0664 Val_Loss: 227.4476  BEST VAL Loss: 227.4476\n",
            "\n",
            "Epoch 1421: Validation loss decreased (227.447556 --> 227.385849).\n",
            "\t Train_Loss: 91.0337 Val_Loss: 227.3858  BEST VAL Loss: 227.3858\n",
            "\n",
            "Epoch 1422: Validation loss decreased (227.385849 --> 227.324020).\n",
            "\t Train_Loss: 91.0009 Val_Loss: 227.3240  BEST VAL Loss: 227.3240\n",
            "\n",
            "Epoch 1423: Validation loss decreased (227.324020 --> 227.262436).\n",
            "\t Train_Loss: 90.9682 Val_Loss: 227.2624  BEST VAL Loss: 227.2624\n",
            "\n",
            "Epoch 1424: Validation loss decreased (227.262436 --> 227.200607).\n",
            "\t Train_Loss: 90.9355 Val_Loss: 227.2006  BEST VAL Loss: 227.2006\n",
            "\n",
            "Epoch 1425: Validation loss decreased (227.200607 --> 227.138779).\n",
            "\t Train_Loss: 90.9027 Val_Loss: 227.1388  BEST VAL Loss: 227.1388\n",
            "\n",
            "Epoch 1426: Validation loss decreased (227.138779 --> 227.077393).\n",
            "\t Train_Loss: 90.8700 Val_Loss: 227.0774  BEST VAL Loss: 227.0774\n",
            "\n",
            "Epoch 1427: Validation loss decreased (227.077393 --> 227.015625).\n",
            "\t Train_Loss: 90.8373 Val_Loss: 227.0156  BEST VAL Loss: 227.0156\n",
            "\n",
            "Epoch 1428: Validation loss decreased (227.015625 --> 226.953857).\n",
            "\t Train_Loss: 90.8046 Val_Loss: 226.9539  BEST VAL Loss: 226.9539\n",
            "\n",
            "Epoch 1429: Validation loss decreased (226.953857 --> 226.892258).\n",
            "\t Train_Loss: 90.7720 Val_Loss: 226.8923  BEST VAL Loss: 226.8923\n",
            "\n",
            "Epoch 1430: Validation loss decreased (226.892258 --> 226.830811).\n",
            "\t Train_Loss: 90.7393 Val_Loss: 226.8308  BEST VAL Loss: 226.8308\n",
            "\n",
            "Epoch 1431: Validation loss decreased (226.830811 --> 226.769043).\n",
            "\t Train_Loss: 90.7067 Val_Loss: 226.7690  BEST VAL Loss: 226.7690\n",
            "\n",
            "Epoch 1432: Validation loss decreased (226.769043 --> 226.707230).\n",
            "\t Train_Loss: 90.6741 Val_Loss: 226.7072  BEST VAL Loss: 226.7072\n",
            "\n",
            "Epoch 1433: Validation loss decreased (226.707230 --> 226.645798).\n",
            "\t Train_Loss: 90.6414 Val_Loss: 226.6458  BEST VAL Loss: 226.6458\n",
            "\n",
            "Epoch 1434: Validation loss decreased (226.645798 --> 226.584106).\n",
            "\t Train_Loss: 90.6088 Val_Loss: 226.5841  BEST VAL Loss: 226.5841\n",
            "\n",
            "Epoch 1435: Validation loss decreased (226.584106 --> 226.522507).\n",
            "\t Train_Loss: 90.5762 Val_Loss: 226.5225  BEST VAL Loss: 226.5225\n",
            "\n",
            "Epoch 1436: Validation loss decreased (226.522507 --> 226.460968).\n",
            "\t Train_Loss: 90.5435 Val_Loss: 226.4610  BEST VAL Loss: 226.4610\n",
            "\n",
            "Epoch 1437: Validation loss decreased (226.460968 --> 226.399399).\n",
            "\t Train_Loss: 90.5109 Val_Loss: 226.3994  BEST VAL Loss: 226.3994\n",
            "\n",
            "Epoch 1438: Validation loss decreased (226.399399 --> 226.337646).\n",
            "\t Train_Loss: 90.4783 Val_Loss: 226.3376  BEST VAL Loss: 226.3376\n",
            "\n",
            "Epoch 1439: Validation loss decreased (226.337646 --> 226.275925).\n",
            "\t Train_Loss: 90.4458 Val_Loss: 226.2759  BEST VAL Loss: 226.2759\n",
            "\n",
            "Epoch 1440: Validation loss decreased (226.275925 --> 226.214752).\n",
            "\t Train_Loss: 90.4132 Val_Loss: 226.2148  BEST VAL Loss: 226.2148\n",
            "\n",
            "Epoch 1441: Validation loss decreased (226.214752 --> 226.153122).\n",
            "\t Train_Loss: 90.3806 Val_Loss: 226.1531  BEST VAL Loss: 226.1531\n",
            "\n",
            "Epoch 1442: Validation loss decreased (226.153122 --> 226.091461).\n",
            "\t Train_Loss: 90.3481 Val_Loss: 226.0915  BEST VAL Loss: 226.0915\n",
            "\n",
            "Epoch 1443: Validation loss decreased (226.091461 --> 226.029892).\n",
            "\t Train_Loss: 90.3155 Val_Loss: 226.0299  BEST VAL Loss: 226.0299\n",
            "\n",
            "Epoch 1444: Validation loss decreased (226.029892 --> 225.968369).\n",
            "\t Train_Loss: 90.2830 Val_Loss: 225.9684  BEST VAL Loss: 225.9684\n",
            "\n",
            "Epoch 1445: Validation loss decreased (225.968369 --> 225.906738).\n",
            "\t Train_Loss: 90.2504 Val_Loss: 225.9067  BEST VAL Loss: 225.9067\n",
            "\n",
            "Epoch 1446: Validation loss decreased (225.906738 --> 225.845367).\n",
            "\t Train_Loss: 90.2179 Val_Loss: 225.8454  BEST VAL Loss: 225.8454\n",
            "\n",
            "Epoch 1447: Validation loss decreased (225.845367 --> 225.784256).\n",
            "\t Train_Loss: 90.1853 Val_Loss: 225.7843  BEST VAL Loss: 225.7843\n",
            "\n",
            "Epoch 1448: Validation loss decreased (225.784256 --> 225.722565).\n",
            "\t Train_Loss: 90.1528 Val_Loss: 225.7226  BEST VAL Loss: 225.7226\n",
            "\n",
            "Epoch 1449: Validation loss decreased (225.722565 --> 225.661011).\n",
            "\t Train_Loss: 90.1203 Val_Loss: 225.6610  BEST VAL Loss: 225.6610\n",
            "\n",
            "Epoch 1450: Validation loss decreased (225.661011 --> 225.599274).\n",
            "\t Train_Loss: 90.0878 Val_Loss: 225.5993  BEST VAL Loss: 225.5993\n",
            "\n",
            "Epoch 1451: Validation loss decreased (225.599274 --> 225.538086).\n",
            "\t Train_Loss: 90.0552 Val_Loss: 225.5381  BEST VAL Loss: 225.5381\n",
            "\n",
            "Epoch 1452: Validation loss decreased (225.538086 --> 225.476837).\n",
            "\t Train_Loss: 90.0227 Val_Loss: 225.4768  BEST VAL Loss: 225.4768\n",
            "\n",
            "Epoch 1453: Validation loss decreased (225.476837 --> 225.415085).\n",
            "\t Train_Loss: 89.9901 Val_Loss: 225.4151  BEST VAL Loss: 225.4151\n",
            "\n",
            "Epoch 1454: Validation loss decreased (225.415085 --> 225.353760).\n",
            "\t Train_Loss: 89.9576 Val_Loss: 225.3538  BEST VAL Loss: 225.3538\n",
            "\n",
            "Epoch 1455: Validation loss decreased (225.353760 --> 225.292236).\n",
            "\t Train_Loss: 89.9250 Val_Loss: 225.2922  BEST VAL Loss: 225.2922\n",
            "\n",
            "Epoch 1456: Validation loss decreased (225.292236 --> 225.230957).\n",
            "\t Train_Loss: 89.8924 Val_Loss: 225.2310  BEST VAL Loss: 225.2310\n",
            "\n",
            "Epoch 1457: Validation loss decreased (225.230957 --> 225.169708).\n",
            "\t Train_Loss: 89.8597 Val_Loss: 225.1697  BEST VAL Loss: 225.1697\n",
            "\n",
            "Epoch 1458: Validation loss decreased (225.169708 --> 225.108276).\n",
            "\t Train_Loss: 89.8270 Val_Loss: 225.1083  BEST VAL Loss: 225.1083\n",
            "\n",
            "Epoch 1459: Validation loss decreased (225.108276 --> 225.047028).\n",
            "\t Train_Loss: 89.7943 Val_Loss: 225.0470  BEST VAL Loss: 225.0470\n",
            "\n",
            "Epoch 1460: Validation loss decreased (225.047028 --> 224.985580).\n",
            "\t Train_Loss: 89.7615 Val_Loss: 224.9856  BEST VAL Loss: 224.9856\n",
            "\n",
            "Epoch 1461: Validation loss decreased (224.985580 --> 224.924469).\n",
            "\t Train_Loss: 89.7286 Val_Loss: 224.9245  BEST VAL Loss: 224.9245\n",
            "\n",
            "Epoch 1462: Validation loss decreased (224.924469 --> 224.863525).\n",
            "\t Train_Loss: 89.6956 Val_Loss: 224.8635  BEST VAL Loss: 224.8635\n",
            "\n",
            "Epoch 1463: Validation loss decreased (224.863525 --> 224.802231).\n",
            "\t Train_Loss: 89.6624 Val_Loss: 224.8022  BEST VAL Loss: 224.8022\n",
            "\n",
            "Epoch 1464: Validation loss decreased (224.802231 --> 224.741089).\n",
            "\t Train_Loss: 89.6289 Val_Loss: 224.7411  BEST VAL Loss: 224.7411\n",
            "\n",
            "Epoch 1465: Validation loss decreased (224.741089 --> 224.680466).\n",
            "\t Train_Loss: 89.5949 Val_Loss: 224.6805  BEST VAL Loss: 224.6805\n",
            "\n",
            "Epoch 1466: Validation loss decreased (224.680466 --> 224.620468).\n",
            "\t Train_Loss: 89.5604 Val_Loss: 224.6205  BEST VAL Loss: 224.6205\n",
            "\n",
            "Epoch 1467: Validation loss decreased (224.620468 --> 224.560425).\n",
            "\t Train_Loss: 89.5247 Val_Loss: 224.5604  BEST VAL Loss: 224.5604\n",
            "\n",
            "Epoch 1468: Validation loss decreased (224.560425 --> 224.501984).\n",
            "\t Train_Loss: 89.4872 Val_Loss: 224.5020  BEST VAL Loss: 224.5020\n",
            "\n",
            "Epoch 1469: Validation loss decreased (224.501984 --> 224.446823).\n",
            "\t Train_Loss: 89.4460 Val_Loss: 224.4468  BEST VAL Loss: 224.4468\n",
            "\n",
            "Epoch 1470: Validation loss decreased (224.446823 --> 224.400833).\n",
            "\t Train_Loss: 89.3966 Val_Loss: 224.4008  BEST VAL Loss: 224.4008\n",
            "\n",
            "Epoch 1471: Validation loss decreased (224.400833 --> 224.399704).\n",
            "\t Train_Loss: 89.3246 Val_Loss: 224.3997  BEST VAL Loss: 224.3997\n",
            "\n",
            "Epoch 1472: Validation loss did not decrease\n",
            "\t Train_Loss: 89.1635 Val_Loss: 224.9106  BEST VAL Loss: 224.3997\n",
            "\n",
            "Epoch 1473: Validation loss did not decrease\n",
            "\t Train_Loss: 88.2764 Val_Loss: 279.5345  BEST VAL Loss: 224.3997\n",
            "\n",
            "Epoch 1474: Validation loss decreased (224.399704 --> 223.440216).\n",
            "\t Train_Loss: 91.2647 Val_Loss: 223.4402  BEST VAL Loss: 223.4402\n",
            "\n",
            "Epoch 1475: Validation loss decreased (223.440216 --> 222.048462).\n",
            "\t Train_Loss: 88.0667 Val_Loss: 222.0485  BEST VAL Loss: 222.0485\n",
            "\n",
            "Epoch 1476: Validation loss decreased (222.048462 --> 221.811768).\n",
            "\t Train_Loss: 89.3005 Val_Loss: 221.8118  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1477: Validation loss did not decrease\n",
            "\t Train_Loss: 89.3511 Val_Loss: 222.1155  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1478: Validation loss did not decrease\n",
            "\t Train_Loss: 89.2610 Val_Loss: 222.8156  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1479: Validation loss did not decrease\n",
            "\t Train_Loss: 89.1646 Val_Loss: 223.6218  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1480: Validation loss did not decrease\n",
            "\t Train_Loss: 89.1405 Val_Loss: 224.1249  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1481: Validation loss did not decrease\n",
            "\t Train_Loss: 89.1684 Val_Loss: 224.0435  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1482: Validation loss did not decrease\n",
            "\t Train_Loss: 89.1427 Val_Loss: 223.4136  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1483: Validation loss did not decrease\n",
            "\t Train_Loss: 89.0571 Val_Loss: 222.5413  BEST VAL Loss: 221.8118\n",
            "\n",
            "Epoch 1484: Validation loss decreased (221.811768 --> 221.783981).\n",
            "\t Train_Loss: 88.9993 Val_Loss: 221.7840  BEST VAL Loss: 221.7840\n",
            "\n",
            "Epoch 1485: Validation loss decreased (221.783981 --> 221.355270).\n",
            "\t Train_Loss: 88.9904 Val_Loss: 221.3553  BEST VAL Loss: 221.3553\n",
            "\n",
            "Epoch 1486: Validation loss decreased (221.355270 --> 221.309189).\n",
            "\t Train_Loss: 88.9853 Val_Loss: 221.3092  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1487: Validation loss did not decrease\n",
            "\t Train_Loss: 88.9502 Val_Loss: 221.6003  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1488: Validation loss did not decrease\n",
            "\t Train_Loss: 88.8910 Val_Loss: 222.0972  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1489: Validation loss did not decrease\n",
            "\t Train_Loss: 88.8402 Val_Loss: 222.5821  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1490: Validation loss did not decrease\n",
            "\t Train_Loss: 88.8189 Val_Loss: 222.8256  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1491: Validation loss did not decrease\n",
            "\t Train_Loss: 88.8060 Val_Loss: 222.7111  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1492: Validation loss did not decrease\n",
            "\t Train_Loss: 88.7692 Val_Loss: 222.3017  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1493: Validation loss did not decrease\n",
            "\t Train_Loss: 88.7177 Val_Loss: 221.7893  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1494: Validation loss did not decrease\n",
            "\t Train_Loss: 88.6801 Val_Loss: 221.3742  BEST VAL Loss: 221.3092\n",
            "\n",
            "Epoch 1495: Validation loss decreased (221.309189 --> 221.178940).\n",
            "\t Train_Loss: 88.6584 Val_Loss: 221.1789  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1496: Validation loss did not decrease\n",
            "\t Train_Loss: 88.6343 Val_Loss: 221.2311  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1497: Validation loss did not decrease\n",
            "\t Train_Loss: 88.5973 Val_Loss: 221.4779  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1498: Validation loss did not decrease\n",
            "\t Train_Loss: 88.5543 Val_Loss: 221.8037  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1499: Validation loss did not decrease\n",
            "\t Train_Loss: 88.5193 Val_Loss: 222.0579  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1500: Validation loss did not decrease\n",
            "\t Train_Loss: 88.4942 Val_Loss: 222.1220  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1501: Validation loss did not decrease\n",
            "\t Train_Loss: 88.4666 Val_Loss: 221.9692  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1502: Validation loss did not decrease\n",
            "\t Train_Loss: 88.4294 Val_Loss: 221.6741  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1503: Validation loss did not decrease\n",
            "\t Train_Loss: 88.3913 Val_Loss: 221.3639  BEST VAL Loss: 221.1789\n",
            "\n",
            "Epoch 1504: Validation loss decreased (221.178940 --> 221.151642).\n",
            "\t Train_Loss: 88.3604 Val_Loss: 221.1516  BEST VAL Loss: 221.1516\n",
            "\n",
            "Epoch 1505: Validation loss decreased (221.151642 --> 221.094147).\n",
            "\t Train_Loss: 88.3330 Val_Loss: 221.0941  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1506: Validation loss did not decrease\n",
            "\t Train_Loss: 88.3020 Val_Loss: 221.1848  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1507: Validation loss did not decrease\n",
            "\t Train_Loss: 88.2664 Val_Loss: 221.3633  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1508: Validation loss did not decrease\n",
            "\t Train_Loss: 88.2316 Val_Loss: 221.5366  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1509: Validation loss did not decrease\n",
            "\t Train_Loss: 88.2012 Val_Loss: 221.6162  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1510: Validation loss did not decrease\n",
            "\t Train_Loss: 88.1722 Val_Loss: 221.5597  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1511: Validation loss did not decrease\n",
            "\t Train_Loss: 88.1399 Val_Loss: 221.3881  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1512: Validation loss did not decrease\n",
            "\t Train_Loss: 88.1054 Val_Loss: 221.1718  BEST VAL Loss: 221.0941\n",
            "\n",
            "Epoch 1513: Validation loss decreased (221.094147 --> 220.989548).\n",
            "\t Train_Loss: 88.0729 Val_Loss: 220.9895  BEST VAL Loss: 220.9895\n",
            "\n",
            "Epoch 1514: Validation loss decreased (220.989548 --> 220.895370).\n",
            "\t Train_Loss: 88.0428 Val_Loss: 220.8954  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1515: Validation loss did not decrease\n",
            "\t Train_Loss: 88.0121 Val_Loss: 220.9016  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1516: Validation loss did not decrease\n",
            "\t Train_Loss: 87.9795 Val_Loss: 220.9792  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1517: Validation loss did not decrease\n",
            "\t Train_Loss: 87.9463 Val_Loss: 221.0724  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1518: Validation loss did not decrease\n",
            "\t Train_Loss: 87.9147 Val_Loss: 221.1213  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1519: Validation loss did not decrease\n",
            "\t Train_Loss: 87.8841 Val_Loss: 221.0894  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1520: Validation loss did not decrease\n",
            "\t Train_Loss: 87.8528 Val_Loss: 220.9784  BEST VAL Loss: 220.8954\n",
            "\n",
            "Epoch 1521: Validation loss decreased (220.895370 --> 220.823685).\n",
            "\t Train_Loss: 87.8202 Val_Loss: 220.8237  BEST VAL Loss: 220.8237\n",
            "\n",
            "Epoch 1522: Validation loss decreased (220.823685 --> 220.676620).\n",
            "\t Train_Loss: 87.7879 Val_Loss: 220.6766  BEST VAL Loss: 220.6766\n",
            "\n",
            "Epoch 1523: Validation loss decreased (220.676620 --> 220.576813).\n",
            "\t Train_Loss: 87.7568 Val_Loss: 220.5768  BEST VAL Loss: 220.5768\n",
            "\n",
            "Epoch 1524: Validation loss decreased (220.576813 --> 220.541168).\n",
            "\t Train_Loss: 87.7257 Val_Loss: 220.5412  BEST VAL Loss: 220.5412\n",
            "\n",
            "Epoch 1525: Validation loss did not decrease\n",
            "\t Train_Loss: 87.6940 Val_Loss: 220.5577  BEST VAL Loss: 220.5412\n",
            "\n",
            "Epoch 1526: Validation loss did not decrease\n",
            "\t Train_Loss: 87.6618 Val_Loss: 220.5934  BEST VAL Loss: 220.5412\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 87.6301 Val_Loss: 220.6106  BEST VAL Loss: 220.5412\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 87.5989 Val_Loss: 220.5813  BEST VAL Loss: 220.5412\n",
            "\n",
            "Epoch 1529: Validation loss decreased (220.541168 --> 220.500397).\n",
            "\t Train_Loss: 87.5676 Val_Loss: 220.5004  BEST VAL Loss: 220.5004\n",
            "\n",
            "Epoch 1530: Validation loss decreased (220.500397 --> 220.385651).\n",
            "\t Train_Loss: 87.5358 Val_Loss: 220.3857  BEST VAL Loss: 220.3857\n",
            "\n",
            "Epoch 1531: Validation loss decreased (220.385651 --> 220.267624).\n",
            "\t Train_Loss: 87.5040 Val_Loss: 220.2676  BEST VAL Loss: 220.2676\n",
            "\n",
            "Epoch 1532: Validation loss decreased (220.267624 --> 220.174713).\n",
            "\t Train_Loss: 87.4726 Val_Loss: 220.1747  BEST VAL Loss: 220.1747\n",
            "\n",
            "Epoch 1533: Validation loss decreased (220.174713 --> 220.120880).\n",
            "\t Train_Loss: 87.4413 Val_Loss: 220.1209  BEST VAL Loss: 220.1209\n",
            "\n",
            "Epoch 1534: Validation loss decreased (220.120880 --> 220.103027).\n",
            "\t Train_Loss: 87.4098 Val_Loss: 220.1030  BEST VAL Loss: 220.1030\n",
            "\n",
            "Epoch 1535: Validation loss decreased (220.103027 --> 220.102829).\n",
            "\t Train_Loss: 87.3782 Val_Loss: 220.1028  BEST VAL Loss: 220.1028\n",
            "\n",
            "Epoch 1536: Validation loss decreased (220.102829 --> 220.096588).\n",
            "\t Train_Loss: 87.3466 Val_Loss: 220.0966  BEST VAL Loss: 220.0966\n",
            "\n",
            "Epoch 1537: Validation loss decreased (220.096588 --> 220.063919).\n",
            "\t Train_Loss: 87.3153 Val_Loss: 220.0639  BEST VAL Loss: 220.0639\n",
            "\n",
            "Epoch 1538: Validation loss decreased (220.063919 --> 219.999466).\n",
            "\t Train_Loss: 87.2840 Val_Loss: 219.9995  BEST VAL Loss: 219.9995\n",
            "\n",
            "Epoch 1539: Validation loss decreased (219.999466 --> 219.910522).\n",
            "\t Train_Loss: 87.2524 Val_Loss: 219.9105  BEST VAL Loss: 219.9105\n",
            "\n",
            "Epoch 1540: Validation loss decreased (219.910522 --> 219.814743).\n",
            "\t Train_Loss: 87.2209 Val_Loss: 219.8147  BEST VAL Loss: 219.8147\n",
            "\n",
            "Epoch 1541: Validation loss decreased (219.814743 --> 219.731369).\n",
            "\t Train_Loss: 87.1895 Val_Loss: 219.7314  BEST VAL Loss: 219.7314\n",
            "\n",
            "Epoch 1542: Validation loss decreased (219.731369 --> 219.671829).\n",
            "\t Train_Loss: 87.1582 Val_Loss: 219.6718  BEST VAL Loss: 219.6718\n",
            "\n",
            "Epoch 1543: Validation loss decreased (219.671829 --> 219.635895).\n",
            "\t Train_Loss: 87.1268 Val_Loss: 219.6359  BEST VAL Loss: 219.6359\n",
            "\n",
            "Epoch 1544: Validation loss decreased (219.635895 --> 219.614258).\n",
            "\t Train_Loss: 87.0954 Val_Loss: 219.6143  BEST VAL Loss: 219.6143\n",
            "\n",
            "Epoch 1545: Validation loss decreased (219.614258 --> 219.591599).\n",
            "\t Train_Loss: 87.0640 Val_Loss: 219.5916  BEST VAL Loss: 219.5916\n",
            "\n",
            "Epoch 1546: Validation loss decreased (219.591599 --> 219.554855).\n",
            "\t Train_Loss: 87.0326 Val_Loss: 219.5549  BEST VAL Loss: 219.5549\n",
            "\n",
            "Epoch 1547: Validation loss decreased (219.554855 --> 219.498169).\n",
            "\t Train_Loss: 87.0013 Val_Loss: 219.4982  BEST VAL Loss: 219.4982\n",
            "\n",
            "Epoch 1548: Validation loss decreased (219.498169 --> 219.424316).\n",
            "\t Train_Loss: 86.9699 Val_Loss: 219.4243  BEST VAL Loss: 219.4243\n",
            "\n",
            "Epoch 1549: Validation loss decreased (219.424316 --> 219.344193).\n",
            "\t Train_Loss: 86.9385 Val_Loss: 219.3442  BEST VAL Loss: 219.3442\n",
            "\n",
            "Epoch 1550: Validation loss decreased (219.344193 --> 219.269562).\n",
            "\t Train_Loss: 86.9071 Val_Loss: 219.2696  BEST VAL Loss: 219.2696\n",
            "\n",
            "Epoch 1551: Validation loss decreased (219.269562 --> 219.208893).\n",
            "\t Train_Loss: 86.8758 Val_Loss: 219.2089  BEST VAL Loss: 219.2089\n",
            "\n",
            "Epoch 1552: Validation loss decreased (219.208893 --> 219.163727).\n",
            "\t Train_Loss: 86.8445 Val_Loss: 219.1637  BEST VAL Loss: 219.1637\n",
            "\n",
            "Epoch 1553: Validation loss decreased (219.163727 --> 219.128906).\n",
            "\t Train_Loss: 86.8131 Val_Loss: 219.1289  BEST VAL Loss: 219.1289\n",
            "\n",
            "Epoch 1554: Validation loss decreased (219.128906 --> 219.094955).\n",
            "\t Train_Loss: 86.7817 Val_Loss: 219.0950  BEST VAL Loss: 219.0950\n",
            "\n",
            "Epoch 1555: Validation loss decreased (219.094955 --> 219.053009).\n",
            "\t Train_Loss: 86.7503 Val_Loss: 219.0530  BEST VAL Loss: 219.0530\n",
            "\n",
            "Epoch 1556: Validation loss decreased (219.053009 --> 218.998764).\n",
            "\t Train_Loss: 86.7189 Val_Loss: 218.9988  BEST VAL Loss: 218.9988\n",
            "\n",
            "Epoch 1557: Validation loss decreased (218.998764 --> 218.932861).\n",
            "\t Train_Loss: 86.6874 Val_Loss: 218.9329  BEST VAL Loss: 218.9329\n",
            "\n",
            "Epoch 1558: Validation loss decreased (218.932861 --> 218.862045).\n",
            "\t Train_Loss: 86.6559 Val_Loss: 218.8620  BEST VAL Loss: 218.8620\n",
            "\n",
            "Epoch 1559: Validation loss decreased (218.862045 --> 218.793411).\n",
            "\t Train_Loss: 86.6243 Val_Loss: 218.7934  BEST VAL Loss: 218.7934\n",
            "\n",
            "Epoch 1560: Validation loss decreased (218.793411 --> 218.733505).\n",
            "\t Train_Loss: 86.5927 Val_Loss: 218.7335  BEST VAL Loss: 218.7335\n",
            "\n",
            "Epoch 1561: Validation loss decreased (218.733505 --> 218.683029).\n",
            "\t Train_Loss: 86.5609 Val_Loss: 218.6830  BEST VAL Loss: 218.6830\n",
            "\n",
            "Epoch 1562: Validation loss decreased (218.683029 --> 218.639893).\n",
            "\t Train_Loss: 86.5290 Val_Loss: 218.6399  BEST VAL Loss: 218.6399\n",
            "\n",
            "Epoch 1563: Validation loss decreased (218.639893 --> 218.597809).\n",
            "\t Train_Loss: 86.4969 Val_Loss: 218.5978  BEST VAL Loss: 218.5978\n",
            "\n",
            "Epoch 1564: Validation loss decreased (218.597809 --> 218.551224).\n",
            "\t Train_Loss: 86.4644 Val_Loss: 218.5512  BEST VAL Loss: 218.5512\n",
            "\n",
            "Epoch 1565: Validation loss decreased (218.551224 --> 218.497421).\n",
            "\t Train_Loss: 86.4316 Val_Loss: 218.4974  BEST VAL Loss: 218.4974\n",
            "\n",
            "Epoch 1566: Validation loss decreased (218.497421 --> 218.436142).\n",
            "\t Train_Loss: 86.3979 Val_Loss: 218.4361  BEST VAL Loss: 218.4361\n",
            "\n",
            "Epoch 1567: Validation loss decreased (218.436142 --> 218.371735).\n",
            "\t Train_Loss: 86.3630 Val_Loss: 218.3717  BEST VAL Loss: 218.3717\n",
            "\n",
            "Epoch 1568: Validation loss decreased (218.371735 --> 218.310165).\n",
            "\t Train_Loss: 86.3259 Val_Loss: 218.3102  BEST VAL Loss: 218.3102\n",
            "\n",
            "Epoch 1569: Validation loss decreased (218.310165 --> 218.257034).\n",
            "\t Train_Loss: 86.2842 Val_Loss: 218.2570  BEST VAL Loss: 218.2570\n",
            "\n",
            "Epoch 1570: Validation loss decreased (218.257034 --> 218.221359).\n",
            "\t Train_Loss: 86.2323 Val_Loss: 218.2214  BEST VAL Loss: 218.2214\n",
            "\n",
            "Epoch 1571: Validation loss did not decrease\n",
            "\t Train_Loss: 86.1495 Val_Loss: 218.2518  BEST VAL Loss: 218.2214\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 85.9286 Val_Loss: 219.3379  BEST VAL Loss: 218.2214\n",
            "\n",
            "Epoch 1573: Validation loss did not decrease\n",
            "\t Train_Loss: 84.3148 Val_Loss: 292.2422  BEST VAL Loss: 218.2214\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 92.8235 Val_Loss: 233.5902  BEST VAL Loss: 218.2214\n",
            "\n",
            "Epoch 1575: Validation loss decreased (218.221359 --> 215.243317).\n",
            "\t Train_Loss: 80.7040 Val_Loss: 215.2433  BEST VAL Loss: 215.2433\n",
            "\n",
            "Epoch 1576: Validation loss decreased (215.243317 --> 214.447800).\n",
            "\t Train_Loss: 85.2960 Val_Loss: 214.4478  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1577: Validation loss did not decrease\n",
            "\t Train_Loss: 86.2089 Val_Loss: 214.6600  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 86.2082 Val_Loss: 215.5685  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1579: Validation loss did not decrease\n",
            "\t Train_Loss: 86.0543 Val_Loss: 216.8486  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 85.9956 Val_Loss: 217.8133  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1581: Validation loss did not decrease\n",
            "\t Train_Loss: 86.0913 Val_Loss: 217.8584  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1582: Validation loss did not decrease\n",
            "\t Train_Loss: 86.1042 Val_Loss: 216.9558  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1583: Validation loss did not decrease\n",
            "\t Train_Loss: 85.9596 Val_Loss: 215.6151  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 85.8738 Val_Loss: 214.4810  BEST VAL Loss: 214.4478\n",
            "\n",
            "Epoch 1585: Validation loss decreased (214.447800 --> 213.882721).\n",
            "\t Train_Loss: 85.8946 Val_Loss: 213.8827  BEST VAL Loss: 213.8827\n",
            "\n",
            "Epoch 1586: Validation loss decreased (213.882721 --> 213.864487).\n",
            "\t Train_Loss: 85.9179 Val_Loss: 213.8645  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1587: Validation loss did not decrease\n",
            "\t Train_Loss: 85.8785 Val_Loss: 214.3570  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1588: Validation loss did not decrease\n",
            "\t Train_Loss: 85.7911 Val_Loss: 215.1790  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1589: Validation loss did not decrease\n",
            "\t Train_Loss: 85.7220 Val_Loss: 215.9755  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 85.7177 Val_Loss: 216.3407  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1591: Validation loss did not decrease\n",
            "\t Train_Loss: 85.7251 Val_Loss: 216.0998  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1592: Validation loss did not decrease\n",
            "\t Train_Loss: 85.6703 Val_Loss: 215.4206  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1593: Validation loss did not decrease\n",
            "\t Train_Loss: 85.5917 Val_Loss: 214.7052  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1594: Validation loss did not decrease\n",
            "\t Train_Loss: 85.4394 Val_Loss: 245.7385  BEST VAL Loss: 213.8645\n",
            "\n",
            "Epoch 1595: Validation loss decreased (213.864487 --> 213.147247).\n",
            "\t Train_Loss: 82.3241 Val_Loss: 213.1472  BEST VAL Loss: 213.1472\n",
            "\n",
            "Epoch 1596: Validation loss decreased (213.147247 --> 212.720123).\n",
            "\t Train_Loss: 85.3888 Val_Loss: 212.7201  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1597: Validation loss did not decrease\n",
            "\t Train_Loss: 85.6434 Val_Loss: 213.0343  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 85.5540 Val_Loss: 213.9042  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1599: Validation loss did not decrease\n",
            "\t Train_Loss: 85.4328 Val_Loss: 215.0023  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 85.3928 Val_Loss: 215.7708  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1601: Validation loss did not decrease\n",
            "\t Train_Loss: 85.4395 Val_Loss: 215.7845  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1602: Validation loss did not decrease\n",
            "\t Train_Loss: 85.4198 Val_Loss: 215.0767  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 85.3156 Val_Loss: 214.0789  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1604: Validation loss did not decrease\n",
            "\t Train_Loss: 85.2602 Val_Loss: 213.2852  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 85.2673 Val_Loss: 212.9566  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1606: Validation loss did not decrease\n",
            "\t Train_Loss: 85.2637 Val_Loss: 213.1306  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1607: Validation loss did not decrease\n",
            "\t Train_Loss: 85.2136 Val_Loss: 213.7072  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1608: Validation loss did not decrease\n",
            "\t Train_Loss: 85.1434 Val_Loss: 214.4403  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 85.1054 Val_Loss: 214.9739  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1610: Validation loss did not decrease\n",
            "\t Train_Loss: 85.1044 Val_Loss: 215.0386  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 85.0803 Val_Loss: 214.6380  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 85.0209 Val_Loss: 214.0278  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 84.9779 Val_Loss: 213.5251  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 84.9626 Val_Loss: 213.3305  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 84.9432 Val_Loss: 213.4850  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 84.9025 Val_Loss: 213.8949  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1617: Validation loss did not decrease\n",
            "\t Train_Loss: 84.8560 Val_Loss: 214.3591  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1618: Validation loss did not decrease\n",
            "\t Train_Loss: 84.8268 Val_Loss: 214.6420  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 84.8086 Val_Loss: 214.6069  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1620: Validation loss did not decrease\n",
            "\t Train_Loss: 84.7766 Val_Loss: 214.2964  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 84.7342 Val_Loss: 213.8959  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 84.7021 Val_Loss: 213.6047  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1623: Validation loss did not decrease\n",
            "\t Train_Loss: 84.6792 Val_Loss: 213.5368  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 84.6509 Val_Loss: 213.6927  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1625: Validation loss did not decrease\n",
            "\t Train_Loss: 84.6141 Val_Loss: 213.9741  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 84.5792 Val_Loss: 214.2263  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 84.5524 Val_Loss: 214.3116  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1628: Validation loss did not decrease\n",
            "\t Train_Loss: 84.5257 Val_Loss: 214.1883  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 84.4919 Val_Loss: 213.9287  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 84.4576 Val_Loss: 213.6661  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1631: Validation loss did not decrease\n",
            "\t Train_Loss: 84.4288 Val_Loss: 213.5147  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 84.4012 Val_Loss: 213.5202  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 84.3697 Val_Loss: 213.6496  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1634: Validation loss did not decrease\n",
            "\t Train_Loss: 84.3364 Val_Loss: 213.8141  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 84.3058 Val_Loss: 213.9115  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1636: Validation loss did not decrease\n",
            "\t Train_Loss: 84.2775 Val_Loss: 213.8798  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 84.2471 Val_Loss: 213.7283  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 84.2146 Val_Loss: 213.5280  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 84.1836 Val_Loss: 213.3651  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 84.1543 Val_Loss: 213.2949  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 84.1242 Val_Loss: 213.3232  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 84.0925 Val_Loss: 213.4068  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 84.0609 Val_Loss: 213.4795  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1644: Validation loss did not decrease\n",
            "\t Train_Loss: 84.0307 Val_Loss: 213.4843  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 84.0003 Val_Loss: 213.4036  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1646: Validation loss did not decrease\n",
            "\t Train_Loss: 83.9686 Val_Loss: 213.2653  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1647: Validation loss did not decrease\n",
            "\t Train_Loss: 83.9364 Val_Loss: 213.1233  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 83.9048 Val_Loss: 213.0262  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 83.8728 Val_Loss: 212.9962  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 83.8394 Val_Loss: 213.0197  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 83.8042 Val_Loss: 213.0591  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 83.7672 Val_Loss: 213.0723  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 83.7264 Val_Loss: 213.0336  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1654: Validation loss did not decrease\n",
            "\t Train_Loss: 83.6767 Val_Loss: 212.9497  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1655: Validation loss did not decrease\n",
            "\t Train_Loss: 83.6062 Val_Loss: 212.8588  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1656: Validation loss did not decrease\n",
            "\t Train_Loss: 83.4725 Val_Loss: 212.8645  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1657: Validation loss did not decrease\n",
            "\t Train_Loss: 83.0305 Val_Loss: 214.8031  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1658: Validation loss did not decrease\n",
            "\t Train_Loss: 79.5742 Val_Loss: 288.8331  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1659: Validation loss did not decrease\n",
            "\t Train_Loss: 91.0848 Val_Loss: 279.1414  BEST VAL Loss: 212.7201\n",
            "\n",
            "Epoch 1660: Validation loss decreased (212.720123 --> 212.345215).\n",
            "\t Train_Loss: 88.5778 Val_Loss: 212.3452  BEST VAL Loss: 212.3452\n",
            "\n",
            "Epoch 1661: Validation loss decreased (212.345215 --> 206.531570).\n",
            "\t Train_Loss: 76.6942 Val_Loss: 206.5316  BEST VAL Loss: 206.5316\n",
            "\n",
            "Epoch 1662: Validation loss decreased (206.531570 --> 206.319504).\n",
            "\t Train_Loss: 81.1312 Val_Loss: 206.3195  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1663: Validation loss did not decrease\n",
            "\t Train_Loss: 79.8475 Val_Loss: 212.9032  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1664: Validation loss did not decrease\n",
            "\t Train_Loss: 74.2963 Val_Loss: 239.2923  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1665: Validation loss did not decrease\n",
            "\t Train_Loss: 78.3968 Val_Loss: 226.0441  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1666: Validation loss did not decrease\n",
            "\t Train_Loss: 74.9205 Val_Loss: 210.8967  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1667: Validation loss did not decrease\n",
            "\t Train_Loss: 73.2710 Val_Loss: 207.3484  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1668: Validation loss did not decrease\n",
            "\t Train_Loss: 75.1509 Val_Loss: 208.0193  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1669: Validation loss did not decrease\n",
            "\t Train_Loss: 74.1093 Val_Loss: 212.8705  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1670: Validation loss did not decrease\n",
            "\t Train_Loss: 72.0973 Val_Loss: 220.2972  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1671: Validation loss did not decrease\n",
            "\t Train_Loss: 72.4497 Val_Loss: 222.5462  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1672: Validation loss did not decrease\n",
            "\t Train_Loss: 73.0655 Val_Loss: 218.1714  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1673: Validation loss did not decrease\n",
            "\t Train_Loss: 72.1984 Val_Loss: 210.6074  BEST VAL Loss: 206.3195\n",
            "\n",
            "Epoch 1674: Validation loss decreased (206.319504 --> 204.549545).\n",
            "\t Train_Loss: 71.2372 Val_Loss: 204.5495  BEST VAL Loss: 204.5495\n",
            "\n",
            "Epoch 1675: Validation loss decreased (204.549545 --> 201.955093).\n",
            "\t Train_Loss: 71.3099 Val_Loss: 201.9551  BEST VAL Loss: 201.9551\n",
            "\n",
            "Epoch 1676: Validation loss decreased (201.955093 --> 201.899567).\n",
            "\t Train_Loss: 71.7497 Val_Loss: 201.8996  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1677: Validation loss did not decrease\n",
            "\t Train_Loss: 71.4607 Val_Loss: 204.1366  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1678: Validation loss did not decrease\n",
            "\t Train_Loss: 70.7466 Val_Loss: 208.5045  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1679: Validation loss did not decrease\n",
            "\t Train_Loss: 70.4134 Val_Loss: 213.0678  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1680: Validation loss did not decrease\n",
            "\t Train_Loss: 70.4668 Val_Loss: 214.9422  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1681: Validation loss did not decrease\n",
            "\t Train_Loss: 70.5557 Val_Loss: 213.0997  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1682: Validation loss did not decrease\n",
            "\t Train_Loss: 70.2185 Val_Loss: 209.0408  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1683: Validation loss did not decrease\n",
            "\t Train_Loss: 69.7560 Val_Loss: 205.4546  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1684: Validation loss did not decrease\n",
            "\t Train_Loss: 69.6716 Val_Loss: 203.3459  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1685: Validation loss did not decrease\n",
            "\t Train_Loss: 69.6961 Val_Loss: 202.6744  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1686: Validation loss did not decrease\n",
            "\t Train_Loss: 69.4792 Val_Loss: 203.4668  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1687: Validation loss did not decrease\n",
            "\t Train_Loss: 69.1445 Val_Loss: 205.5376  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1688: Validation loss did not decrease\n",
            "\t Train_Loss: 68.9080 Val_Loss: 207.7431  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1689: Validation loss did not decrease\n",
            "\t Train_Loss: 68.8672 Val_Loss: 208.3457  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1690: Validation loss did not decrease\n",
            "\t Train_Loss: 68.8228 Val_Loss: 206.8035  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1691: Validation loss did not decrease\n",
            "\t Train_Loss: 68.5614 Val_Loss: 204.0958  BEST VAL Loss: 201.8996\n",
            "\n",
            "Epoch 1692: Validation loss decreased (201.899567 --> 201.855103).\n",
            "\t Train_Loss: 68.3018 Val_Loss: 201.8551  BEST VAL Loss: 201.8551\n",
            "\n",
            "Epoch 1693: Validation loss decreased (201.855103 --> 200.840744).\n",
            "\t Train_Loss: 68.2328 Val_Loss: 200.8407  BEST VAL Loss: 200.8407\n",
            "\n",
            "Epoch 1694: Validation loss did not decrease\n",
            "\t Train_Loss: 68.1862 Val_Loss: 200.9172  BEST VAL Loss: 200.8407\n",
            "\n",
            "Epoch 1695: Validation loss did not decrease\n",
            "\t Train_Loss: 68.0297 Val_Loss: 201.6808  BEST VAL Loss: 200.8407\n",
            "\n",
            "Epoch 1696: Validation loss did not decrease\n",
            "\t Train_Loss: 67.8514 Val_Loss: 202.4985  BEST VAL Loss: 200.8407\n",
            "\n",
            "Epoch 1697: Validation loss did not decrease\n",
            "\t Train_Loss: 67.7700 Val_Loss: 202.5802  BEST VAL Loss: 200.8407\n",
            "\n",
            "Epoch 1698: Validation loss did not decrease\n",
            "\t Train_Loss: 67.6604 Val_Loss: 201.6799  BEST VAL Loss: 200.8407\n",
            "\n",
            "Epoch 1699: Validation loss decreased (200.840744 --> 200.239792).\n",
            "\t Train_Loss: 67.5058 Val_Loss: 200.2398  BEST VAL Loss: 200.2398\n",
            "\n",
            "Epoch 1700: Validation loss decreased (200.239792 --> 199.063629).\n",
            "\t Train_Loss: 67.4219 Val_Loss: 199.0636  BEST VAL Loss: 199.0636\n",
            "\n",
            "Epoch 1701: Validation loss decreased (199.063629 --> 198.595947).\n",
            "\t Train_Loss: 67.3563 Val_Loss: 198.5959  BEST VAL Loss: 198.5959\n",
            "\n",
            "Epoch 1702: Validation loss did not decrease\n",
            "\t Train_Loss: 67.2245 Val_Loss: 198.6621  BEST VAL Loss: 198.5959\n",
            "\n",
            "Epoch 1703: Validation loss did not decrease\n",
            "\t Train_Loss: 67.0731 Val_Loss: 198.7546  BEST VAL Loss: 198.5959\n",
            "\n",
            "Epoch 1704: Validation loss decreased (198.595947 --> 198.500549).\n",
            "\t Train_Loss: 66.9883 Val_Loss: 198.5005  BEST VAL Loss: 198.5005\n",
            "\n",
            "Epoch 1705: Validation loss decreased (198.500549 --> 197.838135).\n",
            "\t Train_Loss: 66.9227 Val_Loss: 197.8381  BEST VAL Loss: 197.8381\n",
            "\n",
            "Epoch 1706: Validation loss decreased (197.838135 --> 196.941879).\n",
            "\t Train_Loss: 66.8024 Val_Loss: 196.9419  BEST VAL Loss: 196.9419\n",
            "\n",
            "Epoch 1707: Validation loss decreased (196.941879 --> 196.107330).\n",
            "\t Train_Loss: 66.6709 Val_Loss: 196.1073  BEST VAL Loss: 196.1073\n",
            "\n",
            "Epoch 1708: Validation loss decreased (196.107330 --> 195.586349).\n",
            "\t Train_Loss: 66.5812 Val_Loss: 195.5863  BEST VAL Loss: 195.5863\n",
            "\n",
            "Epoch 1709: Validation loss decreased (195.586349 --> 195.396942).\n",
            "\t Train_Loss: 66.5042 Val_Loss: 195.3969  BEST VAL Loss: 195.3969\n",
            "\n",
            "Epoch 1710: Validation loss decreased (195.396942 --> 195.309067).\n",
            "\t Train_Loss: 66.4025 Val_Loss: 195.3091  BEST VAL Loss: 195.3091\n",
            "\n",
            "Epoch 1711: Validation loss decreased (195.309067 --> 195.069611).\n",
            "\t Train_Loss: 66.2882 Val_Loss: 195.0696  BEST VAL Loss: 195.0696\n",
            "\n",
            "Epoch 1712: Validation loss decreased (195.069611 --> 194.611267).\n",
            "\t Train_Loss: 66.1922 Val_Loss: 194.6113  BEST VAL Loss: 194.6113\n",
            "\n",
            "Epoch 1713: Validation loss decreased (194.611267 --> 194.020142).\n",
            "\t Train_Loss: 66.1133 Val_Loss: 194.0201  BEST VAL Loss: 194.0201\n",
            "\n",
            "Epoch 1714: Validation loss decreased (194.020142 --> 193.416412).\n",
            "\t Train_Loss: 66.0247 Val_Loss: 193.4164  BEST VAL Loss: 193.4164\n",
            "\n",
            "Epoch 1715: Validation loss decreased (193.416412 --> 192.892990).\n",
            "\t Train_Loss: 65.9225 Val_Loss: 192.8930  BEST VAL Loss: 192.8930\n",
            "\n",
            "Epoch 1716: Validation loss decreased (192.892990 --> 192.474289).\n",
            "\t Train_Loss: 65.8298 Val_Loss: 192.4743  BEST VAL Loss: 192.4743\n",
            "\n",
            "Epoch 1717: Validation loss decreased (192.474289 --> 192.117828).\n",
            "\t Train_Loss: 65.7535 Val_Loss: 192.1178  BEST VAL Loss: 192.1178\n",
            "\n",
            "Epoch 1718: Validation loss decreased (192.117828 --> 191.773514).\n",
            "\t Train_Loss: 65.6733 Val_Loss: 191.7735  BEST VAL Loss: 191.7735\n",
            "\n",
            "Epoch 1719: Validation loss decreased (191.773514 --> 191.443359).\n",
            "\t Train_Loss: 65.5808 Val_Loss: 191.4434  BEST VAL Loss: 191.4434\n",
            "\n",
            "Epoch 1720: Validation loss decreased (191.443359 --> 191.156433).\n",
            "\t Train_Loss: 65.4946 Val_Loss: 191.1564  BEST VAL Loss: 191.1564\n",
            "\n",
            "Epoch 1721: Validation loss decreased (191.156433 --> 190.907776).\n",
            "\t Train_Loss: 65.4212 Val_Loss: 190.9078  BEST VAL Loss: 190.9078\n",
            "\n",
            "Epoch 1722: Validation loss decreased (190.907776 --> 190.641235).\n",
            "\t Train_Loss: 65.3450 Val_Loss: 190.6412  BEST VAL Loss: 190.6412\n",
            "\n",
            "Epoch 1723: Validation loss decreased (190.641235 --> 190.277878).\n",
            "\t Train_Loss: 65.2634 Val_Loss: 190.2779  BEST VAL Loss: 190.2779\n",
            "\n",
            "Epoch 1724: Validation loss decreased (190.277878 --> 189.787048).\n",
            "\t Train_Loss: 65.1864 Val_Loss: 189.7870  BEST VAL Loss: 189.7870\n",
            "\n",
            "Epoch 1725: Validation loss decreased (189.787048 --> 189.254547).\n",
            "\t Train_Loss: 65.1137 Val_Loss: 189.2545  BEST VAL Loss: 189.2545\n",
            "\n",
            "Epoch 1726: Validation loss decreased (189.254547 --> 188.830673).\n",
            "\t Train_Loss: 65.0417 Val_Loss: 188.8307  BEST VAL Loss: 188.8307\n",
            "\n",
            "Epoch 1727: Validation loss decreased (188.830673 --> 188.598923).\n",
            "\t Train_Loss: 64.9716 Val_Loss: 188.5989  BEST VAL Loss: 188.5989\n",
            "\n",
            "Epoch 1728: Validation loss decreased (188.598923 --> 188.514511).\n",
            "\t Train_Loss: 64.9002 Val_Loss: 188.5145  BEST VAL Loss: 188.5145\n",
            "\n",
            "Epoch 1729: Validation loss decreased (188.514511 --> 188.441452).\n",
            "\t Train_Loss: 64.8289 Val_Loss: 188.4415  BEST VAL Loss: 188.4415\n",
            "\n",
            "Epoch 1730: Validation loss decreased (188.441452 --> 188.241577).\n",
            "\t Train_Loss: 64.7634 Val_Loss: 188.2416  BEST VAL Loss: 188.2416\n",
            "\n",
            "Epoch 1731: Validation loss decreased (188.241577 --> 187.883423).\n",
            "\t Train_Loss: 64.6990 Val_Loss: 187.8834  BEST VAL Loss: 187.8834\n",
            "\n",
            "Epoch 1732: Validation loss decreased (187.883423 --> 187.457428).\n",
            "\t Train_Loss: 64.6321 Val_Loss: 187.4574  BEST VAL Loss: 187.4574\n",
            "\n",
            "Epoch 1733: Validation loss decreased (187.457428 --> 187.079926).\n",
            "\t Train_Loss: 64.5668 Val_Loss: 187.0799  BEST VAL Loss: 187.0799\n",
            "\n",
            "Epoch 1734: Validation loss decreased (187.079926 --> 186.804382).\n",
            "\t Train_Loss: 64.5040 Val_Loss: 186.8044  BEST VAL Loss: 186.8044\n",
            "\n",
            "Epoch 1735: Validation loss decreased (186.804382 --> 186.604965).\n",
            "\t Train_Loss: 64.4418 Val_Loss: 186.6050  BEST VAL Loss: 186.6050\n",
            "\n",
            "Epoch 1736: Validation loss decreased (186.604965 --> 186.418594).\n",
            "\t Train_Loss: 64.3810 Val_Loss: 186.4186  BEST VAL Loss: 186.4186\n",
            "\n",
            "Epoch 1737: Validation loss decreased (186.418594 --> 186.219162).\n",
            "\t Train_Loss: 64.3196 Val_Loss: 186.2192  BEST VAL Loss: 186.2192\n",
            "\n",
            "Epoch 1738: Validation loss decreased (186.219162 --> 186.028107).\n",
            "\t Train_Loss: 64.2580 Val_Loss: 186.0281  BEST VAL Loss: 186.0281\n",
            "\n",
            "Epoch 1739: Validation loss decreased (186.028107 --> 185.863525).\n",
            "\t Train_Loss: 64.1988 Val_Loss: 185.8635  BEST VAL Loss: 185.8635\n",
            "\n",
            "Epoch 1740: Validation loss decreased (185.863525 --> 185.703094).\n",
            "\t Train_Loss: 64.1402 Val_Loss: 185.7031  BEST VAL Loss: 185.7031\n",
            "\n",
            "Epoch 1741: Validation loss decreased (185.703094 --> 185.500137).\n",
            "\t Train_Loss: 64.0811 Val_Loss: 185.5001  BEST VAL Loss: 185.5001\n",
            "\n",
            "Epoch 1742: Validation loss decreased (185.500137 --> 185.233261).\n",
            "\t Train_Loss: 64.0228 Val_Loss: 185.2333  BEST VAL Loss: 185.2333\n",
            "\n",
            "Epoch 1743: Validation loss decreased (185.233261 --> 184.940308).\n",
            "\t Train_Loss: 63.9644 Val_Loss: 184.9403  BEST VAL Loss: 184.9403\n",
            "\n",
            "Epoch 1744: Validation loss decreased (184.940308 --> 184.688995).\n",
            "\t Train_Loss: 63.9067 Val_Loss: 184.6890  BEST VAL Loss: 184.6890\n",
            "\n",
            "Epoch 1745: Validation loss decreased (184.688995 --> 184.512375).\n",
            "\t Train_Loss: 63.8502 Val_Loss: 184.5124  BEST VAL Loss: 184.5124\n",
            "\n",
            "Epoch 1746: Validation loss decreased (184.512375 --> 184.387985).\n",
            "\t Train_Loss: 63.7931 Val_Loss: 184.3880  BEST VAL Loss: 184.3880\n",
            "\n",
            "Epoch 1747: Validation loss decreased (184.387985 --> 184.263489).\n",
            "\t Train_Loss: 63.7362 Val_Loss: 184.2635  BEST VAL Loss: 184.2635\n",
            "\n",
            "Epoch 1748: Validation loss decreased (184.263489 --> 184.110153).\n",
            "\t Train_Loss: 63.6799 Val_Loss: 184.1102  BEST VAL Loss: 184.1102\n",
            "\n",
            "Epoch 1749: Validation loss decreased (184.110153 --> 183.937622).\n",
            "\t Train_Loss: 63.6238 Val_Loss: 183.9376  BEST VAL Loss: 183.9376\n",
            "\n",
            "Epoch 1750: Validation loss decreased (183.937622 --> 183.765274).\n",
            "\t Train_Loss: 63.5685 Val_Loss: 183.7653  BEST VAL Loss: 183.7653\n",
            "\n",
            "Epoch 1751: Validation loss decreased (183.765274 --> 183.591782).\n",
            "\t Train_Loss: 63.5131 Val_Loss: 183.5918  BEST VAL Loss: 183.5918\n",
            "\n",
            "Epoch 1752: Validation loss decreased (183.591782 --> 183.401901).\n",
            "\t Train_Loss: 63.4577 Val_Loss: 183.4019  BEST VAL Loss: 183.4019\n",
            "\n",
            "Epoch 1753: Validation loss decreased (183.401901 --> 183.193588).\n",
            "\t Train_Loss: 63.4031 Val_Loss: 183.1936  BEST VAL Loss: 183.1936\n",
            "\n",
            "Epoch 1754: Validation loss decreased (183.193588 --> 182.994110).\n",
            "\t Train_Loss: 63.3486 Val_Loss: 182.9941  BEST VAL Loss: 182.9941\n",
            "\n",
            "Epoch 1755: Validation loss decreased (182.994110 --> 182.832397).\n",
            "\t Train_Loss: 63.2946 Val_Loss: 182.8324  BEST VAL Loss: 182.8324\n",
            "\n",
            "Epoch 1756: Validation loss decreased (182.832397 --> 182.708084).\n",
            "\t Train_Loss: 63.2409 Val_Loss: 182.7081  BEST VAL Loss: 182.7081\n",
            "\n",
            "Epoch 1757: Validation loss decreased (182.708084 --> 182.592697).\n",
            "\t Train_Loss: 63.1872 Val_Loss: 182.5927  BEST VAL Loss: 182.5927\n",
            "\n",
            "Epoch 1758: Validation loss decreased (182.592697 --> 182.457611).\n",
            "\t Train_Loss: 63.1341 Val_Loss: 182.4576  BEST VAL Loss: 182.4576\n",
            "\n",
            "Epoch 1759: Validation loss decreased (182.457611 --> 182.301193).\n",
            "\t Train_Loss: 63.0814 Val_Loss: 182.3012  BEST VAL Loss: 182.3012\n",
            "\n",
            "Epoch 1760: Validation loss decreased (182.301193 --> 182.138489).\n",
            "\t Train_Loss: 63.0289 Val_Loss: 182.1385  BEST VAL Loss: 182.1385\n",
            "\n",
            "Epoch 1761: Validation loss decreased (182.138489 --> 181.977356).\n",
            "\t Train_Loss: 62.9768 Val_Loss: 181.9774  BEST VAL Loss: 181.9774\n",
            "\n",
            "Epoch 1762: Validation loss decreased (181.977356 --> 181.812424).\n",
            "\t Train_Loss: 62.9249 Val_Loss: 181.8124  BEST VAL Loss: 181.8124\n",
            "\n",
            "Epoch 1763: Validation loss decreased (181.812424 --> 181.641312).\n",
            "\t Train_Loss: 62.8734 Val_Loss: 181.6413  BEST VAL Loss: 181.6413\n",
            "\n",
            "Epoch 1764: Validation loss decreased (181.641312 --> 181.476196).\n",
            "\t Train_Loss: 62.8223 Val_Loss: 181.4762  BEST VAL Loss: 181.4762\n",
            "\n",
            "Epoch 1765: Validation loss decreased (181.476196 --> 181.332687).\n",
            "\t Train_Loss: 62.7714 Val_Loss: 181.3327  BEST VAL Loss: 181.3327\n",
            "\n",
            "Epoch 1766: Validation loss decreased (181.332687 --> 181.209091).\n",
            "\t Train_Loss: 62.7209 Val_Loss: 181.2091  BEST VAL Loss: 181.2091\n",
            "\n",
            "Epoch 1767: Validation loss decreased (181.209091 --> 181.086914).\n",
            "\t Train_Loss: 62.6706 Val_Loss: 181.0869  BEST VAL Loss: 181.0869\n",
            "\n",
            "Epoch 1768: Validation loss decreased (181.086914 --> 180.951599).\n",
            "\t Train_Loss: 62.6207 Val_Loss: 180.9516  BEST VAL Loss: 180.9516\n",
            "\n",
            "Epoch 1769: Validation loss decreased (180.951599 --> 180.803070).\n",
            "\t Train_Loss: 62.5710 Val_Loss: 180.8031  BEST VAL Loss: 180.8031\n",
            "\n",
            "Epoch 1770: Validation loss decreased (180.803070 --> 180.652588).\n",
            "\t Train_Loss: 62.5216 Val_Loss: 180.6526  BEST VAL Loss: 180.6526\n",
            "\n",
            "Epoch 1771: Validation loss decreased (180.652588 --> 180.502411).\n",
            "\t Train_Loss: 62.4725 Val_Loss: 180.5024  BEST VAL Loss: 180.5024\n",
            "\n",
            "Epoch 1772: Validation loss decreased (180.502411 --> 180.350143).\n",
            "\t Train_Loss: 62.4237 Val_Loss: 180.3501  BEST VAL Loss: 180.3501\n",
            "\n",
            "Epoch 1773: Validation loss decreased (180.350143 --> 180.197342).\n",
            "\t Train_Loss: 62.3751 Val_Loss: 180.1973  BEST VAL Loss: 180.1973\n",
            "\n",
            "Epoch 1774: Validation loss decreased (180.197342 --> 180.052490).\n",
            "\t Train_Loss: 62.3268 Val_Loss: 180.0525  BEST VAL Loss: 180.0525\n",
            "\n",
            "Epoch 1775: Validation loss decreased (180.052490 --> 179.921921).\n",
            "\t Train_Loss: 62.2787 Val_Loss: 179.9219  BEST VAL Loss: 179.9219\n",
            "\n",
            "Epoch 1776: Validation loss decreased (179.921921 --> 179.799057).\n",
            "\t Train_Loss: 62.2309 Val_Loss: 179.7991  BEST VAL Loss: 179.7991\n",
            "\n",
            "Epoch 1777: Validation loss decreased (179.799057 --> 179.672531).\n",
            "\t Train_Loss: 62.1833 Val_Loss: 179.6725  BEST VAL Loss: 179.6725\n",
            "\n",
            "Epoch 1778: Validation loss decreased (179.672531 --> 179.537842).\n",
            "\t Train_Loss: 62.1360 Val_Loss: 179.5378  BEST VAL Loss: 179.5378\n",
            "\n",
            "Epoch 1779: Validation loss decreased (179.537842 --> 179.399643).\n",
            "\t Train_Loss: 62.0889 Val_Loss: 179.3996  BEST VAL Loss: 179.3996\n",
            "\n",
            "Epoch 1780: Validation loss decreased (179.399643 --> 179.260925).\n",
            "\t Train_Loss: 62.0420 Val_Loss: 179.2609  BEST VAL Loss: 179.2609\n",
            "\n",
            "Epoch 1781: Validation loss decreased (179.260925 --> 179.121689).\n",
            "\t Train_Loss: 61.9953 Val_Loss: 179.1217  BEST VAL Loss: 179.1217\n",
            "\n",
            "Epoch 1782: Validation loss decreased (179.121689 --> 178.981247).\n",
            "\t Train_Loss: 61.9489 Val_Loss: 178.9812  BEST VAL Loss: 178.9812\n",
            "\n",
            "Epoch 1783: Validation loss decreased (178.981247 --> 178.844894).\n",
            "\t Train_Loss: 61.9027 Val_Loss: 178.8449  BEST VAL Loss: 178.8449\n",
            "\n",
            "Epoch 1784: Validation loss decreased (178.844894 --> 178.717651).\n",
            "\t Train_Loss: 61.8567 Val_Loss: 178.7177  BEST VAL Loss: 178.7177\n",
            "\n",
            "Epoch 1785: Validation loss decreased (178.717651 --> 178.596664).\n",
            "\t Train_Loss: 61.8109 Val_Loss: 178.5967  BEST VAL Loss: 178.5967\n",
            "\n",
            "Epoch 1786: Validation loss decreased (178.596664 --> 178.476730).\n",
            "\t Train_Loss: 61.7653 Val_Loss: 178.4767  BEST VAL Loss: 178.4767\n",
            "\n",
            "Epoch 1787: Validation loss decreased (178.476730 --> 178.353561).\n",
            "\t Train_Loss: 61.7199 Val_Loss: 178.3536  BEST VAL Loss: 178.3536\n",
            "\n",
            "Epoch 1788: Validation loss decreased (178.353561 --> 178.228256).\n",
            "\t Train_Loss: 61.6747 Val_Loss: 178.2283  BEST VAL Loss: 178.2283\n",
            "\n",
            "Epoch 1789: Validation loss decreased (178.228256 --> 178.102646).\n",
            "\t Train_Loss: 61.6298 Val_Loss: 178.1026  BEST VAL Loss: 178.1026\n",
            "\n",
            "Epoch 1790: Validation loss decreased (178.102646 --> 177.975830).\n",
            "\t Train_Loss: 61.5850 Val_Loss: 177.9758  BEST VAL Loss: 177.9758\n",
            "\n",
            "Epoch 1791: Validation loss decreased (177.975830 --> 177.848526).\n",
            "\t Train_Loss: 61.5404 Val_Loss: 177.8485  BEST VAL Loss: 177.8485\n",
            "\n",
            "Epoch 1792: Validation loss decreased (177.848526 --> 177.723709).\n",
            "\t Train_Loss: 61.4960 Val_Loss: 177.7237  BEST VAL Loss: 177.7237\n",
            "\n",
            "Epoch 1793: Validation loss decreased (177.723709 --> 177.604156).\n",
            "\t Train_Loss: 61.4518 Val_Loss: 177.6042  BEST VAL Loss: 177.6042\n",
            "\n",
            "Epoch 1794: Validation loss decreased (177.604156 --> 177.489502).\n",
            "\t Train_Loss: 61.4078 Val_Loss: 177.4895  BEST VAL Loss: 177.4895\n",
            "\n",
            "Epoch 1795: Validation loss decreased (177.489502 --> 177.376328).\n",
            "\t Train_Loss: 61.3639 Val_Loss: 177.3763  BEST VAL Loss: 177.3763\n",
            "\n",
            "Epoch 1796: Validation loss decreased (177.376328 --> 177.263031).\n",
            "\t Train_Loss: 61.3203 Val_Loss: 177.2630  BEST VAL Loss: 177.2630\n",
            "\n",
            "Epoch 1797: Validation loss decreased (177.263031 --> 177.149368).\n",
            "\t Train_Loss: 61.2768 Val_Loss: 177.1494  BEST VAL Loss: 177.1494\n",
            "\n",
            "Epoch 1798: Validation loss decreased (177.149368 --> 177.035141).\n",
            "\t Train_Loss: 61.2335 Val_Loss: 177.0351  BEST VAL Loss: 177.0351\n",
            "\n",
            "Epoch 1799: Validation loss decreased (177.035141 --> 176.919800).\n",
            "\t Train_Loss: 61.1903 Val_Loss: 176.9198  BEST VAL Loss: 176.9198\n",
            "\n",
            "Epoch 1800: Validation loss decreased (176.919800 --> 176.803909).\n",
            "\t Train_Loss: 61.1474 Val_Loss: 176.8039  BEST VAL Loss: 176.8039\n",
            "\n",
            "Epoch 1801: Validation loss decreased (176.803909 --> 176.689774).\n",
            "\t Train_Loss: 61.1045 Val_Loss: 176.6898  BEST VAL Loss: 176.6898\n",
            "\n",
            "Epoch 1802: Validation loss decreased (176.689774 --> 176.578720).\n",
            "\t Train_Loss: 61.0619 Val_Loss: 176.5787  BEST VAL Loss: 176.5787\n",
            "\n",
            "Epoch 1803: Validation loss decreased (176.578720 --> 176.470413).\n",
            "\t Train_Loss: 61.0194 Val_Loss: 176.4704  BEST VAL Loss: 176.4704\n",
            "\n",
            "Epoch 1804: Validation loss decreased (176.470413 --> 176.363632).\n",
            "\t Train_Loss: 60.9770 Val_Loss: 176.3636  BEST VAL Loss: 176.3636\n",
            "\n",
            "Epoch 1805: Validation loss decreased (176.363632 --> 176.257736).\n",
            "\t Train_Loss: 60.9348 Val_Loss: 176.2577  BEST VAL Loss: 176.2577\n",
            "\n",
            "Epoch 1806: Validation loss decreased (176.257736 --> 176.152344).\n",
            "\t Train_Loss: 60.8928 Val_Loss: 176.1523  BEST VAL Loss: 176.1523\n",
            "\n",
            "Epoch 1807: Validation loss decreased (176.152344 --> 176.046738).\n",
            "\t Train_Loss: 60.8509 Val_Loss: 176.0467  BEST VAL Loss: 176.0467\n",
            "\n",
            "Epoch 1808: Validation loss decreased (176.046738 --> 175.940018).\n",
            "\t Train_Loss: 60.8092 Val_Loss: 175.9400  BEST VAL Loss: 175.9400\n",
            "\n",
            "Epoch 1809: Validation loss decreased (175.940018 --> 175.832840).\n",
            "\t Train_Loss: 60.7676 Val_Loss: 175.8328  BEST VAL Loss: 175.8328\n",
            "\n",
            "Epoch 1810: Validation loss decreased (175.832840 --> 175.726425).\n",
            "\t Train_Loss: 60.7261 Val_Loss: 175.7264  BEST VAL Loss: 175.7264\n",
            "\n",
            "Epoch 1811: Validation loss decreased (175.726425 --> 175.621872).\n",
            "\t Train_Loss: 60.6848 Val_Loss: 175.6219  BEST VAL Loss: 175.6219\n",
            "\n",
            "Epoch 1812: Validation loss decreased (175.621872 --> 175.518936).\n",
            "\t Train_Loss: 60.6436 Val_Loss: 175.5189  BEST VAL Loss: 175.5189\n",
            "\n",
            "Epoch 1813: Validation loss decreased (175.518936 --> 175.417480).\n",
            "\t Train_Loss: 60.6025 Val_Loss: 175.4175  BEST VAL Loss: 175.4175\n",
            "\n",
            "Epoch 1814: Validation loss decreased (175.417480 --> 175.317093).\n",
            "\t Train_Loss: 60.5616 Val_Loss: 175.3171  BEST VAL Loss: 175.3171\n",
            "\n",
            "Epoch 1815: Validation loss decreased (175.317093 --> 175.217285).\n",
            "\t Train_Loss: 60.5208 Val_Loss: 175.2173  BEST VAL Loss: 175.2173\n",
            "\n",
            "Epoch 1816: Validation loss decreased (175.217285 --> 175.117340).\n",
            "\t Train_Loss: 60.4801 Val_Loss: 175.1173  BEST VAL Loss: 175.1173\n",
            "\n",
            "Epoch 1817: Validation loss decreased (175.117340 --> 175.016693).\n",
            "\t Train_Loss: 60.4396 Val_Loss: 175.0167  BEST VAL Loss: 175.0167\n",
            "\n",
            "Epoch 1818: Validation loss decreased (175.016693 --> 174.916382).\n",
            "\t Train_Loss: 60.3991 Val_Loss: 174.9164  BEST VAL Loss: 174.9164\n",
            "\n",
            "Epoch 1819: Validation loss decreased (174.916382 --> 174.815842).\n",
            "\t Train_Loss: 60.3588 Val_Loss: 174.8158  BEST VAL Loss: 174.8158\n",
            "\n",
            "Epoch 1820: Validation loss decreased (174.815842 --> 174.716370).\n",
            "\t Train_Loss: 60.3186 Val_Loss: 174.7164  BEST VAL Loss: 174.7164\n",
            "\n",
            "Epoch 1821: Validation loss decreased (174.716370 --> 174.618057).\n",
            "\t Train_Loss: 60.2786 Val_Loss: 174.6181  BEST VAL Loss: 174.6181\n",
            "\n",
            "Epoch 1822: Validation loss decreased (174.618057 --> 174.520798).\n",
            "\t Train_Loss: 60.2386 Val_Loss: 174.5208  BEST VAL Loss: 174.5208\n",
            "\n",
            "Epoch 1823: Validation loss decreased (174.520798 --> 174.424591).\n",
            "\t Train_Loss: 60.1988 Val_Loss: 174.4246  BEST VAL Loss: 174.4246\n",
            "\n",
            "Epoch 1824: Validation loss decreased (174.424591 --> 174.328842).\n",
            "\t Train_Loss: 60.1590 Val_Loss: 174.3288  BEST VAL Loss: 174.3288\n",
            "\n",
            "Epoch 1825: Validation loss decreased (174.328842 --> 174.233521).\n",
            "\t Train_Loss: 60.1194 Val_Loss: 174.2335  BEST VAL Loss: 174.2335\n",
            "\n",
            "Epoch 1826: Validation loss decreased (174.233521 --> 174.137756).\n",
            "\t Train_Loss: 60.0799 Val_Loss: 174.1378  BEST VAL Loss: 174.1378\n",
            "\n",
            "Epoch 1827: Validation loss decreased (174.137756 --> 174.042038).\n",
            "\t Train_Loss: 60.0405 Val_Loss: 174.0420  BEST VAL Loss: 174.0420\n",
            "\n",
            "Epoch 1828: Validation loss decreased (174.042038 --> 173.946823).\n",
            "\t Train_Loss: 60.0012 Val_Loss: 173.9468  BEST VAL Loss: 173.9468\n",
            "\n",
            "Epoch 1829: Validation loss decreased (173.946823 --> 173.851791).\n",
            "\t Train_Loss: 59.9620 Val_Loss: 173.8518  BEST VAL Loss: 173.8518\n",
            "\n",
            "Epoch 1830: Validation loss decreased (173.851791 --> 173.757431).\n",
            "\t Train_Loss: 59.9229 Val_Loss: 173.7574  BEST VAL Loss: 173.7574\n",
            "\n",
            "Epoch 1831: Validation loss decreased (173.757431 --> 173.664047).\n",
            "\t Train_Loss: 59.8839 Val_Loss: 173.6640  BEST VAL Loss: 173.6640\n",
            "\n",
            "Epoch 1832: Validation loss decreased (173.664047 --> 173.571213).\n",
            "\t Train_Loss: 59.8451 Val_Loss: 173.5712  BEST VAL Loss: 173.5712\n",
            "\n",
            "Epoch 1833: Validation loss decreased (173.571213 --> 173.479370).\n",
            "\t Train_Loss: 59.8063 Val_Loss: 173.4794  BEST VAL Loss: 173.4794\n",
            "\n",
            "Epoch 1834: Validation loss decreased (173.479370 --> 173.387405).\n",
            "\t Train_Loss: 59.7676 Val_Loss: 173.3874  BEST VAL Loss: 173.3874\n",
            "\n",
            "Epoch 1835: Validation loss decreased (173.387405 --> 173.295761).\n",
            "\t Train_Loss: 59.7289 Val_Loss: 173.2958  BEST VAL Loss: 173.2958\n",
            "\n",
            "Epoch 1836: Validation loss decreased (173.295761 --> 173.204102).\n",
            "\t Train_Loss: 59.6904 Val_Loss: 173.2041  BEST VAL Loss: 173.2041\n",
            "\n",
            "Epoch 1837: Validation loss decreased (173.204102 --> 173.112595).\n",
            "\t Train_Loss: 59.6521 Val_Loss: 173.1126  BEST VAL Loss: 173.1126\n",
            "\n",
            "Epoch 1838: Validation loss decreased (173.112595 --> 173.021301).\n",
            "\t Train_Loss: 59.6137 Val_Loss: 173.0213  BEST VAL Loss: 173.0213\n",
            "\n",
            "Epoch 1839: Validation loss decreased (173.021301 --> 172.930557).\n",
            "\t Train_Loss: 59.5755 Val_Loss: 172.9306  BEST VAL Loss: 172.9306\n",
            "\n",
            "Epoch 1840: Validation loss decreased (172.930557 --> 172.840469).\n",
            "\t Train_Loss: 59.5374 Val_Loss: 172.8405  BEST VAL Loss: 172.8405\n",
            "\n",
            "Epoch 1841: Validation loss decreased (172.840469 --> 172.751053).\n",
            "\t Train_Loss: 59.4993 Val_Loss: 172.7511  BEST VAL Loss: 172.7511\n",
            "\n",
            "Epoch 1842: Validation loss decreased (172.751053 --> 172.661911).\n",
            "\t Train_Loss: 59.4613 Val_Loss: 172.6619  BEST VAL Loss: 172.6619\n",
            "\n",
            "Epoch 1843: Validation loss decreased (172.661911 --> 172.572998).\n",
            "\t Train_Loss: 59.4235 Val_Loss: 172.5730  BEST VAL Loss: 172.5730\n",
            "\n",
            "Epoch 1844: Validation loss decreased (172.572998 --> 172.484665).\n",
            "\t Train_Loss: 59.3857 Val_Loss: 172.4847  BEST VAL Loss: 172.4847\n",
            "\n",
            "Epoch 1845: Validation loss decreased (172.484665 --> 172.396179).\n",
            "\t Train_Loss: 59.3480 Val_Loss: 172.3962  BEST VAL Loss: 172.3962\n",
            "\n",
            "Epoch 1846: Validation loss decreased (172.396179 --> 172.308151).\n",
            "\t Train_Loss: 59.3104 Val_Loss: 172.3082  BEST VAL Loss: 172.3082\n",
            "\n",
            "Epoch 1847: Validation loss decreased (172.308151 --> 172.219635).\n",
            "\t Train_Loss: 59.2728 Val_Loss: 172.2196  BEST VAL Loss: 172.2196\n",
            "\n",
            "Epoch 1848: Validation loss decreased (172.219635 --> 172.131744).\n",
            "\t Train_Loss: 59.2354 Val_Loss: 172.1317  BEST VAL Loss: 172.1317\n",
            "\n",
            "Epoch 1849: Validation loss decreased (172.131744 --> 172.044754).\n",
            "\t Train_Loss: 59.1980 Val_Loss: 172.0448  BEST VAL Loss: 172.0448\n",
            "\n",
            "Epoch 1850: Validation loss decreased (172.044754 --> 171.957947).\n",
            "\t Train_Loss: 59.1606 Val_Loss: 171.9579  BEST VAL Loss: 171.9579\n",
            "\n",
            "Epoch 1851: Validation loss decreased (171.957947 --> 171.871201).\n",
            "\t Train_Loss: 59.1234 Val_Loss: 171.8712  BEST VAL Loss: 171.8712\n",
            "\n",
            "Epoch 1852: Validation loss decreased (171.871201 --> 171.785095).\n",
            "\t Train_Loss: 59.0863 Val_Loss: 171.7851  BEST VAL Loss: 171.7851\n",
            "\n",
            "Epoch 1853: Validation loss decreased (171.785095 --> 171.699112).\n",
            "\t Train_Loss: 59.0492 Val_Loss: 171.6991  BEST VAL Loss: 171.6991\n",
            "\n",
            "Epoch 1854: Validation loss decreased (171.699112 --> 171.613403).\n",
            "\t Train_Loss: 59.0122 Val_Loss: 171.6134  BEST VAL Loss: 171.6134\n",
            "\n",
            "Epoch 1855: Validation loss decreased (171.613403 --> 171.527954).\n",
            "\t Train_Loss: 58.9753 Val_Loss: 171.5280  BEST VAL Loss: 171.5280\n",
            "\n",
            "Epoch 1856: Validation loss decreased (171.527954 --> 171.442307).\n",
            "\t Train_Loss: 58.9384 Val_Loss: 171.4423  BEST VAL Loss: 171.4423\n",
            "\n",
            "Epoch 1857: Validation loss decreased (171.442307 --> 171.357117).\n",
            "\t Train_Loss: 58.9017 Val_Loss: 171.3571  BEST VAL Loss: 171.3571\n",
            "\n",
            "Epoch 1858: Validation loss decreased (171.357117 --> 171.272202).\n",
            "\t Train_Loss: 58.8650 Val_Loss: 171.2722  BEST VAL Loss: 171.2722\n",
            "\n",
            "Epoch 1859: Validation loss decreased (171.272202 --> 171.187515).\n",
            "\t Train_Loss: 58.8283 Val_Loss: 171.1875  BEST VAL Loss: 171.1875\n",
            "\n",
            "Epoch 1860: Validation loss decreased (171.187515 --> 171.103409).\n",
            "\t Train_Loss: 58.7918 Val_Loss: 171.1034  BEST VAL Loss: 171.1034\n",
            "\n",
            "Epoch 1861: Validation loss decreased (171.103409 --> 171.019455).\n",
            "\t Train_Loss: 58.7553 Val_Loss: 171.0195  BEST VAL Loss: 171.0195\n",
            "\n",
            "Epoch 1862: Validation loss decreased (171.019455 --> 170.935593).\n",
            "\t Train_Loss: 58.7188 Val_Loss: 170.9356  BEST VAL Loss: 170.9356\n",
            "\n",
            "Epoch 1863: Validation loss decreased (170.935593 --> 170.852341).\n",
            "\t Train_Loss: 58.6825 Val_Loss: 170.8523  BEST VAL Loss: 170.8523\n",
            "\n",
            "Epoch 1864: Validation loss decreased (170.852341 --> 170.769028).\n",
            "\t Train_Loss: 58.6462 Val_Loss: 170.7690  BEST VAL Loss: 170.7690\n",
            "\n",
            "Epoch 1865: Validation loss decreased (170.769028 --> 170.685699).\n",
            "\t Train_Loss: 58.6100 Val_Loss: 170.6857  BEST VAL Loss: 170.6857\n",
            "\n",
            "Epoch 1866: Validation loss decreased (170.685699 --> 170.602524).\n",
            "\t Train_Loss: 58.5738 Val_Loss: 170.6025  BEST VAL Loss: 170.6025\n",
            "\n",
            "Epoch 1867: Validation loss decreased (170.602524 --> 170.520340).\n",
            "\t Train_Loss: 58.5377 Val_Loss: 170.5203  BEST VAL Loss: 170.5203\n",
            "\n",
            "Epoch 1868: Validation loss decreased (170.520340 --> 170.437683).\n",
            "\t Train_Loss: 58.5017 Val_Loss: 170.4377  BEST VAL Loss: 170.4377\n",
            "\n",
            "Epoch 1869: Validation loss decreased (170.437683 --> 170.355667).\n",
            "\t Train_Loss: 58.4657 Val_Loss: 170.3557  BEST VAL Loss: 170.3557\n",
            "\n",
            "Epoch 1870: Validation loss decreased (170.355667 --> 170.273514).\n",
            "\t Train_Loss: 58.4299 Val_Loss: 170.2735  BEST VAL Loss: 170.2735\n",
            "\n",
            "Epoch 1871: Validation loss decreased (170.273514 --> 170.191879).\n",
            "\t Train_Loss: 58.3940 Val_Loss: 170.1919  BEST VAL Loss: 170.1919\n",
            "\n",
            "Epoch 1872: Validation loss decreased (170.191879 --> 170.110596).\n",
            "\t Train_Loss: 58.3582 Val_Loss: 170.1106  BEST VAL Loss: 170.1106\n",
            "\n",
            "Epoch 1873: Validation loss decreased (170.110596 --> 170.029221).\n",
            "\t Train_Loss: 58.3225 Val_Loss: 170.0292  BEST VAL Loss: 170.0292\n",
            "\n",
            "Epoch 1874: Validation loss decreased (170.029221 --> 169.948105).\n",
            "\t Train_Loss: 58.2869 Val_Loss: 169.9481  BEST VAL Loss: 169.9481\n",
            "\n",
            "Epoch 1875: Validation loss decreased (169.948105 --> 169.867188).\n",
            "\t Train_Loss: 58.2513 Val_Loss: 169.8672  BEST VAL Loss: 169.8672\n",
            "\n",
            "Epoch 1876: Validation loss decreased (169.867188 --> 169.786758).\n",
            "\t Train_Loss: 58.2158 Val_Loss: 169.7868  BEST VAL Loss: 169.7868\n",
            "\n",
            "Epoch 1877: Validation loss decreased (169.786758 --> 169.706100).\n",
            "\t Train_Loss: 58.1803 Val_Loss: 169.7061  BEST VAL Loss: 169.7061\n",
            "\n",
            "Epoch 1878: Validation loss decreased (169.706100 --> 169.626114).\n",
            "\t Train_Loss: 58.1449 Val_Loss: 169.6261  BEST VAL Loss: 169.6261\n",
            "\n",
            "Epoch 1879: Validation loss decreased (169.626114 --> 169.545807).\n",
            "\t Train_Loss: 58.1096 Val_Loss: 169.5458  BEST VAL Loss: 169.5458\n",
            "\n",
            "Epoch 1880: Validation loss decreased (169.545807 --> 169.466019).\n",
            "\t Train_Loss: 58.0743 Val_Loss: 169.4660  BEST VAL Loss: 169.4660\n",
            "\n",
            "Epoch 1881: Validation loss decreased (169.466019 --> 169.386551).\n",
            "\t Train_Loss: 58.0391 Val_Loss: 169.3866  BEST VAL Loss: 169.3866\n",
            "\n",
            "Epoch 1882: Validation loss decreased (169.386551 --> 169.307373).\n",
            "\t Train_Loss: 58.0039 Val_Loss: 169.3074  BEST VAL Loss: 169.3074\n",
            "\n",
            "Epoch 1883: Validation loss decreased (169.307373 --> 169.228027).\n",
            "\t Train_Loss: 57.9688 Val_Loss: 169.2280  BEST VAL Loss: 169.2280\n",
            "\n",
            "Epoch 1884: Validation loss decreased (169.228027 --> 169.148956).\n",
            "\t Train_Loss: 57.9337 Val_Loss: 169.1490  BEST VAL Loss: 169.1490\n",
            "\n",
            "Epoch 1885: Validation loss decreased (169.148956 --> 169.070328).\n",
            "\t Train_Loss: 57.8987 Val_Loss: 169.0703  BEST VAL Loss: 169.0703\n",
            "\n",
            "Epoch 1886: Validation loss decreased (169.070328 --> 168.991714).\n",
            "\t Train_Loss: 57.8638 Val_Loss: 168.9917  BEST VAL Loss: 168.9917\n",
            "\n",
            "Epoch 1887: Validation loss decreased (168.991714 --> 168.913010).\n",
            "\t Train_Loss: 57.8289 Val_Loss: 168.9130  BEST VAL Loss: 168.9130\n",
            "\n",
            "Epoch 1888: Validation loss decreased (168.913010 --> 168.834793).\n",
            "\t Train_Loss: 57.7941 Val_Loss: 168.8348  BEST VAL Loss: 168.8348\n",
            "\n",
            "Epoch 1889: Validation loss decreased (168.834793 --> 168.756821).\n",
            "\t Train_Loss: 57.7593 Val_Loss: 168.7568  BEST VAL Loss: 168.7568\n",
            "\n",
            "Epoch 1890: Validation loss decreased (168.756821 --> 168.679169).\n",
            "\t Train_Loss: 57.7246 Val_Loss: 168.6792  BEST VAL Loss: 168.6792\n",
            "\n",
            "Epoch 1891: Validation loss decreased (168.679169 --> 168.601608).\n",
            "\t Train_Loss: 57.6899 Val_Loss: 168.6016  BEST VAL Loss: 168.6016\n",
            "\n",
            "Epoch 1892: Validation loss decreased (168.601608 --> 168.524124).\n",
            "\t Train_Loss: 57.6553 Val_Loss: 168.5241  BEST VAL Loss: 168.5241\n",
            "\n",
            "Epoch 1893: Validation loss decreased (168.524124 --> 168.446869).\n",
            "\t Train_Loss: 57.6208 Val_Loss: 168.4469  BEST VAL Loss: 168.4469\n",
            "\n",
            "Epoch 1894: Validation loss decreased (168.446869 --> 168.369629).\n",
            "\t Train_Loss: 57.5863 Val_Loss: 168.3696  BEST VAL Loss: 168.3696\n",
            "\n",
            "Epoch 1895: Validation loss decreased (168.369629 --> 168.292755).\n",
            "\t Train_Loss: 57.5518 Val_Loss: 168.2928  BEST VAL Loss: 168.2928\n",
            "\n",
            "Epoch 1896: Validation loss decreased (168.292755 --> 168.215820).\n",
            "\t Train_Loss: 57.5174 Val_Loss: 168.2158  BEST VAL Loss: 168.2158\n",
            "\n",
            "Epoch 1897: Validation loss decreased (168.215820 --> 168.139374).\n",
            "\t Train_Loss: 57.4831 Val_Loss: 168.1394  BEST VAL Loss: 168.1394\n",
            "\n",
            "Epoch 1898: Validation loss decreased (168.139374 --> 168.063019).\n",
            "\t Train_Loss: 57.4488 Val_Loss: 168.0630  BEST VAL Loss: 168.0630\n",
            "\n",
            "Epoch 1899: Validation loss decreased (168.063019 --> 167.987015).\n",
            "\t Train_Loss: 57.4145 Val_Loss: 167.9870  BEST VAL Loss: 167.9870\n",
            "\n",
            "Epoch 1900: Validation loss decreased (167.987015 --> 167.910919).\n",
            "\t Train_Loss: 57.3803 Val_Loss: 167.9109  BEST VAL Loss: 167.9109\n",
            "\n",
            "Epoch 1901: Validation loss decreased (167.910919 --> 167.835037).\n",
            "\t Train_Loss: 57.3462 Val_Loss: 167.8350  BEST VAL Loss: 167.8350\n",
            "\n",
            "Epoch 1902: Validation loss decreased (167.835037 --> 167.759445).\n",
            "\t Train_Loss: 57.3121 Val_Loss: 167.7594  BEST VAL Loss: 167.7594\n",
            "\n",
            "Epoch 1903: Validation loss decreased (167.759445 --> 167.683762).\n",
            "\t Train_Loss: 57.2781 Val_Loss: 167.6838  BEST VAL Loss: 167.6838\n",
            "\n",
            "Epoch 1904: Validation loss decreased (167.683762 --> 167.608490).\n",
            "\t Train_Loss: 57.2441 Val_Loss: 167.6085  BEST VAL Loss: 167.6085\n",
            "\n",
            "Epoch 1905: Validation loss decreased (167.608490 --> 167.533234).\n",
            "\t Train_Loss: 57.2102 Val_Loss: 167.5332  BEST VAL Loss: 167.5332\n",
            "\n",
            "Epoch 1906: Validation loss decreased (167.533234 --> 167.458298).\n",
            "\t Train_Loss: 57.1762 Val_Loss: 167.4583  BEST VAL Loss: 167.4583\n",
            "\n",
            "Epoch 1907: Validation loss decreased (167.458298 --> 167.383453).\n",
            "\t Train_Loss: 57.1424 Val_Loss: 167.3835  BEST VAL Loss: 167.3835\n",
            "\n",
            "Epoch 1908: Validation loss decreased (167.383453 --> 167.308823).\n",
            "\t Train_Loss: 57.1086 Val_Loss: 167.3088  BEST VAL Loss: 167.3088\n",
            "\n",
            "Epoch 1909: Validation loss decreased (167.308823 --> 167.234360).\n",
            "\t Train_Loss: 57.0749 Val_Loss: 167.2344  BEST VAL Loss: 167.2344\n",
            "\n",
            "Epoch 1910: Validation loss decreased (167.234360 --> 167.159958).\n",
            "\t Train_Loss: 57.0412 Val_Loss: 167.1600  BEST VAL Loss: 167.1600\n",
            "\n",
            "Epoch 1911: Validation loss decreased (167.159958 --> 167.085846).\n",
            "\t Train_Loss: 57.0076 Val_Loss: 167.0858  BEST VAL Loss: 167.0858\n",
            "\n",
            "Epoch 1912: Validation loss decreased (167.085846 --> 167.011719).\n",
            "\t Train_Loss: 56.9740 Val_Loss: 167.0117  BEST VAL Loss: 167.0117\n",
            "\n",
            "Epoch 1913: Validation loss decreased (167.011719 --> 166.938095).\n",
            "\t Train_Loss: 56.9404 Val_Loss: 166.9381  BEST VAL Loss: 166.9381\n",
            "\n",
            "Epoch 1914: Validation loss decreased (166.938095 --> 166.864212).\n",
            "\t Train_Loss: 56.9070 Val_Loss: 166.8642  BEST VAL Loss: 166.8642\n",
            "\n",
            "Epoch 1915: Validation loss decreased (166.864212 --> 166.790665).\n",
            "\t Train_Loss: 56.8735 Val_Loss: 166.7907  BEST VAL Loss: 166.7907\n",
            "\n",
            "Epoch 1916: Validation loss decreased (166.790665 --> 166.717285).\n",
            "\t Train_Loss: 56.8401 Val_Loss: 166.7173  BEST VAL Loss: 166.7173\n",
            "\n",
            "Epoch 1917: Validation loss decreased (166.717285 --> 166.644287).\n",
            "\t Train_Loss: 56.8067 Val_Loss: 166.6443  BEST VAL Loss: 166.6443\n",
            "\n",
            "Epoch 1918: Validation loss decreased (166.644287 --> 166.571335).\n",
            "\t Train_Loss: 56.7735 Val_Loss: 166.5713  BEST VAL Loss: 166.5713\n",
            "\n",
            "Epoch 1919: Validation loss decreased (166.571335 --> 166.498352).\n",
            "\t Train_Loss: 56.7402 Val_Loss: 166.4984  BEST VAL Loss: 166.4984\n",
            "\n",
            "Epoch 1920: Validation loss decreased (166.498352 --> 166.425613).\n",
            "\t Train_Loss: 56.7070 Val_Loss: 166.4256  BEST VAL Loss: 166.4256\n",
            "\n",
            "Epoch 1921: Validation loss decreased (166.425613 --> 166.352905).\n",
            "\t Train_Loss: 56.6738 Val_Loss: 166.3529  BEST VAL Loss: 166.3529\n",
            "\n",
            "Epoch 1922: Validation loss decreased (166.352905 --> 166.280380).\n",
            "\t Train_Loss: 56.6407 Val_Loss: 166.2804  BEST VAL Loss: 166.2804\n",
            "\n",
            "Epoch 1923: Validation loss decreased (166.280380 --> 166.208282).\n",
            "\t Train_Loss: 56.6076 Val_Loss: 166.2083  BEST VAL Loss: 166.2083\n",
            "\n",
            "Epoch 1924: Validation loss decreased (166.208282 --> 166.136246).\n",
            "\t Train_Loss: 56.5746 Val_Loss: 166.1362  BEST VAL Loss: 166.1362\n",
            "\n",
            "Epoch 1925: Validation loss decreased (166.136246 --> 166.064087).\n",
            "\t Train_Loss: 56.5417 Val_Loss: 166.0641  BEST VAL Loss: 166.0641\n",
            "\n",
            "Epoch 1926: Validation loss decreased (166.064087 --> 165.992554).\n",
            "\t Train_Loss: 56.5087 Val_Loss: 165.9926  BEST VAL Loss: 165.9926\n",
            "\n",
            "Epoch 1927: Validation loss decreased (165.992554 --> 165.920441).\n",
            "\t Train_Loss: 56.4758 Val_Loss: 165.9204  BEST VAL Loss: 165.9204\n",
            "\n",
            "Epoch 1928: Validation loss decreased (165.920441 --> 165.849182).\n",
            "\t Train_Loss: 56.4430 Val_Loss: 165.8492  BEST VAL Loss: 165.8492\n",
            "\n",
            "Epoch 1929: Validation loss decreased (165.849182 --> 165.777817).\n",
            "\t Train_Loss: 56.4102 Val_Loss: 165.7778  BEST VAL Loss: 165.7778\n",
            "\n",
            "Epoch 1930: Validation loss decreased (165.777817 --> 165.706360).\n",
            "\t Train_Loss: 56.3775 Val_Loss: 165.7064  BEST VAL Loss: 165.7064\n",
            "\n",
            "Epoch 1931: Validation loss decreased (165.706360 --> 165.635406).\n",
            "\t Train_Loss: 56.3448 Val_Loss: 165.6354  BEST VAL Loss: 165.6354\n",
            "\n",
            "Epoch 1932: Validation loss decreased (165.635406 --> 165.564651).\n",
            "\t Train_Loss: 56.3121 Val_Loss: 165.5647  BEST VAL Loss: 165.5647\n",
            "\n",
            "Epoch 1933: Validation loss decreased (165.564651 --> 165.493774).\n",
            "\t Train_Loss: 56.2795 Val_Loss: 165.4938  BEST VAL Loss: 165.4938\n",
            "\n",
            "Epoch 1934: Validation loss decreased (165.493774 --> 165.423096).\n",
            "\t Train_Loss: 56.2469 Val_Loss: 165.4231  BEST VAL Loss: 165.4231\n",
            "\n",
            "Epoch 1935: Validation loss decreased (165.423096 --> 165.352646).\n",
            "\t Train_Loss: 56.2144 Val_Loss: 165.3526  BEST VAL Loss: 165.3526\n",
            "\n",
            "Epoch 1936: Validation loss decreased (165.352646 --> 165.282349).\n",
            "\t Train_Loss: 56.1819 Val_Loss: 165.2823  BEST VAL Loss: 165.2823\n",
            "\n",
            "Epoch 1937: Validation loss decreased (165.282349 --> 165.211960).\n",
            "\t Train_Loss: 56.1495 Val_Loss: 165.2120  BEST VAL Loss: 165.2120\n",
            "\n",
            "Epoch 1938: Validation loss decreased (165.211960 --> 165.141754).\n",
            "\t Train_Loss: 56.1171 Val_Loss: 165.1418  BEST VAL Loss: 165.1418\n",
            "\n",
            "Epoch 1939: Validation loss decreased (165.141754 --> 165.071899).\n",
            "\t Train_Loss: 56.0848 Val_Loss: 165.0719  BEST VAL Loss: 165.0719\n",
            "\n",
            "Epoch 1940: Validation loss decreased (165.071899 --> 165.002228).\n",
            "\t Train_Loss: 56.0525 Val_Loss: 165.0022  BEST VAL Loss: 165.0022\n",
            "\n",
            "Epoch 1941: Validation loss decreased (165.002228 --> 164.932266).\n",
            "\t Train_Loss: 56.0202 Val_Loss: 164.9323  BEST VAL Loss: 164.9323\n",
            "\n",
            "Epoch 1942: Validation loss decreased (164.932266 --> 164.863205).\n",
            "\t Train_Loss: 55.9880 Val_Loss: 164.8632  BEST VAL Loss: 164.8632\n",
            "\n",
            "Epoch 1943: Validation loss decreased (164.863205 --> 164.793564).\n",
            "\t Train_Loss: 55.9558 Val_Loss: 164.7936  BEST VAL Loss: 164.7936\n",
            "\n",
            "Epoch 1944: Validation loss decreased (164.793564 --> 164.724518).\n",
            "\t Train_Loss: 55.9237 Val_Loss: 164.7245  BEST VAL Loss: 164.7245\n",
            "\n",
            "Epoch 1945: Validation loss decreased (164.724518 --> 164.655090).\n",
            "\t Train_Loss: 55.8916 Val_Loss: 164.6551  BEST VAL Loss: 164.6551\n",
            "\n",
            "Epoch 1946: Validation loss decreased (164.655090 --> 164.586212).\n",
            "\t Train_Loss: 55.8596 Val_Loss: 164.5862  BEST VAL Loss: 164.5862\n",
            "\n",
            "Epoch 1947: Validation loss decreased (164.586212 --> 164.517609).\n",
            "\t Train_Loss: 55.8276 Val_Loss: 164.5176  BEST VAL Loss: 164.5176\n",
            "\n",
            "Epoch 1948: Validation loss decreased (164.517609 --> 164.448730).\n",
            "\t Train_Loss: 55.7956 Val_Loss: 164.4487  BEST VAL Loss: 164.4487\n",
            "\n",
            "Epoch 1949: Validation loss decreased (164.448730 --> 164.379929).\n",
            "\t Train_Loss: 55.7637 Val_Loss: 164.3799  BEST VAL Loss: 164.3799\n",
            "\n",
            "Epoch 1950: Validation loss decreased (164.379929 --> 164.311539).\n",
            "\t Train_Loss: 55.7318 Val_Loss: 164.3115  BEST VAL Loss: 164.3115\n",
            "\n",
            "Epoch 1951: Validation loss decreased (164.311539 --> 164.243439).\n",
            "\t Train_Loss: 55.7000 Val_Loss: 164.2434  BEST VAL Loss: 164.2434\n",
            "\n",
            "Epoch 1952: Validation loss decreased (164.243439 --> 164.175293).\n",
            "\t Train_Loss: 55.6682 Val_Loss: 164.1753  BEST VAL Loss: 164.1753\n",
            "\n",
            "Epoch 1953: Validation loss decreased (164.175293 --> 164.107254).\n",
            "\t Train_Loss: 55.6365 Val_Loss: 164.1073  BEST VAL Loss: 164.1073\n",
            "\n",
            "Epoch 1954: Validation loss decreased (164.107254 --> 164.039230).\n",
            "\t Train_Loss: 55.6048 Val_Loss: 164.0392  BEST VAL Loss: 164.0392\n",
            "\n",
            "Epoch 1955: Validation loss decreased (164.039230 --> 163.971451).\n",
            "\t Train_Loss: 55.5732 Val_Loss: 163.9715  BEST VAL Loss: 163.9715\n",
            "\n",
            "Epoch 1956: Validation loss decreased (163.971451 --> 163.903824).\n",
            "\t Train_Loss: 55.5415 Val_Loss: 163.9038  BEST VAL Loss: 163.9038\n",
            "\n",
            "Epoch 1957: Validation loss decreased (163.903824 --> 163.836426).\n",
            "\t Train_Loss: 55.5100 Val_Loss: 163.8364  BEST VAL Loss: 163.8364\n",
            "\n",
            "Epoch 1958: Validation loss decreased (163.836426 --> 163.768951).\n",
            "\t Train_Loss: 55.4785 Val_Loss: 163.7690  BEST VAL Loss: 163.7690\n",
            "\n",
            "Epoch 1959: Validation loss decreased (163.768951 --> 163.701569).\n",
            "\t Train_Loss: 55.4470 Val_Loss: 163.7016  BEST VAL Loss: 163.7016\n",
            "\n",
            "Epoch 1960: Validation loss decreased (163.701569 --> 163.634506).\n",
            "\t Train_Loss: 55.4155 Val_Loss: 163.6345  BEST VAL Loss: 163.6345\n",
            "\n",
            "Epoch 1961: Validation loss decreased (163.634506 --> 163.567551).\n",
            "\t Train_Loss: 55.3841 Val_Loss: 163.5676  BEST VAL Loss: 163.5676\n",
            "\n",
            "Epoch 1962: Validation loss decreased (163.567551 --> 163.500626).\n",
            "\t Train_Loss: 55.3528 Val_Loss: 163.5006  BEST VAL Loss: 163.5006\n",
            "\n",
            "Epoch 1963: Validation loss decreased (163.500626 --> 163.433929).\n",
            "\t Train_Loss: 55.3215 Val_Loss: 163.4339  BEST VAL Loss: 163.4339\n",
            "\n",
            "Epoch 1964: Validation loss decreased (163.433929 --> 163.367172).\n",
            "\t Train_Loss: 55.2902 Val_Loss: 163.3672  BEST VAL Loss: 163.3672\n",
            "\n",
            "Epoch 1965: Validation loss decreased (163.367172 --> 163.300568).\n",
            "\t Train_Loss: 55.2589 Val_Loss: 163.3006  BEST VAL Loss: 163.3006\n",
            "\n",
            "Epoch 1966: Validation loss decreased (163.300568 --> 163.234268).\n",
            "\t Train_Loss: 55.2278 Val_Loss: 163.2343  BEST VAL Loss: 163.2343\n",
            "\n",
            "Epoch 1967: Validation loss decreased (163.234268 --> 163.167984).\n",
            "\t Train_Loss: 55.1966 Val_Loss: 163.1680  BEST VAL Loss: 163.1680\n",
            "\n",
            "Epoch 1968: Validation loss decreased (163.167984 --> 163.101685).\n",
            "\t Train_Loss: 55.1655 Val_Loss: 163.1017  BEST VAL Loss: 163.1017\n",
            "\n",
            "Epoch 1969: Validation loss decreased (163.101685 --> 163.035812).\n",
            "\t Train_Loss: 55.1344 Val_Loss: 163.0358  BEST VAL Loss: 163.0358\n",
            "\n",
            "Epoch 1970: Validation loss decreased (163.035812 --> 162.970093).\n",
            "\t Train_Loss: 55.1034 Val_Loss: 162.9701  BEST VAL Loss: 162.9701\n",
            "\n",
            "Epoch 1971: Validation loss decreased (162.970093 --> 162.904160).\n",
            "\t Train_Loss: 55.0724 Val_Loss: 162.9042  BEST VAL Loss: 162.9042\n",
            "\n",
            "Epoch 1972: Validation loss decreased (162.904160 --> 162.838547).\n",
            "\t Train_Loss: 55.0415 Val_Loss: 162.8385  BEST VAL Loss: 162.8385\n",
            "\n",
            "Epoch 1973: Validation loss decreased (162.838547 --> 162.772827).\n",
            "\t Train_Loss: 55.0105 Val_Loss: 162.7728  BEST VAL Loss: 162.7728\n",
            "\n",
            "Epoch 1974: Validation loss decreased (162.772827 --> 162.707520).\n",
            "\t Train_Loss: 54.9797 Val_Loss: 162.7075  BEST VAL Loss: 162.7075\n",
            "\n",
            "Epoch 1975: Validation loss decreased (162.707520 --> 162.642242).\n",
            "\t Train_Loss: 54.9489 Val_Loss: 162.6422  BEST VAL Loss: 162.6422\n",
            "\n",
            "Epoch 1976: Validation loss decreased (162.642242 --> 162.577347).\n",
            "\t Train_Loss: 54.9181 Val_Loss: 162.5773  BEST VAL Loss: 162.5773\n",
            "\n",
            "Epoch 1977: Validation loss decreased (162.577347 --> 162.512100).\n",
            "\t Train_Loss: 54.8873 Val_Loss: 162.5121  BEST VAL Loss: 162.5121\n",
            "\n",
            "Epoch 1978: Validation loss decreased (162.512100 --> 162.447098).\n",
            "\t Train_Loss: 54.8566 Val_Loss: 162.4471  BEST VAL Loss: 162.4471\n",
            "\n",
            "Epoch 1979: Validation loss decreased (162.447098 --> 162.382385).\n",
            "\t Train_Loss: 54.8260 Val_Loss: 162.3824  BEST VAL Loss: 162.3824\n",
            "\n",
            "Epoch 1980: Validation loss decreased (162.382385 --> 162.317459).\n",
            "\t Train_Loss: 54.7953 Val_Loss: 162.3175  BEST VAL Loss: 162.3175\n",
            "\n",
            "Epoch 1981: Validation loss decreased (162.317459 --> 162.253036).\n",
            "\t Train_Loss: 54.7647 Val_Loss: 162.2530  BEST VAL Loss: 162.2530\n",
            "\n",
            "Epoch 1982: Validation loss decreased (162.253036 --> 162.188629).\n",
            "\t Train_Loss: 54.7342 Val_Loss: 162.1886  BEST VAL Loss: 162.1886\n",
            "\n",
            "Epoch 1983: Validation loss decreased (162.188629 --> 162.124252).\n",
            "\t Train_Loss: 54.7037 Val_Loss: 162.1243  BEST VAL Loss: 162.1243\n",
            "\n",
            "Epoch 1984: Validation loss decreased (162.124252 --> 162.060165).\n",
            "\t Train_Loss: 54.6732 Val_Loss: 162.0602  BEST VAL Loss: 162.0602\n",
            "\n",
            "Epoch 1985: Validation loss decreased (162.060165 --> 161.995895).\n",
            "\t Train_Loss: 54.6428 Val_Loss: 161.9959  BEST VAL Loss: 161.9959\n",
            "\n",
            "Epoch 1986: Validation loss decreased (161.995895 --> 161.931946).\n",
            "\t Train_Loss: 54.6124 Val_Loss: 161.9319  BEST VAL Loss: 161.9319\n",
            "\n",
            "Epoch 1987: Validation loss decreased (161.931946 --> 161.867966).\n",
            "\t Train_Loss: 54.5820 Val_Loss: 161.8680  BEST VAL Loss: 161.8680\n",
            "\n",
            "Epoch 1988: Validation loss decreased (161.867966 --> 161.804306).\n",
            "\t Train_Loss: 54.5517 Val_Loss: 161.8043  BEST VAL Loss: 161.8043\n",
            "\n",
            "Epoch 1989: Validation loss decreased (161.804306 --> 161.740616).\n",
            "\t Train_Loss: 54.5215 Val_Loss: 161.7406  BEST VAL Loss: 161.7406\n",
            "\n",
            "Epoch 1990: Validation loss decreased (161.740616 --> 161.677109).\n",
            "\t Train_Loss: 54.4912 Val_Loss: 161.6771  BEST VAL Loss: 161.6771\n",
            "\n",
            "Epoch 1991: Validation loss decreased (161.677109 --> 161.613754).\n",
            "\t Train_Loss: 54.4610 Val_Loss: 161.6138  BEST VAL Loss: 161.6138\n",
            "\n",
            "Epoch 1992: Validation loss decreased (161.613754 --> 161.550629).\n",
            "\t Train_Loss: 54.4309 Val_Loss: 161.5506  BEST VAL Loss: 161.5506\n",
            "\n",
            "Epoch 1993: Validation loss decreased (161.550629 --> 161.487274).\n",
            "\t Train_Loss: 54.4008 Val_Loss: 161.4873  BEST VAL Loss: 161.4873\n",
            "\n",
            "Epoch 1994: Validation loss decreased (161.487274 --> 161.424072).\n",
            "\t Train_Loss: 54.3707 Val_Loss: 161.4241  BEST VAL Loss: 161.4241\n",
            "\n",
            "Epoch 1995: Validation loss decreased (161.424072 --> 161.361038).\n",
            "\t Train_Loss: 54.3406 Val_Loss: 161.3610  BEST VAL Loss: 161.3610\n",
            "\n",
            "Epoch 1996: Validation loss decreased (161.361038 --> 161.298203).\n",
            "\t Train_Loss: 54.3106 Val_Loss: 161.2982  BEST VAL Loss: 161.2982\n",
            "\n",
            "Epoch 1997: Validation loss decreased (161.298203 --> 161.235397).\n",
            "\t Train_Loss: 54.2807 Val_Loss: 161.2354  BEST VAL Loss: 161.2354\n",
            "\n",
            "Epoch 1998: Validation loss decreased (161.235397 --> 161.172882).\n",
            "\t Train_Loss: 54.2508 Val_Loss: 161.1729  BEST VAL Loss: 161.1729\n",
            "\n",
            "Epoch 1999: Validation loss decreased (161.172882 --> 161.110443).\n",
            "\t Train_Loss: 54.2209 Val_Loss: 161.1104  BEST VAL Loss: 161.1104\n",
            "\n"
          ]
        }
      ],
      "source": [
        "SimpleRNN_best_model, train_losses, val_losses = trainer(SimpleRNN_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8x-goeR4Ef0o",
        "outputId": "2ca8d7e2-b650-4fdb-9fc0-906d5fefad72"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-qi4WfFZQF-"
      },
      "source": [
        "---\n",
        "### 4.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VCb4QQCPRrzm"
      },
      "outputs": [],
      "source": [
        "val_predict_RNN = SimpleRNN_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "kjQclrpURuOf",
        "outputId": "93363a93-7e09-40af-bb36-a3ed6d37e1ee"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_RNN.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by Simple RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvDPP4kXEI-"
      },
      "source": [
        "---\n",
        "---\n",
        "## 5 GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIg9lbrXiKZ"
      },
      "source": [
        "---\n",
        "### 5.1 Define single GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EDjVZTlBXIFj"
      },
      "outputs": [],
      "source": [
        "class GRUCell(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple GRU cell network for educational purposes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_length=10, hidden_size=20, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_length = input_length\n",
        "        self.bias = bias\n",
        "\n",
        "        \"\"\"Define Needed Layers \"\"\"\n",
        "        self.reset_gate_Wxh = torch.nn.Linear(input_length, hidden_size, bias=bias)\n",
        "        self.reset_gate_Whh = torch.nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "        self.update_gate_Wxh = torch.nn.Linear(input_length, hidden_size, bias=bias)\n",
        "        self.update_gate_Whh = torch.nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "        self.output_gate_Wxh = torch.nn.Linear(input_length, hidden_size, bias=bias)\n",
        "        self.output_gate_Whh = torch.nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        bias_params = [nn.Parameter(torch.Tensor(hidden_size)) if self.bias else 0 for _ in range(3)]\n",
        "        self.br, self.bu, self.bo = bias_params\n",
        "\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "    def reset_gate(self, x, h):\n",
        "        return torch.sigmoid(self.reset_gate_Wxh(x) + self.reset_gate_Whh(h) + self.br)\n",
        "\n",
        "\n",
        "    def update_gate(self, x, h):\n",
        "        return torch.sigmoid(self.update_gate_Wxh(x) + self.update_gate_Whh(h) + self.bu)\n",
        "\n",
        "\n",
        "    def output_gate(self, x,h,r):\n",
        "        return torch.tanh(self.output_gate_Wxh(x) + r * self.output_gate_Whh(h) + self.bo)\n",
        "\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        \"\"\"Define Forward pass\"\"\"\n",
        "        if h is None:\n",
        "          h = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "        r = self.reset_gate(x, h)\n",
        "        z = self.update_gate(x, h)\n",
        "        n_t = self.output_gate(x, h, r)\n",
        "        h_new = (1 - z) * n_t + z * h\n",
        "\n",
        "        return h_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SlbDcQ-XuYx"
      },
      "source": [
        "---\n",
        "### 5.2 GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g6frzQdtXyWW"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru_cells = nn.ModuleList([GRUCell(input_size if layer == 0 else hidden_size,\n",
        "                                              hidden_size, bias) for layer in range(num_layers)])\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        if hx is None:\n",
        "            hx = [torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device) for _ in range(self.num_layers)]\n",
        "\n",
        "        output = []\n",
        "        for t in range(input.size(1)):\n",
        "            next_input = input[:,t]\n",
        "            for layer, gru_cell in enumerate(self.gru_cells):\n",
        "                hx[layer] = gru_cell(next_input, hx[layer])\n",
        "                next_input = hx[layer]\n",
        "\n",
        "            output.append(hx[-1])\n",
        "\n",
        "        output = torch.stack(output)\n",
        "        output = self.fc(output[-1])\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIm3f8yX9XF"
      },
      "source": [
        "---\n",
        "### 5.3 Train GRU model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TTE5topYD_j",
        "outputId": "b6d17437-7c31-45ab-9640-695e5a010667"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GRU(\n",
              "  (gru_cells): ModuleList(\n",
              "    (0): GRUCell(\n",
              "      (reset_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (reset_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (update_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (update_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "      (output_gate_Wxh): Linear(in_features=1, out_features=50, bias=True)\n",
              "      (output_gate_Whh): Linear(in_features=50, out_features=50, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GRU_model = GRU(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "GRU_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SdCfe-xRYKtx"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(GRU_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1xTqKgoYNNM",
        "outputId": "25312db9-998b-4c9b-e84b-49535807600a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\t Train_Loss: 201.1709 Val_Loss: 594.1055  BEST VAL Loss: 594.1055\n",
            "\n",
            "Epoch 334: Validation loss decreased (594.105530 --> 590.431824).\n",
            "\t Train_Loss: 199.8498 Val_Loss: 590.4318  BEST VAL Loss: 590.4318\n",
            "\n",
            "Epoch 335: Validation loss decreased (590.431824 --> 586.803406).\n",
            "\t Train_Loss: 198.5544 Val_Loss: 586.8034  BEST VAL Loss: 586.8034\n",
            "\n",
            "Epoch 336: Validation loss decreased (586.803406 --> 583.220337).\n",
            "\t Train_Loss: 197.2838 Val_Loss: 583.2203  BEST VAL Loss: 583.2203\n",
            "\n",
            "Epoch 337: Validation loss decreased (583.220337 --> 579.682129).\n",
            "\t Train_Loss: 196.0378 Val_Loss: 579.6821  BEST VAL Loss: 579.6821\n",
            "\n",
            "Epoch 338: Validation loss decreased (579.682129 --> 576.188660).\n",
            "\t Train_Loss: 194.8161 Val_Loss: 576.1887  BEST VAL Loss: 576.1887\n",
            "\n",
            "Epoch 339: Validation loss decreased (576.188660 --> 572.738770).\n",
            "\t Train_Loss: 193.6180 Val_Loss: 572.7388  BEST VAL Loss: 572.7388\n",
            "\n",
            "Epoch 340: Validation loss decreased (572.738770 --> 569.332153).\n",
            "\t Train_Loss: 192.4431 Val_Loss: 569.3322  BEST VAL Loss: 569.3322\n",
            "\n",
            "Epoch 341: Validation loss decreased (569.332153 --> 565.968079).\n",
            "\t Train_Loss: 191.2914 Val_Loss: 565.9681  BEST VAL Loss: 565.9681\n",
            "\n",
            "Epoch 342: Validation loss decreased (565.968079 --> 562.645691).\n",
            "\t Train_Loss: 190.1620 Val_Loss: 562.6457  BEST VAL Loss: 562.6457\n",
            "\n",
            "Epoch 343: Validation loss decreased (562.645691 --> 559.365173).\n",
            "\t Train_Loss: 189.0550 Val_Loss: 559.3652  BEST VAL Loss: 559.3652\n",
            "\n",
            "Epoch 344: Validation loss decreased (559.365173 --> 556.125671).\n",
            "\t Train_Loss: 187.9695 Val_Loss: 556.1257  BEST VAL Loss: 556.1257\n",
            "\n",
            "Epoch 345: Validation loss decreased (556.125671 --> 552.926697).\n",
            "\t Train_Loss: 186.9052 Val_Loss: 552.9267  BEST VAL Loss: 552.9267\n",
            "\n",
            "Epoch 346: Validation loss decreased (552.926697 --> 549.767395).\n",
            "\t Train_Loss: 185.8622 Val_Loss: 549.7674  BEST VAL Loss: 549.7674\n",
            "\n",
            "Epoch 347: Validation loss decreased (549.767395 --> 546.648621).\n",
            "\t Train_Loss: 184.8396 Val_Loss: 546.6486  BEST VAL Loss: 546.6486\n",
            "\n",
            "Epoch 348: Validation loss decreased (546.648621 --> 543.567749).\n",
            "\t Train_Loss: 183.8373 Val_Loss: 543.5677  BEST VAL Loss: 543.5677\n",
            "\n",
            "Epoch 349: Validation loss decreased (543.567749 --> 540.526306).\n",
            "\t Train_Loss: 182.8548 Val_Loss: 540.5263  BEST VAL Loss: 540.5263\n",
            "\n",
            "Epoch 350: Validation loss decreased (540.526306 --> 537.522644).\n",
            "\t Train_Loss: 181.8919 Val_Loss: 537.5226  BEST VAL Loss: 537.5226\n",
            "\n",
            "Epoch 351: Validation loss decreased (537.522644 --> 534.556763).\n",
            "\t Train_Loss: 180.9482 Val_Loss: 534.5568  BEST VAL Loss: 534.5568\n",
            "\n",
            "Epoch 352: Validation loss decreased (534.556763 --> 531.628052).\n",
            "\t Train_Loss: 180.0233 Val_Loss: 531.6281  BEST VAL Loss: 531.6281\n",
            "\n",
            "Epoch 353: Validation loss decreased (531.628052 --> 528.736084).\n",
            "\t Train_Loss: 179.1168 Val_Loss: 528.7361  BEST VAL Loss: 528.7361\n",
            "\n",
            "Epoch 354: Validation loss decreased (528.736084 --> 525.880066).\n",
            "\t Train_Loss: 178.2285 Val_Loss: 525.8801  BEST VAL Loss: 525.8801\n",
            "\n",
            "Epoch 355: Validation loss decreased (525.880066 --> 523.059875).\n",
            "\t Train_Loss: 177.3582 Val_Loss: 523.0599  BEST VAL Loss: 523.0599\n",
            "\n",
            "Epoch 356: Validation loss decreased (523.059875 --> 520.275574).\n",
            "\t Train_Loss: 176.5051 Val_Loss: 520.2756  BEST VAL Loss: 520.2756\n",
            "\n",
            "Epoch 357: Validation loss decreased (520.275574 --> 517.525635).\n",
            "\t Train_Loss: 175.6693 Val_Loss: 517.5256  BEST VAL Loss: 517.5256\n",
            "\n",
            "Epoch 358: Validation loss decreased (517.525635 --> 514.810242).\n",
            "\t Train_Loss: 174.8506 Val_Loss: 514.8102  BEST VAL Loss: 514.8102\n",
            "\n",
            "Epoch 359: Validation loss decreased (514.810242 --> 512.129395).\n",
            "\t Train_Loss: 174.0483 Val_Loss: 512.1294  BEST VAL Loss: 512.1294\n",
            "\n",
            "Epoch 360: Validation loss decreased (512.129395 --> 509.482178).\n",
            "\t Train_Loss: 173.2624 Val_Loss: 509.4822  BEST VAL Loss: 509.4822\n",
            "\n",
            "Epoch 361: Validation loss decreased (509.482178 --> 506.867767).\n",
            "\t Train_Loss: 172.4922 Val_Loss: 506.8678  BEST VAL Loss: 506.8678\n",
            "\n",
            "Epoch 362: Validation loss decreased (506.867767 --> 504.286041).\n",
            "\t Train_Loss: 171.7379 Val_Loss: 504.2860  BEST VAL Loss: 504.2860\n",
            "\n",
            "Epoch 363: Validation loss decreased (504.286041 --> 501.736877).\n",
            "\t Train_Loss: 170.9989 Val_Loss: 501.7369  BEST VAL Loss: 501.7369\n",
            "\n",
            "Epoch 364: Validation loss decreased (501.736877 --> 499.219818).\n",
            "\t Train_Loss: 170.2751 Val_Loss: 499.2198  BEST VAL Loss: 499.2198\n",
            "\n",
            "Epoch 365: Validation loss decreased (499.219818 --> 496.734680).\n",
            "\t Train_Loss: 169.5660 Val_Loss: 496.7347  BEST VAL Loss: 496.7347\n",
            "\n",
            "Epoch 366: Validation loss decreased (496.734680 --> 494.280273).\n",
            "\t Train_Loss: 168.8716 Val_Loss: 494.2803  BEST VAL Loss: 494.2803\n",
            "\n",
            "Epoch 367: Validation loss decreased (494.280273 --> 491.857025).\n",
            "\t Train_Loss: 168.1914 Val_Loss: 491.8570  BEST VAL Loss: 491.8570\n",
            "\n",
            "Epoch 368: Validation loss decreased (491.857025 --> 489.464081).\n",
            "\t Train_Loss: 167.5252 Val_Loss: 489.4641  BEST VAL Loss: 489.4641\n",
            "\n",
            "Epoch 369: Validation loss decreased (489.464081 --> 487.101227).\n",
            "\t Train_Loss: 166.8728 Val_Loss: 487.1012  BEST VAL Loss: 487.1012\n",
            "\n",
            "Epoch 370: Validation loss decreased (487.101227 --> 484.767975).\n",
            "\t Train_Loss: 166.2339 Val_Loss: 484.7680  BEST VAL Loss: 484.7680\n",
            "\n",
            "Epoch 371: Validation loss decreased (484.767975 --> 482.464447).\n",
            "\t Train_Loss: 165.6082 Val_Loss: 482.4644  BEST VAL Loss: 482.4644\n",
            "\n",
            "Epoch 372: Validation loss decreased (482.464447 --> 480.189545).\n",
            "\t Train_Loss: 164.9955 Val_Loss: 480.1895  BEST VAL Loss: 480.1895\n",
            "\n",
            "Epoch 373: Validation loss decreased (480.189545 --> 477.942688).\n",
            "\t Train_Loss: 164.3956 Val_Loss: 477.9427  BEST VAL Loss: 477.9427\n",
            "\n",
            "Epoch 374: Validation loss decreased (477.942688 --> 475.725311).\n",
            "\t Train_Loss: 163.8083 Val_Loss: 475.7253  BEST VAL Loss: 475.7253\n",
            "\n",
            "Epoch 375: Validation loss decreased (475.725311 --> 473.534973).\n",
            "\t Train_Loss: 163.2331 Val_Loss: 473.5350  BEST VAL Loss: 473.5350\n",
            "\n",
            "Epoch 376: Validation loss decreased (473.534973 --> 471.372772).\n",
            "\t Train_Loss: 162.6700 Val_Loss: 471.3728  BEST VAL Loss: 471.3728\n",
            "\n",
            "Epoch 377: Validation loss decreased (471.372772 --> 469.237396).\n",
            "\t Train_Loss: 162.1188 Val_Loss: 469.2374  BEST VAL Loss: 469.2374\n",
            "\n",
            "Epoch 378: Validation loss decreased (469.237396 --> 467.128815).\n",
            "\t Train_Loss: 161.5791 Val_Loss: 467.1288  BEST VAL Loss: 467.1288\n",
            "\n",
            "Epoch 379: Validation loss decreased (467.128815 --> 465.047272).\n",
            "\t Train_Loss: 161.0508 Val_Loss: 465.0473  BEST VAL Loss: 465.0473\n",
            "\n",
            "Epoch 380: Validation loss decreased (465.047272 --> 462.991364).\n",
            "\t Train_Loss: 160.5336 Val_Loss: 462.9914  BEST VAL Loss: 462.9914\n",
            "\n",
            "Epoch 381: Validation loss decreased (462.991364 --> 460.961761).\n",
            "\t Train_Loss: 160.0274 Val_Loss: 460.9618  BEST VAL Loss: 460.9618\n",
            "\n",
            "Epoch 382: Validation loss decreased (460.961761 --> 458.957825).\n",
            "\t Train_Loss: 159.5320 Val_Loss: 458.9578  BEST VAL Loss: 458.9578\n",
            "\n",
            "Epoch 383: Validation loss decreased (458.957825 --> 456.978729).\n",
            "\t Train_Loss: 159.0471 Val_Loss: 456.9787  BEST VAL Loss: 456.9787\n",
            "\n",
            "Epoch 384: Validation loss decreased (456.978729 --> 455.024872).\n",
            "\t Train_Loss: 158.5723 Val_Loss: 455.0249  BEST VAL Loss: 455.0249\n",
            "\n",
            "Epoch 385: Validation loss decreased (455.024872 --> 453.095520).\n",
            "\t Train_Loss: 158.1078 Val_Loss: 453.0955  BEST VAL Loss: 453.0955\n",
            "\n",
            "Epoch 386: Validation loss decreased (453.095520 --> 451.190338).\n",
            "\t Train_Loss: 157.6533 Val_Loss: 451.1903  BEST VAL Loss: 451.1903\n",
            "\n",
            "Epoch 387: Validation loss decreased (451.190338 --> 449.309387).\n",
            "\t Train_Loss: 157.2083 Val_Loss: 449.3094  BEST VAL Loss: 449.3094\n",
            "\n",
            "Epoch 388: Validation loss decreased (449.309387 --> 447.451904).\n",
            "\t Train_Loss: 156.7731 Val_Loss: 447.4519  BEST VAL Loss: 447.4519\n",
            "\n",
            "Epoch 389: Validation loss decreased (447.451904 --> 445.618805).\n",
            "\t Train_Loss: 156.3470 Val_Loss: 445.6188  BEST VAL Loss: 445.6188\n",
            "\n",
            "Epoch 390: Validation loss decreased (445.618805 --> 443.807526).\n",
            "\t Train_Loss: 155.9303 Val_Loss: 443.8075  BEST VAL Loss: 443.8075\n",
            "\n",
            "Epoch 391: Validation loss decreased (443.807526 --> 442.019928).\n",
            "\t Train_Loss: 155.5223 Val_Loss: 442.0199  BEST VAL Loss: 442.0199\n",
            "\n",
            "Epoch 392: Validation loss decreased (442.019928 --> 440.254883).\n",
            "\t Train_Loss: 155.1235 Val_Loss: 440.2549  BEST VAL Loss: 440.2549\n",
            "\n",
            "Epoch 393: Validation loss decreased (440.254883 --> 438.511810).\n",
            "\t Train_Loss: 154.7330 Val_Loss: 438.5118  BEST VAL Loss: 438.5118\n",
            "\n",
            "Epoch 394: Validation loss decreased (438.511810 --> 436.791168).\n",
            "\t Train_Loss: 154.3511 Val_Loss: 436.7912  BEST VAL Loss: 436.7912\n",
            "\n",
            "Epoch 395: Validation loss decreased (436.791168 --> 435.092041).\n",
            "\t Train_Loss: 153.9776 Val_Loss: 435.0920  BEST VAL Loss: 435.0920\n",
            "\n",
            "Epoch 396: Validation loss decreased (435.092041 --> 433.413971).\n",
            "\t Train_Loss: 153.6121 Val_Loss: 433.4140  BEST VAL Loss: 433.4140\n",
            "\n",
            "Epoch 397: Validation loss decreased (433.413971 --> 431.758118).\n",
            "\t Train_Loss: 153.2546 Val_Loss: 431.7581  BEST VAL Loss: 431.7581\n",
            "\n",
            "Epoch 398: Validation loss decreased (431.758118 --> 430.122467).\n",
            "\t Train_Loss: 152.9049 Val_Loss: 430.1225  BEST VAL Loss: 430.1225\n",
            "\n",
            "Epoch 399: Validation loss decreased (430.122467 --> 428.507904).\n",
            "\t Train_Loss: 152.5630 Val_Loss: 428.5079  BEST VAL Loss: 428.5079\n",
            "\n",
            "Epoch 400: Validation loss decreased (428.507904 --> 426.912994).\n",
            "\t Train_Loss: 152.2285 Val_Loss: 426.9130  BEST VAL Loss: 426.9130\n",
            "\n",
            "Epoch 401: Validation loss decreased (426.912994 --> 425.338959).\n",
            "\t Train_Loss: 151.9013 Val_Loss: 425.3390  BEST VAL Loss: 425.3390\n",
            "\n",
            "Epoch 402: Validation loss decreased (425.338959 --> 423.784393).\n",
            "\t Train_Loss: 151.5815 Val_Loss: 423.7844  BEST VAL Loss: 423.7844\n",
            "\n",
            "Epoch 403: Validation loss decreased (423.784393 --> 422.249603).\n",
            "\t Train_Loss: 151.2686 Val_Loss: 422.2496  BEST VAL Loss: 422.2496\n",
            "\n",
            "Epoch 404: Validation loss decreased (422.249603 --> 420.734863).\n",
            "\t Train_Loss: 150.9626 Val_Loss: 420.7349  BEST VAL Loss: 420.7349\n",
            "\n",
            "Epoch 405: Validation loss decreased (420.734863 --> 419.238922).\n",
            "\t Train_Loss: 150.6634 Val_Loss: 419.2389  BEST VAL Loss: 419.2389\n",
            "\n",
            "Epoch 406: Validation loss decreased (419.238922 --> 417.761536).\n",
            "\t Train_Loss: 150.3708 Val_Loss: 417.7615  BEST VAL Loss: 417.7615\n",
            "\n",
            "Epoch 407: Validation loss decreased (417.761536 --> 416.303619).\n",
            "\t Train_Loss: 150.0849 Val_Loss: 416.3036  BEST VAL Loss: 416.3036\n",
            "\n",
            "Epoch 408: Validation loss decreased (416.303619 --> 414.863922).\n",
            "\t Train_Loss: 149.8052 Val_Loss: 414.8639  BEST VAL Loss: 414.8639\n",
            "\n",
            "Epoch 409: Validation loss decreased (414.863922 --> 413.442291).\n",
            "\t Train_Loss: 149.5319 Val_Loss: 413.4423  BEST VAL Loss: 413.4423\n",
            "\n",
            "Epoch 410: Validation loss decreased (413.442291 --> 412.038422).\n",
            "\t Train_Loss: 149.2646 Val_Loss: 412.0384  BEST VAL Loss: 412.0384\n",
            "\n",
            "Epoch 411: Validation loss decreased (412.038422 --> 410.652985).\n",
            "\t Train_Loss: 149.0033 Val_Loss: 410.6530  BEST VAL Loss: 410.6530\n",
            "\n",
            "Epoch 412: Validation loss decreased (410.652985 --> 409.284821).\n",
            "\t Train_Loss: 148.7478 Val_Loss: 409.2848  BEST VAL Loss: 409.2848\n",
            "\n",
            "Epoch 413: Validation loss decreased (409.284821 --> 407.934296).\n",
            "\t Train_Loss: 148.4981 Val_Loss: 407.9343  BEST VAL Loss: 407.9343\n",
            "\n",
            "Epoch 414: Validation loss decreased (407.934296 --> 406.600586).\n",
            "\t Train_Loss: 148.2541 Val_Loss: 406.6006  BEST VAL Loss: 406.6006\n",
            "\n",
            "Epoch 415: Validation loss decreased (406.600586 --> 405.284515).\n",
            "\t Train_Loss: 148.0155 Val_Loss: 405.2845  BEST VAL Loss: 405.2845\n",
            "\n",
            "Epoch 416: Validation loss decreased (405.284515 --> 403.984619).\n",
            "\t Train_Loss: 147.7823 Val_Loss: 403.9846  BEST VAL Loss: 403.9846\n",
            "\n",
            "Epoch 417: Validation loss decreased (403.984619 --> 402.701324).\n",
            "\t Train_Loss: 147.5545 Val_Loss: 402.7013  BEST VAL Loss: 402.7013\n",
            "\n",
            "Epoch 418: Validation loss decreased (402.701324 --> 401.434174).\n",
            "\t Train_Loss: 147.3318 Val_Loss: 401.4342  BEST VAL Loss: 401.4342\n",
            "\n",
            "Epoch 419: Validation loss decreased (401.434174 --> 400.183411).\n",
            "\t Train_Loss: 147.1142 Val_Loss: 400.1834  BEST VAL Loss: 400.1834\n",
            "\n",
            "Epoch 420: Validation loss decreased (400.183411 --> 398.948700).\n",
            "\t Train_Loss: 146.9015 Val_Loss: 398.9487  BEST VAL Loss: 398.9487\n",
            "\n",
            "Epoch 421: Validation loss decreased (398.948700 --> 397.729065).\n",
            "\t Train_Loss: 146.6938 Val_Loss: 397.7291  BEST VAL Loss: 397.7291\n",
            "\n",
            "Epoch 422: Validation loss decreased (397.729065 --> 396.525848).\n",
            "\t Train_Loss: 146.4906 Val_Loss: 396.5258  BEST VAL Loss: 396.5258\n",
            "\n",
            "Epoch 423: Validation loss decreased (396.525848 --> 395.337585).\n",
            "\t Train_Loss: 146.2923 Val_Loss: 395.3376  BEST VAL Loss: 395.3376\n",
            "\n",
            "Epoch 424: Validation loss decreased (395.337585 --> 394.164215).\n",
            "\t Train_Loss: 146.0984 Val_Loss: 394.1642  BEST VAL Loss: 394.1642\n",
            "\n",
            "Epoch 425: Validation loss decreased (394.164215 --> 393.006439).\n",
            "\t Train_Loss: 145.9090 Val_Loss: 393.0064  BEST VAL Loss: 393.0064\n",
            "\n",
            "Epoch 426: Validation loss decreased (393.006439 --> 391.862701).\n",
            "\t Train_Loss: 145.7240 Val_Loss: 391.8627  BEST VAL Loss: 391.8627\n",
            "\n",
            "Epoch 427: Validation loss decreased (391.862701 --> 390.733856).\n",
            "\t Train_Loss: 145.5433 Val_Loss: 390.7339  BEST VAL Loss: 390.7339\n",
            "\n",
            "Epoch 428: Validation loss decreased (390.733856 --> 389.619598).\n",
            "\t Train_Loss: 145.3667 Val_Loss: 389.6196  BEST VAL Loss: 389.6196\n",
            "\n",
            "Epoch 429: Validation loss decreased (389.619598 --> 388.519348).\n",
            "\t Train_Loss: 145.1943 Val_Loss: 388.5193  BEST VAL Loss: 388.5193\n",
            "\n",
            "Epoch 430: Validation loss decreased (388.519348 --> 387.433319).\n",
            "\t Train_Loss: 145.0258 Val_Loss: 387.4333  BEST VAL Loss: 387.4333\n",
            "\n",
            "Epoch 431: Validation loss decreased (387.433319 --> 386.360931).\n",
            "\t Train_Loss: 144.8612 Val_Loss: 386.3609  BEST VAL Loss: 386.3609\n",
            "\n",
            "Epoch 432: Validation loss decreased (386.360931 --> 385.302338).\n",
            "\t Train_Loss: 144.7005 Val_Loss: 385.3023  BEST VAL Loss: 385.3023\n",
            "\n",
            "Epoch 433: Validation loss decreased (385.302338 --> 384.257782).\n",
            "\t Train_Loss: 144.5436 Val_Loss: 384.2578  BEST VAL Loss: 384.2578\n",
            "\n",
            "Epoch 434: Validation loss decreased (384.257782 --> 383.226105).\n",
            "\t Train_Loss: 144.3903 Val_Loss: 383.2261  BEST VAL Loss: 383.2261\n",
            "\n",
            "Epoch 435: Validation loss decreased (383.226105 --> 382.207825).\n",
            "\t Train_Loss: 144.2406 Val_Loss: 382.2078  BEST VAL Loss: 382.2078\n",
            "\n",
            "Epoch 436: Validation loss decreased (382.207825 --> 381.202545).\n",
            "\t Train_Loss: 144.0943 Val_Loss: 381.2025  BEST VAL Loss: 381.2025\n",
            "\n",
            "Epoch 437: Validation loss decreased (381.202545 --> 380.210266).\n",
            "\t Train_Loss: 143.9516 Val_Loss: 380.2103  BEST VAL Loss: 380.2103\n",
            "\n",
            "Epoch 438: Validation loss decreased (380.210266 --> 379.230438).\n",
            "\t Train_Loss: 143.8122 Val_Loss: 379.2304  BEST VAL Loss: 379.2304\n",
            "\n",
            "Epoch 439: Validation loss decreased (379.230438 --> 378.263702).\n",
            "\t Train_Loss: 143.6762 Val_Loss: 378.2637  BEST VAL Loss: 378.2637\n",
            "\n",
            "Epoch 440: Validation loss decreased (378.263702 --> 377.309021).\n",
            "\t Train_Loss: 143.5432 Val_Loss: 377.3090  BEST VAL Loss: 377.3090\n",
            "\n",
            "Epoch 441: Validation loss decreased (377.309021 --> 376.366302).\n",
            "\t Train_Loss: 143.4135 Val_Loss: 376.3663  BEST VAL Loss: 376.3663\n",
            "\n",
            "Epoch 442: Validation loss decreased (376.366302 --> 375.436371).\n",
            "\t Train_Loss: 143.2868 Val_Loss: 375.4364  BEST VAL Loss: 375.4364\n",
            "\n",
            "Epoch 443: Validation loss decreased (375.436371 --> 374.517487).\n",
            "\t Train_Loss: 143.1632 Val_Loss: 374.5175  BEST VAL Loss: 374.5175\n",
            "\n",
            "Epoch 444: Validation loss decreased (374.517487 --> 373.610992).\n",
            "\t Train_Loss: 143.0424 Val_Loss: 373.6110  BEST VAL Loss: 373.6110\n",
            "\n",
            "Epoch 445: Validation loss decreased (373.610992 --> 372.716278).\n",
            "\t Train_Loss: 142.9246 Val_Loss: 372.7163  BEST VAL Loss: 372.7163\n",
            "\n",
            "Epoch 446: Validation loss decreased (372.716278 --> 371.832764).\n",
            "\t Train_Loss: 142.8096 Val_Loss: 371.8328  BEST VAL Loss: 371.8328\n",
            "\n",
            "Epoch 447: Validation loss decreased (371.832764 --> 370.960938).\n",
            "\t Train_Loss: 142.6974 Val_Loss: 370.9609  BEST VAL Loss: 370.9609\n",
            "\n",
            "Epoch 448: Validation loss decreased (370.960938 --> 370.100525).\n",
            "\t Train_Loss: 142.5878 Val_Loss: 370.1005  BEST VAL Loss: 370.1005\n",
            "\n",
            "Epoch 449: Validation loss decreased (370.100525 --> 369.250946).\n",
            "\t Train_Loss: 142.4809 Val_Loss: 369.2509  BEST VAL Loss: 369.2509\n",
            "\n",
            "Epoch 450: Validation loss decreased (369.250946 --> 368.411926).\n",
            "\t Train_Loss: 142.3765 Val_Loss: 368.4119  BEST VAL Loss: 368.4119\n",
            "\n",
            "Epoch 451: Validation loss decreased (368.411926 --> 367.584137).\n",
            "\t Train_Loss: 142.2746 Val_Loss: 367.5841  BEST VAL Loss: 367.5841\n",
            "\n",
            "Epoch 452: Validation loss decreased (367.584137 --> 366.767090).\n",
            "\t Train_Loss: 142.1753 Val_Loss: 366.7671  BEST VAL Loss: 366.7671\n",
            "\n",
            "Epoch 453: Validation loss decreased (366.767090 --> 365.960602).\n",
            "\t Train_Loss: 142.0782 Val_Loss: 365.9606  BEST VAL Loss: 365.9606\n",
            "\n",
            "Epoch 454: Validation loss decreased (365.960602 --> 365.164368).\n",
            "\t Train_Loss: 141.9836 Val_Loss: 365.1644  BEST VAL Loss: 365.1644\n",
            "\n",
            "Epoch 455: Validation loss decreased (365.164368 --> 364.378876).\n",
            "\t Train_Loss: 141.8914 Val_Loss: 364.3789  BEST VAL Loss: 364.3789\n",
            "\n",
            "Epoch 456: Validation loss decreased (364.378876 --> 363.603424).\n",
            "\t Train_Loss: 141.8012 Val_Loss: 363.6034  BEST VAL Loss: 363.6034\n",
            "\n",
            "Epoch 457: Validation loss decreased (363.603424 --> 362.837250).\n",
            "\t Train_Loss: 141.7133 Val_Loss: 362.8372  BEST VAL Loss: 362.8372\n",
            "\n",
            "Epoch 458: Validation loss decreased (362.837250 --> 362.082275).\n",
            "\t Train_Loss: 141.6276 Val_Loss: 362.0823  BEST VAL Loss: 362.0823\n",
            "\n",
            "Epoch 459: Validation loss decreased (362.082275 --> 361.335999).\n",
            "\t Train_Loss: 141.5439 Val_Loss: 361.3360  BEST VAL Loss: 361.3360\n",
            "\n",
            "Epoch 460: Validation loss decreased (361.335999 --> 360.599854).\n",
            "\t Train_Loss: 141.4624 Val_Loss: 360.5999  BEST VAL Loss: 360.5999\n",
            "\n",
            "Epoch 461: Validation loss decreased (360.599854 --> 359.873291).\n",
            "\t Train_Loss: 141.3827 Val_Loss: 359.8733  BEST VAL Loss: 359.8733\n",
            "\n",
            "Epoch 462: Validation loss decreased (359.873291 --> 359.156097).\n",
            "\t Train_Loss: 141.3051 Val_Loss: 359.1561  BEST VAL Loss: 359.1561\n",
            "\n",
            "Epoch 463: Validation loss decreased (359.156097 --> 358.448395).\n",
            "\t Train_Loss: 141.2293 Val_Loss: 358.4484  BEST VAL Loss: 358.4484\n",
            "\n",
            "Epoch 464: Validation loss decreased (358.448395 --> 357.750000).\n",
            "\t Train_Loss: 141.1555 Val_Loss: 357.7500  BEST VAL Loss: 357.7500\n",
            "\n",
            "Epoch 465: Validation loss decreased (357.750000 --> 357.060516).\n",
            "\t Train_Loss: 141.0834 Val_Loss: 357.0605  BEST VAL Loss: 357.0605\n",
            "\n",
            "Epoch 466: Validation loss decreased (357.060516 --> 356.379852).\n",
            "\t Train_Loss: 141.0132 Val_Loss: 356.3799  BEST VAL Loss: 356.3799\n",
            "\n",
            "Epoch 467: Validation loss decreased (356.379852 --> 355.708191).\n",
            "\t Train_Loss: 140.9447 Val_Loss: 355.7082  BEST VAL Loss: 355.7082\n",
            "\n",
            "Epoch 468: Validation loss decreased (355.708191 --> 355.045471).\n",
            "\t Train_Loss: 140.8778 Val_Loss: 355.0455  BEST VAL Loss: 355.0455\n",
            "\n",
            "Epoch 469: Validation loss decreased (355.045471 --> 354.391113).\n",
            "\t Train_Loss: 140.8126 Val_Loss: 354.3911  BEST VAL Loss: 354.3911\n",
            "\n",
            "Epoch 470: Validation loss decreased (354.391113 --> 353.745392).\n",
            "\t Train_Loss: 140.7491 Val_Loss: 353.7454  BEST VAL Loss: 353.7454\n",
            "\n",
            "Epoch 471: Validation loss decreased (353.745392 --> 353.108124).\n",
            "\t Train_Loss: 140.6872 Val_Loss: 353.1081  BEST VAL Loss: 353.1081\n",
            "\n",
            "Epoch 472: Validation loss decreased (353.108124 --> 352.479645).\n",
            "\t Train_Loss: 140.6268 Val_Loss: 352.4796  BEST VAL Loss: 352.4796\n",
            "\n",
            "Epoch 473: Validation loss decreased (352.479645 --> 351.858978).\n",
            "\t Train_Loss: 140.5679 Val_Loss: 351.8590  BEST VAL Loss: 351.8590\n",
            "\n",
            "Epoch 474: Validation loss decreased (351.858978 --> 351.246216).\n",
            "\t Train_Loss: 140.5105 Val_Loss: 351.2462  BEST VAL Loss: 351.2462\n",
            "\n",
            "Epoch 475: Validation loss decreased (351.246216 --> 350.641724).\n",
            "\t Train_Loss: 140.4546 Val_Loss: 350.6417  BEST VAL Loss: 350.6417\n",
            "\n",
            "Epoch 476: Validation loss decreased (350.641724 --> 350.045135).\n",
            "\t Train_Loss: 140.4000 Val_Loss: 350.0451  BEST VAL Loss: 350.0451\n",
            "\n",
            "Epoch 477: Validation loss decreased (350.045135 --> 349.456421).\n",
            "\t Train_Loss: 140.3468 Val_Loss: 349.4564  BEST VAL Loss: 349.4564\n",
            "\n",
            "Epoch 478: Validation loss decreased (349.456421 --> 348.875580).\n",
            "\t Train_Loss: 140.2949 Val_Loss: 348.8756  BEST VAL Loss: 348.8756\n",
            "\n",
            "Epoch 479: Validation loss decreased (348.875580 --> 348.302399).\n",
            "\t Train_Loss: 140.2444 Val_Loss: 348.3024  BEST VAL Loss: 348.3024\n",
            "\n",
            "Epoch 480: Validation loss decreased (348.302399 --> 347.736572).\n",
            "\t Train_Loss: 140.1951 Val_Loss: 347.7366  BEST VAL Loss: 347.7366\n",
            "\n",
            "Epoch 481: Validation loss decreased (347.736572 --> 347.178223).\n",
            "\t Train_Loss: 140.1471 Val_Loss: 347.1782  BEST VAL Loss: 347.1782\n",
            "\n",
            "Epoch 482: Validation loss decreased (347.178223 --> 346.627502).\n",
            "\t Train_Loss: 140.1004 Val_Loss: 346.6275  BEST VAL Loss: 346.6275\n",
            "\n",
            "Epoch 483: Validation loss decreased (346.627502 --> 346.083313).\n",
            "\t Train_Loss: 140.0548 Val_Loss: 346.0833  BEST VAL Loss: 346.0833\n",
            "\n",
            "Epoch 484: Validation loss decreased (346.083313 --> 345.547089).\n",
            "\t Train_Loss: 140.0104 Val_Loss: 345.5471  BEST VAL Loss: 345.5471\n",
            "\n",
            "Epoch 485: Validation loss decreased (345.547089 --> 345.017639).\n",
            "\t Train_Loss: 139.9671 Val_Loss: 345.0176  BEST VAL Loss: 345.0176\n",
            "\n",
            "Epoch 486: Validation loss decreased (345.017639 --> 344.495117).\n",
            "\t Train_Loss: 139.9249 Val_Loss: 344.4951  BEST VAL Loss: 344.4951\n",
            "\n",
            "Epoch 487: Validation loss decreased (344.495117 --> 343.979614).\n",
            "\t Train_Loss: 139.8838 Val_Loss: 343.9796  BEST VAL Loss: 343.9796\n",
            "\n",
            "Epoch 488: Validation loss decreased (343.979614 --> 343.471039).\n",
            "\t Train_Loss: 139.8438 Val_Loss: 343.4710  BEST VAL Loss: 343.4710\n",
            "\n",
            "Epoch 489: Validation loss decreased (343.471039 --> 342.968872).\n",
            "\t Train_Loss: 139.8047 Val_Loss: 342.9689  BEST VAL Loss: 342.9689\n",
            "\n",
            "Epoch 490: Validation loss decreased (342.968872 --> 342.473602).\n",
            "\t Train_Loss: 139.7668 Val_Loss: 342.4736  BEST VAL Loss: 342.4736\n",
            "\n",
            "Epoch 491: Validation loss decreased (342.473602 --> 341.985016).\n",
            "\t Train_Loss: 139.7297 Val_Loss: 341.9850  BEST VAL Loss: 341.9850\n",
            "\n",
            "Epoch 492: Validation loss decreased (341.985016 --> 341.503265).\n",
            "\t Train_Loss: 139.6937 Val_Loss: 341.5033  BEST VAL Loss: 341.5033\n",
            "\n",
            "Epoch 493: Validation loss decreased (341.503265 --> 341.027069).\n",
            "\t Train_Loss: 139.6584 Val_Loss: 341.0271  BEST VAL Loss: 341.0271\n",
            "\n",
            "Epoch 494: Validation loss decreased (341.027069 --> 340.557709).\n",
            "\t Train_Loss: 139.6239 Val_Loss: 340.5577  BEST VAL Loss: 340.5577\n",
            "\n",
            "Epoch 495: Validation loss decreased (340.557709 --> 340.096405).\n",
            "\t Train_Loss: 139.5895 Val_Loss: 340.0964  BEST VAL Loss: 340.0964\n",
            "\n",
            "Epoch 496: Validation loss decreased (340.096405 --> 339.903351).\n",
            "\t Train_Loss: 139.5364 Val_Loss: 339.9034  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 497: Validation loss did not decrease\n",
            "\t Train_Loss: 138.3568 Val_Loss: 359.0089  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 498: Validation loss did not decrease\n",
            "\t Train_Loss: 132.0413 Val_Loss: 341.8039  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 499: Validation loss did not decrease\n",
            "\t Train_Loss: 133.4114 Val_Loss: 354.9771  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 500: Validation loss did not decrease\n",
            "\t Train_Loss: 130.2579 Val_Loss: 353.3650  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 501: Validation loss did not decrease\n",
            "\t Train_Loss: 129.1773 Val_Loss: 341.3442  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 502: Validation loss did not decrease\n",
            "\t Train_Loss: 129.4108 Val_Loss: 348.1675  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 503: Validation loss did not decrease\n",
            "\t Train_Loss: 126.8407 Val_Loss: 352.8266  BEST VAL Loss: 339.9034\n",
            "\n",
            "Epoch 504: Validation loss decreased (339.903351 --> 338.793793).\n",
            "\t Train_Loss: 126.8322 Val_Loss: 338.7938  BEST VAL Loss: 338.7938\n",
            "\n",
            "Epoch 505: Validation loss did not decrease\n",
            "\t Train_Loss: 126.3287 Val_Loss: 339.3327  BEST VAL Loss: 338.7938\n",
            "\n",
            "Epoch 506: Validation loss did not decrease\n",
            "\t Train_Loss: 124.6332 Val_Loss: 349.3857  BEST VAL Loss: 338.7938\n",
            "\n",
            "Epoch 507: Validation loss did not decrease\n",
            "\t Train_Loss: 124.8241 Val_Loss: 347.5504  BEST VAL Loss: 338.7938\n",
            "\n",
            "Epoch 508: Validation loss decreased (338.793793 --> 337.397614).\n",
            "\t Train_Loss: 121.7235 Val_Loss: 337.3976  BEST VAL Loss: 337.3976\n",
            "\n",
            "Epoch 509: Validation loss decreased (337.397614 --> 337.057709).\n",
            "\t Train_Loss: 121.9332 Val_Loss: 337.0577  BEST VAL Loss: 337.0577\n",
            "\n",
            "Epoch 510: Validation loss did not decrease\n",
            "\t Train_Loss: 121.1065 Val_Loss: 339.4330  BEST VAL Loss: 337.0577\n",
            "\n",
            "Epoch 511: Validation loss did not decrease\n",
            "\t Train_Loss: 118.5509 Val_Loss: 381.9013  BEST VAL Loss: 337.0577\n",
            "\n",
            "Epoch 512: Validation loss did not decrease\n",
            "\t Train_Loss: 127.7214 Val_Loss: 347.2135  BEST VAL Loss: 337.0577\n",
            "\n",
            "Epoch 513: Validation loss decreased (337.057709 --> 334.108215).\n",
            "\t Train_Loss: 118.6290 Val_Loss: 334.1082  BEST VAL Loss: 334.1082\n",
            "\n",
            "Epoch 514: Validation loss decreased (334.108215 --> 328.729462).\n",
            "\t Train_Loss: 118.6172 Val_Loss: 328.7295  BEST VAL Loss: 328.7295\n",
            "\n",
            "Epoch 515: Validation loss decreased (328.729462 --> 327.804962).\n",
            "\t Train_Loss: 120.7772 Val_Loss: 327.8050  BEST VAL Loss: 327.8050\n",
            "\n",
            "Epoch 516: Validation loss did not decrease\n",
            "\t Train_Loss: 120.5463 Val_Loss: 329.6354  BEST VAL Loss: 327.8050\n",
            "\n",
            "Epoch 517: Validation loss did not decrease\n",
            "\t Train_Loss: 118.9805 Val_Loss: 333.5753  BEST VAL Loss: 327.8050\n",
            "\n",
            "Epoch 518: Validation loss did not decrease\n",
            "\t Train_Loss: 120.2421 Val_Loss: 329.3283  BEST VAL Loss: 327.8050\n",
            "\n",
            "Epoch 519: Validation loss decreased (327.804962 --> 324.849487).\n",
            "\t Train_Loss: 118.9239 Val_Loss: 324.8495  BEST VAL Loss: 324.8495\n",
            "\n",
            "Epoch 520: Validation loss decreased (324.849487 --> 323.338867).\n",
            "\t Train_Loss: 118.6552 Val_Loss: 323.3389  BEST VAL Loss: 323.3389\n",
            "\n",
            "Epoch 521: Validation loss decreased (323.338867 --> 323.194916).\n",
            "\t Train_Loss: 119.1425 Val_Loss: 323.1949  BEST VAL Loss: 323.1949\n",
            "\n",
            "Epoch 522: Validation loss did not decrease\n",
            "\t Train_Loss: 118.1307 Val_Loss: 324.8105  BEST VAL Loss: 323.1949\n",
            "\n",
            "Epoch 523: Validation loss did not decrease\n",
            "\t Train_Loss: 117.9391 Val_Loss: 325.4066  BEST VAL Loss: 323.1949\n",
            "\n",
            "Epoch 524: Validation loss decreased (323.194916 --> 322.296478).\n",
            "\t Train_Loss: 118.2568 Val_Loss: 322.2965  BEST VAL Loss: 322.2965\n",
            "\n",
            "Epoch 525: Validation loss decreased (322.296478 --> 319.732483).\n",
            "\t Train_Loss: 117.4092 Val_Loss: 319.7325  BEST VAL Loss: 319.7325\n",
            "\n",
            "Epoch 526: Validation loss decreased (319.732483 --> 318.577789).\n",
            "\t Train_Loss: 117.3947 Val_Loss: 318.5778  BEST VAL Loss: 318.5778\n",
            "\n",
            "Epoch 527: Validation loss decreased (318.577789 --> 318.331055).\n",
            "\t Train_Loss: 117.5112 Val_Loss: 318.3311  BEST VAL Loss: 318.3311\n",
            "\n",
            "Epoch 528: Validation loss did not decrease\n",
            "\t Train_Loss: 116.9415 Val_Loss: 319.0073  BEST VAL Loss: 318.3311\n",
            "\n",
            "Epoch 529: Validation loss did not decrease\n",
            "\t Train_Loss: 116.7802 Val_Loss: 319.1597  BEST VAL Loss: 318.3311\n",
            "\n",
            "Epoch 530: Validation loss decreased (318.331055 --> 317.316376).\n",
            "\t Train_Loss: 116.9066 Val_Loss: 317.3164  BEST VAL Loss: 317.3164\n",
            "\n",
            "Epoch 531: Validation loss decreased (317.316376 --> 315.307678).\n",
            "\t Train_Loss: 116.4683 Val_Loss: 315.3077  BEST VAL Loss: 315.3077\n",
            "\n",
            "Epoch 532: Validation loss decreased (315.307678 --> 314.129211).\n",
            "\t Train_Loss: 116.3000 Val_Loss: 314.1292  BEST VAL Loss: 314.1292\n",
            "\n",
            "Epoch 533: Validation loss decreased (314.129211 --> 313.624268).\n",
            "\t Train_Loss: 116.3633 Val_Loss: 313.6243  BEST VAL Loss: 313.6243\n",
            "\n",
            "Epoch 534: Validation loss did not decrease\n",
            "\t Train_Loss: 116.0875 Val_Loss: 313.7138  BEST VAL Loss: 313.6243\n",
            "\n",
            "Epoch 535: Validation loss did not decrease\n",
            "\t Train_Loss: 115.8461 Val_Loss: 313.8336  BEST VAL Loss: 313.6243\n",
            "\n",
            "Epoch 536: Validation loss decreased (313.624268 --> 312.913330).\n",
            "\t Train_Loss: 115.8777 Val_Loss: 312.9133  BEST VAL Loss: 312.9133\n",
            "\n",
            "Epoch 537: Validation loss decreased (312.913330 --> 311.337585).\n",
            "\t Train_Loss: 115.7047 Val_Loss: 311.3376  BEST VAL Loss: 311.3376\n",
            "\n",
            "Epoch 538: Validation loss decreased (311.337585 --> 310.078583).\n",
            "\t Train_Loss: 115.4670 Val_Loss: 310.0786  BEST VAL Loss: 310.0786\n",
            "\n",
            "Epoch 539: Validation loss decreased (310.078583 --> 309.334473).\n",
            "\t Train_Loss: 115.4404 Val_Loss: 309.3345  BEST VAL Loss: 309.3345\n",
            "\n",
            "Epoch 540: Validation loss decreased (309.334473 --> 309.042297).\n",
            "\t Train_Loss: 115.3449 Val_Loss: 309.0423  BEST VAL Loss: 309.0423\n",
            "\n",
            "Epoch 541: Validation loss decreased (309.042297 --> 309.010559).\n",
            "\t Train_Loss: 115.1319 Val_Loss: 309.0106  BEST VAL Loss: 309.0106\n",
            "\n",
            "Epoch 542: Validation loss decreased (309.010559 --> 308.659424).\n",
            "\t Train_Loss: 115.0413 Val_Loss: 308.6594  BEST VAL Loss: 308.6594\n",
            "\n",
            "Epoch 543: Validation loss decreased (308.659424 --> 307.651215).\n",
            "\t Train_Loss: 114.9889 Val_Loss: 307.6512  BEST VAL Loss: 307.6512\n",
            "\n",
            "Epoch 544: Validation loss decreased (307.651215 --> 306.453644).\n",
            "\t Train_Loss: 114.8200 Val_Loss: 306.4536  BEST VAL Loss: 306.4536\n",
            "\n",
            "Epoch 545: Validation loss decreased (306.453644 --> 305.524139).\n",
            "\t Train_Loss: 114.6949 Val_Loss: 305.5241  BEST VAL Loss: 305.5241\n",
            "\n",
            "Epoch 546: Validation loss decreased (305.524139 --> 304.954010).\n",
            "\t Train_Loss: 114.6412 Val_Loss: 304.9540  BEST VAL Loss: 304.9540\n",
            "\n",
            "Epoch 547: Validation loss decreased (304.954010 --> 304.683746).\n",
            "\t Train_Loss: 114.5247 Val_Loss: 304.6837  BEST VAL Loss: 304.6837\n",
            "\n",
            "Epoch 548: Validation loss decreased (304.683746 --> 304.491547).\n",
            "\t Train_Loss: 114.3841 Val_Loss: 304.4915  BEST VAL Loss: 304.4915\n",
            "\n",
            "Epoch 549: Validation loss decreased (304.491547 --> 304.005341).\n",
            "\t Train_Loss: 114.3089 Val_Loss: 304.0053  BEST VAL Loss: 304.0053\n",
            "\n",
            "Epoch 550: Validation loss decreased (304.005341 --> 303.132385).\n",
            "\t Train_Loss: 114.2267 Val_Loss: 303.1324  BEST VAL Loss: 303.1324\n",
            "\n",
            "Epoch 551: Validation loss decreased (303.132385 --> 302.175995).\n",
            "\t Train_Loss: 114.0967 Val_Loss: 302.1760  BEST VAL Loss: 302.1760\n",
            "\n",
            "Epoch 552: Validation loss decreased (302.175995 --> 301.404358).\n",
            "\t Train_Loss: 113.9926 Val_Loss: 301.4044  BEST VAL Loss: 301.4044\n",
            "\n",
            "Epoch 553: Validation loss decreased (301.404358 --> 300.882172).\n",
            "\t Train_Loss: 113.8973 Val_Loss: 300.8822  BEST VAL Loss: 300.8822\n",
            "\n",
            "Epoch 554: Validation loss decreased (300.882172 --> 300.510529).\n",
            "\t Train_Loss: 113.6446 Val_Loss: 300.5105  BEST VAL Loss: 300.5105\n",
            "\n",
            "Epoch 555: Validation loss decreased (300.510529 --> 300.273651).\n",
            "\t Train_Loss: 112.7954 Val_Loss: 300.2737  BEST VAL Loss: 300.2737\n",
            "\n",
            "Epoch 556: Validation loss decreased (300.273651 --> 297.924988).\n",
            "\t Train_Loss: 113.7122 Val_Loss: 297.9250  BEST VAL Loss: 297.9250\n",
            "\n",
            "Epoch 557: Validation loss decreased (297.924988 --> 296.995087).\n",
            "\t Train_Loss: 111.6693 Val_Loss: 296.9951  BEST VAL Loss: 296.9951\n",
            "\n",
            "Epoch 558: Validation loss decreased (296.995087 --> 296.433441).\n",
            "\t Train_Loss: 112.8657 Val_Loss: 296.4334  BEST VAL Loss: 296.4334\n",
            "\n",
            "Epoch 559: Validation loss decreased (296.433441 --> 296.405701).\n",
            "\t Train_Loss: 111.1797 Val_Loss: 296.4057  BEST VAL Loss: 296.4057\n",
            "\n",
            "Epoch 560: Validation loss decreased (296.405701 --> 295.521790).\n",
            "\t Train_Loss: 111.9225 Val_Loss: 295.5218  BEST VAL Loss: 295.5218\n",
            "\n",
            "Epoch 561: Validation loss decreased (295.521790 --> 294.784882).\n",
            "\t Train_Loss: 109.8266 Val_Loss: 294.7849  BEST VAL Loss: 294.7849\n",
            "\n",
            "Epoch 562: Validation loss decreased (294.784882 --> 294.302216).\n",
            "\t Train_Loss: 110.2419 Val_Loss: 294.3022  BEST VAL Loss: 294.3022\n",
            "\n",
            "Epoch 563: Validation loss decreased (294.302216 --> 294.153229).\n",
            "\t Train_Loss: 110.2792 Val_Loss: 294.1532  BEST VAL Loss: 294.1532\n",
            "\n",
            "Epoch 564: Validation loss did not decrease\n",
            "\t Train_Loss: 109.2432 Val_Loss: 294.2871  BEST VAL Loss: 294.1532\n",
            "\n",
            "Epoch 565: Validation loss decreased (294.153229 --> 293.712402).\n",
            "\t Train_Loss: 109.0612 Val_Loss: 293.7124  BEST VAL Loss: 293.7124\n",
            "\n",
            "Epoch 566: Validation loss decreased (293.712402 --> 292.504333).\n",
            "\t Train_Loss: 108.8422 Val_Loss: 292.5043  BEST VAL Loss: 292.5043\n",
            "\n",
            "Epoch 567: Validation loss decreased (292.504333 --> 291.678558).\n",
            "\t Train_Loss: 107.5549 Val_Loss: 291.6786  BEST VAL Loss: 291.6786\n",
            "\n",
            "Epoch 568: Validation loss decreased (291.678558 --> 291.268646).\n",
            "\t Train_Loss: 107.7844 Val_Loss: 291.2686  BEST VAL Loss: 291.2686\n",
            "\n",
            "Epoch 569: Validation loss did not decrease\n",
            "\t Train_Loss: 107.2005 Val_Loss: 291.3477  BEST VAL Loss: 291.2686\n",
            "\n",
            "Epoch 570: Validation loss decreased (291.268646 --> 291.191162).\n",
            "\t Train_Loss: 106.6541 Val_Loss: 291.1912  BEST VAL Loss: 291.1912\n",
            "\n",
            "Epoch 571: Validation loss decreased (291.191162 --> 290.088470).\n",
            "\t Train_Loss: 106.6413 Val_Loss: 290.0885  BEST VAL Loss: 290.0885\n",
            "\n",
            "Epoch 572: Validation loss decreased (290.088470 --> 289.101715).\n",
            "\t Train_Loss: 105.6895 Val_Loss: 289.1017  BEST VAL Loss: 289.1017\n",
            "\n",
            "Epoch 573: Validation loss decreased (289.101715 --> 288.522064).\n",
            "\t Train_Loss: 105.3310 Val_Loss: 288.5221  BEST VAL Loss: 288.5221\n",
            "\n",
            "Epoch 574: Validation loss decreased (288.522064 --> 288.209015).\n",
            "\t Train_Loss: 105.0387 Val_Loss: 288.2090  BEST VAL Loss: 288.2090\n",
            "\n",
            "Epoch 575: Validation loss decreased (288.209015 --> 287.933319).\n",
            "\t Train_Loss: 104.4043 Val_Loss: 287.9333  BEST VAL Loss: 287.9333\n",
            "\n",
            "Epoch 576: Validation loss decreased (287.933319 --> 287.278656).\n",
            "\t Train_Loss: 104.2839 Val_Loss: 287.2787  BEST VAL Loss: 287.2787\n",
            "\n",
            "Epoch 577: Validation loss decreased (287.278656 --> 286.439911).\n",
            "\t Train_Loss: 103.7475 Val_Loss: 286.4399  BEST VAL Loss: 286.4399\n",
            "\n",
            "Epoch 578: Validation loss decreased (286.439911 --> 285.766571).\n",
            "\t Train_Loss: 103.1623 Val_Loss: 285.7666  BEST VAL Loss: 285.7666\n",
            "\n",
            "Epoch 579: Validation loss decreased (285.766571 --> 285.314453).\n",
            "\t Train_Loss: 102.9117 Val_Loss: 285.3145  BEST VAL Loss: 285.3145\n",
            "\n",
            "Epoch 580: Validation loss decreased (285.314453 --> 284.994781).\n",
            "\t Train_Loss: 102.3308 Val_Loss: 284.9948  BEST VAL Loss: 284.9948\n",
            "\n",
            "Epoch 581: Validation loss decreased (284.994781 --> 284.487396).\n",
            "\t Train_Loss: 102.0332 Val_Loss: 284.4874  BEST VAL Loss: 284.4874\n",
            "\n",
            "Epoch 582: Validation loss decreased (284.487396 --> 283.723297).\n",
            "\t Train_Loss: 101.7014 Val_Loss: 283.7233  BEST VAL Loss: 283.7233\n",
            "\n",
            "Epoch 583: Validation loss decreased (283.723297 --> 282.998260).\n",
            "\t Train_Loss: 101.1713 Val_Loss: 282.9983  BEST VAL Loss: 282.9983\n",
            "\n",
            "Epoch 584: Validation loss decreased (282.998260 --> 282.435242).\n",
            "\t Train_Loss: 100.8549 Val_Loss: 282.4352  BEST VAL Loss: 282.4352\n",
            "\n",
            "Epoch 585: Validation loss decreased (282.435242 --> 281.965637).\n",
            "\t Train_Loss: 100.3605 Val_Loss: 281.9656  BEST VAL Loss: 281.9656\n",
            "\n",
            "Epoch 586: Validation loss decreased (281.965637 --> 281.437225).\n",
            "\t Train_Loss: 99.9066 Val_Loss: 281.4372  BEST VAL Loss: 281.4372\n",
            "\n",
            "Epoch 587: Validation loss decreased (281.437225 --> 280.786682).\n",
            "\t Train_Loss: 99.5201 Val_Loss: 280.7867  BEST VAL Loss: 280.7867\n",
            "\n",
            "Epoch 588: Validation loss decreased (280.786682 --> 280.121918).\n",
            "\t Train_Loss: 98.9234 Val_Loss: 280.1219  BEST VAL Loss: 280.1219\n",
            "\n",
            "Epoch 589: Validation loss decreased (280.121918 --> 279.546967).\n",
            "\t Train_Loss: 98.2420 Val_Loss: 279.5470  BEST VAL Loss: 279.5470\n",
            "\n",
            "Epoch 590: Validation loss did not decrease\n",
            "\t Train_Loss: 96.7906 Val_Loss: 279.7951  BEST VAL Loss: 279.5470\n",
            "\n",
            "Epoch 591: Validation loss did not decrease\n",
            "\t Train_Loss: 89.4721 Val_Loss: 352.3103  BEST VAL Loss: 279.5470\n",
            "\n",
            "Epoch 592: Validation loss did not decrease\n",
            "\t Train_Loss: 106.8371 Val_Loss: 288.6266  BEST VAL Loss: 279.5470\n",
            "\n",
            "Epoch 593: Validation loss decreased (279.546967 --> 276.935577).\n",
            "\t Train_Loss: 85.8462 Val_Loss: 276.9356  BEST VAL Loss: 276.9356\n",
            "\n",
            "Epoch 594: Validation loss decreased (276.935577 --> 276.851715).\n",
            "\t Train_Loss: 84.1409 Val_Loss: 276.8517  BEST VAL Loss: 276.8517\n",
            "\n",
            "Epoch 595: Validation loss did not decrease\n",
            "\t Train_Loss: 84.1948 Val_Loss: 277.2897  BEST VAL Loss: 276.8517\n",
            "\n",
            "Epoch 596: Validation loss did not decrease\n",
            "\t Train_Loss: 84.8132 Val_Loss: 279.1720  BEST VAL Loss: 276.8517\n",
            "\n",
            "Epoch 597: Validation loss did not decrease\n",
            "\t Train_Loss: 85.3660 Val_Loss: 280.6110  BEST VAL Loss: 276.8517\n",
            "\n",
            "Epoch 598: Validation loss did not decrease\n",
            "\t Train_Loss: 86.2435 Val_Loss: 277.9610  BEST VAL Loss: 276.8517\n",
            "\n",
            "Epoch 599: Validation loss decreased (276.851715 --> 274.315430).\n",
            "\t Train_Loss: 84.5958 Val_Loss: 274.3154  BEST VAL Loss: 274.3154\n",
            "\n",
            "Epoch 600: Validation loss decreased (274.315430 --> 271.779694).\n",
            "\t Train_Loss: 83.4906 Val_Loss: 271.7797  BEST VAL Loss: 271.7797\n",
            "\n",
            "Epoch 601: Validation loss decreased (271.779694 --> 270.413086).\n",
            "\t Train_Loss: 81.8417 Val_Loss: 270.4131  BEST VAL Loss: 270.4131\n",
            "\n",
            "Epoch 602: Validation loss decreased (270.413086 --> 269.703888).\n",
            "\t Train_Loss: 81.9218 Val_Loss: 269.7039  BEST VAL Loss: 269.7039\n",
            "\n",
            "Epoch 603: Validation loss decreased (269.703888 --> 269.054260).\n",
            "\t Train_Loss: 81.1638 Val_Loss: 269.0543  BEST VAL Loss: 269.0543\n",
            "\n",
            "Epoch 604: Validation loss decreased (269.054260 --> 267.987762).\n",
            "\t Train_Loss: 81.6913 Val_Loss: 267.9878  BEST VAL Loss: 267.9878\n",
            "\n",
            "Epoch 605: Validation loss decreased (267.987762 --> 266.821686).\n",
            "\t Train_Loss: 81.3176 Val_Loss: 266.8217  BEST VAL Loss: 266.8217\n",
            "\n",
            "Epoch 606: Validation loss decreased (266.821686 --> 265.813324).\n",
            "\t Train_Loss: 81.2786 Val_Loss: 265.8133  BEST VAL Loss: 265.8133\n",
            "\n",
            "Epoch 607: Validation loss decreased (265.813324 --> 264.912079).\n",
            "\t Train_Loss: 80.5508 Val_Loss: 264.9121  BEST VAL Loss: 264.9121\n",
            "\n",
            "Epoch 608: Validation loss decreased (264.912079 --> 263.963440).\n",
            "\t Train_Loss: 79.9156 Val_Loss: 263.9634  BEST VAL Loss: 263.9634\n",
            "\n",
            "Epoch 609: Validation loss decreased (263.963440 --> 262.956512).\n",
            "\t Train_Loss: 79.5059 Val_Loss: 262.9565  BEST VAL Loss: 262.9565\n",
            "\n",
            "Epoch 610: Validation loss decreased (262.956512 --> 262.021667).\n",
            "\t Train_Loss: 79.0950 Val_Loss: 262.0217  BEST VAL Loss: 262.0217\n",
            "\n",
            "Epoch 611: Validation loss decreased (262.021667 --> 261.216156).\n",
            "\t Train_Loss: 79.1132 Val_Loss: 261.2162  BEST VAL Loss: 261.2162\n",
            "\n",
            "Epoch 612: Validation loss decreased (261.216156 --> 260.427704).\n",
            "\t Train_Loss: 78.7360 Val_Loss: 260.4277  BEST VAL Loss: 260.4277\n",
            "\n",
            "Epoch 613: Validation loss decreased (260.427704 --> 259.513092).\n",
            "\t Train_Loss: 78.6442 Val_Loss: 259.5131  BEST VAL Loss: 259.5131\n",
            "\n",
            "Epoch 614: Validation loss decreased (259.513092 --> 258.553925).\n",
            "\t Train_Loss: 78.2405 Val_Loss: 258.5539  BEST VAL Loss: 258.5539\n",
            "\n",
            "Epoch 615: Validation loss decreased (258.553925 --> 257.705933).\n",
            "\t Train_Loss: 77.9679 Val_Loss: 257.7059  BEST VAL Loss: 257.7059\n",
            "\n",
            "Epoch 616: Validation loss decreased (257.705933 --> 256.947815).\n",
            "\t Train_Loss: 77.5874 Val_Loss: 256.9478  BEST VAL Loss: 256.9478\n",
            "\n",
            "Epoch 617: Validation loss decreased (256.947815 --> 256.135315).\n",
            "\t Train_Loss: 77.3103 Val_Loss: 256.1353  BEST VAL Loss: 256.1353\n",
            "\n",
            "Epoch 618: Validation loss decreased (256.135315 --> 255.280136).\n",
            "\t Train_Loss: 76.9825 Val_Loss: 255.2801  BEST VAL Loss: 255.2801\n",
            "\n",
            "Epoch 619: Validation loss decreased (255.280136 --> 254.512451).\n",
            "\t Train_Loss: 76.7450 Val_Loss: 254.5125  BEST VAL Loss: 254.5125\n",
            "\n",
            "Epoch 620: Validation loss decreased (254.512451 --> 253.824051).\n",
            "\t Train_Loss: 76.5568 Val_Loss: 253.8241  BEST VAL Loss: 253.8241\n",
            "\n",
            "Epoch 621: Validation loss decreased (253.824051 --> 253.040237).\n",
            "\t Train_Loss: 76.3166 Val_Loss: 253.0402  BEST VAL Loss: 253.0402\n",
            "\n",
            "Epoch 622: Validation loss decreased (253.040237 --> 252.087112).\n",
            "\t Train_Loss: 76.1236 Val_Loss: 252.0871  BEST VAL Loss: 252.0871\n",
            "\n",
            "Epoch 623: Validation loss decreased (252.087112 --> 251.122757).\n",
            "\t Train_Loss: 75.7778 Val_Loss: 251.1228  BEST VAL Loss: 251.1228\n",
            "\n",
            "Epoch 624: Validation loss decreased (251.122757 --> 250.247177).\n",
            "\t Train_Loss: 75.5455 Val_Loss: 250.2472  BEST VAL Loss: 250.2472\n",
            "\n",
            "Epoch 625: Validation loss decreased (250.247177 --> 249.395370).\n",
            "\t Train_Loss: 75.2346 Val_Loss: 249.3954  BEST VAL Loss: 249.3954\n",
            "\n",
            "Epoch 626: Validation loss decreased (249.395370 --> 248.489426).\n",
            "\t Train_Loss: 75.0369 Val_Loss: 248.4894  BEST VAL Loss: 248.4894\n",
            "\n",
            "Epoch 627: Validation loss decreased (248.489426 --> 247.589218).\n",
            "\t Train_Loss: 74.7878 Val_Loss: 247.5892  BEST VAL Loss: 247.5892\n",
            "\n",
            "Epoch 628: Validation loss decreased (247.589218 --> 246.769318).\n",
            "\t Train_Loss: 74.6068 Val_Loss: 246.7693  BEST VAL Loss: 246.7693\n",
            "\n",
            "Epoch 629: Validation loss decreased (246.769318 --> 245.996811).\n",
            "\t Train_Loss: 74.3845 Val_Loss: 245.9968  BEST VAL Loss: 245.9968\n",
            "\n",
            "Epoch 630: Validation loss decreased (245.996811 --> 245.196884).\n",
            "\t Train_Loss: 74.1898 Val_Loss: 245.1969  BEST VAL Loss: 245.1969\n",
            "\n",
            "Epoch 631: Validation loss decreased (245.196884 --> 244.383423).\n",
            "\t Train_Loss: 73.9645 Val_Loss: 244.3834  BEST VAL Loss: 244.3834\n",
            "\n",
            "Epoch 632: Validation loss decreased (244.383423 --> 243.626907).\n",
            "\t Train_Loss: 73.7414 Val_Loss: 243.6269  BEST VAL Loss: 243.6269\n",
            "\n",
            "Epoch 633: Validation loss decreased (243.626907 --> 242.924179).\n",
            "\t Train_Loss: 73.5108 Val_Loss: 242.9242  BEST VAL Loss: 242.9242\n",
            "\n",
            "Epoch 634: Validation loss decreased (242.924179 --> 242.205566).\n",
            "\t Train_Loss: 73.2782 Val_Loss: 242.2056  BEST VAL Loss: 242.2056\n",
            "\n",
            "Epoch 635: Validation loss decreased (242.205566 --> 241.457184).\n",
            "\t Train_Loss: 73.0558 Val_Loss: 241.4572  BEST VAL Loss: 241.4572\n",
            "\n",
            "Epoch 636: Validation loss decreased (241.457184 --> 240.739502).\n",
            "\t Train_Loss: 72.7885 Val_Loss: 240.7395  BEST VAL Loss: 240.7395\n",
            "\n",
            "Epoch 637: Validation loss decreased (240.739502 --> 240.089142).\n",
            "\t Train_Loss: 70.4698 Val_Loss: 240.0891  BEST VAL Loss: 240.0891\n",
            "\n",
            "Epoch 638: Validation loss decreased (240.089142 --> 239.827637).\n",
            "\t Train_Loss: 68.3196 Val_Loss: 239.8276  BEST VAL Loss: 239.8276\n",
            "\n",
            "Epoch 639: Validation loss did not decrease\n",
            "\t Train_Loss: 68.0006 Val_Loss: 240.4905  BEST VAL Loss: 239.8276\n",
            "\n",
            "Epoch 640: Validation loss decreased (239.827637 --> 237.955811).\n",
            "\t Train_Loss: 68.8115 Val_Loss: 237.9558  BEST VAL Loss: 237.9558\n",
            "\n",
            "Epoch 641: Validation loss decreased (237.955811 --> 237.070786).\n",
            "\t Train_Loss: 67.7092 Val_Loss: 237.0708  BEST VAL Loss: 237.0708\n",
            "\n",
            "Epoch 642: Validation loss decreased (237.070786 --> 236.293411).\n",
            "\t Train_Loss: 67.0473 Val_Loss: 236.2934  BEST VAL Loss: 236.2934\n",
            "\n",
            "Epoch 643: Validation loss decreased (236.293411 --> 235.243805).\n",
            "\t Train_Loss: 67.0207 Val_Loss: 235.2438  BEST VAL Loss: 235.2438\n",
            "\n",
            "Epoch 644: Validation loss decreased (235.243805 --> 234.248444).\n",
            "\t Train_Loss: 66.8419 Val_Loss: 234.2484  BEST VAL Loss: 234.2484\n",
            "\n",
            "Epoch 645: Validation loss decreased (234.248444 --> 233.389648).\n",
            "\t Train_Loss: 66.3510 Val_Loss: 233.3896  BEST VAL Loss: 233.3896\n",
            "\n",
            "Epoch 646: Validation loss decreased (233.389648 --> 232.508179).\n",
            "\t Train_Loss: 65.8703 Val_Loss: 232.5082  BEST VAL Loss: 232.5082\n",
            "\n",
            "Epoch 647: Validation loss decreased (232.508179 --> 231.613144).\n",
            "\t Train_Loss: 65.7070 Val_Loss: 231.6131  BEST VAL Loss: 231.6131\n",
            "\n",
            "Epoch 648: Validation loss decreased (231.613144 --> 230.863770).\n",
            "\t Train_Loss: 65.5561 Val_Loss: 230.8638  BEST VAL Loss: 230.8638\n",
            "\n",
            "Epoch 649: Validation loss decreased (230.863770 --> 230.222778).\n",
            "\t Train_Loss: 65.2280 Val_Loss: 230.2228  BEST VAL Loss: 230.2228\n",
            "\n",
            "Epoch 650: Validation loss decreased (230.222778 --> 229.450241).\n",
            "\t Train_Loss: 65.0306 Val_Loss: 229.4502  BEST VAL Loss: 229.4502\n",
            "\n",
            "Epoch 651: Validation loss decreased (229.450241 --> 228.704010).\n",
            "\t Train_Loss: 64.5788 Val_Loss: 228.7040  BEST VAL Loss: 228.7040\n",
            "\n",
            "Epoch 652: Validation loss decreased (228.704010 --> 228.149124).\n",
            "\t Train_Loss: 64.3263 Val_Loss: 228.1491  BEST VAL Loss: 228.1491\n",
            "\n",
            "Epoch 653: Validation loss decreased (228.149124 --> 227.557129).\n",
            "\t Train_Loss: 63.9398 Val_Loss: 227.5571  BEST VAL Loss: 227.5571\n",
            "\n",
            "Epoch 654: Validation loss decreased (227.557129 --> 226.762650).\n",
            "\t Train_Loss: 63.7598 Val_Loss: 226.7626  BEST VAL Loss: 226.7626\n",
            "\n",
            "Epoch 655: Validation loss decreased (226.762650 --> 226.017197).\n",
            "\t Train_Loss: 63.5546 Val_Loss: 226.0172  BEST VAL Loss: 226.0172\n",
            "\n",
            "Epoch 656: Validation loss decreased (226.017197 --> 225.349411).\n",
            "\t Train_Loss: 63.3950 Val_Loss: 225.3494  BEST VAL Loss: 225.3494\n",
            "\n",
            "Epoch 657: Validation loss decreased (225.349411 --> 224.531937).\n",
            "\t Train_Loss: 63.0927 Val_Loss: 224.5319  BEST VAL Loss: 224.5319\n",
            "\n",
            "Epoch 658: Validation loss decreased (224.531937 --> 223.546555).\n",
            "\t Train_Loss: 62.8023 Val_Loss: 223.5466  BEST VAL Loss: 223.5466\n",
            "\n",
            "Epoch 659: Validation loss decreased (223.546555 --> 222.644150).\n",
            "\t Train_Loss: 62.4892 Val_Loss: 222.6441  BEST VAL Loss: 222.6441\n",
            "\n",
            "Epoch 660: Validation loss decreased (222.644150 --> 221.842438).\n",
            "\t Train_Loss: 62.2368 Val_Loss: 221.8424  BEST VAL Loss: 221.8424\n",
            "\n",
            "Epoch 661: Validation loss decreased (221.842438 --> 220.972885).\n",
            "\t Train_Loss: 61.9859 Val_Loss: 220.9729  BEST VAL Loss: 220.9729\n",
            "\n",
            "Epoch 662: Validation loss decreased (220.972885 --> 220.085129).\n",
            "\t Train_Loss: 61.7447 Val_Loss: 220.0851  BEST VAL Loss: 220.0851\n",
            "\n",
            "Epoch 663: Validation loss decreased (220.085129 --> 219.311600).\n",
            "\t Train_Loss: 61.5665 Val_Loss: 219.3116  BEST VAL Loss: 219.3116\n",
            "\n",
            "Epoch 664: Validation loss decreased (219.311600 --> 218.578278).\n",
            "\t Train_Loss: 61.3552 Val_Loss: 218.5783  BEST VAL Loss: 218.5783\n",
            "\n",
            "Epoch 665: Validation loss decreased (218.578278 --> 217.794144).\n",
            "\t Train_Loss: 61.1441 Val_Loss: 217.7941  BEST VAL Loss: 217.7941\n",
            "\n",
            "Epoch 666: Validation loss decreased (217.794144 --> 217.056107).\n",
            "\t Train_Loss: 60.8873 Val_Loss: 217.0561  BEST VAL Loss: 217.0561\n",
            "\n",
            "Epoch 667: Validation loss decreased (217.056107 --> 216.422272).\n",
            "\t Train_Loss: 60.6819 Val_Loss: 216.4223  BEST VAL Loss: 216.4223\n",
            "\n",
            "Epoch 668: Validation loss decreased (216.422272 --> 215.779861).\n",
            "\t Train_Loss: 60.4498 Val_Loss: 215.7799  BEST VAL Loss: 215.7799\n",
            "\n",
            "Epoch 669: Validation loss decreased (215.779861 --> 215.098007).\n",
            "\t Train_Loss: 60.2206 Val_Loss: 215.0980  BEST VAL Loss: 215.0980\n",
            "\n",
            "Epoch 670: Validation loss decreased (215.098007 --> 214.473907).\n",
            "\t Train_Loss: 59.9917 Val_Loss: 214.4739  BEST VAL Loss: 214.4739\n",
            "\n",
            "Epoch 671: Validation loss decreased (214.473907 --> 213.863968).\n",
            "\t Train_Loss: 59.7780 Val_Loss: 213.8640  BEST VAL Loss: 213.8640\n",
            "\n",
            "Epoch 672: Validation loss decreased (213.863968 --> 213.154617).\n",
            "\t Train_Loss: 59.5871 Val_Loss: 213.1546  BEST VAL Loss: 213.1546\n",
            "\n",
            "Epoch 673: Validation loss decreased (213.154617 --> 212.384674).\n",
            "\t Train_Loss: 59.3782 Val_Loss: 212.3847  BEST VAL Loss: 212.3847\n",
            "\n",
            "Epoch 674: Validation loss decreased (212.384674 --> 211.642487).\n",
            "\t Train_Loss: 59.1695 Val_Loss: 211.6425  BEST VAL Loss: 211.6425\n",
            "\n",
            "Epoch 675: Validation loss decreased (211.642487 --> 210.904495).\n",
            "\t Train_Loss: 58.9597 Val_Loss: 210.9045  BEST VAL Loss: 210.9045\n",
            "\n",
            "Epoch 676: Validation loss decreased (210.904495 --> 210.120651).\n",
            "\t Train_Loss: 58.7712 Val_Loss: 210.1207  BEST VAL Loss: 210.1207\n",
            "\n",
            "Epoch 677: Validation loss decreased (210.120651 --> 209.338699).\n",
            "\t Train_Loss: 58.5730 Val_Loss: 209.3387  BEST VAL Loss: 209.3387\n",
            "\n",
            "Epoch 678: Validation loss decreased (209.338699 --> 208.616898).\n",
            "\t Train_Loss: 58.3742 Val_Loss: 208.6169  BEST VAL Loss: 208.6169\n",
            "\n",
            "Epoch 679: Validation loss decreased (208.616898 --> 207.927048).\n",
            "\t Train_Loss: 58.1761 Val_Loss: 207.9270  BEST VAL Loss: 207.9270\n",
            "\n",
            "Epoch 680: Validation loss decreased (207.927048 --> 207.221603).\n",
            "\t Train_Loss: 58.0002 Val_Loss: 207.2216  BEST VAL Loss: 207.2216\n",
            "\n",
            "Epoch 681: Validation loss decreased (207.221603 --> 206.524612).\n",
            "\t Train_Loss: 57.8173 Val_Loss: 206.5246  BEST VAL Loss: 206.5246\n",
            "\n",
            "Epoch 682: Validation loss decreased (206.524612 --> 205.882126).\n",
            "\t Train_Loss: 57.6276 Val_Loss: 205.8821  BEST VAL Loss: 205.8821\n",
            "\n",
            "Epoch 683: Validation loss decreased (205.882126 --> 205.278320).\n",
            "\t Train_Loss: 57.4390 Val_Loss: 205.2783  BEST VAL Loss: 205.2783\n",
            "\n",
            "Epoch 684: Validation loss decreased (205.278320 --> 204.665649).\n",
            "\t Train_Loss: 57.2607 Val_Loss: 204.6656  BEST VAL Loss: 204.6656\n",
            "\n",
            "Epoch 685: Validation loss decreased (204.665649 --> 204.049515).\n",
            "\t Train_Loss: 57.0771 Val_Loss: 204.0495  BEST VAL Loss: 204.0495\n",
            "\n",
            "Epoch 686: Validation loss decreased (204.049515 --> 203.459091).\n",
            "\t Train_Loss: 56.8972 Val_Loss: 203.4591  BEST VAL Loss: 203.4591\n",
            "\n",
            "Epoch 687: Validation loss decreased (203.459091 --> 202.872803).\n",
            "\t Train_Loss: 56.7248 Val_Loss: 202.8728  BEST VAL Loss: 202.8728\n",
            "\n",
            "Epoch 688: Validation loss decreased (202.872803 --> 202.248276).\n",
            "\t Train_Loss: 56.5558 Val_Loss: 202.2483  BEST VAL Loss: 202.2483\n",
            "\n",
            "Epoch 689: Validation loss decreased (202.248276 --> 201.592941).\n",
            "\t Train_Loss: 56.3806 Val_Loss: 201.5929  BEST VAL Loss: 201.5929\n",
            "\n",
            "Epoch 690: Validation loss decreased (201.592941 --> 200.949249).\n",
            "\t Train_Loss: 56.2044 Val_Loss: 200.9492  BEST VAL Loss: 200.9492\n",
            "\n",
            "Epoch 691: Validation loss decreased (200.949249 --> 200.326584).\n",
            "\t Train_Loss: 56.0336 Val_Loss: 200.3266  BEST VAL Loss: 200.3266\n",
            "\n",
            "Epoch 692: Validation loss decreased (200.326584 --> 199.702042).\n",
            "\t Train_Loss: 55.8644 Val_Loss: 199.7020  BEST VAL Loss: 199.7020\n",
            "\n",
            "Epoch 693: Validation loss decreased (199.702042 --> 199.074448).\n",
            "\t Train_Loss: 55.6972 Val_Loss: 199.0744  BEST VAL Loss: 199.0744\n",
            "\n",
            "Epoch 694: Validation loss decreased (199.074448 --> 198.469025).\n",
            "\t Train_Loss: 55.5334 Val_Loss: 198.4690  BEST VAL Loss: 198.4690\n",
            "\n",
            "Epoch 695: Validation loss decreased (198.469025 --> 197.892319).\n",
            "\t Train_Loss: 55.3738 Val_Loss: 197.8923  BEST VAL Loss: 197.8923\n",
            "\n",
            "Epoch 696: Validation loss decreased (197.892319 --> 197.325012).\n",
            "\t Train_Loss: 55.2118 Val_Loss: 197.3250  BEST VAL Loss: 197.3250\n",
            "\n",
            "Epoch 697: Validation loss decreased (197.325012 --> 196.754929).\n",
            "\t Train_Loss: 55.0503 Val_Loss: 196.7549  BEST VAL Loss: 196.7549\n",
            "\n",
            "Epoch 698: Validation loss decreased (196.754929 --> 196.191391).\n",
            "\t Train_Loss: 54.8910 Val_Loss: 196.1914  BEST VAL Loss: 196.1914\n",
            "\n",
            "Epoch 699: Validation loss decreased (196.191391 --> 195.644485).\n",
            "\t Train_Loss: 54.7354 Val_Loss: 195.6445  BEST VAL Loss: 195.6445\n",
            "\n",
            "Epoch 700: Validation loss decreased (195.644485 --> 195.102264).\n",
            "\t Train_Loss: 54.5788 Val_Loss: 195.1023  BEST VAL Loss: 195.1023\n",
            "\n",
            "Epoch 701: Validation loss decreased (195.102264 --> 194.548630).\n",
            "\t Train_Loss: 54.4236 Val_Loss: 194.5486  BEST VAL Loss: 194.5486\n",
            "\n",
            "Epoch 702: Validation loss decreased (194.548630 --> 193.990295).\n",
            "\t Train_Loss: 54.2711 Val_Loss: 193.9903  BEST VAL Loss: 193.9903\n",
            "\n",
            "Epoch 703: Validation loss decreased (193.990295 --> 193.441727).\n",
            "\t Train_Loss: 54.1200 Val_Loss: 193.4417  BEST VAL Loss: 193.4417\n",
            "\n",
            "Epoch 704: Validation loss decreased (193.441727 --> 192.902130).\n",
            "\t Train_Loss: 53.9686 Val_Loss: 192.9021  BEST VAL Loss: 192.9021\n",
            "\n",
            "Epoch 705: Validation loss decreased (192.902130 --> 192.359421).\n",
            "\t Train_Loss: 53.8192 Val_Loss: 192.3594  BEST VAL Loss: 192.3594\n",
            "\n",
            "Epoch 706: Validation loss decreased (192.359421 --> 191.816101).\n",
            "\t Train_Loss: 53.6710 Val_Loss: 191.8161  BEST VAL Loss: 191.8161\n",
            "\n",
            "Epoch 707: Validation loss decreased (191.816101 --> 191.288040).\n",
            "\t Train_Loss: 53.5224 Val_Loss: 191.2880  BEST VAL Loss: 191.2880\n",
            "\n",
            "Epoch 708: Validation loss decreased (191.288040 --> 190.771301).\n",
            "\t Train_Loss: 53.3744 Val_Loss: 190.7713  BEST VAL Loss: 190.7713\n",
            "\n",
            "Epoch 709: Validation loss decreased (190.771301 --> 190.248322).\n",
            "\t Train_Loss: 53.2279 Val_Loss: 190.2483  BEST VAL Loss: 190.2483\n",
            "\n",
            "Epoch 710: Validation loss decreased (190.248322 --> 189.710648).\n",
            "\t Train_Loss: 53.0828 Val_Loss: 189.7106  BEST VAL Loss: 189.7106\n",
            "\n",
            "Epoch 711: Validation loss decreased (189.710648 --> 189.190948).\n",
            "\t Train_Loss: 52.9380 Val_Loss: 189.1909  BEST VAL Loss: 189.1909\n",
            "\n",
            "Epoch 712: Validation loss decreased (189.190948 --> 188.690659).\n",
            "\t Train_Loss: 52.7955 Val_Loss: 188.6907  BEST VAL Loss: 188.6907\n",
            "\n",
            "Epoch 713: Validation loss decreased (188.690659 --> 188.170074).\n",
            "\t Train_Loss: 52.6510 Val_Loss: 188.1701  BEST VAL Loss: 188.1701\n",
            "\n",
            "Epoch 714: Validation loss decreased (188.170074 --> 187.650925).\n",
            "\t Train_Loss: 52.5056 Val_Loss: 187.6509  BEST VAL Loss: 187.6509\n",
            "\n",
            "Epoch 715: Validation loss decreased (187.650925 --> 187.149094).\n",
            "\t Train_Loss: 52.3622 Val_Loss: 187.1491  BEST VAL Loss: 187.1491\n",
            "\n",
            "Epoch 716: Validation loss decreased (187.149094 --> 186.641525).\n",
            "\t Train_Loss: 52.2161 Val_Loss: 186.6415  BEST VAL Loss: 186.6415\n",
            "\n",
            "Epoch 717: Validation loss decreased (186.641525 --> 186.156128).\n",
            "\t Train_Loss: 52.0785 Val_Loss: 186.1561  BEST VAL Loss: 186.1561\n",
            "\n",
            "Epoch 718: Validation loss decreased (186.156128 --> 185.638275).\n",
            "\t Train_Loss: 51.9229 Val_Loss: 185.6383  BEST VAL Loss: 185.6383\n",
            "\n",
            "Epoch 719: Validation loss decreased (185.638275 --> 185.138535).\n",
            "\t Train_Loss: 51.7693 Val_Loss: 185.1385  BEST VAL Loss: 185.1385\n",
            "\n",
            "Epoch 720: Validation loss decreased (185.138535 --> 184.672913).\n",
            "\t Train_Loss: 51.6156 Val_Loss: 184.6729  BEST VAL Loss: 184.6729\n",
            "\n",
            "Epoch 721: Validation loss decreased (184.672913 --> 184.109711).\n",
            "\t Train_Loss: 51.5449 Val_Loss: 184.1097  BEST VAL Loss: 184.1097\n",
            "\n",
            "Epoch 722: Validation loss decreased (184.109711 --> 183.701263).\n",
            "\t Train_Loss: 51.4850 Val_Loss: 183.7013  BEST VAL Loss: 183.7013\n",
            "\n",
            "Epoch 723: Validation loss decreased (183.701263 --> 183.341919).\n",
            "\t Train_Loss: 51.3299 Val_Loss: 183.3419  BEST VAL Loss: 183.3419\n",
            "\n",
            "Epoch 724: Validation loss decreased (183.341919 --> 182.823166).\n",
            "\t Train_Loss: 51.2249 Val_Loss: 182.8232  BEST VAL Loss: 182.8232\n",
            "\n",
            "Epoch 725: Validation loss decreased (182.823166 --> 182.315231).\n",
            "\t Train_Loss: 51.1206 Val_Loss: 182.3152  BEST VAL Loss: 182.3152\n",
            "\n",
            "Epoch 726: Validation loss decreased (182.315231 --> 181.914581).\n",
            "\t Train_Loss: 50.9258 Val_Loss: 181.9146  BEST VAL Loss: 181.9146\n",
            "\n",
            "Epoch 727: Validation loss decreased (181.914581 --> 181.496170).\n",
            "\t Train_Loss: 50.7836 Val_Loss: 181.4962  BEST VAL Loss: 181.4962\n",
            "\n",
            "Epoch 728: Validation loss decreased (181.496170 --> 181.061234).\n",
            "\t Train_Loss: 50.6103 Val_Loss: 181.0612  BEST VAL Loss: 181.0612\n",
            "\n",
            "Epoch 729: Validation loss decreased (181.061234 --> 180.652084).\n",
            "\t Train_Loss: 50.2790 Val_Loss: 180.6521  BEST VAL Loss: 180.6521\n",
            "\n",
            "Epoch 730: Validation loss decreased (180.652084 --> 180.116989).\n",
            "\t Train_Loss: 50.5779 Val_Loss: 180.1170  BEST VAL Loss: 180.1170\n",
            "\n",
            "Epoch 731: Validation loss decreased (180.116989 --> 179.787659).\n",
            "\t Train_Loss: 50.2026 Val_Loss: 179.7877  BEST VAL Loss: 179.7877\n",
            "\n",
            "Epoch 732: Validation loss decreased (179.787659 --> 179.550201).\n",
            "\t Train_Loss: 50.2374 Val_Loss: 179.5502  BEST VAL Loss: 179.5502\n",
            "\n",
            "Epoch 733: Validation loss decreased (179.550201 --> 179.003677).\n",
            "\t Train_Loss: 50.0523 Val_Loss: 179.0037  BEST VAL Loss: 179.0037\n",
            "\n",
            "Epoch 734: Validation loss decreased (179.003677 --> 178.425156).\n",
            "\t Train_Loss: 49.7142 Val_Loss: 178.4252  BEST VAL Loss: 178.4252\n",
            "\n",
            "Epoch 735: Validation loss decreased (178.425156 --> 177.935684).\n",
            "\t Train_Loss: 49.6589 Val_Loss: 177.9357  BEST VAL Loss: 177.9357\n",
            "\n",
            "Epoch 736: Validation loss decreased (177.935684 --> 177.305206).\n",
            "\t Train_Loss: 49.2731 Val_Loss: 177.3052  BEST VAL Loss: 177.3052\n",
            "\n",
            "Epoch 737: Validation loss decreased (177.305206 --> 176.745117).\n",
            "\t Train_Loss: 49.2103 Val_Loss: 176.7451  BEST VAL Loss: 176.7451\n",
            "\n",
            "Epoch 738: Validation loss decreased (176.745117 --> 176.406738).\n",
            "\t Train_Loss: 49.1265 Val_Loss: 176.4067  BEST VAL Loss: 176.4067\n",
            "\n",
            "Epoch 739: Validation loss decreased (176.406738 --> 176.094925).\n",
            "\t Train_Loss: 48.7966 Val_Loss: 176.0949  BEST VAL Loss: 176.0949\n",
            "\n",
            "Epoch 740: Validation loss decreased (176.094925 --> 175.709961).\n",
            "\t Train_Loss: 48.6693 Val_Loss: 175.7100  BEST VAL Loss: 175.7100\n",
            "\n",
            "Epoch 741: Validation loss decreased (175.709961 --> 175.341995).\n",
            "\t Train_Loss: 48.5876 Val_Loss: 175.3420  BEST VAL Loss: 175.3420\n",
            "\n",
            "Epoch 742: Validation loss decreased (175.341995 --> 174.846115).\n",
            "\t Train_Loss: 48.4248 Val_Loss: 174.8461  BEST VAL Loss: 174.8461\n",
            "\n",
            "Epoch 743: Validation loss decreased (174.846115 --> 174.247879).\n",
            "\t Train_Loss: 48.1871 Val_Loss: 174.2479  BEST VAL Loss: 174.2479\n",
            "\n",
            "Epoch 744: Validation loss did not decrease\n",
            "\t Train_Loss: 48.0070 Val_Loss: 174.3801  BEST VAL Loss: 174.2479\n",
            "\n",
            "Epoch 745: Validation loss decreased (174.247879 --> 173.375198).\n",
            "\t Train_Loss: 47.9374 Val_Loss: 173.3752  BEST VAL Loss: 173.3752\n",
            "\n",
            "Epoch 746: Validation loss decreased (173.375198 --> 172.982056).\n",
            "\t Train_Loss: 47.7903 Val_Loss: 172.9821  BEST VAL Loss: 172.9821\n",
            "\n",
            "Epoch 747: Validation loss decreased (172.982056 --> 172.638657).\n",
            "\t Train_Loss: 47.5672 Val_Loss: 172.6387  BEST VAL Loss: 172.6387\n",
            "\n",
            "Epoch 748: Validation loss decreased (172.638657 --> 172.323090).\n",
            "\t Train_Loss: 47.4343 Val_Loss: 172.3231  BEST VAL Loss: 172.3231\n",
            "\n",
            "Epoch 749: Validation loss decreased (172.323090 --> 171.879013).\n",
            "\t Train_Loss: 47.2959 Val_Loss: 171.8790  BEST VAL Loss: 171.8790\n",
            "\n",
            "Epoch 750: Validation loss decreased (171.879013 --> 171.339264).\n",
            "\t Train_Loss: 47.1168 Val_Loss: 171.3393  BEST VAL Loss: 171.3393\n",
            "\n",
            "Epoch 751: Validation loss decreased (171.339264 --> 170.799515).\n",
            "\t Train_Loss: 46.9360 Val_Loss: 170.7995  BEST VAL Loss: 170.7995\n",
            "\n",
            "Epoch 752: Validation loss decreased (170.799515 --> 170.294083).\n",
            "\t Train_Loss: 46.7937 Val_Loss: 170.2941  BEST VAL Loss: 170.2941\n",
            "\n",
            "Epoch 753: Validation loss decreased (170.294083 --> 169.904526).\n",
            "\t Train_Loss: 46.6734 Val_Loss: 169.9045  BEST VAL Loss: 169.9045\n",
            "\n",
            "Epoch 754: Validation loss decreased (169.904526 --> 169.587936).\n",
            "\t Train_Loss: 46.5004 Val_Loss: 169.5879  BEST VAL Loss: 169.5879\n",
            "\n",
            "Epoch 755: Validation loss decreased (169.587936 --> 169.228577).\n",
            "\t Train_Loss: 46.3447 Val_Loss: 169.2286  BEST VAL Loss: 169.2286\n",
            "\n",
            "Epoch 756: Validation loss decreased (169.228577 --> 168.839340).\n",
            "\t Train_Loss: 46.1976 Val_Loss: 168.8393  BEST VAL Loss: 168.8393\n",
            "\n",
            "Epoch 757: Validation loss decreased (168.839340 --> 168.394638).\n",
            "\t Train_Loss: 46.0552 Val_Loss: 168.3946  BEST VAL Loss: 168.3946\n",
            "\n",
            "Epoch 758: Validation loss decreased (168.394638 --> 167.962479).\n",
            "\t Train_Loss: 45.8828 Val_Loss: 167.9625  BEST VAL Loss: 167.9625\n",
            "\n",
            "Epoch 759: Validation loss decreased (167.962479 --> 167.554337).\n",
            "\t Train_Loss: 45.7341 Val_Loss: 167.5543  BEST VAL Loss: 167.5543\n",
            "\n",
            "Epoch 760: Validation loss decreased (167.554337 --> 167.149063).\n",
            "\t Train_Loss: 45.6023 Val_Loss: 167.1491  BEST VAL Loss: 167.1491\n",
            "\n",
            "Epoch 761: Validation loss decreased (167.149063 --> 166.824982).\n",
            "\t Train_Loss: 45.4491 Val_Loss: 166.8250  BEST VAL Loss: 166.8250\n",
            "\n",
            "Epoch 762: Validation loss decreased (166.824982 --> 166.510162).\n",
            "\t Train_Loss: 45.2936 Val_Loss: 166.5102  BEST VAL Loss: 166.5102\n",
            "\n",
            "Epoch 763: Validation loss decreased (166.510162 --> 166.116989).\n",
            "\t Train_Loss: 45.1461 Val_Loss: 166.1170  BEST VAL Loss: 166.1170\n",
            "\n",
            "Epoch 764: Validation loss decreased (166.116989 --> 165.649933).\n",
            "\t Train_Loss: 45.0072 Val_Loss: 165.6499  BEST VAL Loss: 165.6499\n",
            "\n",
            "Epoch 765: Validation loss decreased (165.649933 --> 165.111130).\n",
            "\t Train_Loss: 44.8457 Val_Loss: 165.1111  BEST VAL Loss: 165.1111\n",
            "\n",
            "Epoch 766: Validation loss decreased (165.111130 --> 164.653137).\n",
            "\t Train_Loss: 44.6965 Val_Loss: 164.6531  BEST VAL Loss: 164.6531\n",
            "\n",
            "Epoch 767: Validation loss decreased (164.653137 --> 164.187424).\n",
            "\t Train_Loss: 44.5436 Val_Loss: 164.1874  BEST VAL Loss: 164.1874\n",
            "\n",
            "Epoch 768: Validation loss decreased (164.187424 --> 163.790878).\n",
            "\t Train_Loss: 44.3898 Val_Loss: 163.7909  BEST VAL Loss: 163.7909\n",
            "\n",
            "Epoch 769: Validation loss decreased (163.790878 --> 163.294479).\n",
            "\t Train_Loss: 44.2420 Val_Loss: 163.2945  BEST VAL Loss: 163.2945\n",
            "\n",
            "Epoch 770: Validation loss decreased (163.294479 --> 163.010849).\n",
            "\t Train_Loss: 44.1627 Val_Loss: 163.0108  BEST VAL Loss: 163.0108\n",
            "\n",
            "Epoch 771: Validation loss decreased (163.010849 --> 162.321457).\n",
            "\t Train_Loss: 44.1919 Val_Loss: 162.3215  BEST VAL Loss: 162.3215\n",
            "\n",
            "Epoch 772: Validation loss did not decrease\n",
            "\t Train_Loss: 44.1963 Val_Loss: 163.5991  BEST VAL Loss: 162.3215\n",
            "\n",
            "Epoch 773: Validation loss decreased (162.321457 --> 161.698456).\n",
            "\t Train_Loss: 43.7616 Val_Loss: 161.6985  BEST VAL Loss: 161.6985\n",
            "\n",
            "Epoch 774: Validation loss decreased (161.698456 --> 161.093658).\n",
            "\t Train_Loss: 43.5246 Val_Loss: 161.0937  BEST VAL Loss: 161.0937\n",
            "\n",
            "Epoch 775: Validation loss decreased (161.093658 --> 160.892868).\n",
            "\t Train_Loss: 43.5826 Val_Loss: 160.8929  BEST VAL Loss: 160.8929\n",
            "\n",
            "Epoch 776: Validation loss decreased (160.892868 --> 160.538651).\n",
            "\t Train_Loss: 43.3119 Val_Loss: 160.5387  BEST VAL Loss: 160.5387\n",
            "\n",
            "Epoch 777: Validation loss decreased (160.538651 --> 160.127136).\n",
            "\t Train_Loss: 42.9362 Val_Loss: 160.1271  BEST VAL Loss: 160.1271\n",
            "\n",
            "Epoch 778: Validation loss decreased (160.127136 --> 159.889572).\n",
            "\t Train_Loss: 41.7979 Val_Loss: 159.8896  BEST VAL Loss: 159.8896\n",
            "\n",
            "Epoch 779: Validation loss did not decrease\n",
            "\t Train_Loss: 41.3747 Val_Loss: 160.5502  BEST VAL Loss: 159.8896\n",
            "\n",
            "Epoch 780: Validation loss decreased (159.889572 --> 159.325363).\n",
            "\t Train_Loss: 41.8690 Val_Loss: 159.3254  BEST VAL Loss: 159.3254\n",
            "\n",
            "Epoch 781: Validation loss decreased (159.325363 --> 158.795151).\n",
            "\t Train_Loss: 41.7925 Val_Loss: 158.7952  BEST VAL Loss: 158.7952\n",
            "\n",
            "Epoch 782: Validation loss did not decrease\n",
            "\t Train_Loss: 42.1586 Val_Loss: 158.8381  BEST VAL Loss: 158.7952\n",
            "\n",
            "Epoch 783: Validation loss did not decrease\n",
            "\t Train_Loss: 40.6737 Val_Loss: 160.0784  BEST VAL Loss: 158.7952\n",
            "\n",
            "Epoch 784: Validation loss decreased (158.795151 --> 158.248688).\n",
            "\t Train_Loss: 41.5707 Val_Loss: 158.2487  BEST VAL Loss: 158.2487\n",
            "\n",
            "Epoch 785: Validation loss decreased (158.248688 --> 157.628403).\n",
            "\t Train_Loss: 40.4763 Val_Loss: 157.6284  BEST VAL Loss: 157.6284\n",
            "\n",
            "Epoch 786: Validation loss decreased (157.628403 --> 157.351303).\n",
            "\t Train_Loss: 40.1873 Val_Loss: 157.3513  BEST VAL Loss: 157.3513\n",
            "\n",
            "Epoch 787: Validation loss decreased (157.351303 --> 156.926102).\n",
            "\t Train_Loss: 40.3713 Val_Loss: 156.9261  BEST VAL Loss: 156.9261\n",
            "\n",
            "Epoch 788: Validation loss did not decrease\n",
            "\t Train_Loss: 39.5196 Val_Loss: 157.1187  BEST VAL Loss: 156.9261\n",
            "\n",
            "Epoch 789: Validation loss did not decrease\n",
            "\t Train_Loss: 39.6439 Val_Loss: 157.1758  BEST VAL Loss: 156.9261\n",
            "\n",
            "Epoch 790: Validation loss decreased (156.926102 --> 156.346313).\n",
            "\t Train_Loss: 39.5065 Val_Loss: 156.3463  BEST VAL Loss: 156.3463\n",
            "\n",
            "Epoch 791: Validation loss decreased (156.346313 --> 155.738861).\n",
            "\t Train_Loss: 38.9950 Val_Loss: 155.7389  BEST VAL Loss: 155.7389\n",
            "\n",
            "Epoch 792: Validation loss did not decrease\n",
            "\t Train_Loss: 38.9797 Val_Loss: 156.0025  BEST VAL Loss: 155.7389\n",
            "\n",
            "Epoch 793: Validation loss did not decrease\n",
            "\t Train_Loss: 38.6850 Val_Loss: 156.0743  BEST VAL Loss: 155.7389\n",
            "\n",
            "Epoch 794: Validation loss decreased (155.738861 --> 155.563492).\n",
            "\t Train_Loss: 38.3958 Val_Loss: 155.5635  BEST VAL Loss: 155.5635\n",
            "\n",
            "Epoch 795: Validation loss decreased (155.563492 --> 154.695389).\n",
            "\t Train_Loss: 38.2999 Val_Loss: 154.6954  BEST VAL Loss: 154.6954\n",
            "\n",
            "Epoch 796: Validation loss decreased (154.695389 --> 154.419098).\n",
            "\t Train_Loss: 38.1163 Val_Loss: 154.4191  BEST VAL Loss: 154.4191\n",
            "\n",
            "Epoch 797: Validation loss decreased (154.419098 --> 154.240814).\n",
            "\t Train_Loss: 37.9518 Val_Loss: 154.2408  BEST VAL Loss: 154.2408\n",
            "\n",
            "Epoch 798: Validation loss decreased (154.240814 --> 153.480270).\n",
            "\t Train_Loss: 37.6901 Val_Loss: 153.4803  BEST VAL Loss: 153.4803\n",
            "\n",
            "Epoch 799: Validation loss decreased (153.480270 --> 152.696075).\n",
            "\t Train_Loss: 37.5694 Val_Loss: 152.6961  BEST VAL Loss: 152.6961\n",
            "\n",
            "Epoch 800: Validation loss did not decrease\n",
            "\t Train_Loss: 37.4228 Val_Loss: 152.7300  BEST VAL Loss: 152.6961\n",
            "\n",
            "Epoch 801: Validation loss decreased (152.696075 --> 152.208786).\n",
            "\t Train_Loss: 37.1641 Val_Loss: 152.2088  BEST VAL Loss: 152.2088\n",
            "\n",
            "Epoch 802: Validation loss decreased (152.208786 --> 151.564987).\n",
            "\t Train_Loss: 37.1377 Val_Loss: 151.5650  BEST VAL Loss: 151.5650\n",
            "\n",
            "Epoch 803: Validation loss decreased (151.564987 --> 151.099854).\n",
            "\t Train_Loss: 36.7914 Val_Loss: 151.0999  BEST VAL Loss: 151.0999\n",
            "\n",
            "Epoch 804: Validation loss decreased (151.099854 --> 151.033508).\n",
            "\t Train_Loss: 36.6504 Val_Loss: 151.0335  BEST VAL Loss: 151.0335\n",
            "\n",
            "Epoch 805: Validation loss decreased (151.033508 --> 150.781952).\n",
            "\t Train_Loss: 36.5214 Val_Loss: 150.7820  BEST VAL Loss: 150.7820\n",
            "\n",
            "Epoch 806: Validation loss decreased (150.781952 --> 150.050491).\n",
            "\t Train_Loss: 36.4091 Val_Loss: 150.0505  BEST VAL Loss: 150.0505\n",
            "\n",
            "Epoch 807: Validation loss decreased (150.050491 --> 149.255035).\n",
            "\t Train_Loss: 36.1719 Val_Loss: 149.2550  BEST VAL Loss: 149.2550\n",
            "\n",
            "Epoch 808: Validation loss decreased (149.255035 --> 148.973801).\n",
            "\t Train_Loss: 36.1266 Val_Loss: 148.9738  BEST VAL Loss: 148.9738\n",
            "\n",
            "Epoch 809: Validation loss decreased (148.973801 --> 148.676132).\n",
            "\t Train_Loss: 35.9231 Val_Loss: 148.6761  BEST VAL Loss: 148.6761\n",
            "\n",
            "Epoch 810: Validation loss decreased (148.676132 --> 148.431808).\n",
            "\t Train_Loss: 35.7461 Val_Loss: 148.4318  BEST VAL Loss: 148.4318\n",
            "\n",
            "Epoch 811: Validation loss decreased (148.431808 --> 147.964737).\n",
            "\t Train_Loss: 35.6280 Val_Loss: 147.9647  BEST VAL Loss: 147.9647\n",
            "\n",
            "Epoch 812: Validation loss decreased (147.964737 --> 147.335037).\n",
            "\t Train_Loss: 35.5014 Val_Loss: 147.3350  BEST VAL Loss: 147.3350\n",
            "\n",
            "Epoch 813: Validation loss decreased (147.335037 --> 147.179642).\n",
            "\t Train_Loss: 35.3841 Val_Loss: 147.1796  BEST VAL Loss: 147.1796\n",
            "\n",
            "Epoch 814: Validation loss decreased (147.179642 --> 146.562744).\n",
            "\t Train_Loss: 35.3644 Val_Loss: 146.5627  BEST VAL Loss: 146.5627\n",
            "\n",
            "Epoch 815: Validation loss decreased (146.562744 --> 146.360336).\n",
            "\t Train_Loss: 35.7161 Val_Loss: 146.3603  BEST VAL Loss: 146.3603\n",
            "\n",
            "Epoch 816: Validation loss did not decrease\n",
            "\t Train_Loss: 34.9935 Val_Loss: 146.7421  BEST VAL Loss: 146.3603\n",
            "\n",
            "Epoch 817: Validation loss decreased (146.360336 --> 145.551926).\n",
            "\t Train_Loss: 35.5450 Val_Loss: 145.5519  BEST VAL Loss: 145.5519\n",
            "\n",
            "Epoch 818: Validation loss decreased (145.551926 --> 144.888229).\n",
            "\t Train_Loss: 34.9558 Val_Loss: 144.8882  BEST VAL Loss: 144.8882\n",
            "\n",
            "Epoch 819: Validation loss decreased (144.888229 --> 144.722565).\n",
            "\t Train_Loss: 35.0618 Val_Loss: 144.7226  BEST VAL Loss: 144.7226\n",
            "\n",
            "Epoch 820: Validation loss did not decrease\n",
            "\t Train_Loss: 34.9403 Val_Loss: 144.7316  BEST VAL Loss: 144.7226\n",
            "\n",
            "Epoch 821: Validation loss decreased (144.722565 --> 144.535400).\n",
            "\t Train_Loss: 34.5897 Val_Loss: 144.5354  BEST VAL Loss: 144.5354\n",
            "\n",
            "Epoch 822: Validation loss decreased (144.535400 --> 143.690582).\n",
            "\t Train_Loss: 34.6357 Val_Loss: 143.6906  BEST VAL Loss: 143.6906\n",
            "\n",
            "Epoch 823: Validation loss decreased (143.690582 --> 143.437347).\n",
            "\t Train_Loss: 34.2622 Val_Loss: 143.4373  BEST VAL Loss: 143.4373\n",
            "\n",
            "Epoch 824: Validation loss did not decrease\n",
            "\t Train_Loss: 34.2772 Val_Loss: 143.5044  BEST VAL Loss: 143.4373\n",
            "\n",
            "Epoch 825: Validation loss decreased (143.437347 --> 143.109848).\n",
            "\t Train_Loss: 33.9641 Val_Loss: 143.1098  BEST VAL Loss: 143.1098\n",
            "\n",
            "Epoch 826: Validation loss decreased (143.109848 --> 142.694672).\n",
            "\t Train_Loss: 33.7508 Val_Loss: 142.6947  BEST VAL Loss: 142.6947\n",
            "\n",
            "Epoch 827: Validation loss did not decrease\n",
            "\t Train_Loss: 33.8497 Val_Loss: 142.7466  BEST VAL Loss: 142.6947\n",
            "\n",
            "Epoch 828: Validation loss decreased (142.694672 --> 142.265381).\n",
            "\t Train_Loss: 33.5814 Val_Loss: 142.2654  BEST VAL Loss: 142.2654\n",
            "\n",
            "Epoch 829: Validation loss decreased (142.265381 --> 141.897659).\n",
            "\t Train_Loss: 33.4295 Val_Loss: 141.8977  BEST VAL Loss: 141.8977\n",
            "\n",
            "Epoch 830: Validation loss decreased (141.897659 --> 141.502594).\n",
            "\t Train_Loss: 33.2645 Val_Loss: 141.5026  BEST VAL Loss: 141.5026\n",
            "\n",
            "Epoch 831: Validation loss decreased (141.502594 --> 141.119980).\n",
            "\t Train_Loss: 33.1753 Val_Loss: 141.1200  BEST VAL Loss: 141.1200\n",
            "\n",
            "Epoch 832: Validation loss decreased (141.119980 --> 141.013702).\n",
            "\t Train_Loss: 33.0572 Val_Loss: 141.0137  BEST VAL Loss: 141.0137\n",
            "\n",
            "Epoch 833: Validation loss decreased (141.013702 --> 140.658386).\n",
            "\t Train_Loss: 32.9209 Val_Loss: 140.6584  BEST VAL Loss: 140.6584\n",
            "\n",
            "Epoch 834: Validation loss decreased (140.658386 --> 140.166107).\n",
            "\t Train_Loss: 32.7906 Val_Loss: 140.1661  BEST VAL Loss: 140.1661\n",
            "\n",
            "Epoch 835: Validation loss decreased (140.166107 --> 140.014114).\n",
            "\t Train_Loss: 32.6876 Val_Loss: 140.0141  BEST VAL Loss: 140.0141\n",
            "\n",
            "Epoch 836: Validation loss decreased (140.014114 --> 139.476562).\n",
            "\t Train_Loss: 32.6136 Val_Loss: 139.4766  BEST VAL Loss: 139.4766\n",
            "\n",
            "Epoch 837: Validation loss decreased (139.476562 --> 139.462891).\n",
            "\t Train_Loss: 32.6091 Val_Loss: 139.4629  BEST VAL Loss: 139.4629\n",
            "\n",
            "Epoch 838: Validation loss decreased (139.462891 --> 138.780930).\n",
            "\t Train_Loss: 32.3977 Val_Loss: 138.7809  BEST VAL Loss: 138.7809\n",
            "\n",
            "Epoch 839: Validation loss decreased (138.780930 --> 138.365952).\n",
            "\t Train_Loss: 32.2158 Val_Loss: 138.3660  BEST VAL Loss: 138.3660\n",
            "\n",
            "Epoch 840: Validation loss decreased (138.365952 --> 138.187653).\n",
            "\t Train_Loss: 32.1033 Val_Loss: 138.1877  BEST VAL Loss: 138.1877\n",
            "\n",
            "Epoch 841: Validation loss decreased (138.187653 --> 137.686279).\n",
            "\t Train_Loss: 32.0169 Val_Loss: 137.6863  BEST VAL Loss: 137.6863\n",
            "\n",
            "Epoch 842: Validation loss decreased (137.686279 --> 137.516968).\n",
            "\t Train_Loss: 31.9042 Val_Loss: 137.5170  BEST VAL Loss: 137.5170\n",
            "\n",
            "Epoch 843: Validation loss decreased (137.516968 --> 137.230515).\n",
            "\t Train_Loss: 31.7362 Val_Loss: 137.2305  BEST VAL Loss: 137.2305\n",
            "\n",
            "Epoch 844: Validation loss decreased (137.230515 --> 136.810654).\n",
            "\t Train_Loss: 31.6308 Val_Loss: 136.8107  BEST VAL Loss: 136.8107\n",
            "\n",
            "Epoch 845: Validation loss did not decrease\n",
            "\t Train_Loss: 31.5688 Val_Loss: 136.8618  BEST VAL Loss: 136.8107\n",
            "\n",
            "Epoch 846: Validation loss decreased (136.810654 --> 136.011597).\n",
            "\t Train_Loss: 31.5489 Val_Loss: 136.0116  BEST VAL Loss: 136.0116\n",
            "\n",
            "Epoch 847: Validation loss decreased (136.011597 --> 135.861496).\n",
            "\t Train_Loss: 31.4972 Val_Loss: 135.8615  BEST VAL Loss: 135.8615\n",
            "\n",
            "Epoch 848: Validation loss decreased (135.861496 --> 135.755188).\n",
            "\t Train_Loss: 31.2010 Val_Loss: 135.7552  BEST VAL Loss: 135.7552\n",
            "\n",
            "Epoch 849: Validation loss decreased (135.755188 --> 135.153091).\n",
            "\t Train_Loss: 31.1804 Val_Loss: 135.1531  BEST VAL Loss: 135.1531\n",
            "\n",
            "Epoch 850: Validation loss decreased (135.153091 --> 134.860275).\n",
            "\t Train_Loss: 31.0829 Val_Loss: 134.8603  BEST VAL Loss: 134.8603\n",
            "\n",
            "Epoch 851: Validation loss did not decrease\n",
            "\t Train_Loss: 30.8894 Val_Loss: 134.8834  BEST VAL Loss: 134.8603\n",
            "\n",
            "Epoch 852: Validation loss decreased (134.860275 --> 134.190170).\n",
            "\t Train_Loss: 30.9064 Val_Loss: 134.1902  BEST VAL Loss: 134.1902\n",
            "\n",
            "Epoch 853: Validation loss decreased (134.190170 --> 134.048141).\n",
            "\t Train_Loss: 30.7527 Val_Loss: 134.0481  BEST VAL Loss: 134.0481\n",
            "\n",
            "Epoch 854: Validation loss decreased (134.048141 --> 133.876312).\n",
            "\t Train_Loss: 30.5435 Val_Loss: 133.8763  BEST VAL Loss: 133.8763\n",
            "\n",
            "Epoch 855: Validation loss decreased (133.876312 --> 133.221069).\n",
            "\t Train_Loss: 30.5147 Val_Loss: 133.2211  BEST VAL Loss: 133.2211\n",
            "\n",
            "Epoch 856: Validation loss decreased (133.221069 --> 133.069748).\n",
            "\t Train_Loss: 30.4595 Val_Loss: 133.0697  BEST VAL Loss: 133.0697\n",
            "\n",
            "Epoch 857: Validation loss decreased (133.069748 --> 132.797592).\n",
            "\t Train_Loss: 30.2503 Val_Loss: 132.7976  BEST VAL Loss: 132.7976\n",
            "\n",
            "Epoch 858: Validation loss decreased (132.797592 --> 132.218918).\n",
            "\t Train_Loss: 30.1972 Val_Loss: 132.2189  BEST VAL Loss: 132.2189\n",
            "\n",
            "Epoch 859: Validation loss decreased (132.218918 --> 132.042221).\n",
            "\t Train_Loss: 30.1029 Val_Loss: 132.0422  BEST VAL Loss: 132.0422\n",
            "\n",
            "Epoch 860: Validation loss decreased (132.042221 --> 132.005295).\n",
            "\t Train_Loss: 29.9424 Val_Loss: 132.0053  BEST VAL Loss: 132.0053\n",
            "\n",
            "Epoch 861: Validation loss decreased (132.005295 --> 131.420212).\n",
            "\t Train_Loss: 29.9065 Val_Loss: 131.4202  BEST VAL Loss: 131.4202\n",
            "\n",
            "Epoch 862: Validation loss decreased (131.420212 --> 131.155655).\n",
            "\t Train_Loss: 29.8091 Val_Loss: 131.1557  BEST VAL Loss: 131.1557\n",
            "\n",
            "Epoch 863: Validation loss decreased (131.155655 --> 130.848053).\n",
            "\t Train_Loss: 29.6540 Val_Loss: 130.8481  BEST VAL Loss: 130.8481\n",
            "\n",
            "Epoch 864: Validation loss decreased (130.848053 --> 130.410477).\n",
            "\t Train_Loss: 29.5675 Val_Loss: 130.4105  BEST VAL Loss: 130.4105\n",
            "\n",
            "Epoch 865: Validation loss decreased (130.410477 --> 130.222946).\n",
            "\t Train_Loss: 29.5026 Val_Loss: 130.2229  BEST VAL Loss: 130.2229\n",
            "\n",
            "Epoch 866: Validation loss decreased (130.222946 --> 129.916061).\n",
            "\t Train_Loss: 29.3719 Val_Loss: 129.9161  BEST VAL Loss: 129.9161\n",
            "\n",
            "Epoch 867: Validation loss decreased (129.916061 --> 129.513214).\n",
            "\t Train_Loss: 29.2786 Val_Loss: 129.5132  BEST VAL Loss: 129.5132\n",
            "\n",
            "Epoch 868: Validation loss decreased (129.513214 --> 129.340881).\n",
            "\t Train_Loss: 29.2076 Val_Loss: 129.3409  BEST VAL Loss: 129.3409\n",
            "\n",
            "Epoch 869: Validation loss decreased (129.340881 --> 128.933701).\n",
            "\t Train_Loss: 29.0965 Val_Loss: 128.9337  BEST VAL Loss: 128.9337\n",
            "\n",
            "Epoch 870: Validation loss decreased (128.933701 --> 128.568954).\n",
            "\t Train_Loss: 28.9831 Val_Loss: 128.5690  BEST VAL Loss: 128.5690\n",
            "\n",
            "Epoch 871: Validation loss decreased (128.568954 --> 128.409958).\n",
            "\t Train_Loss: 28.9067 Val_Loss: 128.4100  BEST VAL Loss: 128.4100\n",
            "\n",
            "Epoch 872: Validation loss decreased (128.409958 --> 127.985497).\n",
            "\t Train_Loss: 28.8262 Val_Loss: 127.9855  BEST VAL Loss: 127.9855\n",
            "\n",
            "Epoch 873: Validation loss decreased (127.985497 --> 127.641846).\n",
            "\t Train_Loss: 28.7108 Val_Loss: 127.6418  BEST VAL Loss: 127.6418\n",
            "\n",
            "Epoch 874: Validation loss decreased (127.641846 --> 127.416313).\n",
            "\t Train_Loss: 28.6127 Val_Loss: 127.4163  BEST VAL Loss: 127.4163\n",
            "\n",
            "Epoch 875: Validation loss decreased (127.416313 --> 127.057152).\n",
            "\t Train_Loss: 28.5359 Val_Loss: 127.0572  BEST VAL Loss: 127.0572\n",
            "\n",
            "Epoch 876: Validation loss decreased (127.057152 --> 126.872414).\n",
            "\t Train_Loss: 28.4522 Val_Loss: 126.8724  BEST VAL Loss: 126.8724\n",
            "\n",
            "Epoch 877: Validation loss decreased (126.872414 --> 126.491531).\n",
            "\t Train_Loss: 28.3499 Val_Loss: 126.4915  BEST VAL Loss: 126.4915\n",
            "\n",
            "Epoch 878: Validation loss decreased (126.491531 --> 126.228455).\n",
            "\t Train_Loss: 28.2409 Val_Loss: 126.2285  BEST VAL Loss: 126.2285\n",
            "\n",
            "Epoch 879: Validation loss decreased (126.228455 --> 125.938698).\n",
            "\t Train_Loss: 28.1383 Val_Loss: 125.9387  BEST VAL Loss: 125.9387\n",
            "\n",
            "Epoch 880: Validation loss decreased (125.938698 --> 125.586914).\n",
            "\t Train_Loss: 28.0495 Val_Loss: 125.5869  BEST VAL Loss: 125.5869\n",
            "\n",
            "Epoch 881: Validation loss decreased (125.586914 --> 125.419022).\n",
            "\t Train_Loss: 27.9667 Val_Loss: 125.4190  BEST VAL Loss: 125.4190\n",
            "\n",
            "Epoch 882: Validation loss decreased (125.419022 --> 125.088310).\n",
            "\t Train_Loss: 27.8907 Val_Loss: 125.0883  BEST VAL Loss: 125.0883\n",
            "\n",
            "Epoch 883: Validation loss decreased (125.088310 --> 124.890846).\n",
            "\t Train_Loss: 27.8100 Val_Loss: 124.8908  BEST VAL Loss: 124.8908\n",
            "\n",
            "Epoch 884: Validation loss decreased (124.890846 --> 124.523277).\n",
            "\t Train_Loss: 27.7288 Val_Loss: 124.5233  BEST VAL Loss: 124.5233\n",
            "\n",
            "Epoch 885: Validation loss decreased (124.523277 --> 124.292847).\n",
            "\t Train_Loss: 27.6133 Val_Loss: 124.2928  BEST VAL Loss: 124.2928\n",
            "\n",
            "Epoch 886: Validation loss decreased (124.292847 --> 123.983864).\n",
            "\t Train_Loss: 27.5055 Val_Loss: 123.9839  BEST VAL Loss: 123.9839\n",
            "\n",
            "Epoch 887: Validation loss decreased (123.983864 --> 123.784645).\n",
            "\t Train_Loss: 27.4022 Val_Loss: 123.7846  BEST VAL Loss: 123.7846\n",
            "\n",
            "Epoch 888: Validation loss decreased (123.784645 --> 123.518509).\n",
            "\t Train_Loss: 27.3079 Val_Loss: 123.5185  BEST VAL Loss: 123.5185\n",
            "\n",
            "Epoch 889: Validation loss decreased (123.518509 --> 123.288750).\n",
            "\t Train_Loss: 27.2193 Val_Loss: 123.2887  BEST VAL Loss: 123.2887\n",
            "\n",
            "Epoch 890: Validation loss decreased (123.288750 --> 122.971947).\n",
            "\t Train_Loss: 27.1391 Val_Loss: 122.9719  BEST VAL Loss: 122.9719\n",
            "\n",
            "Epoch 891: Validation loss decreased (122.971947 --> 122.786583).\n",
            "\t Train_Loss: 27.0695 Val_Loss: 122.7866  BEST VAL Loss: 122.7866\n",
            "\n",
            "Epoch 892: Validation loss decreased (122.786583 --> 122.493797).\n",
            "\t Train_Loss: 27.0456 Val_Loss: 122.4938  BEST VAL Loss: 122.4938\n",
            "\n",
            "Epoch 893: Validation loss decreased (122.493797 --> 122.370140).\n",
            "\t Train_Loss: 27.0030 Val_Loss: 122.3701  BEST VAL Loss: 122.3701\n",
            "\n",
            "Epoch 894: Validation loss decreased (122.370140 --> 122.078590).\n",
            "\t Train_Loss: 27.0012 Val_Loss: 122.0786  BEST VAL Loss: 122.0786\n",
            "\n",
            "Epoch 895: Validation loss decreased (122.078590 --> 121.746841).\n",
            "\t Train_Loss: 26.7694 Val_Loss: 121.7468  BEST VAL Loss: 121.7468\n",
            "\n",
            "Epoch 896: Validation loss decreased (121.746841 --> 121.503838).\n",
            "\t Train_Loss: 26.5786 Val_Loss: 121.5038  BEST VAL Loss: 121.5038\n",
            "\n",
            "Epoch 897: Validation loss decreased (121.503838 --> 121.337379).\n",
            "\t Train_Loss: 26.4812 Val_Loss: 121.3374  BEST VAL Loss: 121.3374\n",
            "\n",
            "Epoch 898: Validation loss did not decrease\n",
            "\t Train_Loss: 26.3766 Val_Loss: 121.3710  BEST VAL Loss: 121.3374\n",
            "\n",
            "Epoch 899: Validation loss decreased (121.337379 --> 121.279030).\n",
            "\t Train_Loss: 26.0870 Val_Loss: 121.2790  BEST VAL Loss: 121.2790\n",
            "\n",
            "Epoch 900: Validation loss decreased (121.279030 --> 120.924858).\n",
            "\t Train_Loss: 25.7239 Val_Loss: 120.9249  BEST VAL Loss: 120.9249\n",
            "\n",
            "Epoch 901: Validation loss decreased (120.924858 --> 120.407005).\n",
            "\t Train_Loss: 25.8325 Val_Loss: 120.4070  BEST VAL Loss: 120.4070\n",
            "\n",
            "Epoch 902: Validation loss decreased (120.407005 --> 119.869530).\n",
            "\t Train_Loss: 25.5419 Val_Loss: 119.8695  BEST VAL Loss: 119.8695\n",
            "\n",
            "Epoch 903: Validation loss decreased (119.869530 --> 119.770073).\n",
            "\t Train_Loss: 26.1171 Val_Loss: 119.7701  BEST VAL Loss: 119.7701\n",
            "\n",
            "Epoch 904: Validation loss decreased (119.770073 --> 119.762939).\n",
            "\t Train_Loss: 25.2163 Val_Loss: 119.7629  BEST VAL Loss: 119.7629\n",
            "\n",
            "Epoch 905: Validation loss decreased (119.762939 --> 119.521790).\n",
            "\t Train_Loss: 25.3690 Val_Loss: 119.5218  BEST VAL Loss: 119.5218\n",
            "\n",
            "Epoch 906: Validation loss decreased (119.521790 --> 119.155418).\n",
            "\t Train_Loss: 25.1872 Val_Loss: 119.1554  BEST VAL Loss: 119.1554\n",
            "\n",
            "Epoch 907: Validation loss decreased (119.155418 --> 118.832375).\n",
            "\t Train_Loss: 25.1401 Val_Loss: 118.8324  BEST VAL Loss: 118.8324\n",
            "\n",
            "Epoch 908: Validation loss decreased (118.832375 --> 118.512100).\n",
            "\t Train_Loss: 25.6501 Val_Loss: 118.5121  BEST VAL Loss: 118.5121\n",
            "\n",
            "Epoch 909: Validation loss did not decrease\n",
            "\t Train_Loss: 24.9312 Val_Loss: 120.6766  BEST VAL Loss: 118.5121\n",
            "\n",
            "Epoch 910: Validation loss did not decrease\n",
            "\t Train_Loss: 26.2068 Val_Loss: 120.5707  BEST VAL Loss: 118.5121\n",
            "\n",
            "Epoch 911: Validation loss decreased (118.512100 --> 118.171021).\n",
            "\t Train_Loss: 26.0698 Val_Loss: 118.1710  BEST VAL Loss: 118.1710\n",
            "\n",
            "Epoch 912: Validation loss decreased (118.171021 --> 117.745094).\n",
            "\t Train_Loss: 25.5348 Val_Loss: 117.7451  BEST VAL Loss: 117.7451\n",
            "\n",
            "Epoch 913: Validation loss decreased (117.745094 --> 117.394386).\n",
            "\t Train_Loss: 26.2633 Val_Loss: 117.3944  BEST VAL Loss: 117.3944\n",
            "\n",
            "Epoch 914: Validation loss did not decrease\n",
            "\t Train_Loss: 25.9768 Val_Loss: 117.5135  BEST VAL Loss: 117.3944\n",
            "\n",
            "Epoch 915: Validation loss did not decrease\n",
            "\t Train_Loss: 25.1646 Val_Loss: 119.2656  BEST VAL Loss: 117.3944\n",
            "\n",
            "Epoch 916: Validation loss did not decrease\n",
            "\t Train_Loss: 25.5425 Val_Loss: 119.1690  BEST VAL Loss: 117.3944\n",
            "\n",
            "Epoch 917: Validation loss did not decrease\n",
            "\t Train_Loss: 25.5662 Val_Loss: 117.4615  BEST VAL Loss: 117.3944\n",
            "\n",
            "Epoch 918: Validation loss decreased (117.394386 --> 116.691040).\n",
            "\t Train_Loss: 24.8186 Val_Loss: 116.6910  BEST VAL Loss: 116.6910\n",
            "\n",
            "Epoch 919: Validation loss decreased (116.691040 --> 116.041679).\n",
            "\t Train_Loss: 25.1601 Val_Loss: 116.0417  BEST VAL Loss: 116.0417\n",
            "\n",
            "Epoch 920: Validation loss did not decrease\n",
            "\t Train_Loss: 25.1183 Val_Loss: 116.1293  BEST VAL Loss: 116.0417\n",
            "\n",
            "Epoch 921: Validation loss did not decrease\n",
            "\t Train_Loss: 24.5334 Val_Loss: 116.7781  BEST VAL Loss: 116.0417\n",
            "\n",
            "Epoch 922: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3687 Val_Loss: 116.3081  BEST VAL Loss: 116.0417\n",
            "\n",
            "Epoch 923: Validation loss decreased (116.041679 --> 115.197121).\n",
            "\t Train_Loss: 24.5543 Val_Loss: 115.1971  BEST VAL Loss: 115.1971\n",
            "\n",
            "Epoch 924: Validation loss decreased (115.197121 --> 114.730179).\n",
            "\t Train_Loss: 24.5857 Val_Loss: 114.7302  BEST VAL Loss: 114.7302\n",
            "\n",
            "Epoch 925: Validation loss did not decrease\n",
            "\t Train_Loss: 23.9621 Val_Loss: 114.8495  BEST VAL Loss: 114.7302\n",
            "\n",
            "Epoch 926: Validation loss did not decrease\n",
            "\t Train_Loss: 23.5572 Val_Loss: 114.8854  BEST VAL Loss: 114.7302\n",
            "\n",
            "Epoch 927: Validation loss decreased (114.730179 --> 114.176445).\n",
            "\t Train_Loss: 23.5557 Val_Loss: 114.1764  BEST VAL Loss: 114.1764\n",
            "\n",
            "Epoch 928: Validation loss did not decrease\n",
            "\t Train_Loss: 23.0818 Val_Loss: 114.7611  BEST VAL Loss: 114.1764\n",
            "\n",
            "Epoch 929: Validation loss did not decrease\n",
            "\t Train_Loss: 23.3343 Val_Loss: 114.5139  BEST VAL Loss: 114.1764\n",
            "\n",
            "Epoch 930: Validation loss decreased (114.176445 --> 113.104546).\n",
            "\t Train_Loss: 23.0837 Val_Loss: 113.1045  BEST VAL Loss: 113.1045\n",
            "\n",
            "Epoch 931: Validation loss decreased (113.104546 --> 112.637817).\n",
            "\t Train_Loss: 23.2946 Val_Loss: 112.6378  BEST VAL Loss: 112.6378\n",
            "\n",
            "Epoch 932: Validation loss decreased (112.637817 --> 112.310783).\n",
            "\t Train_Loss: 22.8501 Val_Loss: 112.3108  BEST VAL Loss: 112.3108\n",
            "\n",
            "Epoch 933: Validation loss decreased (112.310783 --> 112.132072).\n",
            "\t Train_Loss: 22.8013 Val_Loss: 112.1321  BEST VAL Loss: 112.1321\n",
            "\n",
            "Epoch 934: Validation loss decreased (112.132072 --> 112.110161).\n",
            "\t Train_Loss: 22.3099 Val_Loss: 112.1102  BEST VAL Loss: 112.1102\n",
            "\n",
            "Epoch 935: Validation loss did not decrease\n",
            "\t Train_Loss: 22.1312 Val_Loss: 112.4706  BEST VAL Loss: 112.1102\n",
            "\n",
            "Epoch 936: Validation loss did not decrease\n",
            "\t Train_Loss: 22.1117 Val_Loss: 112.2587  BEST VAL Loss: 112.1102\n",
            "\n",
            "Epoch 937: Validation loss decreased (112.110161 --> 111.792664).\n",
            "\t Train_Loss: 22.7199 Val_Loss: 111.7927  BEST VAL Loss: 111.7927\n",
            "\n",
            "Epoch 938: Validation loss decreased (111.792664 --> 111.121338).\n",
            "\t Train_Loss: 22.3764 Val_Loss: 111.1213  BEST VAL Loss: 111.1213\n",
            "\n",
            "Epoch 939: Validation loss decreased (111.121338 --> 110.685135).\n",
            "\t Train_Loss: 22.2636 Val_Loss: 110.6851  BEST VAL Loss: 110.6851\n",
            "\n",
            "Epoch 940: Validation loss decreased (110.685135 --> 110.228661).\n",
            "\t Train_Loss: 21.6849 Val_Loss: 110.2287  BEST VAL Loss: 110.2287\n",
            "\n",
            "Epoch 941: Validation loss decreased (110.228661 --> 110.118103).\n",
            "\t Train_Loss: 21.5340 Val_Loss: 110.1181  BEST VAL Loss: 110.1181\n",
            "\n",
            "Epoch 942: Validation loss did not decrease\n",
            "\t Train_Loss: 21.4435 Val_Loss: 110.4874  BEST VAL Loss: 110.1181\n",
            "\n",
            "Epoch 943: Validation loss did not decrease\n",
            "\t Train_Loss: 21.5858 Val_Loss: 111.0220  BEST VAL Loss: 110.1181\n",
            "\n",
            "Epoch 944: Validation loss decreased (110.118103 --> 109.749527).\n",
            "\t Train_Loss: 22.4051 Val_Loss: 109.7495  BEST VAL Loss: 109.7495\n",
            "\n",
            "Epoch 945: Validation loss did not decrease\n",
            "\t Train_Loss: 21.1375 Val_Loss: 110.7950  BEST VAL Loss: 109.7495\n",
            "\n",
            "Epoch 946: Validation loss decreased (109.749527 --> 108.658813).\n",
            "\t Train_Loss: 21.9177 Val_Loss: 108.6588  BEST VAL Loss: 108.6588\n",
            "\n",
            "Epoch 947: Validation loss decreased (108.658813 --> 108.497559).\n",
            "\t Train_Loss: 21.5713 Val_Loss: 108.4976  BEST VAL Loss: 108.4976\n",
            "\n",
            "Epoch 948: Validation loss decreased (108.497559 --> 107.970665).\n",
            "\t Train_Loss: 22.1581 Val_Loss: 107.9707  BEST VAL Loss: 107.9707\n",
            "\n",
            "Epoch 949: Validation loss did not decrease\n",
            "\t Train_Loss: 20.8923 Val_Loss: 109.0754  BEST VAL Loss: 107.9707\n",
            "\n",
            "Epoch 950: Validation loss did not decrease\n",
            "\t Train_Loss: 21.5934 Val_Loss: 108.2271  BEST VAL Loss: 107.9707\n",
            "\n",
            "Epoch 951: Validation loss did not decrease\n",
            "\t Train_Loss: 21.2350 Val_Loss: 107.9980  BEST VAL Loss: 107.9707\n",
            "\n",
            "Epoch 952: Validation loss did not decrease\n",
            "\t Train_Loss: 21.3992 Val_Loss: 108.8967  BEST VAL Loss: 107.9707\n",
            "\n",
            "Epoch 953: Validation loss did not decrease\n",
            "\t Train_Loss: 21.0165 Val_Loss: 109.3935  BEST VAL Loss: 107.9707\n",
            "\n",
            "Epoch 954: Validation loss decreased (107.970665 --> 106.860733).\n",
            "\t Train_Loss: 21.7067 Val_Loss: 106.8607  BEST VAL Loss: 106.8607\n",
            "\n",
            "Epoch 955: Validation loss decreased (106.860733 --> 106.132324).\n",
            "\t Train_Loss: 20.7027 Val_Loss: 106.1323  BEST VAL Loss: 106.1323\n",
            "\n",
            "Epoch 956: Validation loss decreased (106.132324 --> 106.075279).\n",
            "\t Train_Loss: 21.7669 Val_Loss: 106.0753  BEST VAL Loss: 106.0753\n",
            "\n",
            "Epoch 957: Validation loss decreased (106.075279 --> 105.954979).\n",
            "\t Train_Loss: 21.2105 Val_Loss: 105.9550  BEST VAL Loss: 105.9550\n",
            "\n",
            "Epoch 958: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9520 Val_Loss: 107.9492  BEST VAL Loss: 105.9550\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 21.5881 Val_Loss: 106.6424  BEST VAL Loss: 105.9550\n",
            "\n",
            "Epoch 960: Validation loss decreased (105.954979 --> 105.485413).\n",
            "\t Train_Loss: 20.1298 Val_Loss: 105.4854  BEST VAL Loss: 105.4854\n",
            "\n",
            "Epoch 961: Validation loss decreased (105.485413 --> 104.892754).\n",
            "\t Train_Loss: 20.8948 Val_Loss: 104.8928  BEST VAL Loss: 104.8928\n",
            "\n",
            "Epoch 962: Validation loss did not decrease\n",
            "\t Train_Loss: 20.5888 Val_Loss: 105.7842  BEST VAL Loss: 104.8928\n",
            "\n",
            "Epoch 963: Validation loss did not decrease\n",
            "\t Train_Loss: 20.0369 Val_Loss: 105.8996  BEST VAL Loss: 104.8928\n",
            "\n",
            "Epoch 964: Validation loss decreased (104.892754 --> 103.979507).\n",
            "\t Train_Loss: 20.1090 Val_Loss: 103.9795  BEST VAL Loss: 103.9795\n",
            "\n",
            "Epoch 965: Validation loss decreased (103.979507 --> 103.744995).\n",
            "\t Train_Loss: 20.2161 Val_Loss: 103.7450  BEST VAL Loss: 103.7450\n",
            "\n",
            "Epoch 966: Validation loss did not decrease\n",
            "\t Train_Loss: 20.5776 Val_Loss: 104.1261  BEST VAL Loss: 103.7450\n",
            "\n",
            "Epoch 967: Validation loss did not decrease\n",
            "\t Train_Loss: 19.5852 Val_Loss: 106.4406  BEST VAL Loss: 103.7450\n",
            "\n",
            "Epoch 968: Validation loss did not decrease\n",
            "\t Train_Loss: 20.8489 Val_Loss: 104.0005  BEST VAL Loss: 103.7450\n",
            "\n",
            "Epoch 969: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3239 Val_Loss: 105.3634  BEST VAL Loss: 103.7450\n",
            "\n",
            "Epoch 970: Validation loss decreased (103.744995 --> 102.751587).\n",
            "\t Train_Loss: 21.3965 Val_Loss: 102.7516  BEST VAL Loss: 102.7516\n",
            "\n",
            "Epoch 971: Validation loss did not decrease\n",
            "\t Train_Loss: 20.1593 Val_Loss: 103.9501  BEST VAL Loss: 102.7516\n",
            "\n",
            "Epoch 972: Validation loss did not decrease\n",
            "\t Train_Loss: 19.4611 Val_Loss: 106.8740  BEST VAL Loss: 102.7516\n",
            "\n",
            "Epoch 973: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6822 Val_Loss: 103.7504  BEST VAL Loss: 102.7516\n",
            "\n",
            "Epoch 974: Validation loss decreased (102.751587 --> 101.507278).\n",
            "\t Train_Loss: 20.2312 Val_Loss: 101.5073  BEST VAL Loss: 101.5073\n",
            "\n",
            "Epoch 975: Validation loss did not decrease\n",
            "\t Train_Loss: 19.5537 Val_Loss: 102.5428  BEST VAL Loss: 101.5073\n",
            "\n",
            "Epoch 976: Validation loss did not decrease\n",
            "\t Train_Loss: 20.5795 Val_Loss: 101.7151  BEST VAL Loss: 101.5073\n",
            "\n",
            "Epoch 977: Validation loss decreased (101.507278 --> 101.054039).\n",
            "\t Train_Loss: 19.7598 Val_Loss: 101.0540  BEST VAL Loss: 101.0540\n",
            "\n",
            "Epoch 978: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8127 Val_Loss: 104.2591  BEST VAL Loss: 101.0540\n",
            "\n",
            "Epoch 979: Validation loss did not decrease\n",
            "\t Train_Loss: 19.9366 Val_Loss: 107.9353  BEST VAL Loss: 101.0540\n",
            "\n",
            "Epoch 980: Validation loss decreased (101.054039 --> 100.424614).\n",
            "\t Train_Loss: 20.6834 Val_Loss: 100.4246  BEST VAL Loss: 100.4246\n",
            "\n",
            "Epoch 981: Validation loss did not decrease\n",
            "\t Train_Loss: 19.1710 Val_Loss: 100.7208  BEST VAL Loss: 100.4246\n",
            "\n",
            "Epoch 982: Validation loss decreased (100.424614 --> 99.646935).\n",
            "\t Train_Loss: 19.7608 Val_Loss: 99.6469  BEST VAL Loss: 99.6469\n",
            "\n",
            "Epoch 983: Validation loss did not decrease\n",
            "\t Train_Loss: 18.6374 Val_Loss: 100.5444  BEST VAL Loss: 99.6469\n",
            "\n",
            "Epoch 984: Validation loss did not decrease\n",
            "\t Train_Loss: 19.1375 Val_Loss: 100.5452  BEST VAL Loss: 99.6469\n",
            "\n",
            "Epoch 985: Validation loss decreased (99.646935 --> 99.339027).\n",
            "\t Train_Loss: 18.9502 Val_Loss: 99.3390  BEST VAL Loss: 99.3390\n",
            "\n",
            "Epoch 986: Validation loss did not decrease\n",
            "\t Train_Loss: 18.9841 Val_Loss: 99.5460  BEST VAL Loss: 99.3390\n",
            "\n",
            "Epoch 987: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3750 Val_Loss: 99.3451  BEST VAL Loss: 99.3390\n",
            "\n",
            "Epoch 988: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8082 Val_Loss: 99.8320  BEST VAL Loss: 99.3390\n",
            "\n",
            "Epoch 989: Validation loss did not decrease\n",
            "\t Train_Loss: 18.2666 Val_Loss: 99.9745  BEST VAL Loss: 99.3390\n",
            "\n",
            "Epoch 990: Validation loss decreased (99.339027 --> 97.800560).\n",
            "\t Train_Loss: 19.1003 Val_Loss: 97.8006  BEST VAL Loss: 97.8006\n",
            "\n",
            "Epoch 991: Validation loss did not decrease\n",
            "\t Train_Loss: 18.3290 Val_Loss: 98.0778  BEST VAL Loss: 97.8006\n",
            "\n",
            "Epoch 992: Validation loss decreased (97.800560 --> 97.111870).\n",
            "\t Train_Loss: 18.8415 Val_Loss: 97.1119  BEST VAL Loss: 97.1119\n",
            "\n",
            "Epoch 993: Validation loss did not decrease\n",
            "\t Train_Loss: 18.5761 Val_Loss: 98.0785  BEST VAL Loss: 97.1119\n",
            "\n",
            "Epoch 994: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0789 Val_Loss: 100.4612  BEST VAL Loss: 97.1119\n",
            "\n",
            "Epoch 995: Validation loss did not decrease\n",
            "\t Train_Loss: 18.5189 Val_Loss: 99.0268  BEST VAL Loss: 97.1119\n",
            "\n",
            "Epoch 996: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0002 Val_Loss: 97.3774  BEST VAL Loss: 97.1119\n",
            "\n",
            "Epoch 997: Validation loss did not decrease\n",
            "\t Train_Loss: 17.7145 Val_Loss: 97.3188  BEST VAL Loss: 97.1119\n",
            "\n",
            "Epoch 998: Validation loss decreased (97.111870 --> 96.994469).\n",
            "\t Train_Loss: 17.9409 Val_Loss: 96.9945  BEST VAL Loss: 96.9945\n",
            "\n",
            "Epoch 999: Validation loss decreased (96.994469 --> 96.668312).\n",
            "\t Train_Loss: 18.0798 Val_Loss: 96.6683  BEST VAL Loss: 96.6683\n",
            "\n",
            "Epoch 1000: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6353 Val_Loss: 96.7101  BEST VAL Loss: 96.6683\n",
            "\n",
            "Epoch 1001: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6408 Val_Loss: 96.7906  BEST VAL Loss: 96.6683\n",
            "\n",
            "Epoch 1002: Validation loss did not decrease\n",
            "\t Train_Loss: 17.7854 Val_Loss: 96.7395  BEST VAL Loss: 96.6683\n",
            "\n",
            "Epoch 1003: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6892 Val_Loss: 96.7537  BEST VAL Loss: 96.6683\n",
            "\n",
            "Epoch 1004: Validation loss decreased (96.668312 --> 96.657967).\n",
            "\t Train_Loss: 17.4488 Val_Loss: 96.6580  BEST VAL Loss: 96.6580\n",
            "\n",
            "Epoch 1005: Validation loss decreased (96.657967 --> 95.510559).\n",
            "\t Train_Loss: 17.2068 Val_Loss: 95.5106  BEST VAL Loss: 95.5106\n",
            "\n",
            "Epoch 1006: Validation loss decreased (95.510559 --> 94.705406).\n",
            "\t Train_Loss: 17.4545 Val_Loss: 94.7054  BEST VAL Loss: 94.7054\n",
            "\n",
            "Epoch 1007: Validation loss decreased (94.705406 --> 94.566551).\n",
            "\t Train_Loss: 17.2533 Val_Loss: 94.5666  BEST VAL Loss: 94.5666\n",
            "\n",
            "Epoch 1008: Validation loss decreased (94.566551 --> 94.425438).\n",
            "\t Train_Loss: 17.4789 Val_Loss: 94.4254  BEST VAL Loss: 94.4254\n",
            "\n",
            "Epoch 1009: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9795 Val_Loss: 95.2934  BEST VAL Loss: 94.4254\n",
            "\n",
            "Epoch 1010: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9781 Val_Loss: 95.2283  BEST VAL Loss: 94.4254\n",
            "\n",
            "Epoch 1011: Validation loss did not decrease\n",
            "\t Train_Loss: 16.7348 Val_Loss: 94.7337  BEST VAL Loss: 94.4254\n",
            "\n",
            "Epoch 1012: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9033 Val_Loss: 94.5327  BEST VAL Loss: 94.4254\n",
            "\n",
            "Epoch 1013: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8197 Val_Loss: 95.1771  BEST VAL Loss: 94.4254\n",
            "\n",
            "Epoch 1014: Validation loss decreased (94.425438 --> 94.210693).\n",
            "\t Train_Loss: 16.6717 Val_Loss: 94.2107  BEST VAL Loss: 94.2107\n",
            "\n",
            "Epoch 1015: Validation loss decreased (94.210693 --> 93.639648).\n",
            "\t Train_Loss: 16.4346 Val_Loss: 93.6396  BEST VAL Loss: 93.6396\n",
            "\n",
            "Epoch 1016: Validation loss decreased (93.639648 --> 93.499924).\n",
            "\t Train_Loss: 16.5238 Val_Loss: 93.4999  BEST VAL Loss: 93.4999\n",
            "\n",
            "Epoch 1017: Validation loss did not decrease\n",
            "\t Train_Loss: 16.3291 Val_Loss: 93.6047  BEST VAL Loss: 93.4999\n",
            "\n",
            "Epoch 1018: Validation loss did not decrease\n",
            "\t Train_Loss: 16.1630 Val_Loss: 93.6407  BEST VAL Loss: 93.4999\n",
            "\n",
            "Epoch 1019: Validation loss decreased (93.499924 --> 93.453110).\n",
            "\t Train_Loss: 16.1007 Val_Loss: 93.4531  BEST VAL Loss: 93.4531\n",
            "\n",
            "Epoch 1020: Validation loss decreased (93.453110 --> 93.085869).\n",
            "\t Train_Loss: 16.1774 Val_Loss: 93.0859  BEST VAL Loss: 93.0859\n",
            "\n",
            "Epoch 1021: Validation loss did not decrease\n",
            "\t Train_Loss: 16.2527 Val_Loss: 93.3564  BEST VAL Loss: 93.0859\n",
            "\n",
            "Epoch 1022: Validation loss decreased (93.085869 --> 92.427643).\n",
            "\t Train_Loss: 16.2386 Val_Loss: 92.4276  BEST VAL Loss: 92.4276\n",
            "\n",
            "Epoch 1023: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0187 Val_Loss: 92.4955  BEST VAL Loss: 92.4276\n",
            "\n",
            "Epoch 1024: Validation loss did not decrease\n",
            "\t Train_Loss: 15.8826 Val_Loss: 93.0567  BEST VAL Loss: 92.4276\n",
            "\n",
            "Epoch 1025: Validation loss decreased (92.427643 --> 92.209953).\n",
            "\t Train_Loss: 16.1859 Val_Loss: 92.2100  BEST VAL Loss: 92.2100\n",
            "\n",
            "Epoch 1026: Validation loss decreased (92.209953 --> 90.916222).\n",
            "\t Train_Loss: 15.7638 Val_Loss: 90.9162  BEST VAL Loss: 90.9162\n",
            "\n",
            "Epoch 1027: Validation loss did not decrease\n",
            "\t Train_Loss: 16.4296 Val_Loss: 90.9478  BEST VAL Loss: 90.9162\n",
            "\n",
            "Epoch 1028: Validation loss did not decrease\n",
            "\t Train_Loss: 16.4418 Val_Loss: 92.2612  BEST VAL Loss: 90.9162\n",
            "\n",
            "Epoch 1029: Validation loss did not decrease\n",
            "\t Train_Loss: 15.9813 Val_Loss: 91.1456  BEST VAL Loss: 90.9162\n",
            "\n",
            "Epoch 1030: Validation loss decreased (90.916222 --> 90.854088).\n",
            "\t Train_Loss: 16.1247 Val_Loss: 90.8541  BEST VAL Loss: 90.8541\n",
            "\n",
            "Epoch 1031: Validation loss decreased (90.854088 --> 90.647301).\n",
            "\t Train_Loss: 16.1098 Val_Loss: 90.6473  BEST VAL Loss: 90.6473\n",
            "\n",
            "Epoch 1032: Validation loss did not decrease\n",
            "\t Train_Loss: 15.8489 Val_Loss: 91.1616  BEST VAL Loss: 90.6473\n",
            "\n",
            "Epoch 1033: Validation loss decreased (90.647301 --> 90.580399).\n",
            "\t Train_Loss: 15.9175 Val_Loss: 90.5804  BEST VAL Loss: 90.5804\n",
            "\n",
            "Epoch 1034: Validation loss decreased (90.580399 --> 90.168480).\n",
            "\t Train_Loss: 15.7437 Val_Loss: 90.1685  BEST VAL Loss: 90.1685\n",
            "\n",
            "Epoch 1035: Validation loss decreased (90.168480 --> 89.866394).\n",
            "\t Train_Loss: 16.0161 Val_Loss: 89.8664  BEST VAL Loss: 89.8664\n",
            "\n",
            "Epoch 1036: Validation loss did not decrease\n",
            "\t Train_Loss: 15.4297 Val_Loss: 90.5391  BEST VAL Loss: 89.8664\n",
            "\n",
            "Epoch 1037: Validation loss did not decrease\n",
            "\t Train_Loss: 15.4733 Val_Loss: 90.2353  BEST VAL Loss: 89.8664\n",
            "\n",
            "Epoch 1038: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3292 Val_Loss: 90.1702  BEST VAL Loss: 89.8664\n",
            "\n",
            "Epoch 1039: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5865 Val_Loss: 90.8153  BEST VAL Loss: 89.8664\n",
            "\n",
            "Epoch 1040: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3013 Val_Loss: 90.3055  BEST VAL Loss: 89.8664\n",
            "\n",
            "Epoch 1041: Validation loss decreased (89.866394 --> 89.492958).\n",
            "\t Train_Loss: 15.5493 Val_Loss: 89.4930  BEST VAL Loss: 89.4930\n",
            "\n",
            "Epoch 1042: Validation loss decreased (89.492958 --> 88.769524).\n",
            "\t Train_Loss: 15.2462 Val_Loss: 88.7695  BEST VAL Loss: 88.7695\n",
            "\n",
            "Epoch 1043: Validation loss decreased (88.769524 --> 88.670639).\n",
            "\t Train_Loss: 15.2239 Val_Loss: 88.6706  BEST VAL Loss: 88.6706\n",
            "\n",
            "Epoch 1044: Validation loss decreased (88.670639 --> 88.063377).\n",
            "\t Train_Loss: 15.2316 Val_Loss: 88.0634  BEST VAL Loss: 88.0634\n",
            "\n",
            "Epoch 1045: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3489 Val_Loss: 88.3617  BEST VAL Loss: 88.0634\n",
            "\n",
            "Epoch 1046: Validation loss did not decrease\n",
            "\t Train_Loss: 15.2313 Val_Loss: 88.6671  BEST VAL Loss: 88.0634\n",
            "\n",
            "Epoch 1047: Validation loss decreased (88.063377 --> 87.966034).\n",
            "\t Train_Loss: 15.1309 Val_Loss: 87.9660  BEST VAL Loss: 87.9660\n",
            "\n",
            "Epoch 1048: Validation loss did not decrease\n",
            "\t Train_Loss: 15.2502 Val_Loss: 88.3292  BEST VAL Loss: 87.9660\n",
            "\n",
            "Epoch 1049: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7209 Val_Loss: 89.1158  BEST VAL Loss: 87.9660\n",
            "\n",
            "Epoch 1050: Validation loss decreased (87.966034 --> 87.438988).\n",
            "\t Train_Loss: 15.5464 Val_Loss: 87.4390  BEST VAL Loss: 87.4390\n",
            "\n",
            "Epoch 1051: Validation loss decreased (87.438988 --> 86.740044).\n",
            "\t Train_Loss: 15.2938 Val_Loss: 86.7400  BEST VAL Loss: 86.7400\n",
            "\n",
            "Epoch 1052: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1808 Val_Loss: 87.3142  BEST VAL Loss: 86.7400\n",
            "\n",
            "Epoch 1053: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8178 Val_Loss: 87.0990  BEST VAL Loss: 86.7400\n",
            "\n",
            "Epoch 1054: Validation loss decreased (86.740044 --> 86.528435).\n",
            "\t Train_Loss: 14.9565 Val_Loss: 86.5284  BEST VAL Loss: 86.5284\n",
            "\n",
            "Epoch 1055: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0406 Val_Loss: 86.6994  BEST VAL Loss: 86.5284\n",
            "\n",
            "Epoch 1056: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7290 Val_Loss: 87.4203  BEST VAL Loss: 86.5284\n",
            "\n",
            "Epoch 1057: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8398 Val_Loss: 86.6077  BEST VAL Loss: 86.5284\n",
            "\n",
            "Epoch 1058: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7343 Val_Loss: 86.6890  BEST VAL Loss: 86.5284\n",
            "\n",
            "Epoch 1059: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7165 Val_Loss: 86.7338  BEST VAL Loss: 86.5284\n",
            "\n",
            "Epoch 1060: Validation loss decreased (86.528435 --> 85.911186).\n",
            "\t Train_Loss: 14.7176 Val_Loss: 85.9112  BEST VAL Loss: 85.9112\n",
            "\n",
            "Epoch 1061: Validation loss decreased (85.911186 --> 85.337173).\n",
            "\t Train_Loss: 14.3529 Val_Loss: 85.3372  BEST VAL Loss: 85.3372\n",
            "\n",
            "Epoch 1062: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5825 Val_Loss: 85.4398  BEST VAL Loss: 85.3372\n",
            "\n",
            "Epoch 1063: Validation loss did not decrease\n",
            "\t Train_Loss: 15.4389 Val_Loss: 85.7173  BEST VAL Loss: 85.3372\n",
            "\n",
            "Epoch 1064: Validation loss did not decrease\n",
            "\t Train_Loss: 15.2802 Val_Loss: 85.9955  BEST VAL Loss: 85.3372\n",
            "\n",
            "Epoch 1065: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5677 Val_Loss: 85.5057  BEST VAL Loss: 85.3372\n",
            "\n",
            "Epoch 1066: Validation loss decreased (85.337173 --> 84.947815).\n",
            "\t Train_Loss: 15.5405 Val_Loss: 84.9478  BEST VAL Loss: 84.9478\n",
            "\n",
            "Epoch 1067: Validation loss decreased (84.947815 --> 84.815742).\n",
            "\t Train_Loss: 15.2734 Val_Loss: 84.8157  BEST VAL Loss: 84.8157\n",
            "\n",
            "Epoch 1068: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7009 Val_Loss: 86.7841  BEST VAL Loss: 84.8157\n",
            "\n",
            "Epoch 1069: Validation loss decreased (84.815742 --> 84.316216).\n",
            "\t Train_Loss: 15.7200 Val_Loss: 84.3162  BEST VAL Loss: 84.3162\n",
            "\n",
            "Epoch 1070: Validation loss decreased (84.316216 --> 83.635368).\n",
            "\t Train_Loss: 14.8103 Val_Loss: 83.6354  BEST VAL Loss: 83.6354\n",
            "\n",
            "Epoch 1071: Validation loss did not decrease\n",
            "\t Train_Loss: 14.9656 Val_Loss: 83.9428  BEST VAL Loss: 83.6354\n",
            "\n",
            "Epoch 1072: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0497 Val_Loss: 83.9373  BEST VAL Loss: 83.6354\n",
            "\n",
            "Epoch 1073: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4915 Val_Loss: 85.0250  BEST VAL Loss: 83.6354\n",
            "\n",
            "Epoch 1074: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8948 Val_Loss: 84.0882  BEST VAL Loss: 83.6354\n",
            "\n",
            "Epoch 1075: Validation loss decreased (83.635368 --> 83.597435).\n",
            "\t Train_Loss: 14.6045 Val_Loss: 83.5974  BEST VAL Loss: 83.5974\n",
            "\n",
            "Epoch 1076: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0791 Val_Loss: 84.5941  BEST VAL Loss: 83.5974\n",
            "\n",
            "Epoch 1077: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8249 Val_Loss: 84.1928  BEST VAL Loss: 83.5974\n",
            "\n",
            "Epoch 1078: Validation loss decreased (83.597435 --> 82.854996).\n",
            "\t Train_Loss: 14.7762 Val_Loss: 82.8550  BEST VAL Loss: 82.8550\n",
            "\n",
            "Epoch 1079: Validation loss decreased (82.854996 --> 82.761879).\n",
            "\t Train_Loss: 14.4320 Val_Loss: 82.7619  BEST VAL Loss: 82.7619\n",
            "\n",
            "Epoch 1080: Validation loss decreased (82.761879 --> 81.834038).\n",
            "\t Train_Loss: 15.0512 Val_Loss: 81.8340  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1081: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1742 Val_Loss: 83.6647  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1082: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4601 Val_Loss: 83.6866  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1083: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0613 Val_Loss: 82.0809  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1084: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4038 Val_Loss: 82.5078  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1085: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5546 Val_Loss: 82.0903  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1086: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5230 Val_Loss: 83.7743  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1087: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0629 Val_Loss: 82.6670  BEST VAL Loss: 81.8340\n",
            "\n",
            "Epoch 1088: Validation loss decreased (81.834038 --> 81.267479).\n",
            "\t Train_Loss: 14.6935 Val_Loss: 81.2675  BEST VAL Loss: 81.2675\n",
            "\n",
            "Epoch 1089: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1023 Val_Loss: 83.2506  BEST VAL Loss: 81.2675\n",
            "\n",
            "Epoch 1090: Validation loss decreased (81.267479 --> 80.750603).\n",
            "\t Train_Loss: 16.6491 Val_Loss: 80.7506  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1091: Validation loss did not decrease\n",
            "\t Train_Loss: 15.7410 Val_Loss: 81.2962  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1092: Validation loss did not decrease\n",
            "\t Train_Loss: 14.9623 Val_Loss: 81.5193  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1093: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8839 Val_Loss: 81.0858  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1094: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0084 Val_Loss: 81.9871  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1095: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4736 Val_Loss: 81.6054  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1096: Validation loss did not decrease\n",
            "\t Train_Loss: 14.2489 Val_Loss: 81.1770  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1097: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3162 Val_Loss: 81.3077  BEST VAL Loss: 80.7506\n",
            "\n",
            "Epoch 1098: Validation loss decreased (80.750603 --> 79.927246).\n",
            "\t Train_Loss: 14.5520 Val_Loss: 79.9272  BEST VAL Loss: 79.9272\n",
            "\n",
            "Epoch 1099: Validation loss decreased (79.927246 --> 79.494514).\n",
            "\t Train_Loss: 14.5982 Val_Loss: 79.4945  BEST VAL Loss: 79.4945\n",
            "\n",
            "Epoch 1100: Validation loss decreased (79.494514 --> 79.491333).\n",
            "\t Train_Loss: 14.3317 Val_Loss: 79.4913  BEST VAL Loss: 79.4913\n",
            "\n",
            "Epoch 1101: Validation loss decreased (79.491333 --> 79.399361).\n",
            "\t Train_Loss: 14.9463 Val_Loss: 79.3994  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4136 Val_Loss: 80.1499  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1103: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5154 Val_Loss: 81.8753  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1104: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6435 Val_Loss: 81.0712  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1105: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2728 Val_Loss: 80.4581  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3651 Val_Loss: 80.3500  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1107: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1426 Val_Loss: 80.0855  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1108: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9716 Val_Loss: 80.8217  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1109: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5225 Val_Loss: 80.2836  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1110: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1347 Val_Loss: 79.4964  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1111: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5790 Val_Loss: 79.6776  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1112: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0377 Val_Loss: 79.5470  BEST VAL Loss: 79.3994\n",
            "\n",
            "Epoch 1113: Validation loss decreased (79.399361 --> 78.943077).\n",
            "\t Train_Loss: 13.2585 Val_Loss: 78.9431  BEST VAL Loss: 78.9431\n",
            "\n",
            "Epoch 1114: Validation loss decreased (78.943077 --> 78.814163).\n",
            "\t Train_Loss: 12.6399 Val_Loss: 78.8142  BEST VAL Loss: 78.8142\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9955 Val_Loss: 78.9873  BEST VAL Loss: 78.8142\n",
            "\n",
            "Epoch 1116: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8420 Val_Loss: 79.1607  BEST VAL Loss: 78.8142\n",
            "\n",
            "Epoch 1117: Validation loss decreased (78.814163 --> 78.579124).\n",
            "\t Train_Loss: 13.0576 Val_Loss: 78.5791  BEST VAL Loss: 78.5791\n",
            "\n",
            "Epoch 1118: Validation loss decreased (78.579124 --> 78.164497).\n",
            "\t Train_Loss: 12.4803 Val_Loss: 78.1645  BEST VAL Loss: 78.1645\n",
            "\n",
            "Epoch 1119: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1127 Val_Loss: 78.8191  BEST VAL Loss: 78.1645\n",
            "\n",
            "Epoch 1120: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6705 Val_Loss: 79.4553  BEST VAL Loss: 78.1645\n",
            "\n",
            "Epoch 1121: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8686 Val_Loss: 78.3707  BEST VAL Loss: 78.1645\n",
            "\n",
            "Epoch 1122: Validation loss decreased (78.164497 --> 78.060745).\n",
            "\t Train_Loss: 12.4823 Val_Loss: 78.0607  BEST VAL Loss: 78.0607\n",
            "\n",
            "Epoch 1123: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8736 Val_Loss: 78.1357  BEST VAL Loss: 78.0607\n",
            "\n",
            "Epoch 1124: Validation loss decreased (78.060745 --> 77.961548).\n",
            "\t Train_Loss: 12.7587 Val_Loss: 77.9615  BEST VAL Loss: 77.9615\n",
            "\n",
            "Epoch 1125: Validation loss decreased (77.961548 --> 77.315697).\n",
            "\t Train_Loss: 12.9890 Val_Loss: 77.3157  BEST VAL Loss: 77.3157\n",
            "\n",
            "Epoch 1126: Validation loss decreased (77.315697 --> 76.899658).\n",
            "\t Train_Loss: 12.5089 Val_Loss: 76.8997  BEST VAL Loss: 76.8997\n",
            "\n",
            "Epoch 1127: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7526 Val_Loss: 77.1953  BEST VAL Loss: 76.8997\n",
            "\n",
            "Epoch 1128: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2373 Val_Loss: 77.9249  BEST VAL Loss: 76.8997\n",
            "\n",
            "Epoch 1129: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6343 Val_Loss: 77.2270  BEST VAL Loss: 76.8997\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2703 Val_Loss: 77.1164  BEST VAL Loss: 76.8997\n",
            "\n",
            "Epoch 1131: Validation loss decreased (76.899658 --> 76.854584).\n",
            "\t Train_Loss: 12.1723 Val_Loss: 76.8546  BEST VAL Loss: 76.8546\n",
            "\n",
            "Epoch 1132: Validation loss decreased (76.854584 --> 76.428391).\n",
            "\t Train_Loss: 12.0884 Val_Loss: 76.4284  BEST VAL Loss: 76.4284\n",
            "\n",
            "Epoch 1133: Validation loss decreased (76.428391 --> 76.157372).\n",
            "\t Train_Loss: 12.0635 Val_Loss: 76.1574  BEST VAL Loss: 76.1574\n",
            "\n",
            "Epoch 1134: Validation loss decreased (76.157372 --> 75.926872).\n",
            "\t Train_Loss: 12.0531 Val_Loss: 75.9269  BEST VAL Loss: 75.9269\n",
            "\n",
            "Epoch 1135: Validation loss decreased (75.926872 --> 75.869400).\n",
            "\t Train_Loss: 12.0023 Val_Loss: 75.8694  BEST VAL Loss: 75.8694\n",
            "\n",
            "Epoch 1136: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9104 Val_Loss: 75.9438  BEST VAL Loss: 75.8694\n",
            "\n",
            "Epoch 1137: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8966 Val_Loss: 76.8830  BEST VAL Loss: 75.8694\n",
            "\n",
            "Epoch 1138: Validation loss decreased (75.869400 --> 75.589600).\n",
            "\t Train_Loss: 12.3306 Val_Loss: 75.5896  BEST VAL Loss: 75.5896\n",
            "\n",
            "Epoch 1139: Validation loss decreased (75.589600 --> 74.976387).\n",
            "\t Train_Loss: 11.9814 Val_Loss: 74.9764  BEST VAL Loss: 74.9764\n",
            "\n",
            "Epoch 1140: Validation loss decreased (74.976387 --> 74.636673).\n",
            "\t Train_Loss: 12.4943 Val_Loss: 74.6367  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1141: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2862 Val_Loss: 75.2065  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1142: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0518 Val_Loss: 75.7105  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1542 Val_Loss: 75.4765  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1144: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8145 Val_Loss: 75.1816  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8520 Val_Loss: 75.2658  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6440 Val_Loss: 74.9633  BEST VAL Loss: 74.6367\n",
            "\n",
            "Epoch 1147: Validation loss decreased (74.636673 --> 74.562279).\n",
            "\t Train_Loss: 11.7380 Val_Loss: 74.5623  BEST VAL Loss: 74.5623\n",
            "\n",
            "Epoch 1148: Validation loss decreased (74.562279 --> 73.972588).\n",
            "\t Train_Loss: 11.8499 Val_Loss: 73.9726  BEST VAL Loss: 73.9726\n",
            "\n",
            "Epoch 1149: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5167 Val_Loss: 74.3710  BEST VAL Loss: 73.9726\n",
            "\n",
            "Epoch 1150: Validation loss decreased (73.972588 --> 73.926476).\n",
            "\t Train_Loss: 11.6253 Val_Loss: 73.9265  BEST VAL Loss: 73.9265\n",
            "\n",
            "Epoch 1151: Validation loss decreased (73.926476 --> 73.844772).\n",
            "\t Train_Loss: 11.7051 Val_Loss: 73.8448  BEST VAL Loss: 73.8448\n",
            "\n",
            "Epoch 1152: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5161 Val_Loss: 74.1368  BEST VAL Loss: 73.8448\n",
            "\n",
            "Epoch 1153: Validation loss decreased (73.844772 --> 73.516579).\n",
            "\t Train_Loss: 11.8194 Val_Loss: 73.5166  BEST VAL Loss: 73.5166\n",
            "\n",
            "Epoch 1154: Validation loss decreased (73.516579 --> 73.057747).\n",
            "\t Train_Loss: 11.4220 Val_Loss: 73.0577  BEST VAL Loss: 73.0577\n",
            "\n",
            "Epoch 1155: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6676 Val_Loss: 73.4733  BEST VAL Loss: 73.0577\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3520 Val_Loss: 73.9516  BEST VAL Loss: 73.0577\n",
            "\n",
            "Epoch 1157: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7919 Val_Loss: 73.5227  BEST VAL Loss: 73.0577\n",
            "\n",
            "Epoch 1158: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3807 Val_Loss: 73.0938  BEST VAL Loss: 73.0577\n",
            "\n",
            "Epoch 1159: Validation loss decreased (73.057747 --> 72.849220).\n",
            "\t Train_Loss: 11.6498 Val_Loss: 72.8492  BEST VAL Loss: 72.8492\n",
            "\n",
            "Epoch 1160: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3113 Val_Loss: 73.4756  BEST VAL Loss: 72.8492\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6863 Val_Loss: 73.7091  BEST VAL Loss: 72.8492\n",
            "\n",
            "Epoch 1162: Validation loss decreased (72.849220 --> 72.345589).\n",
            "\t Train_Loss: 11.7407 Val_Loss: 72.3456  BEST VAL Loss: 72.3456\n",
            "\n",
            "Epoch 1163: Validation loss decreased (72.345589 --> 72.178261).\n",
            "\t Train_Loss: 11.7891 Val_Loss: 72.1783  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1164: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1653 Val_Loss: 72.2567  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1165: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6863 Val_Loss: 73.1808  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8575 Val_Loss: 73.5163  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1167: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6090 Val_Loss: 72.4853  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7876 Val_Loss: 72.8232  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1169: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1379 Val_Loss: 73.5949  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1170: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6677 Val_Loss: 72.8887  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1171: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6558 Val_Loss: 72.3568  BEST VAL Loss: 72.1783\n",
            "\n",
            "Epoch 1172: Validation loss decreased (72.178261 --> 71.744179).\n",
            "\t Train_Loss: 11.5120 Val_Loss: 71.7442  BEST VAL Loss: 71.7442\n",
            "\n",
            "Epoch 1173: Validation loss decreased (71.744179 --> 71.657944).\n",
            "\t Train_Loss: 11.6618 Val_Loss: 71.6579  BEST VAL Loss: 71.6579\n",
            "\n",
            "Epoch 1174: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4381 Val_Loss: 71.9879  BEST VAL Loss: 71.6579\n",
            "\n",
            "Epoch 1175: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1105 Val_Loss: 72.9769  BEST VAL Loss: 71.6579\n",
            "\n",
            "Epoch 1176: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7084 Val_Loss: 72.3687  BEST VAL Loss: 71.6579\n",
            "\n",
            "Epoch 1177: Validation loss decreased (71.657944 --> 71.584961).\n",
            "\t Train_Loss: 11.2268 Val_Loss: 71.5850  BEST VAL Loss: 71.5850\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0472 Val_Loss: 74.0311  BEST VAL Loss: 71.5850\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4254 Val_Loss: 73.6954  BEST VAL Loss: 71.5850\n",
            "\n",
            "Epoch 1180: Validation loss decreased (71.584961 --> 71.135429).\n",
            "\t Train_Loss: 13.7950 Val_Loss: 71.1354  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1181: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5783 Val_Loss: 71.3928  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1182: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7510 Val_Loss: 71.2486  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3852 Val_Loss: 71.4652  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1184: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7136 Val_Loss: 72.0929  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0305 Val_Loss: 72.3159  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1186: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1401 Val_Loss: 71.3284  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 16.7408 Val_Loss: 71.9424  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1188: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8820 Val_Loss: 73.3167  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1189: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6649 Val_Loss: 72.5840  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1190: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7389 Val_Loss: 72.4949  BEST VAL Loss: 71.1354\n",
            "\n",
            "Epoch 1191: Validation loss decreased (71.135429 --> 70.522583).\n",
            "\t Train_Loss: 15.7422 Val_Loss: 70.5226  BEST VAL Loss: 70.5226\n",
            "\n",
            "Epoch 1192: Validation loss did not decrease\n",
            "\t Train_Loss: 15.2862 Val_Loss: 71.9096  BEST VAL Loss: 70.5226\n",
            "\n",
            "Epoch 1193: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2132 Val_Loss: 70.9113  BEST VAL Loss: 70.5226\n",
            "\n",
            "Epoch 1194: Validation loss did not decrease\n",
            "\t Train_Loss: 16.2197 Val_Loss: 71.7338  BEST VAL Loss: 70.5226\n",
            "\n",
            "Epoch 1195: Validation loss decreased (70.522583 --> 70.406815).\n",
            "\t Train_Loss: 15.8478 Val_Loss: 70.4068  BEST VAL Loss: 70.4068\n",
            "\n",
            "Epoch 1196: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6685 Val_Loss: 71.5298  BEST VAL Loss: 70.4068\n",
            "\n",
            "Epoch 1197: Validation loss decreased (70.406815 --> 70.392593).\n",
            "\t Train_Loss: 16.1599 Val_Loss: 70.3926  BEST VAL Loss: 70.3926\n",
            "\n",
            "Epoch 1198: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4518 Val_Loss: 71.8359  BEST VAL Loss: 70.3926\n",
            "\n",
            "Epoch 1199: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0468 Val_Loss: 72.4762  BEST VAL Loss: 70.3926\n",
            "\n",
            "Epoch 1200: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1752 Val_Loss: 72.1694  BEST VAL Loss: 70.3926\n",
            "\n",
            "Epoch 1201: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3815 Val_Loss: 73.7889  BEST VAL Loss: 70.3926\n",
            "\n",
            "Epoch 1202: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3666 Val_Loss: 72.6883  BEST VAL Loss: 70.3926\n",
            "\n",
            "Epoch 1203: Validation loss decreased (70.392593 --> 70.026787).\n",
            "\t Train_Loss: 13.4190 Val_Loss: 70.0268  BEST VAL Loss: 70.0268\n",
            "\n",
            "Epoch 1204: Validation loss decreased (70.026787 --> 69.333221).\n",
            "\t Train_Loss: 12.4468 Val_Loss: 69.3332  BEST VAL Loss: 69.3332\n",
            "\n",
            "Epoch 1205: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2210 Val_Loss: 70.0089  BEST VAL Loss: 69.3332\n",
            "\n",
            "Epoch 1206: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0072 Val_Loss: 69.9123  BEST VAL Loss: 69.3332\n",
            "\n",
            "Epoch 1207: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1380 Val_Loss: 69.6942  BEST VAL Loss: 69.3332\n",
            "\n",
            "Epoch 1208: Validation loss decreased (69.333221 --> 68.950867).\n",
            "\t Train_Loss: 12.7123 Val_Loss: 68.9509  BEST VAL Loss: 68.9509\n",
            "\n",
            "Epoch 1209: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6325 Val_Loss: 70.0084  BEST VAL Loss: 68.9509\n",
            "\n",
            "Epoch 1210: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1500 Val_Loss: 71.2729  BEST VAL Loss: 68.9509\n",
            "\n",
            "Epoch 1211: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1619 Val_Loss: 71.3152  BEST VAL Loss: 68.9509\n",
            "\n",
            "Epoch 1212: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9520 Val_Loss: 70.4467  BEST VAL Loss: 68.9509\n",
            "\n",
            "Epoch 1213: Validation loss decreased (68.950867 --> 68.914291).\n",
            "\t Train_Loss: 11.5980 Val_Loss: 68.9143  BEST VAL Loss: 68.9143\n",
            "\n",
            "Epoch 1214: Validation loss decreased (68.914291 --> 68.404076).\n",
            "\t Train_Loss: 11.6745 Val_Loss: 68.4041  BEST VAL Loss: 68.4041\n",
            "\n",
            "Epoch 1215: Validation loss decreased (68.404076 --> 68.300537).\n",
            "\t Train_Loss: 11.8366 Val_Loss: 68.3005  BEST VAL Loss: 68.3005\n",
            "\n",
            "Epoch 1216: Validation loss decreased (68.300537 --> 67.774918).\n",
            "\t Train_Loss: 11.9694 Val_Loss: 67.7749  BEST VAL Loss: 67.7749\n",
            "\n",
            "Epoch 1217: Validation loss decreased (67.774918 --> 67.406967).\n",
            "\t Train_Loss: 11.6539 Val_Loss: 67.4070  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1218: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1899 Val_Loss: 68.5610  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1219: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1897 Val_Loss: 69.2365  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1220: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6167 Val_Loss: 69.8561  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1221: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8556 Val_Loss: 69.6335  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1222: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3624 Val_Loss: 68.2183  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1223: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9639 Val_Loss: 67.6858  BEST VAL Loss: 67.4070\n",
            "\n",
            "Epoch 1224: Validation loss decreased (67.406967 --> 66.677254).\n",
            "\t Train_Loss: 11.2632 Val_Loss: 66.6773  BEST VAL Loss: 66.6773\n",
            "\n",
            "Epoch 1225: Validation loss decreased (66.677254 --> 66.607964).\n",
            "\t Train_Loss: 11.3235 Val_Loss: 66.6080  BEST VAL Loss: 66.6080\n",
            "\n",
            "Epoch 1226: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1509 Val_Loss: 67.2828  BEST VAL Loss: 66.6080\n",
            "\n",
            "Epoch 1227: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9723 Val_Loss: 68.9087  BEST VAL Loss: 66.6080\n",
            "\n",
            "Epoch 1228: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2882 Val_Loss: 68.4089  BEST VAL Loss: 66.6080\n",
            "\n",
            "Epoch 1229: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7854 Val_Loss: 68.2320  BEST VAL Loss: 66.6080\n",
            "\n",
            "Epoch 1230: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1362 Val_Loss: 67.4339  BEST VAL Loss: 66.6080\n",
            "\n",
            "Epoch 1231: Validation loss decreased (66.607964 --> 66.208336).\n",
            "\t Train_Loss: 12.0187 Val_Loss: 66.2083  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1232: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4658 Val_Loss: 66.4889  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1233: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9388 Val_Loss: 66.7312  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1234: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8262 Val_Loss: 66.5347  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1235: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1754 Val_Loss: 66.3048  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1236: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9399 Val_Loss: 67.1740  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1237: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2693 Val_Loss: 67.0785  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1238: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2229 Val_Loss: 68.0431  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1239: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7449 Val_Loss: 67.5905  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1240: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2339 Val_Loss: 66.6701  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1241: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9244 Val_Loss: 66.4712  BEST VAL Loss: 66.2083\n",
            "\n",
            "Epoch 1242: Validation loss decreased (66.208336 --> 65.739311).\n",
            "\t Train_Loss: 13.5799 Val_Loss: 65.7393  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1243: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6389 Val_Loss: 67.1602  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1244: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3765 Val_Loss: 65.8204  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1245: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5343 Val_Loss: 72.2753  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1246: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6700 Val_Loss: 68.2171  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1247: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8369 Val_Loss: 66.5474  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1248: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0333 Val_Loss: 67.9234  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1249: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5644 Val_Loss: 66.5375  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1250: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0785 Val_Loss: 69.3561  BEST VAL Loss: 65.7393\n",
            "\n",
            "Epoch 1251: Validation loss decreased (65.739311 --> 65.635895).\n",
            "\t Train_Loss: 17.6465 Val_Loss: 65.6359  BEST VAL Loss: 65.6359\n",
            "\n",
            "Epoch 1252: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4291 Val_Loss: 68.1059  BEST VAL Loss: 65.6359\n",
            "\n",
            "Epoch 1253: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8838 Val_Loss: 69.6648  BEST VAL Loss: 65.6359\n",
            "\n",
            "Epoch 1254: Validation loss did not decrease\n",
            "\t Train_Loss: 18.3368 Val_Loss: 66.7664  BEST VAL Loss: 65.6359\n",
            "\n",
            "Epoch 1255: Validation loss decreased (65.635895 --> 65.617569).\n",
            "\t Train_Loss: 13.1652 Val_Loss: 65.6176  BEST VAL Loss: 65.6176\n",
            "\n",
            "Epoch 1256: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1304 Val_Loss: 81.8538  BEST VAL Loss: 65.6176\n",
            "\n",
            "Epoch 1257: Validation loss did not decrease\n",
            "\t Train_Loss: 22.9191 Val_Loss: 70.1051  BEST VAL Loss: 65.6176\n",
            "\n",
            "Epoch 1258: Validation loss decreased (65.617569 --> 64.719551).\n",
            "\t Train_Loss: 17.7422 Val_Loss: 64.7196  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1259: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0895 Val_Loss: 66.3541  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1260: Validation loss did not decrease\n",
            "\t Train_Loss: 17.4940 Val_Loss: 67.0034  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1261: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0775 Val_Loss: 65.5018  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1262: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5559 Val_Loss: 66.8339  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1263: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2294 Val_Loss: 67.5131  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1264: Validation loss did not decrease\n",
            "\t Train_Loss: 14.9788 Val_Loss: 66.8243  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1265: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0264 Val_Loss: 64.8940  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1266: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4191 Val_Loss: 66.0624  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1267: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8413 Val_Loss: 66.4910  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1268: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4302 Val_Loss: 66.1710  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1269: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9859 Val_Loss: 65.0415  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1270: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6884 Val_Loss: 65.5836  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1271: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9639 Val_Loss: 67.2803  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1272: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9104 Val_Loss: 68.2217  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1273: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5857 Val_Loss: 68.2256  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1274: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6280 Val_Loss: 67.2965  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1275: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0649 Val_Loss: 66.3315  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1276: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1928 Val_Loss: 65.0450  BEST VAL Loss: 64.7196\n",
            "\n",
            "Epoch 1277: Validation loss decreased (64.719551 --> 64.025551).\n",
            "\t Train_Loss: 12.2404 Val_Loss: 64.0256  BEST VAL Loss: 64.0256\n",
            "\n",
            "Epoch 1278: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8948 Val_Loss: 64.6975  BEST VAL Loss: 64.0256\n",
            "\n",
            "Epoch 1279: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7747 Val_Loss: 65.1700  BEST VAL Loss: 64.0256\n",
            "\n",
            "Epoch 1280: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6136 Val_Loss: 64.6418  BEST VAL Loss: 64.0256\n",
            "\n",
            "Epoch 1281: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5144 Val_Loss: 64.2566  BEST VAL Loss: 64.0256\n",
            "\n",
            "Epoch 1282: Validation loss decreased (64.025551 --> 63.936829).\n",
            "\t Train_Loss: 11.3746 Val_Loss: 63.9368  BEST VAL Loss: 63.9368\n",
            "\n",
            "Epoch 1283: Validation loss decreased (63.936829 --> 63.616177).\n",
            "\t Train_Loss: 11.2093 Val_Loss: 63.6162  BEST VAL Loss: 63.6162\n",
            "\n",
            "Epoch 1284: Validation loss decreased (63.616177 --> 63.477234).\n",
            "\t Train_Loss: 10.9982 Val_Loss: 63.4772  BEST VAL Loss: 63.4772\n",
            "\n",
            "Epoch 1285: Validation loss decreased (63.477234 --> 63.405384).\n",
            "\t Train_Loss: 10.8795 Val_Loss: 63.4054  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1286: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8157 Val_Loss: 63.4635  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1287: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7581 Val_Loss: 63.9117  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1288: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5652 Val_Loss: 64.2805  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1289: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4711 Val_Loss: 64.4016  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1290: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4633 Val_Loss: 64.3933  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1291: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5329 Val_Loss: 64.2761  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1292: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4383 Val_Loss: 63.8644  BEST VAL Loss: 63.4054\n",
            "\n",
            "Epoch 1293: Validation loss decreased (63.405384 --> 63.184662).\n",
            "\t Train_Loss: 10.3464 Val_Loss: 63.1847  BEST VAL Loss: 63.1847\n",
            "\n",
            "Epoch 1294: Validation loss decreased (63.184662 --> 62.949829).\n",
            "\t Train_Loss: 10.1650 Val_Loss: 62.9498  BEST VAL Loss: 62.9498\n",
            "\n",
            "Epoch 1295: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1800 Val_Loss: 63.0168  BEST VAL Loss: 62.9498\n",
            "\n",
            "Epoch 1296: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1539 Val_Loss: 63.1471  BEST VAL Loss: 62.9498\n",
            "\n",
            "Epoch 1297: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0576 Val_Loss: 63.2330  BEST VAL Loss: 62.9498\n",
            "\n",
            "Epoch 1298: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9761 Val_Loss: 63.3047  BEST VAL Loss: 62.9498\n",
            "\n",
            "Epoch 1299: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9704 Val_Loss: 63.0963  BEST VAL Loss: 62.9498\n",
            "\n",
            "Epoch 1300: Validation loss decreased (62.949829 --> 62.768707).\n",
            "\t Train_Loss: 9.9983 Val_Loss: 62.7687  BEST VAL Loss: 62.7687\n",
            "\n",
            "Epoch 1301: Validation loss decreased (62.768707 --> 62.457386).\n",
            "\t Train_Loss: 9.9915 Val_Loss: 62.4574  BEST VAL Loss: 62.4574\n",
            "\n",
            "Epoch 1302: Validation loss decreased (62.457386 --> 62.387878).\n",
            "\t Train_Loss: 9.9088 Val_Loss: 62.3879  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1303: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9333 Val_Loss: 62.4902  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1304: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9259 Val_Loss: 62.5433  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1305: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8901 Val_Loss: 62.5768  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1306: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8319 Val_Loss: 62.7709  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1307: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8166 Val_Loss: 63.0324  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1308: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8263 Val_Loss: 63.0471  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1309: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7897 Val_Loss: 62.8990  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1310: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7414 Val_Loss: 62.5959  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1311: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7406 Val_Loss: 62.4089  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1312: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7394 Val_Loss: 62.6119  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1313: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7349 Val_Loss: 62.6444  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1314: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6794 Val_Loss: 62.7284  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1315: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6890 Val_Loss: 63.2938  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1316: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8006 Val_Loss: 63.1848  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1317: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7088 Val_Loss: 62.7466  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1318: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8917 Val_Loss: 63.0095  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1319: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7338 Val_Loss: 63.1860  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1320: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8862 Val_Loss: 62.6894  BEST VAL Loss: 62.3879\n",
            "\n",
            "Epoch 1321: Validation loss decreased (62.387878 --> 61.987221).\n",
            "\t Train_Loss: 9.7572 Val_Loss: 61.9872  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1322: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8617 Val_Loss: 62.4170  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1323: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6860 Val_Loss: 63.1750  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1324: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8598 Val_Loss: 62.9881  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1325: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6471 Val_Loss: 62.5116  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1326: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0918 Val_Loss: 62.7882  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1327: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8522 Val_Loss: 62.7054  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1328: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2165 Val_Loss: 62.0608  BEST VAL Loss: 61.9872\n",
            "\n",
            "Epoch 1329: Validation loss decreased (61.987221 --> 61.763802).\n",
            "\t Train_Loss: 9.8272 Val_Loss: 61.7638  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1330: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8007 Val_Loss: 61.9654  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1331: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5650 Val_Loss: 62.2302  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1332: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6556 Val_Loss: 62.2619  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1333: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6788 Val_Loss: 62.4869  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1334: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7908 Val_Loss: 63.5488  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1335: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5627 Val_Loss: 67.5819  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1336: Validation loss did not decrease\n",
            "\t Train_Loss: 19.7484 Val_Loss: 69.4298  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1337: Validation loss did not decrease\n",
            "\t Train_Loss: 18.2375 Val_Loss: 65.7043  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1338: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6684 Val_Loss: 62.3294  BEST VAL Loss: 61.7638\n",
            "\n",
            "Epoch 1339: Validation loss decreased (61.763802 --> 60.656750).\n",
            "\t Train_Loss: 14.3416 Val_Loss: 60.6567  BEST VAL Loss: 60.6567\n",
            "\n",
            "Epoch 1340: Validation loss decreased (60.656750 --> 59.788235).\n",
            "\t Train_Loss: 14.0563 Val_Loss: 59.7882  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1341: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7224 Val_Loss: 61.1996  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1342: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3447 Val_Loss: 63.9420  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1343: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5142 Val_Loss: 66.1066  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1344: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3915 Val_Loss: 67.5265  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1345: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7906 Val_Loss: 69.6457  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1346: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1628 Val_Loss: 69.6756  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1347: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9468 Val_Loss: 69.2250  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5296 Val_Loss: 69.6540  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1349: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6359 Val_Loss: 70.3190  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1350: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9406 Val_Loss: 69.1750  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1351: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6211 Val_Loss: 66.9945  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1352: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4949 Val_Loss: 64.6341  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1353: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1274 Val_Loss: 63.3106  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1354: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0914 Val_Loss: 63.3212  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1355: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7350 Val_Loss: 61.6056  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1356: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4201 Val_Loss: 60.5795  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1357: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5729 Val_Loss: 59.9966  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1358: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4803 Val_Loss: 59.9200  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1359: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2183 Val_Loss: 60.0776  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1360: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8306 Val_Loss: 60.0652  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1361: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4455 Val_Loss: 60.0803  BEST VAL Loss: 59.7882\n",
            "\n",
            "Epoch 1362: Validation loss decreased (59.788235 --> 59.647655).\n",
            "\t Train_Loss: 10.2597 Val_Loss: 59.6477  BEST VAL Loss: 59.6477\n",
            "\n",
            "Epoch 1363: Validation loss decreased (59.647655 --> 59.204712).\n",
            "\t Train_Loss: 10.2646 Val_Loss: 59.2047  BEST VAL Loss: 59.2047\n",
            "\n",
            "Epoch 1364: Validation loss decreased (59.204712 --> 59.024426).\n",
            "\t Train_Loss: 10.5965 Val_Loss: 59.0244  BEST VAL Loss: 59.0244\n",
            "\n",
            "Epoch 1365: Validation loss decreased (59.024426 --> 58.802795).\n",
            "\t Train_Loss: 10.3689 Val_Loss: 58.8028  BEST VAL Loss: 58.8028\n",
            "\n",
            "Epoch 1366: Validation loss decreased (58.802795 --> 58.342506).\n",
            "\t Train_Loss: 9.9816 Val_Loss: 58.3425  BEST VAL Loss: 58.3425\n",
            "\n",
            "Epoch 1367: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8451 Val_Loss: 58.6385  BEST VAL Loss: 58.3425\n",
            "\n",
            "Epoch 1368: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9458 Val_Loss: 58.8948  BEST VAL Loss: 58.3425\n",
            "\n",
            "Epoch 1369: Validation loss decreased (58.342506 --> 58.269531).\n",
            "\t Train_Loss: 10.0557 Val_Loss: 58.2695  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1370: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9532 Val_Loss: 58.4040  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1371: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7723 Val_Loss: 60.0907  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1372: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8633 Val_Loss: 60.6461  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1373: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0884 Val_Loss: 60.0100  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1374: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7668 Val_Loss: 59.7344  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1375: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2548 Val_Loss: 60.3872  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1376: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8649 Val_Loss: 59.7300  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1377: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7784 Val_Loss: 58.5994  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1378: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6416 Val_Loss: 58.3382  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1379: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6881 Val_Loss: 58.3520  BEST VAL Loss: 58.2695\n",
            "\n",
            "Epoch 1380: Validation loss decreased (58.269531 --> 58.244759).\n",
            "\t Train_Loss: 9.7849 Val_Loss: 58.2448  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1381: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4600 Val_Loss: 58.7220  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1382: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4162 Val_Loss: 59.4774  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1383: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5808 Val_Loss: 59.0041  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1384: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7340 Val_Loss: 58.8861  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1385: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4898 Val_Loss: 58.9068  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1386: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3571 Val_Loss: 58.5953  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1387: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3521 Val_Loss: 58.3129  BEST VAL Loss: 58.2448\n",
            "\n",
            "Epoch 1388: Validation loss decreased (58.244759 --> 58.186733).\n",
            "\t Train_Loss: 9.3990 Val_Loss: 58.1867  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1389: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4909 Val_Loss: 58.1997  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1390: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3867 Val_Loss: 58.4242  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1391: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3249 Val_Loss: 58.6860  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1392: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2317 Val_Loss: 59.2511  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1393: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3107 Val_Loss: 59.3661  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1394: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2570 Val_Loss: 59.2157  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1395: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2226 Val_Loss: 58.8364  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1396: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4222 Val_Loss: 60.0616  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1397: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8966 Val_Loss: 59.0171  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1398: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7349 Val_Loss: 58.4012  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1399: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2898 Val_Loss: 58.3289  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1400: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8665 Val_Loss: 59.4977  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1401: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4688 Val_Loss: 60.0180  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1402: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1755 Val_Loss: 59.7251  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1403: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8291 Val_Loss: 60.9135  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1404: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0589 Val_Loss: 60.1723  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1405: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9620 Val_Loss: 59.1151  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1406: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4094 Val_Loss: 58.7900  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1407: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6582 Val_Loss: 59.5350  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1408: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7386 Val_Loss: 62.6676  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1409: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4945 Val_Loss: 64.5804  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1410: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3016 Val_Loss: 62.7741  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1411: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8684 Val_Loss: 62.5316  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1412: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7896 Val_Loss: 63.0950  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1413: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3936 Val_Loss: 59.5157  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1414: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3219 Val_Loss: 59.9356  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1415: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3206 Val_Loss: 60.3854  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1416: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1462 Val_Loss: 59.4162  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1417: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9714 Val_Loss: 60.3536  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1418: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3848 Val_Loss: 61.9448  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1419: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1769 Val_Loss: 60.5685  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1420: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7152 Val_Loss: 60.8625  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1421: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1249 Val_Loss: 62.0805  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1422: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7078 Val_Loss: 62.4620  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1423: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2700 Val_Loss: 61.8447  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1424: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8567 Val_Loss: 60.7584  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1425: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5873 Val_Loss: 60.0082  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1426: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5091 Val_Loss: 59.6123  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1427: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6410 Val_Loss: 59.7337  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1428: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5755 Val_Loss: 59.8687  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1429: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3843 Val_Loss: 59.6725  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1430: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2558 Val_Loss: 59.4819  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1431: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0378 Val_Loss: 59.6165  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1432: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1054 Val_Loss: 59.5131  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1433: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8913 Val_Loss: 69.5173  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1434: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2772 Val_Loss: 59.3818  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1435: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2011 Val_Loss: 59.0523  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1436: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1655 Val_Loss: 59.3667  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1437: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3033 Val_Loss: 59.5202  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1438: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7979 Val_Loss: 58.5260  BEST VAL Loss: 58.1867\n",
            "\n",
            "Epoch 1439: Validation loss decreased (58.186733 --> 57.856697).\n",
            "\t Train_Loss: 10.8018 Val_Loss: 57.8567  BEST VAL Loss: 57.8567\n",
            "\n",
            "Epoch 1440: Validation loss decreased (57.856697 --> 57.483032).\n",
            "\t Train_Loss: 11.7349 Val_Loss: 57.4830  BEST VAL Loss: 57.4830\n",
            "\n",
            "Epoch 1441: Validation loss decreased (57.483032 --> 56.755047).\n",
            "\t Train_Loss: 11.8880 Val_Loss: 56.7550  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1442: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5686 Val_Loss: 57.0843  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1443: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3661 Val_Loss: 57.4882  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1444: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1510 Val_Loss: 57.3663  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1445: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4594 Val_Loss: 57.7284  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1446: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3976 Val_Loss: 57.7159  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1447: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4968 Val_Loss: 57.4296  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1448: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7878 Val_Loss: 57.3962  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1449: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7349 Val_Loss: 57.6987  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1450: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4466 Val_Loss: 57.5964  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1451: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6212 Val_Loss: 57.0074  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1452: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3522 Val_Loss: 56.7787  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1453: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3289 Val_Loss: 56.7551  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1454: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1054 Val_Loss: 57.1364  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1455: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0545 Val_Loss: 57.3440  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1456: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0330 Val_Loss: 57.0634  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1457: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8142 Val_Loss: 56.8266  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1458: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8197 Val_Loss: 56.9178  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1459: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7784 Val_Loss: 57.1980  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1460: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6978 Val_Loss: 57.3449  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1461: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5321 Val_Loss: 57.3399  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1462: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5668 Val_Loss: 57.2050  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1463: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5705 Val_Loss: 56.9122  BEST VAL Loss: 56.7550\n",
            "\n",
            "Epoch 1464: Validation loss decreased (56.755047 --> 56.589943).\n",
            "\t Train_Loss: 9.5337 Val_Loss: 56.5899  BEST VAL Loss: 56.5899\n",
            "\n",
            "Epoch 1465: Validation loss decreased (56.589943 --> 56.457325).\n",
            "\t Train_Loss: 9.5671 Val_Loss: 56.4573  BEST VAL Loss: 56.4573\n",
            "\n",
            "Epoch 1466: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5524 Val_Loss: 56.5946  BEST VAL Loss: 56.4573\n",
            "\n",
            "Epoch 1467: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4712 Val_Loss: 56.9065  BEST VAL Loss: 56.4573\n",
            "\n",
            "Epoch 1468: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4465 Val_Loss: 57.0561  BEST VAL Loss: 56.4573\n",
            "\n",
            "Epoch 1469: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4662 Val_Loss: 56.8558  BEST VAL Loss: 56.4573\n",
            "\n",
            "Epoch 1470: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4046 Val_Loss: 56.5086  BEST VAL Loss: 56.4573\n",
            "\n",
            "Epoch 1471: Validation loss decreased (56.457325 --> 56.298840).\n",
            "\t Train_Loss: 9.3411 Val_Loss: 56.2988  BEST VAL Loss: 56.2988\n",
            "\n",
            "Epoch 1472: Validation loss decreased (56.298840 --> 56.261974).\n",
            "\t Train_Loss: 9.3255 Val_Loss: 56.2620  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1473: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3118 Val_Loss: 56.3373  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1474: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2884 Val_Loss: 56.4352  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1475: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2911 Val_Loss: 56.4632  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1476: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2766 Val_Loss: 56.4187  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1477: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2387 Val_Loss: 56.3469  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1478: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2300 Val_Loss: 56.2953  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1479: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2217 Val_Loss: 56.2778  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1480: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1892 Val_Loss: 56.2765  BEST VAL Loss: 56.2620\n",
            "\n",
            "Epoch 1481: Validation loss decreased (56.261974 --> 56.251343).\n",
            "\t Train_Loss: 9.1652 Val_Loss: 56.2513  BEST VAL Loss: 56.2513\n",
            "\n",
            "Epoch 1482: Validation loss decreased (56.251343 --> 56.154846).\n",
            "\t Train_Loss: 9.1521 Val_Loss: 56.1548  BEST VAL Loss: 56.1548\n",
            "\n",
            "Epoch 1483: Validation loss decreased (56.154846 --> 55.992947).\n",
            "\t Train_Loss: 9.1344 Val_Loss: 55.9929  BEST VAL Loss: 55.9929\n",
            "\n",
            "Epoch 1484: Validation loss decreased (55.992947 --> 55.842934).\n",
            "\t Train_Loss: 9.1212 Val_Loss: 55.8429  BEST VAL Loss: 55.8429\n",
            "\n",
            "Epoch 1485: Validation loss decreased (55.842934 --> 55.776142).\n",
            "\t Train_Loss: 9.1119 Val_Loss: 55.7761  BEST VAL Loss: 55.7761\n",
            "\n",
            "Epoch 1486: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0958 Val_Loss: 55.7828  BEST VAL Loss: 55.7761\n",
            "\n",
            "Epoch 1487: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0836 Val_Loss: 55.7843  BEST VAL Loss: 55.7761\n",
            "\n",
            "Epoch 1488: Validation loss decreased (55.776142 --> 55.696255).\n",
            "\t Train_Loss: 9.0773 Val_Loss: 55.6963  BEST VAL Loss: 55.6963\n",
            "\n",
            "Epoch 1489: Validation loss decreased (55.696255 --> 55.524654).\n",
            "\t Train_Loss: 9.0616 Val_Loss: 55.5247  BEST VAL Loss: 55.5247\n",
            "\n",
            "Epoch 1490: Validation loss decreased (55.524654 --> 55.359768).\n",
            "\t Train_Loss: 9.0409 Val_Loss: 55.3598  BEST VAL Loss: 55.3598\n",
            "\n",
            "Epoch 1491: Validation loss decreased (55.359768 --> 55.283245).\n",
            "\t Train_Loss: 9.0293 Val_Loss: 55.2832  BEST VAL Loss: 55.2832\n",
            "\n",
            "Epoch 1492: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0187 Val_Loss: 55.2902  BEST VAL Loss: 55.2832\n",
            "\n",
            "Epoch 1493: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0051 Val_Loss: 55.3046  BEST VAL Loss: 55.2832\n",
            "\n",
            "Epoch 1494: Validation loss decreased (55.283245 --> 55.257168).\n",
            "\t Train_Loss: 8.9944 Val_Loss: 55.2572  BEST VAL Loss: 55.2572\n",
            "\n",
            "Epoch 1495: Validation loss decreased (55.257168 --> 55.161385).\n",
            "\t Train_Loss: 8.9815 Val_Loss: 55.1614  BEST VAL Loss: 55.1614\n",
            "\n",
            "Epoch 1496: Validation loss decreased (55.161385 --> 55.082912).\n",
            "\t Train_Loss: 8.9685 Val_Loss: 55.0829  BEST VAL Loss: 55.0829\n",
            "\n",
            "Epoch 1497: Validation loss decreased (55.082912 --> 55.056160).\n",
            "\t Train_Loss: 8.9586 Val_Loss: 55.0562  BEST VAL Loss: 55.0562\n",
            "\n",
            "Epoch 1498: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9465 Val_Loss: 55.0572  BEST VAL Loss: 55.0562\n",
            "\n",
            "Epoch 1499: Validation loss decreased (55.056160 --> 55.031170).\n",
            "\t Train_Loss: 8.9321 Val_Loss: 55.0312  BEST VAL Loss: 55.0312\n",
            "\n",
            "Epoch 1500: Validation loss decreased (55.031170 --> 54.952633).\n",
            "\t Train_Loss: 8.9198 Val_Loss: 54.9526  BEST VAL Loss: 54.9526\n",
            "\n",
            "Epoch 1501: Validation loss decreased (54.952633 --> 54.852638).\n",
            "\t Train_Loss: 8.9085 Val_Loss: 54.8526  BEST VAL Loss: 54.8526\n",
            "\n",
            "Epoch 1502: Validation loss decreased (54.852638 --> 54.766731).\n",
            "\t Train_Loss: 8.8957 Val_Loss: 54.7667  BEST VAL Loss: 54.7667\n",
            "\n",
            "Epoch 1503: Validation loss decreased (54.766731 --> 54.690563).\n",
            "\t Train_Loss: 8.8829 Val_Loss: 54.6906  BEST VAL Loss: 54.6906\n",
            "\n",
            "Epoch 1504: Validation loss decreased (54.690563 --> 54.596218).\n",
            "\t Train_Loss: 8.8714 Val_Loss: 54.5962  BEST VAL Loss: 54.5962\n",
            "\n",
            "Epoch 1505: Validation loss decreased (54.596218 --> 54.479164).\n",
            "\t Train_Loss: 8.8589 Val_Loss: 54.4792  BEST VAL Loss: 54.4792\n",
            "\n",
            "Epoch 1506: Validation loss decreased (54.479164 --> 54.365932).\n",
            "\t Train_Loss: 8.8462 Val_Loss: 54.3659  BEST VAL Loss: 54.3659\n",
            "\n",
            "Epoch 1507: Validation loss decreased (54.365932 --> 54.270687).\n",
            "\t Train_Loss: 8.8335 Val_Loss: 54.2707  BEST VAL Loss: 54.2707\n",
            "\n",
            "Epoch 1508: Validation loss decreased (54.270687 --> 54.174500).\n",
            "\t Train_Loss: 8.8197 Val_Loss: 54.1745  BEST VAL Loss: 54.1745\n",
            "\n",
            "Epoch 1509: Validation loss decreased (54.174500 --> 54.066448).\n",
            "\t Train_Loss: 8.8076 Val_Loss: 54.0664  BEST VAL Loss: 54.0664\n",
            "\n",
            "Epoch 1510: Validation loss decreased (54.066448 --> 53.974884).\n",
            "\t Train_Loss: 8.7958 Val_Loss: 53.9749  BEST VAL Loss: 53.9749\n",
            "\n",
            "Epoch 1511: Validation loss decreased (53.974884 --> 53.915760).\n",
            "\t Train_Loss: 8.7835 Val_Loss: 53.9158  BEST VAL Loss: 53.9158\n",
            "\n",
            "Epoch 1512: Validation loss decreased (53.915760 --> 53.858521).\n",
            "\t Train_Loss: 8.7713 Val_Loss: 53.8585  BEST VAL Loss: 53.8585\n",
            "\n",
            "Epoch 1513: Validation loss decreased (53.858521 --> 53.770855).\n",
            "\t Train_Loss: 8.7597 Val_Loss: 53.7709  BEST VAL Loss: 53.7709\n",
            "\n",
            "Epoch 1514: Validation loss decreased (53.770855 --> 53.675831).\n",
            "\t Train_Loss: 8.7477 Val_Loss: 53.6758  BEST VAL Loss: 53.6758\n",
            "\n",
            "Epoch 1515: Validation loss decreased (53.675831 --> 53.620098).\n",
            "\t Train_Loss: 8.7360 Val_Loss: 53.6201  BEST VAL Loss: 53.6201\n",
            "\n",
            "Epoch 1516: Validation loss decreased (53.620098 --> 53.595886).\n",
            "\t Train_Loss: 8.7241 Val_Loss: 53.5959  BEST VAL Loss: 53.5959\n",
            "\n",
            "Epoch 1517: Validation loss decreased (53.595886 --> 53.549488).\n",
            "\t Train_Loss: 8.7120 Val_Loss: 53.5495  BEST VAL Loss: 53.5495\n",
            "\n",
            "Epoch 1518: Validation loss decreased (53.549488 --> 53.464375).\n",
            "\t Train_Loss: 8.7006 Val_Loss: 53.4644  BEST VAL Loss: 53.4644\n",
            "\n",
            "Epoch 1519: Validation loss decreased (53.464375 --> 53.376118).\n",
            "\t Train_Loss: 8.6888 Val_Loss: 53.3761  BEST VAL Loss: 53.3761\n",
            "\n",
            "Epoch 1520: Validation loss decreased (53.376118 --> 53.304058).\n",
            "\t Train_Loss: 8.6770 Val_Loss: 53.3041  BEST VAL Loss: 53.3041\n",
            "\n",
            "Epoch 1521: Validation loss decreased (53.304058 --> 53.228382).\n",
            "\t Train_Loss: 8.6656 Val_Loss: 53.2284  BEST VAL Loss: 53.2284\n",
            "\n",
            "Epoch 1522: Validation loss decreased (53.228382 --> 53.143921).\n",
            "\t Train_Loss: 8.6538 Val_Loss: 53.1439  BEST VAL Loss: 53.1439\n",
            "\n",
            "Epoch 1523: Validation loss decreased (53.143921 --> 53.064423).\n",
            "\t Train_Loss: 8.6421 Val_Loss: 53.0644  BEST VAL Loss: 53.0644\n",
            "\n",
            "Epoch 1524: Validation loss decreased (53.064423 --> 52.980774).\n",
            "\t Train_Loss: 8.6302 Val_Loss: 52.9808  BEST VAL Loss: 52.9808\n",
            "\n",
            "Epoch 1525: Validation loss decreased (52.980774 --> 52.883633).\n",
            "\t Train_Loss: 8.6184 Val_Loss: 52.8836  BEST VAL Loss: 52.8836\n",
            "\n",
            "Epoch 1526: Validation loss decreased (52.883633 --> 52.798981).\n",
            "\t Train_Loss: 8.6068 Val_Loss: 52.7990  BEST VAL Loss: 52.7990\n",
            "\n",
            "Epoch 1527: Validation loss decreased (52.798981 --> 52.740692).\n",
            "\t Train_Loss: 8.5949 Val_Loss: 52.7407  BEST VAL Loss: 52.7407\n",
            "\n",
            "Epoch 1528: Validation loss decreased (52.740692 --> 52.677307).\n",
            "\t Train_Loss: 8.5830 Val_Loss: 52.6773  BEST VAL Loss: 52.6773\n",
            "\n",
            "Epoch 1529: Validation loss decreased (52.677307 --> 52.591228).\n",
            "\t Train_Loss: 8.5713 Val_Loss: 52.5912  BEST VAL Loss: 52.5912\n",
            "\n",
            "Epoch 1530: Validation loss decreased (52.591228 --> 52.514027).\n",
            "\t Train_Loss: 8.5592 Val_Loss: 52.5140  BEST VAL Loss: 52.5140\n",
            "\n",
            "Epoch 1531: Validation loss decreased (52.514027 --> 52.460842).\n",
            "\t Train_Loss: 8.5472 Val_Loss: 52.4608  BEST VAL Loss: 52.4608\n",
            "\n",
            "Epoch 1532: Validation loss decreased (52.460842 --> 52.405434).\n",
            "\t Train_Loss: 8.5349 Val_Loss: 52.4054  BEST VAL Loss: 52.4054\n",
            "\n",
            "Epoch 1533: Validation loss decreased (52.405434 --> 52.338146).\n",
            "\t Train_Loss: 8.5225 Val_Loss: 52.3381  BEST VAL Loss: 52.3381\n",
            "\n",
            "Epoch 1534: Validation loss decreased (52.338146 --> 52.272804).\n",
            "\t Train_Loss: 8.5101 Val_Loss: 52.2728  BEST VAL Loss: 52.2728\n",
            "\n",
            "Epoch 1535: Validation loss decreased (52.272804 --> 52.205151).\n",
            "\t Train_Loss: 8.4974 Val_Loss: 52.2052  BEST VAL Loss: 52.2052\n",
            "\n",
            "Epoch 1536: Validation loss decreased (52.205151 --> 52.138744).\n",
            "\t Train_Loss: 8.4848 Val_Loss: 52.1387  BEST VAL Loss: 52.1387\n",
            "\n",
            "Epoch 1537: Validation loss decreased (52.138744 --> 52.083157).\n",
            "\t Train_Loss: 8.4720 Val_Loss: 52.0832  BEST VAL Loss: 52.0832\n",
            "\n",
            "Epoch 1538: Validation loss decreased (52.083157 --> 52.009502).\n",
            "\t Train_Loss: 8.4591 Val_Loss: 52.0095  BEST VAL Loss: 52.0095\n",
            "\n",
            "Epoch 1539: Validation loss decreased (52.009502 --> 51.921947).\n",
            "\t Train_Loss: 8.4460 Val_Loss: 51.9219  BEST VAL Loss: 51.9219\n",
            "\n",
            "Epoch 1540: Validation loss decreased (51.921947 --> 51.856670).\n",
            "\t Train_Loss: 8.4328 Val_Loss: 51.8567  BEST VAL Loss: 51.8567\n",
            "\n",
            "Epoch 1541: Validation loss decreased (51.856670 --> 51.790668).\n",
            "\t Train_Loss: 8.4195 Val_Loss: 51.7907  BEST VAL Loss: 51.7907\n",
            "\n",
            "Epoch 1542: Validation loss decreased (51.790668 --> 51.726284).\n",
            "\t Train_Loss: 8.4060 Val_Loss: 51.7263  BEST VAL Loss: 51.7263\n",
            "\n",
            "Epoch 1543: Validation loss decreased (51.726284 --> 51.650318).\n",
            "\t Train_Loss: 8.3925 Val_Loss: 51.6503  BEST VAL Loss: 51.6503\n",
            "\n",
            "Epoch 1544: Validation loss decreased (51.650318 --> 51.571438).\n",
            "\t Train_Loss: 8.3787 Val_Loss: 51.5714  BEST VAL Loss: 51.5714\n",
            "\n",
            "Epoch 1545: Validation loss decreased (51.571438 --> 51.516113).\n",
            "\t Train_Loss: 8.3650 Val_Loss: 51.5161  BEST VAL Loss: 51.5161\n",
            "\n",
            "Epoch 1546: Validation loss decreased (51.516113 --> 51.451588).\n",
            "\t Train_Loss: 8.3510 Val_Loss: 51.4516  BEST VAL Loss: 51.4516\n",
            "\n",
            "Epoch 1547: Validation loss decreased (51.451588 --> 51.404736).\n",
            "\t Train_Loss: 8.3370 Val_Loss: 51.4047  BEST VAL Loss: 51.4047\n",
            "\n",
            "Epoch 1548: Validation loss decreased (51.404736 --> 51.318390).\n",
            "\t Train_Loss: 8.3228 Val_Loss: 51.3184  BEST VAL Loss: 51.3184\n",
            "\n",
            "Epoch 1549: Validation loss decreased (51.318390 --> 51.293434).\n",
            "\t Train_Loss: 8.3084 Val_Loss: 51.2934  BEST VAL Loss: 51.2934\n",
            "\n",
            "Epoch 1550: Validation loss decreased (51.293434 --> 51.166435).\n",
            "\t Train_Loss: 8.2941 Val_Loss: 51.1664  BEST VAL Loss: 51.1664\n",
            "\n",
            "Epoch 1551: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2816 Val_Loss: 51.2524  BEST VAL Loss: 51.1664\n",
            "\n",
            "Epoch 1552: Validation loss decreased (51.166435 --> 50.971329).\n",
            "\t Train_Loss: 8.2750 Val_Loss: 50.9713  BEST VAL Loss: 50.9713\n",
            "\n",
            "Epoch 1553: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2697 Val_Loss: 51.1713  BEST VAL Loss: 50.9713\n",
            "\n",
            "Epoch 1554: Validation loss decreased (50.971329 --> 50.766151).\n",
            "\t Train_Loss: 8.2679 Val_Loss: 50.7662  BEST VAL Loss: 50.7662\n",
            "\n",
            "Epoch 1555: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2625 Val_Loss: 51.0754  BEST VAL Loss: 50.7662\n",
            "\n",
            "Epoch 1556: Validation loss decreased (50.766151 --> 50.729179).\n",
            "\t Train_Loss: 8.2410 Val_Loss: 50.7292  BEST VAL Loss: 50.7292\n",
            "\n",
            "Epoch 1557: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2084 Val_Loss: 50.8590  BEST VAL Loss: 50.7292\n",
            "\n",
            "Epoch 1558: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1761 Val_Loss: 50.8112  BEST VAL Loss: 50.7292\n",
            "\n",
            "Epoch 1559: Validation loss decreased (50.729179 --> 50.611008).\n",
            "\t Train_Loss: 8.1606 Val_Loss: 50.6110  BEST VAL Loss: 50.6110\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1817 Val_Loss: 51.2633  BEST VAL Loss: 50.6110\n",
            "\n",
            "Epoch 1561: Validation loss decreased (50.611008 --> 50.317013).\n",
            "\t Train_Loss: 8.3910 Val_Loss: 50.3170  BEST VAL Loss: 50.3170\n",
            "\n",
            "Epoch 1562: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4457 Val_Loss: 50.7946  BEST VAL Loss: 50.3170\n",
            "\n",
            "Epoch 1563: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2113 Val_Loss: 50.8652  BEST VAL Loss: 50.3170\n",
            "\n",
            "Epoch 1564: Validation loss decreased (50.317013 --> 49.958218).\n",
            "\t Train_Loss: 8.2996 Val_Loss: 49.9582  BEST VAL Loss: 49.9582\n",
            "\n",
            "Epoch 1565: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1983 Val_Loss: 50.1474  BEST VAL Loss: 49.9582\n",
            "\n",
            "Epoch 1566: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1434 Val_Loss: 51.4008  BEST VAL Loss: 49.9582\n",
            "\n",
            "Epoch 1567: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3940 Val_Loss: 50.7993  BEST VAL Loss: 49.9582\n",
            "\n",
            "Epoch 1568: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1718 Val_Loss: 50.5542  BEST VAL Loss: 49.9582\n",
            "\n",
            "Epoch 1569: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0554 Val_Loss: 50.4699  BEST VAL Loss: 49.9582\n",
            "\n",
            "Epoch 1570: Validation loss decreased (49.958218 --> 49.643536).\n",
            "\t Train_Loss: 8.1691 Val_Loss: 49.6435  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1571: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3006 Val_Loss: 51.1405  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2661 Val_Loss: 51.0138  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1573: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2148 Val_Loss: 50.1213  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1293 Val_Loss: 50.0658  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1575: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1286 Val_Loss: 50.2470  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1576: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1282 Val_Loss: 50.1407  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1577: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2341 Val_Loss: 51.4789  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2655 Val_Loss: 50.7661  BEST VAL Loss: 49.6435\n",
            "\n",
            "Epoch 1579: Validation loss decreased (49.643536 --> 49.270794).\n",
            "\t Train_Loss: 8.0594 Val_Loss: 49.2708  BEST VAL Loss: 49.2708\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3058 Val_Loss: 50.0497  BEST VAL Loss: 49.2708\n",
            "\n",
            "Epoch 1581: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0892 Val_Loss: 50.3788  BEST VAL Loss: 49.2708\n",
            "\n",
            "Epoch 1582: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0209 Val_Loss: 49.6739  BEST VAL Loss: 49.2708\n",
            "\n",
            "Epoch 1583: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2803 Val_Loss: 50.7711  BEST VAL Loss: 49.2708\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0431 Val_Loss: 50.4054  BEST VAL Loss: 49.2708\n",
            "\n",
            "Epoch 1585: Validation loss decreased (49.270794 --> 49.205322).\n",
            "\t Train_Loss: 7.9848 Val_Loss: 49.2053  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1586: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1431 Val_Loss: 50.0348  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1587: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9430 Val_Loss: 50.6097  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1588: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0197 Val_Loss: 49.7461  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1589: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9297 Val_Loss: 49.7519  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8856 Val_Loss: 50.3948  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1591: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9850 Val_Loss: 49.6711  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1592: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1474 Val_Loss: 50.4280  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1593: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9409 Val_Loss: 49.6276  BEST VAL Loss: 49.2053\n",
            "\n",
            "Epoch 1594: Validation loss decreased (49.205322 --> 48.800568).\n",
            "\t Train_Loss: 7.8832 Val_Loss: 48.8006  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1595: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9801 Val_Loss: 49.5052  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1596: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8337 Val_Loss: 50.1889  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1597: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9557 Val_Loss: 49.2891  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8458 Val_Loss: 49.1869  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1599: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8220 Val_Loss: 49.9235  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8683 Val_Loss: 49.2070  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1601: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2625 Val_Loss: 50.1551  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1602: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8258 Val_Loss: 50.7754  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2615 Val_Loss: 49.7124  BEST VAL Loss: 48.8006\n",
            "\n",
            "Epoch 1604: Validation loss decreased (48.800568 --> 48.780575).\n",
            "\t Train_Loss: 7.7809 Val_Loss: 48.7806  BEST VAL Loss: 48.7806\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4475 Val_Loss: 51.7169  BEST VAL Loss: 48.7806\n",
            "\n",
            "Epoch 1606: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4769 Val_Loss: 52.5482  BEST VAL Loss: 48.7806\n",
            "\n",
            "Epoch 1607: Validation loss decreased (48.780575 --> 48.354156).\n",
            "\t Train_Loss: 9.3879 Val_Loss: 48.3542  BEST VAL Loss: 48.3542\n",
            "\n",
            "Epoch 1608: Validation loss decreased (48.354156 --> 48.064846).\n",
            "\t Train_Loss: 11.7918 Val_Loss: 48.0648  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1219 Val_Loss: 53.0482  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1610: Validation loss did not decrease\n",
            "\t Train_Loss: 13.4064 Val_Loss: 55.9072  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1212 Val_Loss: 73.0795  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 22.3857 Val_Loss: 64.8841  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3593 Val_Loss: 67.6387  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4493 Val_Loss: 59.0864  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9082 Val_Loss: 59.4292  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9771 Val_Loss: 59.3005  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1617: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0565 Val_Loss: 60.7079  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1618: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6944 Val_Loss: 56.8792  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5775 Val_Loss: 55.2816  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1620: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5032 Val_Loss: 54.1029  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0541 Val_Loss: 52.2480  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0664 Val_Loss: 52.8205  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1623: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8812 Val_Loss: 50.3924  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5253 Val_Loss: 49.1524  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1625: Validation loss did not decrease\n",
            "\t Train_Loss: 14.3259 Val_Loss: 51.7587  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0622 Val_Loss: 54.9453  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 12.8106 Val_Loss: 60.6896  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1628: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2749 Val_Loss: 64.4918  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6791 Val_Loss: 62.6174  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1688 Val_Loss: 56.6068  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1631: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4884 Val_Loss: 53.7802  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1856 Val_Loss: 55.5106  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1849 Val_Loss: 56.4711  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1634: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3804 Val_Loss: 64.6217  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0816 Val_Loss: 65.2127  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1636: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8905 Val_Loss: 60.8403  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5620 Val_Loss: 62.7881  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7012 Val_Loss: 60.3961  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1239 Val_Loss: 56.7371  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4153 Val_Loss: 54.0499  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8346 Val_Loss: 52.8172  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0862 Val_Loss: 52.4766  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8708 Val_Loss: 53.3235  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1644: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6427 Val_Loss: 53.5005  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6350 Val_Loss: 53.6059  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1646: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4732 Val_Loss: 53.9031  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1647: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1240 Val_Loss: 54.0077  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1112 Val_Loss: 53.2934  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0688 Val_Loss: 52.8615  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9359 Val_Loss: 53.1414  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9221 Val_Loss: 52.7246  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8839 Val_Loss: 51.8196  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6116 Val_Loss: 51.0814  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1654: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6055 Val_Loss: 52.3677  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1655: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7180 Val_Loss: 52.8789  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1656: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0585 Val_Loss: 52.3879  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1657: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3839 Val_Loss: 51.3696  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1658: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9186 Val_Loss: 51.8051  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1659: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3033 Val_Loss: 51.5156  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1660: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2542 Val_Loss: 50.9240  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1661: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4165 Val_Loss: 50.3054  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1662: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1399 Val_Loss: 49.6968  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1663: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8891 Val_Loss: 49.7774  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1664: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9334 Val_Loss: 49.8790  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1665: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8230 Val_Loss: 49.8093  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1666: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7809 Val_Loss: 49.2088  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1667: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6982 Val_Loss: 48.8812  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1668: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8345 Val_Loss: 49.5760  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1669: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5691 Val_Loss: 50.9719  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1670: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7196 Val_Loss: 51.3127  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1671: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5761 Val_Loss: 50.9971  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1672: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5433 Val_Loss: 50.4545  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1673: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5801 Val_Loss: 50.4193  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1674: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4075 Val_Loss: 50.4009  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1675: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3768 Val_Loss: 50.3603  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1676: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4115 Val_Loss: 49.7023  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1677: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1464 Val_Loss: 49.4137  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1678: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2124 Val_Loss: 50.2563  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1679: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3120 Val_Loss: 50.0244  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1680: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2407 Val_Loss: 49.2302  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1681: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4207 Val_Loss: 49.3347  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1682: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4646 Val_Loss: 49.3200  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1683: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5715 Val_Loss: 49.2037  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1684: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2698 Val_Loss: 49.3341  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1685: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7362 Val_Loss: 50.8183  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1686: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0591 Val_Loss: 51.0959  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1687: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6313 Val_Loss: 48.6192  BEST VAL Loss: 48.0648\n",
            "\n",
            "Epoch 1688: Validation loss decreased (48.064846 --> 47.837326).\n",
            "\t Train_Loss: 8.2672 Val_Loss: 47.8373  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1689: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5012 Val_Loss: 48.1304  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1690: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1619 Val_Loss: 48.8914  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1691: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2032 Val_Loss: 49.1057  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1692: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9308 Val_Loss: 49.1020  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1693: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2473 Val_Loss: 50.6932  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1694: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0277 Val_Loss: 50.8545  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1695: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9999 Val_Loss: 48.9219  BEST VAL Loss: 47.8373\n",
            "\n",
            "Epoch 1696: Validation loss decreased (47.837326 --> 47.520008).\n",
            "\t Train_Loss: 8.0389 Val_Loss: 47.5200  BEST VAL Loss: 47.5200\n",
            "\n",
            "Epoch 1697: Validation loss decreased (47.520008 --> 47.156403).\n",
            "\t Train_Loss: 8.0317 Val_Loss: 47.1564  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1698: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1224 Val_Loss: 48.3617  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1699: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9131 Val_Loss: 50.2642  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1700: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9962 Val_Loss: 50.2885  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1701: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8474 Val_Loss: 50.2091  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1702: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0275 Val_Loss: 50.7725  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1703: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0497 Val_Loss: 50.3283  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1704: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8712 Val_Loss: 47.2760  BEST VAL Loss: 47.1564\n",
            "\n",
            "Epoch 1705: Validation loss decreased (47.156403 --> 46.441967).\n",
            "\t Train_Loss: 8.1228 Val_Loss: 46.4420  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1706: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4995 Val_Loss: 47.5293  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1707: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4040 Val_Loss: 48.4682  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1708: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5505 Val_Loss: 48.7448  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1709: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4124 Val_Loss: 48.9729  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1710: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3275 Val_Loss: 48.2352  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1711: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0798 Val_Loss: 47.3594  BEST VAL Loss: 46.4420\n",
            "\n",
            "Epoch 1712: Validation loss decreased (46.441967 --> 44.973572).\n",
            "\t Train_Loss: 7.9873 Val_Loss: 44.9736  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1713: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9010 Val_Loss: 47.1069  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1714: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5750 Val_Loss: 46.8107  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1715: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6405 Val_Loss: 45.1785  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1716: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6614 Val_Loss: 45.0475  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1717: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0763 Val_Loss: 45.8147  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1718: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6587 Val_Loss: 47.4566  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1719: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4604 Val_Loss: 51.1219  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1720: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8302 Val_Loss: 51.0948  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1721: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6677 Val_Loss: 48.0281  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1722: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1073 Val_Loss: 45.9221  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1723: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3096 Val_Loss: 45.2584  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1724: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2111 Val_Loss: 46.0094  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1725: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1417 Val_Loss: 46.8727  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1726: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1110 Val_Loss: 48.0936  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1727: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0917 Val_Loss: 49.1951  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1728: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8533 Val_Loss: 49.1205  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1729: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8089 Val_Loss: 48.1613  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1730: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7812 Val_Loss: 51.1150  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1731: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3249 Val_Loss: 50.2676  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1732: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7385 Val_Loss: 49.5917  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1733: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4236 Val_Loss: 48.9299  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1734: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5895 Val_Loss: 46.9529  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1735: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2957 Val_Loss: 47.9280  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1736: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4767 Val_Loss: 48.3527  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1737: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6102 Val_Loss: 45.9232  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1738: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2769 Val_Loss: 47.4216  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1739: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8398 Val_Loss: 47.9988  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1740: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1176 Val_Loss: 46.7012  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1741: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8731 Val_Loss: 45.2193  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1742: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6374 Val_Loss: 45.0864  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1743: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4930 Val_Loss: 46.6912  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1744: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3488 Val_Loss: 50.6825  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1745: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6875 Val_Loss: 52.0471  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1746: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9146 Val_Loss: 50.5541  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1747: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1532 Val_Loss: 48.0274  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1748: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5930 Val_Loss: 48.6445  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1749: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3871 Val_Loss: 49.7103  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1750: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3626 Val_Loss: 48.7047  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1751: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7799 Val_Loss: 46.8844  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1752: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3663 Val_Loss: 46.6113  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1753: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3305 Val_Loss: 47.4884  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1754: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7194 Val_Loss: 48.0285  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1755: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9800 Val_Loss: 47.5067  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1756: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9592 Val_Loss: 46.3589  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1757: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9797 Val_Loss: 46.3020  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1758: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0135 Val_Loss: 47.5898  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1759: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9134 Val_Loss: 47.4228  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1760: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8666 Val_Loss: 45.6551  BEST VAL Loss: 44.9736\n",
            "\n",
            "Epoch 1761: Validation loss decreased (44.973572 --> 44.198830).\n",
            "\t Train_Loss: 7.6834 Val_Loss: 44.1988  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1762: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0129 Val_Loss: 45.5773  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1763: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6716 Val_Loss: 48.0185  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1764: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6948 Val_Loss: 48.7125  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1765: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8375 Val_Loss: 48.6651  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1766: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7726 Val_Loss: 48.4923  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1767: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6537 Val_Loss: 48.3185  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1768: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6239 Val_Loss: 48.1557  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1769: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5540 Val_Loss: 48.7307  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1770: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7407 Val_Loss: 49.2443  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1771: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4443 Val_Loss: 49.7515  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1772: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5364 Val_Loss: 49.4465  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1773: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5192 Val_Loss: 49.9516  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1774: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4733 Val_Loss: 50.1817  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1775: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4115 Val_Loss: 49.8668  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1776: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3776 Val_Loss: 48.5621  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1777: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4178 Val_Loss: 48.6532  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1778: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3579 Val_Loss: 49.3910  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1779: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2298 Val_Loss: 49.9436  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1780: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2496 Val_Loss: 49.7412  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1781: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2554 Val_Loss: 48.1985  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1782: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4888 Val_Loss: 48.7803  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1783: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3256 Val_Loss: 50.6411  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1784: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1477 Val_Loss: 50.8101  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1785: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8218 Val_Loss: 49.2239  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1786: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9341 Val_Loss: 50.6235  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1787: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1822 Val_Loss: 49.0154  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1788: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2987 Val_Loss: 47.2403  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1789: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7539 Val_Loss: 46.8110  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1790: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6415 Val_Loss: 47.4334  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1791: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1986 Val_Loss: 47.6210  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1792: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9409 Val_Loss: 49.0281  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1793: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6424 Val_Loss: 48.0393  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1794: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4054 Val_Loss: 47.3327  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1795: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5433 Val_Loss: 46.6242  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1796: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6583 Val_Loss: 45.5527  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1797: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0578 Val_Loss: 46.3114  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1798: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7899 Val_Loss: 47.2396  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1799: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3239 Val_Loss: 48.4455  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1800: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3017 Val_Loss: 48.3606  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1801: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7544 Val_Loss: 46.5939  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1802: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8857 Val_Loss: 45.7623  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1803: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8714 Val_Loss: 46.9311  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1804: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0183 Val_Loss: 48.2255  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1805: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1738 Val_Loss: 48.2993  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1806: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0450 Val_Loss: 47.5318  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1807: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8828 Val_Loss: 47.6127  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1808: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8411 Val_Loss: 48.7803  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1809: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6749 Val_Loss: 48.7097  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1810: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7493 Val_Loss: 47.1243  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1811: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3850 Val_Loss: 45.9076  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1812: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5369 Val_Loss: 45.7518  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1813: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6561 Val_Loss: 45.4692  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1814: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6405 Val_Loss: 45.9196  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1815: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5089 Val_Loss: 46.1692  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1816: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4215 Val_Loss: 44.7685  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1817: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5734 Val_Loss: 44.8784  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1818: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4138 Val_Loss: 46.2225  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1819: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5616 Val_Loss: 45.9700  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1820: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3863 Val_Loss: 44.6513  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1821: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4676 Val_Loss: 44.7662  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1822: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1475 Val_Loss: 45.6939  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1823: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5132 Val_Loss: 45.7064  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1824: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2070 Val_Loss: 44.9249  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1825: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9959 Val_Loss: 45.1043  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1826: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7864 Val_Loss: 46.4223  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1827: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1399 Val_Loss: 46.8692  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1828: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6901 Val_Loss: 45.5986  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1829: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2655 Val_Loss: 44.7681  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1830: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3874 Val_Loss: 45.0156  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1831: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3633 Val_Loss: 44.5742  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1832: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1186 Val_Loss: 44.7120  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1833: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1093 Val_Loss: 45.5280  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1834: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1770 Val_Loss: 45.8574  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1835: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1243 Val_Loss: 45.9850  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1836: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1975 Val_Loss: 45.8639  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1837: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2064 Val_Loss: 45.3435  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1838: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1179 Val_Loss: 44.5816  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1839: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6562 Val_Loss: 45.3163  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1840: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4260 Val_Loss: 47.3120  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1841: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4515 Val_Loss: 47.3083  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1842: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4281 Val_Loss: 45.5000  BEST VAL Loss: 44.1988\n",
            "\n",
            "Epoch 1843: Validation loss decreased (44.198830 --> 43.731789).\n",
            "\t Train_Loss: 7.2298 Val_Loss: 43.7318  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1844: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4985 Val_Loss: 44.1002  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1845: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3450 Val_Loss: 46.8115  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1846: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5540 Val_Loss: 47.2674  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1847: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5074 Val_Loss: 46.1751  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1848: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0257 Val_Loss: 45.4576  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1849: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2979 Val_Loss: 45.7114  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1850: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1061 Val_Loss: 45.6751  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1851: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2775 Val_Loss: 45.2556  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1852: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1134 Val_Loss: 45.5374  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1853: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9423 Val_Loss: 46.8339  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1854: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3291 Val_Loss: 46.6398  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1855: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2758 Val_Loss: 45.0613  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1856: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0063 Val_Loss: 44.9674  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1857: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0061 Val_Loss: 46.2466  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1858: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1322 Val_Loss: 46.8086  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1859: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2310 Val_Loss: 46.0277  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1860: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7613 Val_Loss: 45.1328  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1861: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0897 Val_Loss: 45.6165  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1862: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8772 Val_Loss: 45.2771  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1863: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7415 Val_Loss: 44.5324  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1864: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1542 Val_Loss: 46.1483  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1865: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8591 Val_Loss: 46.0963  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1866: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8238 Val_Loss: 45.2419  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1867: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8151 Val_Loss: 45.0602  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1868: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7822 Val_Loss: 45.4018  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1869: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9747 Val_Loss: 44.7791  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1870: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9732 Val_Loss: 45.1429  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1871: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9995 Val_Loss: 45.7884  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1872: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7751 Val_Loss: 45.0440  BEST VAL Loss: 43.7318\n",
            "\n",
            "Epoch 1873: Validation loss decreased (43.731789 --> 43.596214).\n",
            "\t Train_Loss: 6.8688 Val_Loss: 43.5962  BEST VAL Loss: 43.5962\n",
            "\n",
            "Epoch 1874: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4065 Val_Loss: 45.5180  BEST VAL Loss: 43.5962\n",
            "\n",
            "Epoch 1875: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8093 Val_Loss: 47.3611  BEST VAL Loss: 43.5962\n",
            "\n",
            "Epoch 1876: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5654 Val_Loss: 45.4702  BEST VAL Loss: 43.5962\n",
            "\n",
            "Epoch 1877: Validation loss decreased (43.596214 --> 43.509193).\n",
            "\t Train_Loss: 6.8909 Val_Loss: 43.5092  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1878: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1297 Val_Loss: 43.6661  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1879: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4436 Val_Loss: 44.0697  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1880: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1334 Val_Loss: 44.5220  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1881: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9105 Val_Loss: 44.7735  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1882: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9669 Val_Loss: 44.7671  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1883: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9282 Val_Loss: 44.0167  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1884: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8153 Val_Loss: 43.9032  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1885: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6947 Val_Loss: 44.0840  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1886: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7758 Val_Loss: 43.7578  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1887: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0393 Val_Loss: 46.2091  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1888: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1204 Val_Loss: 45.9304  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1889: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8319 Val_Loss: 44.0666  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1890: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2691 Val_Loss: 43.9557  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1891: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1105 Val_Loss: 44.6297  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1892: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8252 Val_Loss: 45.2523  BEST VAL Loss: 43.5092\n",
            "\n",
            "Epoch 1893: Validation loss decreased (43.509193 --> 43.475037).\n",
            "\t Train_Loss: 7.6996 Val_Loss: 43.4750  BEST VAL Loss: 43.4750\n",
            "\n",
            "Epoch 1894: Validation loss decreased (43.475037 --> 42.949291).\n",
            "\t Train_Loss: 6.7752 Val_Loss: 42.9493  BEST VAL Loss: 42.9493\n",
            "\n",
            "Epoch 1895: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9964 Val_Loss: 44.5318  BEST VAL Loss: 42.9493\n",
            "\n",
            "Epoch 1896: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1988 Val_Loss: 45.6216  BEST VAL Loss: 42.9493\n",
            "\n",
            "Epoch 1897: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4675 Val_Loss: 45.4159  BEST VAL Loss: 42.9493\n",
            "\n",
            "Epoch 1898: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3424 Val_Loss: 43.7902  BEST VAL Loss: 42.9493\n",
            "\n",
            "Epoch 1899: Validation loss decreased (42.949291 --> 42.618053).\n",
            "\t Train_Loss: 6.6957 Val_Loss: 42.6181  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1900: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1098 Val_Loss: 43.1455  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1901: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9861 Val_Loss: 44.8136  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1902: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0065 Val_Loss: 45.7693  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1903: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9894 Val_Loss: 45.1714  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1904: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0612 Val_Loss: 44.0820  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1905: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8285 Val_Loss: 43.8986  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1906: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0175 Val_Loss: 44.1517  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1907: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6798 Val_Loss: 44.3079  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1908: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1031 Val_Loss: 45.0869  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1909: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1778 Val_Loss: 45.8229  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1910: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1584 Val_Loss: 45.6482  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1911: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9199 Val_Loss: 45.2767  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1912: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2402 Val_Loss: 43.7277  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1913: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0540 Val_Loss: 43.3937  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1914: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1249 Val_Loss: 44.0054  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1915: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8184 Val_Loss: 44.0240  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1916: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8778 Val_Loss: 43.8726  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1917: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8751 Val_Loss: 42.9926  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1918: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9636 Val_Loss: 43.1822  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1919: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7677 Val_Loss: 45.7469  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1920: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3032 Val_Loss: 45.3467  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1921: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0800 Val_Loss: 43.8082  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1922: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8322 Val_Loss: 43.0374  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1923: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3411 Val_Loss: 43.1130  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1924: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8800 Val_Loss: 46.4445  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1925: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5384 Val_Loss: 46.5091  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1926: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6414 Val_Loss: 43.6701  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1927: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8247 Val_Loss: 44.2245  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1928: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2446 Val_Loss: 44.7305  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1929: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6844 Val_Loss: 44.9039  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1930: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9613 Val_Loss: 43.9374  BEST VAL Loss: 42.6181\n",
            "\n",
            "Epoch 1931: Validation loss decreased (42.618053 --> 42.598667).\n",
            "\t Train_Loss: 6.7795 Val_Loss: 42.5987  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1932: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9850 Val_Loss: 43.4305  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1933: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5524 Val_Loss: 44.0794  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1934: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6568 Val_Loss: 43.4617  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1935: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7177 Val_Loss: 43.1503  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1936: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6061 Val_Loss: 43.1056  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1937: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5424 Val_Loss: 43.8783  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1938: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0436 Val_Loss: 43.5797  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1939: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5930 Val_Loss: 44.3640  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1940: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7618 Val_Loss: 44.5885  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1941: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7935 Val_Loss: 44.2662  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1942: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7291 Val_Loss: 43.2914  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1943: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5579 Val_Loss: 42.7830  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1944: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4602 Val_Loss: 42.6171  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1945: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5300 Val_Loss: 43.4078  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1946: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7263 Val_Loss: 42.9188  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1947: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6735 Val_Loss: 43.1327  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1948: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4998 Val_Loss: 43.2680  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1949: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6676 Val_Loss: 42.7943  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1950: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8675 Val_Loss: 43.4838  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1951: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7031 Val_Loss: 44.4065  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1952: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1002 Val_Loss: 44.2345  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1953: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0439 Val_Loss: 43.5814  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1954: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5932 Val_Loss: 43.1910  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1955: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9914 Val_Loss: 43.1227  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1956: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7896 Val_Loss: 43.4721  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1957: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5207 Val_Loss: 42.7092  BEST VAL Loss: 42.5987\n",
            "\n",
            "Epoch 1958: Validation loss decreased (42.598667 --> 42.359039).\n",
            "\t Train_Loss: 6.7185 Val_Loss: 42.3590  BEST VAL Loss: 42.3590\n",
            "\n",
            "Epoch 1959: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7739 Val_Loss: 42.8774  BEST VAL Loss: 42.3590\n",
            "\n",
            "Epoch 1960: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6400 Val_Loss: 42.4671  BEST VAL Loss: 42.3590\n",
            "\n",
            "Epoch 1961: Validation loss decreased (42.359039 --> 42.194862).\n",
            "\t Train_Loss: 6.6334 Val_Loss: 42.1949  BEST VAL Loss: 42.1949\n",
            "\n",
            "Epoch 1962: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6072 Val_Loss: 42.5338  BEST VAL Loss: 42.1949\n",
            "\n",
            "Epoch 1963: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5291 Val_Loss: 43.5769  BEST VAL Loss: 42.1949\n",
            "\n",
            "Epoch 1964: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4145 Val_Loss: 43.9860  BEST VAL Loss: 42.1949\n",
            "\n",
            "Epoch 1965: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5803 Val_Loss: 44.0019  BEST VAL Loss: 42.1949\n",
            "\n",
            "Epoch 1966: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5970 Val_Loss: 43.3901  BEST VAL Loss: 42.1949\n",
            "\n",
            "Epoch 1967: Validation loss decreased (42.194862 --> 41.908474).\n",
            "\t Train_Loss: 6.4593 Val_Loss: 41.9085  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1968: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6035 Val_Loss: 42.6367  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1969: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5511 Val_Loss: 42.2044  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1970: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5340 Val_Loss: 43.4245  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1971: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4576 Val_Loss: 44.5689  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1972: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8420 Val_Loss: 42.5727  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1973: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8748 Val_Loss: 42.2151  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1974: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8933 Val_Loss: 43.0785  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1975: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4606 Val_Loss: 42.5619  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1976: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4922 Val_Loss: 42.3107  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1977: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0175 Val_Loss: 42.5484  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1978: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4097 Val_Loss: 43.7265  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1979: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6203 Val_Loss: 43.5796  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1980: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4693 Val_Loss: 42.9642  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1981: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5309 Val_Loss: 42.4955  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1982: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7864 Val_Loss: 42.5188  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1983: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6009 Val_Loss: 43.1589  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1984: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7108 Val_Loss: 43.7218  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1985: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5708 Val_Loss: 43.8402  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1986: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6344 Val_Loss: 43.1737  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1987: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7122 Val_Loss: 42.3396  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1988: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5904 Val_Loss: 43.3090  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1989: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5998 Val_Loss: 44.6514  BEST VAL Loss: 41.9085\n",
            "\n",
            "Epoch 1990: Validation loss decreased (41.908474 --> 41.735390).\n",
            "\t Train_Loss: 6.9297 Val_Loss: 41.7354  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1991: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6549 Val_Loss: 42.5535  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1992: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7197 Val_Loss: 43.5352  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1993: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6187 Val_Loss: 44.0616  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1994: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1497 Val_Loss: 43.5627  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1995: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5113 Val_Loss: 42.0871  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1996: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1679 Val_Loss: 42.7840  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1997: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8820 Val_Loss: 43.0834  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1998: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8199 Val_Loss: 43.0807  BEST VAL Loss: 41.7354\n",
            "\n",
            "Epoch 1999: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6168 Val_Loss: 42.8142  BEST VAL Loss: 41.7354\n",
            "\n"
          ]
        }
      ],
      "source": [
        "GRU_best_model, train_losses, val_losses = trainer(GRU_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jaUYwd9vFHuC",
        "outputId": "57cf97fe-bd3c-4132-9431-dd30b844c2b4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jcdsct5Y9a8"
      },
      "source": [
        "---\n",
        "### 5.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PHWWy8uCY7Vg"
      },
      "outputs": [],
      "source": [
        "val_predict_GRU = GRU_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "wThiG7D-ZBA_",
        "outputId": "f66730b0-7bdc-4b26-cb6c-f905e1c17538"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_GRU.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by GRU RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMYeUTH5Ki7"
      },
      "source": [
        "---\n",
        "---\n",
        "## 6 LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1qXDXQ9VfIW"
      },
      "source": [
        "---\n",
        "### 6.1 Define single RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "bVP9ju7m39TL"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple LSTM cell network for educational AI-summer purposes\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length=10, hidden_size=20, bias=True):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        \"\"\"Define Needed Layers \"\"\"\n",
        "        self.forget_gate_Wxh = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.forget_gate_Whh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.input_gate_Wxh = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.input_gate_Whh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.output_gate_Wxh = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.output_gate_Whh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.cell_memory_gate_Wxh = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.cell_memory_gate_Whh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        bias_params = [nn.Parameter(torch.Tensor(hidden_size)) if self.bias else 0 for _ in range(4)]\n",
        "        self.bf, self.bi, self.bc, self.bo = bias_params\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forget(self, x, h):\n",
        "\n",
        "        fg = torch.sigmoid(x.matmul(self.forget_gate_Wxh.T) + h.matmul(self.forget_gate_Whh.T) + self.bf)\n",
        "\n",
        "        return fg\n",
        "\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "        ig = torch.sigmoid(x.matmul(self.input_gate_Wxh.T) + h.matmul(self.input_gate_Whh.T) + self.bi)\n",
        "        return ig\n",
        "\n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        gates = x.matmul(self.cell_memory_gate_Wxh.T) + h.matmul(self.cell_memory_gate_Whh.T) + self.bc\n",
        "        cell = torch.tanh(gates)\n",
        "        c_next = i * cell + f * c_prev\n",
        "        return c_next\n",
        "\n",
        "\n",
        "    def output_gate(self, x, h):\n",
        "        gates = x.matmul(self.output_gate_Wxh.T) + h.matmul(self.output_gate_Whh.T) + self.bo\n",
        "        og = torch.sigmoid(gates)\n",
        "        return og\n",
        "\n",
        "\n",
        "    def forward(self, x, hx=None):\n",
        "        \"\"\"Define Forward pass\"\"\"\n",
        "        if hx is None:\n",
        "            hx = (torch.zeros(x.size(0), self.hidden_size, dtype=x.dtype, device=x.device),\n",
        "                  torch.zeros(x.size(0), self.hidden_size, dtype=x.dtype, device=x.device))\n",
        "\n",
        "        h_prev, c_prev = hx\n",
        "\n",
        "        fg = self.forget(x, h_prev)\n",
        "        ig = self.input_gate(x, h_prev)\n",
        "        og = self.output_gate(x, h_prev)\n",
        "\n",
        "        c_next = self.cell_memory_gate(ig, fg, x, h_prev, c_prev)\n",
        "\n",
        "        h_next = og * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQpH0TuoVpVf"
      },
      "source": [
        "---\n",
        "### 6.2 LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "PshO9vv35REY"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        \"\"\"Define Needed Layers\"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "        self.lstm_cells = nn.ModuleList([LSTMCell(input_size if i == 0 else hidden_size, hidden_size, bias) for i in range(num_layers)])\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        \"\"\"Define Forward pass\"\"\"\n",
        "\n",
        "        batch_size, seq_len, _ = input.size()\n",
        "\n",
        "        if hx is None:\n",
        "            hx = [(torch.zeros(batch_size, self.hidden_size, dtype=input.dtype, device=input.device),\n",
        "                   torch.zeros(batch_size, self.hidden_size, dtype=input.dtype, device=input.device))\n",
        "                  for i in range(self.num_layers)]\n",
        "\n",
        "        h_t, c_t = list(zip(*hx))\n",
        "        h_t = list(h_t)\n",
        "        c_t = list(c_t)\n",
        "\n",
        "        output = []\n",
        "        for t in range(seq_len):\n",
        "            next_input = input[:,t]\n",
        "            for layer in range(self.num_layers):\n",
        "                h_t[layer], c_t[layer] = self.lstm_cells[layer](next_input, (h_t[layer], c_t[layer]))\n",
        "                next_input = h_t[layer]\n",
        "            output.append(h_t[-1])\n",
        "        h_t = tuple(h_t)\n",
        "        c_t = tuple(c_t)\n",
        "        output = torch.stack(output)\n",
        "        output = self.fc(output[-1])\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7G5_9S7VxJ0"
      },
      "source": [
        "---\n",
        "### 6.3 Train LSTM model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQTMa2Xj6maS",
        "outputId": "d4aba2ab-bfa7-42af-e70f-d3e31bc3dd57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm_cells): ModuleList(\n",
              "    (0): LSTMCell()\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LSTM_model = LSTM(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "LSTM_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "XuzlCGTrV_xH"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwpdUK6Z6skV",
        "outputId": "74e751ed-31f7-417b-8f67-cc4381d877ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 150: Validation loss decreased (3509.686035 --> 3479.413330).\n",
            "\t Train_Loss: 2078.7849 Val_Loss: 3479.4133  BEST VAL Loss: 3479.4133\n",
            "\n",
            "Epoch 151: Validation loss decreased (3479.413330 --> 3449.429688).\n",
            "\t Train_Loss: 2055.9531 Val_Loss: 3449.4297  BEST VAL Loss: 3449.4297\n",
            "\n",
            "Epoch 152: Validation loss decreased (3449.429688 --> 3419.731201).\n",
            "\t Train_Loss: 2033.3728 Val_Loss: 3419.7312  BEST VAL Loss: 3419.7312\n",
            "\n",
            "Epoch 153: Validation loss decreased (3419.731201 --> 3390.316895).\n",
            "\t Train_Loss: 2011.0402 Val_Loss: 3390.3169  BEST VAL Loss: 3390.3169\n",
            "\n",
            "Epoch 154: Validation loss decreased (3390.316895 --> 3361.180908).\n",
            "\t Train_Loss: 1988.9525 Val_Loss: 3361.1809  BEST VAL Loss: 3361.1809\n",
            "\n",
            "Epoch 155: Validation loss decreased (3361.180908 --> 3332.322266).\n",
            "\t Train_Loss: 1967.1072 Val_Loss: 3332.3223  BEST VAL Loss: 3332.3223\n",
            "\n",
            "Epoch 156: Validation loss decreased (3332.322266 --> 3303.734863).\n",
            "\t Train_Loss: 1945.5011 Val_Loss: 3303.7349  BEST VAL Loss: 3303.7349\n",
            "\n",
            "Epoch 157: Validation loss decreased (3303.734863 --> 3275.418457).\n",
            "\t Train_Loss: 1924.1315 Val_Loss: 3275.4185  BEST VAL Loss: 3275.4185\n",
            "\n",
            "Epoch 158: Validation loss decreased (3275.418457 --> 3247.367432).\n",
            "\t Train_Loss: 1902.9952 Val_Loss: 3247.3674  BEST VAL Loss: 3247.3674\n",
            "\n",
            "Epoch 159: Validation loss decreased (3247.367432 --> 3219.581055).\n",
            "\t Train_Loss: 1882.0898 Val_Loss: 3219.5811  BEST VAL Loss: 3219.5811\n",
            "\n",
            "Epoch 160: Validation loss decreased (3219.581055 --> 3192.055664).\n",
            "\t Train_Loss: 1861.4121 Val_Loss: 3192.0557  BEST VAL Loss: 3192.0557\n",
            "\n",
            "Epoch 161: Validation loss decreased (3192.055664 --> 3164.786621).\n",
            "\t Train_Loss: 1840.9601 Val_Loss: 3164.7866  BEST VAL Loss: 3164.7866\n",
            "\n",
            "Epoch 162: Validation loss decreased (3164.786621 --> 3137.773682).\n",
            "\t Train_Loss: 1820.7302 Val_Loss: 3137.7737  BEST VAL Loss: 3137.7737\n",
            "\n",
            "Epoch 163: Validation loss decreased (3137.773682 --> 3111.011475).\n",
            "\t Train_Loss: 1800.7206 Val_Loss: 3111.0115  BEST VAL Loss: 3111.0115\n",
            "\n",
            "Epoch 164: Validation loss decreased (3111.011475 --> 3084.498779).\n",
            "\t Train_Loss: 1780.9283 Val_Loss: 3084.4988  BEST VAL Loss: 3084.4988\n",
            "\n",
            "Epoch 165: Validation loss decreased (3084.498779 --> 3058.232666).\n",
            "\t Train_Loss: 1761.3512 Val_Loss: 3058.2327  BEST VAL Loss: 3058.2327\n",
            "\n",
            "Epoch 166: Validation loss decreased (3058.232666 --> 3032.209229).\n",
            "\t Train_Loss: 1741.9858 Val_Loss: 3032.2092  BEST VAL Loss: 3032.2092\n",
            "\n",
            "Epoch 167: Validation loss decreased (3032.209229 --> 3006.427734).\n",
            "\t Train_Loss: 1722.8303 Val_Loss: 3006.4277  BEST VAL Loss: 3006.4277\n",
            "\n",
            "Epoch 168: Validation loss decreased (3006.427734 --> 2980.884277).\n",
            "\t Train_Loss: 1703.8828 Val_Loss: 2980.8843  BEST VAL Loss: 2980.8843\n",
            "\n",
            "Epoch 169: Validation loss decreased (2980.884277 --> 2955.576416).\n",
            "\t Train_Loss: 1685.1399 Val_Loss: 2955.5764  BEST VAL Loss: 2955.5764\n",
            "\n",
            "Epoch 170: Validation loss decreased (2955.576416 --> 2930.501221).\n",
            "\t Train_Loss: 1666.6001 Val_Loss: 2930.5012  BEST VAL Loss: 2930.5012\n",
            "\n",
            "Epoch 171: Validation loss decreased (2930.501221 --> 2905.656738).\n",
            "\t Train_Loss: 1648.2607 Val_Loss: 2905.6567  BEST VAL Loss: 2905.6567\n",
            "\n",
            "Epoch 172: Validation loss decreased (2905.656738 --> 2881.041260).\n",
            "\t Train_Loss: 1630.1191 Val_Loss: 2881.0413  BEST VAL Loss: 2881.0413\n",
            "\n",
            "Epoch 173: Validation loss decreased (2881.041260 --> 2856.651123).\n",
            "\t Train_Loss: 1612.1740 Val_Loss: 2856.6511  BEST VAL Loss: 2856.6511\n",
            "\n",
            "Epoch 174: Validation loss decreased (2856.651123 --> 2832.483643).\n",
            "\t Train_Loss: 1594.4226 Val_Loss: 2832.4836  BEST VAL Loss: 2832.4836\n",
            "\n",
            "Epoch 175: Validation loss decreased (2832.483643 --> 2808.538574).\n",
            "\t Train_Loss: 1576.8628 Val_Loss: 2808.5386  BEST VAL Loss: 2808.5386\n",
            "\n",
            "Epoch 176: Validation loss decreased (2808.538574 --> 2784.811279).\n",
            "\t Train_Loss: 1559.4926 Val_Loss: 2784.8113  BEST VAL Loss: 2784.8113\n",
            "\n",
            "Epoch 177: Validation loss decreased (2784.811279 --> 2761.301270).\n",
            "\t Train_Loss: 1542.3098 Val_Loss: 2761.3013  BEST VAL Loss: 2761.3013\n",
            "\n",
            "Epoch 178: Validation loss decreased (2761.301270 --> 2738.004883).\n",
            "\t Train_Loss: 1525.3130 Val_Loss: 2738.0049  BEST VAL Loss: 2738.0049\n",
            "\n",
            "Epoch 179: Validation loss decreased (2738.004883 --> 2714.921875).\n",
            "\t Train_Loss: 1508.4991 Val_Loss: 2714.9219  BEST VAL Loss: 2714.9219\n",
            "\n",
            "Epoch 180: Validation loss decreased (2714.921875 --> 2692.048584).\n",
            "\t Train_Loss: 1491.8671 Val_Loss: 2692.0486  BEST VAL Loss: 2692.0486\n",
            "\n",
            "Epoch 181: Validation loss decreased (2692.048584 --> 2669.382812).\n",
            "\t Train_Loss: 1475.4148 Val_Loss: 2669.3828  BEST VAL Loss: 2669.3828\n",
            "\n",
            "Epoch 182: Validation loss decreased (2669.382812 --> 2646.923340).\n",
            "\t Train_Loss: 1459.1394 Val_Loss: 2646.9233  BEST VAL Loss: 2646.9233\n",
            "\n",
            "Epoch 183: Validation loss decreased (2646.923340 --> 2624.668457).\n",
            "\t Train_Loss: 1443.0405 Val_Loss: 2624.6685  BEST VAL Loss: 2624.6685\n",
            "\n",
            "Epoch 184: Validation loss decreased (2624.668457 --> 2602.614014).\n",
            "\t Train_Loss: 1427.1152 Val_Loss: 2602.6140  BEST VAL Loss: 2602.6140\n",
            "\n",
            "Epoch 185: Validation loss decreased (2602.614014 --> 2580.761475).\n",
            "\t Train_Loss: 1411.3622 Val_Loss: 2580.7615  BEST VAL Loss: 2580.7615\n",
            "\n",
            "Epoch 186: Validation loss decreased (2580.761475 --> 2559.105713).\n",
            "\t Train_Loss: 1395.7795 Val_Loss: 2559.1057  BEST VAL Loss: 2559.1057\n",
            "\n",
            "Epoch 187: Validation loss decreased (2559.105713 --> 2537.646729).\n",
            "\t Train_Loss: 1380.3656 Val_Loss: 2537.6467  BEST VAL Loss: 2537.6467\n",
            "\n",
            "Epoch 188: Validation loss decreased (2537.646729 --> 2516.381348).\n",
            "\t Train_Loss: 1365.1180 Val_Loss: 2516.3813  BEST VAL Loss: 2516.3813\n",
            "\n",
            "Epoch 189: Validation loss decreased (2516.381348 --> 2495.309326).\n",
            "\t Train_Loss: 1350.0359 Val_Loss: 2495.3093  BEST VAL Loss: 2495.3093\n",
            "\n",
            "Epoch 190: Validation loss decreased (2495.309326 --> 2474.426514).\n",
            "\t Train_Loss: 1335.1176 Val_Loss: 2474.4265  BEST VAL Loss: 2474.4265\n",
            "\n",
            "Epoch 191: Validation loss decreased (2474.426514 --> 2453.733887).\n",
            "\t Train_Loss: 1320.3605 Val_Loss: 2453.7339  BEST VAL Loss: 2453.7339\n",
            "\n",
            "Epoch 192: Validation loss decreased (2453.733887 --> 2433.228516).\n",
            "\t Train_Loss: 1305.7639 Val_Loss: 2433.2285  BEST VAL Loss: 2433.2285\n",
            "\n",
            "Epoch 193: Validation loss decreased (2433.228516 --> 2412.907471).\n",
            "\t Train_Loss: 1291.3254 Val_Loss: 2412.9075  BEST VAL Loss: 2412.9075\n",
            "\n",
            "Epoch 194: Validation loss decreased (2412.907471 --> 2392.771484).\n",
            "\t Train_Loss: 1277.0439 Val_Loss: 2392.7715  BEST VAL Loss: 2392.7715\n",
            "\n",
            "Epoch 195: Validation loss decreased (2392.771484 --> 2372.816162).\n",
            "\t Train_Loss: 1262.9182 Val_Loss: 2372.8162  BEST VAL Loss: 2372.8162\n",
            "\n",
            "Epoch 196: Validation loss decreased (2372.816162 --> 2353.041504).\n",
            "\t Train_Loss: 1248.9460 Val_Loss: 2353.0415  BEST VAL Loss: 2353.0415\n",
            "\n",
            "Epoch 197: Validation loss decreased (2353.041504 --> 2333.445068).\n",
            "\t Train_Loss: 1235.1260 Val_Loss: 2333.4451  BEST VAL Loss: 2333.4451\n",
            "\n",
            "Epoch 198: Validation loss decreased (2333.445068 --> 2314.026855).\n",
            "\t Train_Loss: 1221.4567 Val_Loss: 2314.0269  BEST VAL Loss: 2314.0269\n",
            "\n",
            "Epoch 199: Validation loss decreased (2314.026855 --> 2294.783936).\n",
            "\t Train_Loss: 1207.9366 Val_Loss: 2294.7839  BEST VAL Loss: 2294.7839\n",
            "\n",
            "Epoch 200: Validation loss decreased (2294.783936 --> 2275.712891).\n",
            "\t Train_Loss: 1194.5645 Val_Loss: 2275.7129  BEST VAL Loss: 2275.7129\n",
            "\n",
            "Epoch 201: Validation loss decreased (2275.712891 --> 2256.816162).\n",
            "\t Train_Loss: 1181.3385 Val_Loss: 2256.8162  BEST VAL Loss: 2256.8162\n",
            "\n",
            "Epoch 202: Validation loss decreased (2256.816162 --> 2238.090088).\n",
            "\t Train_Loss: 1168.2573 Val_Loss: 2238.0901  BEST VAL Loss: 2238.0901\n",
            "\n",
            "Epoch 203: Validation loss decreased (2238.090088 --> 2219.532227).\n",
            "\t Train_Loss: 1155.3197 Val_Loss: 2219.5322  BEST VAL Loss: 2219.5322\n",
            "\n",
            "Epoch 204: Validation loss decreased (2219.532227 --> 2201.142578).\n",
            "\t Train_Loss: 1142.5243 Val_Loss: 2201.1426  BEST VAL Loss: 2201.1426\n",
            "\n",
            "Epoch 205: Validation loss decreased (2201.142578 --> 2182.919678).\n",
            "\t Train_Loss: 1129.8693 Val_Loss: 2182.9197  BEST VAL Loss: 2182.9197\n",
            "\n",
            "Epoch 206: Validation loss decreased (2182.919678 --> 2164.861084).\n",
            "\t Train_Loss: 1117.3536 Val_Loss: 2164.8611  BEST VAL Loss: 2164.8611\n",
            "\n",
            "Epoch 207: Validation loss decreased (2164.861084 --> 2146.966797).\n",
            "\t Train_Loss: 1104.9756 Val_Loss: 2146.9668  BEST VAL Loss: 2146.9668\n",
            "\n",
            "Epoch 208: Validation loss decreased (2146.966797 --> 2129.233154).\n",
            "\t Train_Loss: 1092.7345 Val_Loss: 2129.2332  BEST VAL Loss: 2129.2332\n",
            "\n",
            "Epoch 209: Validation loss decreased (2129.233154 --> 2111.659912).\n",
            "\t Train_Loss: 1080.6288 Val_Loss: 2111.6599  BEST VAL Loss: 2111.6599\n",
            "\n",
            "Epoch 210: Validation loss decreased (2111.659912 --> 2094.246582).\n",
            "\t Train_Loss: 1068.6564 Val_Loss: 2094.2466  BEST VAL Loss: 2094.2466\n",
            "\n",
            "Epoch 211: Validation loss decreased (2094.246582 --> 2076.990723).\n",
            "\t Train_Loss: 1056.8171 Val_Loss: 2076.9907  BEST VAL Loss: 2076.9907\n",
            "\n",
            "Epoch 212: Validation loss decreased (2076.990723 --> 2059.892334).\n",
            "\t Train_Loss: 1045.1090 Val_Loss: 2059.8923  BEST VAL Loss: 2059.8923\n",
            "\n",
            "Epoch 213: Validation loss decreased (2059.892334 --> 2042.947266).\n",
            "\t Train_Loss: 1033.5310 Val_Loss: 2042.9473  BEST VAL Loss: 2042.9473\n",
            "\n",
            "Epoch 214: Validation loss decreased (2042.947266 --> 2026.156860).\n",
            "\t Train_Loss: 1022.0815 Val_Loss: 2026.1569  BEST VAL Loss: 2026.1569\n",
            "\n",
            "Epoch 215: Validation loss decreased (2026.156860 --> 2009.517944).\n",
            "\t Train_Loss: 1010.7600 Val_Loss: 2009.5179  BEST VAL Loss: 2009.5179\n",
            "\n",
            "Epoch 216: Validation loss decreased (2009.517944 --> 1993.030884).\n",
            "\t Train_Loss: 999.5648 Val_Loss: 1993.0309  BEST VAL Loss: 1993.0309\n",
            "\n",
            "Epoch 217: Validation loss decreased (1993.030884 --> 1976.692993).\n",
            "\t Train_Loss: 988.4943 Val_Loss: 1976.6930  BEST VAL Loss: 1976.6930\n",
            "\n",
            "Epoch 218: Validation loss decreased (1976.692993 --> 1960.502930).\n",
            "\t Train_Loss: 977.5481 Val_Loss: 1960.5029  BEST VAL Loss: 1960.5029\n",
            "\n",
            "Epoch 219: Validation loss decreased (1960.502930 --> 1944.460938).\n",
            "\t Train_Loss: 966.7244 Val_Loss: 1944.4609  BEST VAL Loss: 1944.4609\n",
            "\n",
            "Epoch 220: Validation loss decreased (1944.460938 --> 1928.564453).\n",
            "\t Train_Loss: 956.0222 Val_Loss: 1928.5645  BEST VAL Loss: 1928.5645\n",
            "\n",
            "Epoch 221: Validation loss decreased (1928.564453 --> 1912.813354).\n",
            "\t Train_Loss: 945.4402 Val_Loss: 1912.8134  BEST VAL Loss: 1912.8134\n",
            "\n",
            "Epoch 222: Validation loss decreased (1912.813354 --> 1897.205322).\n",
            "\t Train_Loss: 934.9779 Val_Loss: 1897.2053  BEST VAL Loss: 1897.2053\n",
            "\n",
            "Epoch 223: Validation loss decreased (1897.205322 --> 1881.738647).\n",
            "\t Train_Loss: 924.6332 Val_Loss: 1881.7386  BEST VAL Loss: 1881.7386\n",
            "\n",
            "Epoch 224: Validation loss decreased (1881.738647 --> 1866.414673).\n",
            "\t Train_Loss: 914.4052 Val_Loss: 1866.4147  BEST VAL Loss: 1866.4147\n",
            "\n",
            "Epoch 225: Validation loss decreased (1866.414673 --> 1851.228516).\n",
            "\t Train_Loss: 904.2933 Val_Loss: 1851.2285  BEST VAL Loss: 1851.2285\n",
            "\n",
            "Epoch 226: Validation loss decreased (1851.228516 --> 1836.182617).\n",
            "\t Train_Loss: 894.2960 Val_Loss: 1836.1826  BEST VAL Loss: 1836.1826\n",
            "\n",
            "Epoch 227: Validation loss decreased (1836.182617 --> 1821.272827).\n",
            "\t Train_Loss: 884.4120 Val_Loss: 1821.2728  BEST VAL Loss: 1821.2728\n",
            "\n",
            "Epoch 228: Validation loss decreased (1821.272827 --> 1806.500366).\n",
            "\t Train_Loss: 874.6405 Val_Loss: 1806.5004  BEST VAL Loss: 1806.5004\n",
            "\n",
            "Epoch 229: Validation loss decreased (1806.500366 --> 1791.862305).\n",
            "\t Train_Loss: 864.9805 Val_Loss: 1791.8623  BEST VAL Loss: 1791.8623\n",
            "\n",
            "Epoch 230: Validation loss decreased (1791.862305 --> 1777.358765).\n",
            "\t Train_Loss: 855.4307 Val_Loss: 1777.3588  BEST VAL Loss: 1777.3588\n",
            "\n",
            "Epoch 231: Validation loss decreased (1777.358765 --> 1762.987549).\n",
            "\t Train_Loss: 845.9901 Val_Loss: 1762.9875  BEST VAL Loss: 1762.9875\n",
            "\n",
            "Epoch 232: Validation loss decreased (1762.987549 --> 1748.747681).\n",
            "\t Train_Loss: 836.6576 Val_Loss: 1748.7477  BEST VAL Loss: 1748.7477\n",
            "\n",
            "Epoch 233: Validation loss decreased (1748.747681 --> 1734.639526).\n",
            "\t Train_Loss: 827.4321 Val_Loss: 1734.6395  BEST VAL Loss: 1734.6395\n",
            "\n",
            "Epoch 234: Validation loss decreased (1734.639526 --> 1720.660156).\n",
            "\t Train_Loss: 818.3128 Val_Loss: 1720.6602  BEST VAL Loss: 1720.6602\n",
            "\n",
            "Epoch 235: Validation loss decreased (1720.660156 --> 1706.809448).\n",
            "\t Train_Loss: 809.2983 Val_Loss: 1706.8094  BEST VAL Loss: 1706.8094\n",
            "\n",
            "Epoch 236: Validation loss decreased (1706.809448 --> 1693.085815).\n",
            "\t Train_Loss: 800.3882 Val_Loss: 1693.0858  BEST VAL Loss: 1693.0858\n",
            "\n",
            "Epoch 237: Validation loss decreased (1693.085815 --> 1679.487671).\n",
            "\t Train_Loss: 791.5808 Val_Loss: 1679.4877  BEST VAL Loss: 1679.4877\n",
            "\n",
            "Epoch 238: Validation loss decreased (1679.487671 --> 1666.015625).\n",
            "\t Train_Loss: 782.8751 Val_Loss: 1666.0156  BEST VAL Loss: 1666.0156\n",
            "\n",
            "Epoch 239: Validation loss decreased (1666.015625 --> 1652.667969).\n",
            "\t Train_Loss: 774.2709 Val_Loss: 1652.6680  BEST VAL Loss: 1652.6680\n",
            "\n",
            "Epoch 240: Validation loss decreased (1652.667969 --> 1639.442261).\n",
            "\t Train_Loss: 765.7667 Val_Loss: 1639.4423  BEST VAL Loss: 1639.4423\n",
            "\n",
            "Epoch 241: Validation loss decreased (1639.442261 --> 1626.340088).\n",
            "\t Train_Loss: 757.3611 Val_Loss: 1626.3401  BEST VAL Loss: 1626.3401\n",
            "\n",
            "Epoch 242: Validation loss decreased (1626.340088 --> 1613.357788).\n",
            "\t Train_Loss: 749.0535 Val_Loss: 1613.3578  BEST VAL Loss: 1613.3578\n",
            "\n",
            "Epoch 243: Validation loss decreased (1613.357788 --> 1600.494995).\n",
            "\t Train_Loss: 740.8432 Val_Loss: 1600.4950  BEST VAL Loss: 1600.4950\n",
            "\n",
            "Epoch 244: Validation loss decreased (1600.494995 --> 1587.751831).\n",
            "\t Train_Loss: 732.7294 Val_Loss: 1587.7518  BEST VAL Loss: 1587.7518\n",
            "\n",
            "Epoch 245: Validation loss decreased (1587.751831 --> 1575.126343).\n",
            "\t Train_Loss: 724.7101 Val_Loss: 1575.1263  BEST VAL Loss: 1575.1263\n",
            "\n",
            "Epoch 246: Validation loss decreased (1575.126343 --> 1562.617920).\n",
            "\t Train_Loss: 716.7855 Val_Loss: 1562.6179  BEST VAL Loss: 1562.6179\n",
            "\n",
            "Epoch 247: Validation loss decreased (1562.617920 --> 1550.225952).\n",
            "\t Train_Loss: 708.9539 Val_Loss: 1550.2260  BEST VAL Loss: 1550.2260\n",
            "\n",
            "Epoch 248: Validation loss decreased (1550.225952 --> 1537.948120).\n",
            "\t Train_Loss: 701.2153 Val_Loss: 1537.9481  BEST VAL Loss: 1537.9481\n",
            "\n",
            "Epoch 249: Validation loss decreased (1537.948120 --> 1525.784790).\n",
            "\t Train_Loss: 693.5676 Val_Loss: 1525.7848  BEST VAL Loss: 1525.7848\n",
            "\n",
            "Epoch 250: Validation loss decreased (1525.784790 --> 1513.733887).\n",
            "\t Train_Loss: 686.0107 Val_Loss: 1513.7339  BEST VAL Loss: 1513.7339\n",
            "\n",
            "Epoch 251: Validation loss decreased (1513.733887 --> 1501.796021).\n",
            "\t Train_Loss: 678.5433 Val_Loss: 1501.7960  BEST VAL Loss: 1501.7960\n",
            "\n",
            "Epoch 252: Validation loss decreased (1501.796021 --> 1489.968506).\n",
            "\t Train_Loss: 671.1651 Val_Loss: 1489.9685  BEST VAL Loss: 1489.9685\n",
            "\n",
            "Epoch 253: Validation loss decreased (1489.968506 --> 1478.251221).\n",
            "\t Train_Loss: 663.8745 Val_Loss: 1478.2512  BEST VAL Loss: 1478.2512\n",
            "\n",
            "Epoch 254: Validation loss decreased (1478.251221 --> 1466.643188).\n",
            "\t Train_Loss: 656.6708 Val_Loss: 1466.6432  BEST VAL Loss: 1466.6432\n",
            "\n",
            "Epoch 255: Validation loss decreased (1466.643188 --> 1455.144043).\n",
            "\t Train_Loss: 649.5538 Val_Loss: 1455.1440  BEST VAL Loss: 1455.1440\n",
            "\n",
            "Epoch 256: Validation loss decreased (1455.144043 --> 1443.751099).\n",
            "\t Train_Loss: 642.5216 Val_Loss: 1443.7511  BEST VAL Loss: 1443.7511\n",
            "\n",
            "Epoch 257: Validation loss decreased (1443.751099 --> 1432.465820).\n",
            "\t Train_Loss: 635.5738 Val_Loss: 1432.4658  BEST VAL Loss: 1432.4658\n",
            "\n",
            "Epoch 258: Validation loss decreased (1432.465820 --> 1421.285400).\n",
            "\t Train_Loss: 628.7095 Val_Loss: 1421.2854  BEST VAL Loss: 1421.2854\n",
            "\n",
            "Epoch 259: Validation loss decreased (1421.285400 --> 1410.209839).\n",
            "\t Train_Loss: 621.9283 Val_Loss: 1410.2098  BEST VAL Loss: 1410.2098\n",
            "\n",
            "Epoch 260: Validation loss decreased (1410.209839 --> 1399.238037).\n",
            "\t Train_Loss: 615.2289 Val_Loss: 1399.2380  BEST VAL Loss: 1399.2380\n",
            "\n",
            "Epoch 261: Validation loss decreased (1399.238037 --> 1388.369751).\n",
            "\t Train_Loss: 608.6107 Val_Loss: 1388.3698  BEST VAL Loss: 1388.3698\n",
            "\n",
            "Epoch 262: Validation loss decreased (1388.369751 --> 1377.602539).\n",
            "\t Train_Loss: 602.0729 Val_Loss: 1377.6025  BEST VAL Loss: 1377.6025\n",
            "\n",
            "Epoch 263: Validation loss decreased (1377.602539 --> 1366.936890).\n",
            "\t Train_Loss: 595.6141 Val_Loss: 1366.9369  BEST VAL Loss: 1366.9369\n",
            "\n",
            "Epoch 264: Validation loss decreased (1366.936890 --> 1356.370972).\n",
            "\t Train_Loss: 589.2344 Val_Loss: 1356.3710  BEST VAL Loss: 1356.3710\n",
            "\n",
            "Epoch 265: Validation loss decreased (1356.370972 --> 1345.904663).\n",
            "\t Train_Loss: 582.9321 Val_Loss: 1345.9047  BEST VAL Loss: 1345.9047\n",
            "\n",
            "Epoch 266: Validation loss decreased (1345.904663 --> 1335.536499).\n",
            "\t Train_Loss: 576.7071 Val_Loss: 1335.5365  BEST VAL Loss: 1335.5365\n",
            "\n",
            "Epoch 267: Validation loss decreased (1335.536499 --> 1325.266846).\n",
            "\t Train_Loss: 570.5580 Val_Loss: 1325.2668  BEST VAL Loss: 1325.2668\n",
            "\n",
            "Epoch 268: Validation loss decreased (1325.266846 --> 1315.094116).\n",
            "\t Train_Loss: 564.4849 Val_Loss: 1315.0941  BEST VAL Loss: 1315.0941\n",
            "\n",
            "Epoch 269: Validation loss decreased (1315.094116 --> 1305.016968).\n",
            "\t Train_Loss: 558.4863 Val_Loss: 1305.0170  BEST VAL Loss: 1305.0170\n",
            "\n",
            "Epoch 270: Validation loss decreased (1305.016968 --> 1295.035400).\n",
            "\t Train_Loss: 552.5618 Val_Loss: 1295.0354  BEST VAL Loss: 1295.0354\n",
            "\n",
            "Epoch 271: Validation loss decreased (1295.035400 --> 1285.147827).\n",
            "\t Train_Loss: 546.7102 Val_Loss: 1285.1478  BEST VAL Loss: 1285.1478\n",
            "\n",
            "Epoch 272: Validation loss decreased (1285.147827 --> 1275.354492).\n",
            "\t Train_Loss: 540.9309 Val_Loss: 1275.3545  BEST VAL Loss: 1275.3545\n",
            "\n",
            "Epoch 273: Validation loss decreased (1275.354492 --> 1265.652344).\n",
            "\t Train_Loss: 535.2235 Val_Loss: 1265.6523  BEST VAL Loss: 1265.6523\n",
            "\n",
            "Epoch 274: Validation loss decreased (1265.652344 --> 1256.043213).\n",
            "\t Train_Loss: 529.5869 Val_Loss: 1256.0432  BEST VAL Loss: 1256.0432\n",
            "\n",
            "Epoch 275: Validation loss decreased (1256.043213 --> 1246.526001).\n",
            "\t Train_Loss: 524.0205 Val_Loss: 1246.5260  BEST VAL Loss: 1246.5260\n",
            "\n",
            "Epoch 276: Validation loss decreased (1246.526001 --> 1237.097900).\n",
            "\t Train_Loss: 518.5240 Val_Loss: 1237.0979  BEST VAL Loss: 1237.0979\n",
            "\n",
            "Epoch 277: Validation loss decreased (1237.097900 --> 1227.759766).\n",
            "\t Train_Loss: 513.0956 Val_Loss: 1227.7598  BEST VAL Loss: 1227.7598\n",
            "\n",
            "Epoch 278: Validation loss decreased (1227.759766 --> 1218.510620).\n",
            "\t Train_Loss: 507.7355 Val_Loss: 1218.5106  BEST VAL Loss: 1218.5106\n",
            "\n",
            "Epoch 279: Validation loss decreased (1218.510620 --> 1209.349121).\n",
            "\t Train_Loss: 502.4424 Val_Loss: 1209.3491  BEST VAL Loss: 1209.3491\n",
            "\n",
            "Epoch 280: Validation loss decreased (1209.349121 --> 1200.275024).\n",
            "\t Train_Loss: 497.2163 Val_Loss: 1200.2750  BEST VAL Loss: 1200.2750\n",
            "\n",
            "Epoch 281: Validation loss decreased (1200.275024 --> 1191.287476).\n",
            "\t Train_Loss: 492.0557 Val_Loss: 1191.2875  BEST VAL Loss: 1191.2875\n",
            "\n",
            "Epoch 282: Validation loss decreased (1191.287476 --> 1182.386108).\n",
            "\t Train_Loss: 486.9604 Val_Loss: 1182.3861  BEST VAL Loss: 1182.3861\n",
            "\n",
            "Epoch 283: Validation loss decreased (1182.386108 --> 1173.568726).\n",
            "\t Train_Loss: 481.9296 Val_Loss: 1173.5687  BEST VAL Loss: 1173.5687\n",
            "\n",
            "Epoch 284: Validation loss decreased (1173.568726 --> 1164.836792).\n",
            "\t Train_Loss: 476.9626 Val_Loss: 1164.8368  BEST VAL Loss: 1164.8368\n",
            "\n",
            "Epoch 285: Validation loss decreased (1164.836792 --> 1156.187744).\n",
            "\t Train_Loss: 472.0586 Val_Loss: 1156.1877  BEST VAL Loss: 1156.1877\n",
            "\n",
            "Epoch 286: Validation loss decreased (1156.187744 --> 1147.621948).\n",
            "\t Train_Loss: 467.2172 Val_Loss: 1147.6219  BEST VAL Loss: 1147.6219\n",
            "\n",
            "Epoch 287: Validation loss decreased (1147.621948 --> 1139.137695).\n",
            "\t Train_Loss: 462.4372 Val_Loss: 1139.1377  BEST VAL Loss: 1139.1377\n",
            "\n",
            "Epoch 288: Validation loss decreased (1139.137695 --> 1130.734497).\n",
            "\t Train_Loss: 457.7181 Val_Loss: 1130.7345  BEST VAL Loss: 1130.7345\n",
            "\n",
            "Epoch 289: Validation loss decreased (1130.734497 --> 1122.411743).\n",
            "\t Train_Loss: 453.0595 Val_Loss: 1122.4117  BEST VAL Loss: 1122.4117\n",
            "\n",
            "Epoch 290: Validation loss decreased (1122.411743 --> 1114.169678).\n",
            "\t Train_Loss: 448.4603 Val_Loss: 1114.1697  BEST VAL Loss: 1114.1697\n",
            "\n",
            "Epoch 291: Validation loss decreased (1114.169678 --> 1106.005493).\n",
            "\t Train_Loss: 443.9199 Val_Loss: 1106.0055  BEST VAL Loss: 1106.0055\n",
            "\n",
            "Epoch 292: Validation loss decreased (1106.005493 --> 1097.919189).\n",
            "\t Train_Loss: 439.4373 Val_Loss: 1097.9192  BEST VAL Loss: 1097.9192\n",
            "\n",
            "Epoch 293: Validation loss decreased (1097.919189 --> 1089.910278).\n",
            "\t Train_Loss: 435.0116 Val_Loss: 1089.9103  BEST VAL Loss: 1089.9103\n",
            "\n",
            "Epoch 294: Validation loss decreased (1089.910278 --> 1081.976440).\n",
            "\t Train_Loss: 430.6411 Val_Loss: 1081.9764  BEST VAL Loss: 1081.9764\n",
            "\n",
            "Epoch 295: Validation loss decreased (1081.976440 --> 1074.115845).\n",
            "\t Train_Loss: 426.3239 Val_Loss: 1074.1158  BEST VAL Loss: 1074.1158\n",
            "\n",
            "Epoch 296: Validation loss decreased (1074.115845 --> 1066.325562).\n",
            "\t Train_Loss: 422.0567 Val_Loss: 1066.3256  BEST VAL Loss: 1066.3256\n",
            "\n",
            "Epoch 297: Validation loss decreased (1066.325562 --> 1058.598267).\n",
            "\t Train_Loss: 417.8355 Val_Loss: 1058.5983  BEST VAL Loss: 1058.5983\n",
            "\n",
            "Epoch 298: Validation loss decreased (1058.598267 --> 1050.921875).\n",
            "\t Train_Loss: 413.6516 Val_Loss: 1050.9219  BEST VAL Loss: 1050.9219\n",
            "\n",
            "Epoch 299: Validation loss decreased (1050.921875 --> 1043.269409).\n",
            "\t Train_Loss: 409.4945 Val_Loss: 1043.2694  BEST VAL Loss: 1043.2694\n",
            "\n",
            "Epoch 300: Validation loss decreased (1043.269409 --> 1035.583252).\n",
            "\t Train_Loss: 405.3453 Val_Loss: 1035.5833  BEST VAL Loss: 1035.5833\n",
            "\n",
            "Epoch 301: Validation loss decreased (1035.583252 --> 1027.739624).\n",
            "\t Train_Loss: 401.1725 Val_Loss: 1027.7396  BEST VAL Loss: 1027.7396\n",
            "\n",
            "Epoch 302: Validation loss decreased (1027.739624 --> 1019.363098).\n",
            "\t Train_Loss: 396.9184 Val_Loss: 1019.3631  BEST VAL Loss: 1019.3631\n",
            "\n",
            "Epoch 303: Validation loss decreased (1019.363098 --> 1009.705872).\n",
            "\t Train_Loss: 392.4315 Val_Loss: 1009.7059  BEST VAL Loss: 1009.7059\n",
            "\n",
            "Epoch 304: Validation loss decreased (1009.705872 --> 1001.592224).\n",
            "\t Train_Loss: 387.7211 Val_Loss: 1001.5922  BEST VAL Loss: 1001.5922\n",
            "\n",
            "Epoch 305: Validation loss decreased (1001.592224 --> 993.539734).\n",
            "\t Train_Loss: 383.5283 Val_Loss: 993.5397  BEST VAL Loss: 993.5397\n",
            "\n",
            "Epoch 306: Validation loss decreased (993.539734 --> 985.443848).\n",
            "\t Train_Loss: 379.3304 Val_Loss: 985.4438  BEST VAL Loss: 985.4438\n",
            "\n",
            "Epoch 307: Validation loss decreased (985.443848 --> 977.335266).\n",
            "\t Train_Loss: 375.1277 Val_Loss: 977.3353  BEST VAL Loss: 977.3353\n",
            "\n",
            "Epoch 308: Validation loss decreased (977.335266 --> 969.234009).\n",
            "\t Train_Loss: 370.9355 Val_Loss: 969.2340  BEST VAL Loss: 969.2340\n",
            "\n",
            "Epoch 309: Validation loss decreased (969.234009 --> 961.160339).\n",
            "\t Train_Loss: 366.7665 Val_Loss: 961.1603  BEST VAL Loss: 961.1603\n",
            "\n",
            "Epoch 310: Validation loss decreased (961.160339 --> 953.126587).\n",
            "\t Train_Loss: 362.6292 Val_Loss: 953.1266  BEST VAL Loss: 953.1266\n",
            "\n",
            "Epoch 311: Validation loss decreased (953.126587 --> 945.144348).\n",
            "\t Train_Loss: 358.5313 Val_Loss: 945.1443  BEST VAL Loss: 945.1443\n",
            "\n",
            "Epoch 312: Validation loss decreased (945.144348 --> 937.221863).\n",
            "\t Train_Loss: 354.4776 Val_Loss: 937.2219  BEST VAL Loss: 937.2219\n",
            "\n",
            "Epoch 313: Validation loss decreased (937.221863 --> 929.366211).\n",
            "\t Train_Loss: 350.4731 Val_Loss: 929.3662  BEST VAL Loss: 929.3662\n",
            "\n",
            "Epoch 314: Validation loss decreased (929.366211 --> 921.583130).\n",
            "\t Train_Loss: 346.5206 Val_Loss: 921.5831  BEST VAL Loss: 921.5831\n",
            "\n",
            "Epoch 315: Validation loss decreased (921.583130 --> 913.877258).\n",
            "\t Train_Loss: 342.6230 Val_Loss: 913.8773  BEST VAL Loss: 913.8773\n",
            "\n",
            "Epoch 316: Validation loss decreased (913.877258 --> 906.251953).\n",
            "\t Train_Loss: 338.7824 Val_Loss: 906.2520  BEST VAL Loss: 906.2520\n",
            "\n",
            "Epoch 317: Validation loss decreased (906.251953 --> 898.709106).\n",
            "\t Train_Loss: 335.0000 Val_Loss: 898.7091  BEST VAL Loss: 898.7091\n",
            "\n",
            "Epoch 318: Validation loss decreased (898.709106 --> 891.251587).\n",
            "\t Train_Loss: 331.2765 Val_Loss: 891.2516  BEST VAL Loss: 891.2516\n",
            "\n",
            "Epoch 319: Validation loss decreased (891.251587 --> 883.881287).\n",
            "\t Train_Loss: 327.6131 Val_Loss: 883.8813  BEST VAL Loss: 883.8813\n",
            "\n",
            "Epoch 320: Validation loss decreased (883.881287 --> 876.598938).\n",
            "\t Train_Loss: 324.0101 Val_Loss: 876.5989  BEST VAL Loss: 876.5989\n",
            "\n",
            "Epoch 321: Validation loss decreased (876.598938 --> 869.403931).\n",
            "\t Train_Loss: 320.4675 Val_Loss: 869.4039  BEST VAL Loss: 869.4039\n",
            "\n",
            "Epoch 322: Validation loss decreased (869.403931 --> 862.299438).\n",
            "\t Train_Loss: 316.9853 Val_Loss: 862.2994  BEST VAL Loss: 862.2994\n",
            "\n",
            "Epoch 323: Validation loss decreased (862.299438 --> 855.283203).\n",
            "\t Train_Loss: 313.5634 Val_Loss: 855.2832  BEST VAL Loss: 855.2832\n",
            "\n",
            "Epoch 324: Validation loss decreased (855.283203 --> 848.356262).\n",
            "\t Train_Loss: 310.2011 Val_Loss: 848.3563  BEST VAL Loss: 848.3563\n",
            "\n",
            "Epoch 325: Validation loss decreased (848.356262 --> 841.518005).\n",
            "\t Train_Loss: 306.8987 Val_Loss: 841.5180  BEST VAL Loss: 841.5180\n",
            "\n",
            "Epoch 326: Validation loss decreased (841.518005 --> 834.768005).\n",
            "\t Train_Loss: 303.6547 Val_Loss: 834.7680  BEST VAL Loss: 834.7680\n",
            "\n",
            "Epoch 327: Validation loss decreased (834.768005 --> 828.106262).\n",
            "\t Train_Loss: 300.4691 Val_Loss: 828.1063  BEST VAL Loss: 828.1063\n",
            "\n",
            "Epoch 328: Validation loss decreased (828.106262 --> 821.531250).\n",
            "\t Train_Loss: 297.3412 Val_Loss: 821.5312  BEST VAL Loss: 821.5312\n",
            "\n",
            "Epoch 329: Validation loss decreased (821.531250 --> 815.042297).\n",
            "\t Train_Loss: 294.2702 Val_Loss: 815.0423  BEST VAL Loss: 815.0423\n",
            "\n",
            "Epoch 330: Validation loss decreased (815.042297 --> 808.639465).\n",
            "\t Train_Loss: 291.2553 Val_Loss: 808.6395  BEST VAL Loss: 808.6395\n",
            "\n",
            "Epoch 331: Validation loss decreased (808.639465 --> 802.321716).\n",
            "\t Train_Loss: 288.2958 Val_Loss: 802.3217  BEST VAL Loss: 802.3217\n",
            "\n",
            "Epoch 332: Validation loss decreased (802.321716 --> 796.087402).\n",
            "\t Train_Loss: 285.3907 Val_Loss: 796.0874  BEST VAL Loss: 796.0874\n",
            "\n",
            "Epoch 333: Validation loss decreased (796.087402 --> 789.934570).\n",
            "\t Train_Loss: 282.5390 Val_Loss: 789.9346  BEST VAL Loss: 789.9346\n",
            "\n",
            "Epoch 334: Validation loss decreased (789.934570 --> 783.864502).\n",
            "\t Train_Loss: 279.7404 Val_Loss: 783.8645  BEST VAL Loss: 783.8645\n",
            "\n",
            "Epoch 335: Validation loss decreased (783.864502 --> 777.874939).\n",
            "\t Train_Loss: 276.9935 Val_Loss: 777.8749  BEST VAL Loss: 777.8749\n",
            "\n",
            "Epoch 336: Validation loss decreased (777.874939 --> 771.965454).\n",
            "\t Train_Loss: 274.2980 Val_Loss: 771.9655  BEST VAL Loss: 771.9655\n",
            "\n",
            "Epoch 337: Validation loss decreased (771.965454 --> 766.133789).\n",
            "\t Train_Loss: 271.6525 Val_Loss: 766.1338  BEST VAL Loss: 766.1338\n",
            "\n",
            "Epoch 338: Validation loss decreased (766.133789 --> 760.379761).\n",
            "\t Train_Loss: 269.0562 Val_Loss: 760.3798  BEST VAL Loss: 760.3798\n",
            "\n",
            "Epoch 339: Validation loss decreased (760.379761 --> 754.702271).\n",
            "\t Train_Loss: 266.5086 Val_Loss: 754.7023  BEST VAL Loss: 754.7023\n",
            "\n",
            "Epoch 340: Validation loss decreased (754.702271 --> 749.100525).\n",
            "\t Train_Loss: 264.0083 Val_Loss: 749.1005  BEST VAL Loss: 749.1005\n",
            "\n",
            "Epoch 341: Validation loss decreased (749.100525 --> 743.572693).\n",
            "\t Train_Loss: 261.5550 Val_Loss: 743.5727  BEST VAL Loss: 743.5727\n",
            "\n",
            "Epoch 342: Validation loss decreased (743.572693 --> 738.117554).\n",
            "\t Train_Loss: 259.1479 Val_Loss: 738.1176  BEST VAL Loss: 738.1176\n",
            "\n",
            "Epoch 343: Validation loss decreased (738.117554 --> 732.735474).\n",
            "\t Train_Loss: 256.7856 Val_Loss: 732.7355  BEST VAL Loss: 732.7355\n",
            "\n",
            "Epoch 344: Validation loss decreased (732.735474 --> 727.424011).\n",
            "\t Train_Loss: 254.4674 Val_Loss: 727.4240  BEST VAL Loss: 727.4240\n",
            "\n",
            "Epoch 345: Validation loss decreased (727.424011 --> 722.181641).\n",
            "\t Train_Loss: 252.1929 Val_Loss: 722.1816  BEST VAL Loss: 722.1816\n",
            "\n",
            "Epoch 346: Validation loss decreased (722.181641 --> 717.009521).\n",
            "\t Train_Loss: 249.9611 Val_Loss: 717.0095  BEST VAL Loss: 717.0095\n",
            "\n",
            "Epoch 347: Validation loss decreased (717.009521 --> 711.904602).\n",
            "\t Train_Loss: 247.7711 Val_Loss: 711.9046  BEST VAL Loss: 711.9046\n",
            "\n",
            "Epoch 348: Validation loss decreased (711.904602 --> 706.867065).\n",
            "\t Train_Loss: 245.6223 Val_Loss: 706.8671  BEST VAL Loss: 706.8671\n",
            "\n",
            "Epoch 349: Validation loss decreased (706.867065 --> 701.894470).\n",
            "\t Train_Loss: 243.5139 Val_Loss: 701.8945  BEST VAL Loss: 701.8945\n",
            "\n",
            "Epoch 350: Validation loss decreased (701.894470 --> 696.987610).\n",
            "\t Train_Loss: 241.4449 Val_Loss: 696.9876  BEST VAL Loss: 696.9876\n",
            "\n",
            "Epoch 351: Validation loss decreased (696.987610 --> 692.144836).\n",
            "\t Train_Loss: 239.4148 Val_Loss: 692.1448  BEST VAL Loss: 692.1448\n",
            "\n",
            "Epoch 352: Validation loss decreased (692.144836 --> 687.365906).\n",
            "\t Train_Loss: 237.4230 Val_Loss: 687.3659  BEST VAL Loss: 687.3659\n",
            "\n",
            "Epoch 353: Validation loss decreased (687.365906 --> 682.647766).\n",
            "\t Train_Loss: 235.4687 Val_Loss: 682.6478  BEST VAL Loss: 682.6478\n",
            "\n",
            "Epoch 354: Validation loss decreased (682.647766 --> 677.991028).\n",
            "\t Train_Loss: 233.5509 Val_Loss: 677.9910  BEST VAL Loss: 677.9910\n",
            "\n",
            "Epoch 355: Validation loss decreased (677.991028 --> 673.394836).\n",
            "\t Train_Loss: 231.6695 Val_Loss: 673.3948  BEST VAL Loss: 673.3948\n",
            "\n",
            "Epoch 356: Validation loss decreased (673.394836 --> 668.858215).\n",
            "\t Train_Loss: 229.8234 Val_Loss: 668.8582  BEST VAL Loss: 668.8582\n",
            "\n",
            "Epoch 357: Validation loss decreased (668.858215 --> 664.380066).\n",
            "\t Train_Loss: 228.0119 Val_Loss: 664.3801  BEST VAL Loss: 664.3801\n",
            "\n",
            "Epoch 358: Validation loss decreased (664.380066 --> 659.959412).\n",
            "\t Train_Loss: 226.2347 Val_Loss: 659.9594  BEST VAL Loss: 659.9594\n",
            "\n",
            "Epoch 359: Validation loss decreased (659.959412 --> 655.595276).\n",
            "\t Train_Loss: 224.4907 Val_Loss: 655.5953  BEST VAL Loss: 655.5953\n",
            "\n",
            "Epoch 360: Validation loss decreased (655.595276 --> 651.287231).\n",
            "\t Train_Loss: 222.7800 Val_Loss: 651.2872  BEST VAL Loss: 651.2872\n",
            "\n",
            "Epoch 361: Validation loss decreased (651.287231 --> 647.034607).\n",
            "\t Train_Loss: 221.1009 Val_Loss: 647.0346  BEST VAL Loss: 647.0346\n",
            "\n",
            "Epoch 362: Validation loss decreased (647.034607 --> 642.836731).\n",
            "\t Train_Loss: 219.4540 Val_Loss: 642.8367  BEST VAL Loss: 642.8367\n",
            "\n",
            "Epoch 363: Validation loss decreased (642.836731 --> 638.691833).\n",
            "\t Train_Loss: 217.8382 Val_Loss: 638.6918  BEST VAL Loss: 638.6918\n",
            "\n",
            "Epoch 364: Validation loss decreased (638.691833 --> 634.600037).\n",
            "\t Train_Loss: 216.2526 Val_Loss: 634.6000  BEST VAL Loss: 634.6000\n",
            "\n",
            "Epoch 365: Validation loss decreased (634.600037 --> 630.560059).\n",
            "\t Train_Loss: 214.6971 Val_Loss: 630.5601  BEST VAL Loss: 630.5601\n",
            "\n",
            "Epoch 366: Validation loss decreased (630.560059 --> 626.571960).\n",
            "\t Train_Loss: 213.1711 Val_Loss: 626.5720  BEST VAL Loss: 626.5720\n",
            "\n",
            "Epoch 367: Validation loss decreased (626.571960 --> 622.633423).\n",
            "\t Train_Loss: 211.6738 Val_Loss: 622.6334  BEST VAL Loss: 622.6334\n",
            "\n",
            "Epoch 368: Validation loss decreased (622.633423 --> 618.745728).\n",
            "\t Train_Loss: 210.2048 Val_Loss: 618.7457  BEST VAL Loss: 618.7457\n",
            "\n",
            "Epoch 369: Validation loss decreased (618.745728 --> 614.906860).\n",
            "\t Train_Loss: 208.7636 Val_Loss: 614.9069  BEST VAL Loss: 614.9069\n",
            "\n",
            "Epoch 370: Validation loss decreased (614.906860 --> 611.116028).\n",
            "\t Train_Loss: 207.3499 Val_Loss: 611.1160  BEST VAL Loss: 611.1160\n",
            "\n",
            "Epoch 371: Validation loss decreased (611.116028 --> 607.374268).\n",
            "\t Train_Loss: 205.9628 Val_Loss: 607.3743  BEST VAL Loss: 607.3743\n",
            "\n",
            "Epoch 372: Validation loss decreased (607.374268 --> 603.678406).\n",
            "\t Train_Loss: 204.6022 Val_Loss: 603.6784  BEST VAL Loss: 603.6784\n",
            "\n",
            "Epoch 373: Validation loss decreased (603.678406 --> 600.029419).\n",
            "\t Train_Loss: 203.2672 Val_Loss: 600.0294  BEST VAL Loss: 600.0294\n",
            "\n",
            "Epoch 374: Validation loss decreased (600.029419 --> 596.426392).\n",
            "\t Train_Loss: 201.9576 Val_Loss: 596.4264  BEST VAL Loss: 596.4264\n",
            "\n",
            "Epoch 375: Validation loss decreased (596.426392 --> 592.868591).\n",
            "\t Train_Loss: 200.6730 Val_Loss: 592.8686  BEST VAL Loss: 592.8686\n",
            "\n",
            "Epoch 376: Validation loss decreased (592.868591 --> 589.355164).\n",
            "\t Train_Loss: 199.4127 Val_Loss: 589.3552  BEST VAL Loss: 589.3552\n",
            "\n",
            "Epoch 377: Validation loss decreased (589.355164 --> 585.884888).\n",
            "\t Train_Loss: 198.1765 Val_Loss: 585.8849  BEST VAL Loss: 585.8849\n",
            "\n",
            "Epoch 378: Validation loss decreased (585.884888 --> 582.459167).\n",
            "\t Train_Loss: 196.9637 Val_Loss: 582.4592  BEST VAL Loss: 582.4592\n",
            "\n",
            "Epoch 379: Validation loss decreased (582.459167 --> 579.075989).\n",
            "\t Train_Loss: 195.7742 Val_Loss: 579.0760  BEST VAL Loss: 579.0760\n",
            "\n",
            "Epoch 380: Validation loss decreased (579.075989 --> 575.734558).\n",
            "\t Train_Loss: 194.6074 Val_Loss: 575.7346  BEST VAL Loss: 575.7346\n",
            "\n",
            "Epoch 381: Validation loss decreased (575.734558 --> 572.434692).\n",
            "\t Train_Loss: 193.4628 Val_Loss: 572.4347  BEST VAL Loss: 572.4347\n",
            "\n",
            "Epoch 382: Validation loss decreased (572.434692 --> 569.175781).\n",
            "\t Train_Loss: 192.3400 Val_Loss: 569.1758  BEST VAL Loss: 569.1758\n",
            "\n",
            "Epoch 383: Validation loss decreased (569.175781 --> 565.958191).\n",
            "\t Train_Loss: 191.2388 Val_Loss: 565.9582  BEST VAL Loss: 565.9582\n",
            "\n",
            "Epoch 384: Validation loss decreased (565.958191 --> 562.779419).\n",
            "\t Train_Loss: 190.1588 Val_Loss: 562.7794  BEST VAL Loss: 562.7794\n",
            "\n",
            "Epoch 385: Validation loss decreased (562.779419 --> 559.640808).\n",
            "\t Train_Loss: 189.0995 Val_Loss: 559.6408  BEST VAL Loss: 559.6408\n",
            "\n",
            "Epoch 386: Validation loss decreased (559.640808 --> 556.540710).\n",
            "\t Train_Loss: 188.0603 Val_Loss: 556.5407  BEST VAL Loss: 556.5407\n",
            "\n",
            "Epoch 387: Validation loss decreased (556.540710 --> 553.479919).\n",
            "\t Train_Loss: 187.0413 Val_Loss: 553.4799  BEST VAL Loss: 553.4799\n",
            "\n",
            "Epoch 388: Validation loss decreased (553.479919 --> 550.455627).\n",
            "\t Train_Loss: 186.0419 Val_Loss: 550.4556  BEST VAL Loss: 550.4556\n",
            "\n",
            "Epoch 389: Validation loss decreased (550.455627 --> 547.469238).\n",
            "\t Train_Loss: 185.0617 Val_Loss: 547.4692  BEST VAL Loss: 547.4692\n",
            "\n",
            "Epoch 390: Validation loss decreased (547.469238 --> 544.520020).\n",
            "\t Train_Loss: 184.1003 Val_Loss: 544.5200  BEST VAL Loss: 544.5200\n",
            "\n",
            "Epoch 391: Validation loss decreased (544.520020 --> 541.606506).\n",
            "\t Train_Loss: 183.1575 Val_Loss: 541.6065  BEST VAL Loss: 541.6065\n",
            "\n",
            "Epoch 392: Validation loss decreased (541.606506 --> 538.729126).\n",
            "\t Train_Loss: 182.2330 Val_Loss: 538.7291  BEST VAL Loss: 538.7291\n",
            "\n",
            "Epoch 393: Validation loss decreased (538.729126 --> 535.887512).\n",
            "\t Train_Loss: 181.3262 Val_Loss: 535.8875  BEST VAL Loss: 535.8875\n",
            "\n",
            "Epoch 394: Validation loss decreased (535.887512 --> 533.080200).\n",
            "\t Train_Loss: 180.4371 Val_Loss: 533.0802  BEST VAL Loss: 533.0802\n",
            "\n",
            "Epoch 395: Validation loss decreased (533.080200 --> 530.307617).\n",
            "\t Train_Loss: 179.5653 Val_Loss: 530.3076  BEST VAL Loss: 530.3076\n",
            "\n",
            "Epoch 396: Validation loss decreased (530.307617 --> 527.568787).\n",
            "\t Train_Loss: 178.7104 Val_Loss: 527.5688  BEST VAL Loss: 527.5688\n",
            "\n",
            "Epoch 397: Validation loss decreased (527.568787 --> 524.864197).\n",
            "\t Train_Loss: 177.8721 Val_Loss: 524.8642  BEST VAL Loss: 524.8642\n",
            "\n",
            "Epoch 398: Validation loss decreased (524.864197 --> 522.192139).\n",
            "\t Train_Loss: 177.0500 Val_Loss: 522.1921  BEST VAL Loss: 522.1921\n",
            "\n",
            "Epoch 399: Validation loss decreased (522.192139 --> 519.553345).\n",
            "\t Train_Loss: 176.2441 Val_Loss: 519.5533  BEST VAL Loss: 519.5533\n",
            "\n",
            "Epoch 400: Validation loss decreased (519.553345 --> 516.946716).\n",
            "\t Train_Loss: 175.4537 Val_Loss: 516.9467  BEST VAL Loss: 516.9467\n",
            "\n",
            "Epoch 401: Validation loss decreased (516.946716 --> 514.371765).\n",
            "\t Train_Loss: 174.6791 Val_Loss: 514.3718  BEST VAL Loss: 514.3718\n",
            "\n",
            "Epoch 402: Validation loss decreased (514.371765 --> 511.828339).\n",
            "\t Train_Loss: 173.9193 Val_Loss: 511.8283  BEST VAL Loss: 511.8283\n",
            "\n",
            "Epoch 403: Validation loss decreased (511.828339 --> 509.316132).\n",
            "\t Train_Loss: 173.1744 Val_Loss: 509.3161  BEST VAL Loss: 509.3161\n",
            "\n",
            "Epoch 404: Validation loss decreased (509.316132 --> 506.835022).\n",
            "\t Train_Loss: 172.4442 Val_Loss: 506.8350  BEST VAL Loss: 506.8350\n",
            "\n",
            "Epoch 405: Validation loss decreased (506.835022 --> 504.383453).\n",
            "\t Train_Loss: 171.7284 Val_Loss: 504.3835  BEST VAL Loss: 504.3835\n",
            "\n",
            "Epoch 406: Validation loss decreased (504.383453 --> 501.961914).\n",
            "\t Train_Loss: 171.0266 Val_Loss: 501.9619  BEST VAL Loss: 501.9619\n",
            "\n",
            "Epoch 407: Validation loss decreased (501.961914 --> 499.570221).\n",
            "\t Train_Loss: 170.3388 Val_Loss: 499.5702  BEST VAL Loss: 499.5702\n",
            "\n",
            "Epoch 408: Validation loss decreased (499.570221 --> 497.207184).\n",
            "\t Train_Loss: 169.6644 Val_Loss: 497.2072  BEST VAL Loss: 497.2072\n",
            "\n",
            "Epoch 409: Validation loss decreased (497.207184 --> 494.873291).\n",
            "\t Train_Loss: 169.0032 Val_Loss: 494.8733  BEST VAL Loss: 494.8733\n",
            "\n",
            "Epoch 410: Validation loss decreased (494.873291 --> 492.567688).\n",
            "\t Train_Loss: 168.3552 Val_Loss: 492.5677  BEST VAL Loss: 492.5677\n",
            "\n",
            "Epoch 411: Validation loss decreased (492.567688 --> 490.290527).\n",
            "\t Train_Loss: 167.7200 Val_Loss: 490.2905  BEST VAL Loss: 490.2905\n",
            "\n",
            "Epoch 412: Validation loss decreased (490.290527 --> 488.040619).\n",
            "\t Train_Loss: 167.0975 Val_Loss: 488.0406  BEST VAL Loss: 488.0406\n",
            "\n",
            "Epoch 413: Validation loss decreased (488.040619 --> 485.818268).\n",
            "\t Train_Loss: 166.4872 Val_Loss: 485.8183  BEST VAL Loss: 485.8183\n",
            "\n",
            "Epoch 414: Validation loss decreased (485.818268 --> 483.622803).\n",
            "\t Train_Loss: 165.8892 Val_Loss: 483.6228  BEST VAL Loss: 483.6228\n",
            "\n",
            "Epoch 415: Validation loss decreased (483.622803 --> 481.453827).\n",
            "\t Train_Loss: 165.3030 Val_Loss: 481.4538  BEST VAL Loss: 481.4538\n",
            "\n",
            "Epoch 416: Validation loss decreased (481.453827 --> 479.311249).\n",
            "\t Train_Loss: 164.7284 Val_Loss: 479.3112  BEST VAL Loss: 479.3112\n",
            "\n",
            "Epoch 417: Validation loss decreased (479.311249 --> 477.194794).\n",
            "\t Train_Loss: 164.1653 Val_Loss: 477.1948  BEST VAL Loss: 477.1948\n",
            "\n",
            "Epoch 418: Validation loss decreased (477.194794 --> 475.103973).\n",
            "\t Train_Loss: 163.6137 Val_Loss: 475.1040  BEST VAL Loss: 475.1040\n",
            "\n",
            "Epoch 419: Validation loss decreased (475.103973 --> 473.038147).\n",
            "\t Train_Loss: 163.0730 Val_Loss: 473.0381  BEST VAL Loss: 473.0381\n",
            "\n",
            "Epoch 420: Validation loss decreased (473.038147 --> 470.998199).\n",
            "\t Train_Loss: 162.5430 Val_Loss: 470.9982  BEST VAL Loss: 470.9982\n",
            "\n",
            "Epoch 421: Validation loss decreased (470.998199 --> 468.982025).\n",
            "\t Train_Loss: 162.0238 Val_Loss: 468.9820  BEST VAL Loss: 468.9820\n",
            "\n",
            "Epoch 422: Validation loss decreased (468.982025 --> 466.990875).\n",
            "\t Train_Loss: 161.5149 Val_Loss: 466.9909  BEST VAL Loss: 466.9909\n",
            "\n",
            "Epoch 423: Validation loss decreased (466.990875 --> 465.023743).\n",
            "\t Train_Loss: 161.0163 Val_Loss: 465.0237  BEST VAL Loss: 465.0237\n",
            "\n",
            "Epoch 424: Validation loss decreased (465.023743 --> 463.080780).\n",
            "\t Train_Loss: 160.5278 Val_Loss: 463.0808  BEST VAL Loss: 463.0808\n",
            "\n",
            "Epoch 425: Validation loss decreased (463.080780 --> 461.160065).\n",
            "\t Train_Loss: 160.0492 Val_Loss: 461.1601  BEST VAL Loss: 461.1601\n",
            "\n",
            "Epoch 426: Validation loss decreased (461.160065 --> 459.263733).\n",
            "\t Train_Loss: 159.5803 Val_Loss: 459.2637  BEST VAL Loss: 459.2637\n",
            "\n",
            "Epoch 427: Validation loss decreased (459.263733 --> 457.389709).\n",
            "\t Train_Loss: 159.1208 Val_Loss: 457.3897  BEST VAL Loss: 457.3897\n",
            "\n",
            "Epoch 428: Validation loss decreased (457.389709 --> 455.539276).\n",
            "\t Train_Loss: 158.6706 Val_Loss: 455.5393  BEST VAL Loss: 455.5393\n",
            "\n",
            "Epoch 429: Validation loss decreased (455.539276 --> 453.710175).\n",
            "\t Train_Loss: 158.2296 Val_Loss: 453.7102  BEST VAL Loss: 453.7102\n",
            "\n",
            "Epoch 430: Validation loss decreased (453.710175 --> 451.904114).\n",
            "\t Train_Loss: 157.7977 Val_Loss: 451.9041  BEST VAL Loss: 451.9041\n",
            "\n",
            "Epoch 431: Validation loss decreased (451.904114 --> 450.118896).\n",
            "\t Train_Loss: 157.3745 Val_Loss: 450.1189  BEST VAL Loss: 450.1189\n",
            "\n",
            "Epoch 432: Validation loss decreased (450.118896 --> 448.355865).\n",
            "\t Train_Loss: 156.9599 Val_Loss: 448.3559  BEST VAL Loss: 448.3559\n",
            "\n",
            "Epoch 433: Validation loss decreased (448.355865 --> 446.613983).\n",
            "\t Train_Loss: 156.5538 Val_Loss: 446.6140  BEST VAL Loss: 446.6140\n",
            "\n",
            "Epoch 434: Validation loss decreased (446.613983 --> 444.893158).\n",
            "\t Train_Loss: 156.1561 Val_Loss: 444.8932  BEST VAL Loss: 444.8932\n",
            "\n",
            "Epoch 435: Validation loss decreased (444.893158 --> 443.192963).\n",
            "\t Train_Loss: 155.7664 Val_Loss: 443.1930  BEST VAL Loss: 443.1930\n",
            "\n",
            "Epoch 436: Validation loss decreased (443.192963 --> 441.513580).\n",
            "\t Train_Loss: 155.3848 Val_Loss: 441.5136  BEST VAL Loss: 441.5136\n",
            "\n",
            "Epoch 437: Validation loss decreased (441.513580 --> 439.854248).\n",
            "\t Train_Loss: 155.0111 Val_Loss: 439.8542  BEST VAL Loss: 439.8542\n",
            "\n",
            "Epoch 438: Validation loss decreased (439.854248 --> 438.214996).\n",
            "\t Train_Loss: 154.6450 Val_Loss: 438.2150  BEST VAL Loss: 438.2150\n",
            "\n",
            "Epoch 439: Validation loss decreased (438.214996 --> 436.595551).\n",
            "\t Train_Loss: 154.2865 Val_Loss: 436.5956  BEST VAL Loss: 436.5956\n",
            "\n",
            "Epoch 440: Validation loss decreased (436.595551 --> 434.995331).\n",
            "\t Train_Loss: 153.9353 Val_Loss: 434.9953  BEST VAL Loss: 434.9953\n",
            "\n",
            "Epoch 441: Validation loss decreased (434.995331 --> 433.414551).\n",
            "\t Train_Loss: 153.5914 Val_Loss: 433.4146  BEST VAL Loss: 433.4146\n",
            "\n",
            "Epoch 442: Validation loss decreased (433.414551 --> 431.853516).\n",
            "\t Train_Loss: 153.2548 Val_Loss: 431.8535  BEST VAL Loss: 431.8535\n",
            "\n",
            "Epoch 443: Validation loss decreased (431.853516 --> 430.310608).\n",
            "\t Train_Loss: 152.9251 Val_Loss: 430.3106  BEST VAL Loss: 430.3106\n",
            "\n",
            "Epoch 444: Validation loss decreased (430.310608 --> 428.786438).\n",
            "\t Train_Loss: 152.6021 Val_Loss: 428.7864  BEST VAL Loss: 428.7864\n",
            "\n",
            "Epoch 445: Validation loss decreased (428.786438 --> 427.280579).\n",
            "\t Train_Loss: 152.2860 Val_Loss: 427.2806  BEST VAL Loss: 427.2806\n",
            "\n",
            "Epoch 446: Validation loss decreased (427.280579 --> 425.792877).\n",
            "\t Train_Loss: 151.9764 Val_Loss: 425.7929  BEST VAL Loss: 425.7929\n",
            "\n",
            "Epoch 447: Validation loss decreased (425.792877 --> 424.323151).\n",
            "\t Train_Loss: 151.6733 Val_Loss: 424.3232  BEST VAL Loss: 424.3232\n",
            "\n",
            "Epoch 448: Validation loss decreased (424.323151 --> 422.871674).\n",
            "\t Train_Loss: 151.3766 Val_Loss: 422.8717  BEST VAL Loss: 422.8717\n",
            "\n",
            "Epoch 449: Validation loss decreased (422.871674 --> 421.436768).\n",
            "\t Train_Loss: 151.0862 Val_Loss: 421.4368  BEST VAL Loss: 421.4368\n",
            "\n",
            "Epoch 450: Validation loss decreased (421.436768 --> 420.019257).\n",
            "\t Train_Loss: 150.8016 Val_Loss: 420.0193  BEST VAL Loss: 420.0193\n",
            "\n",
            "Epoch 451: Validation loss decreased (420.019257 --> 418.619537).\n",
            "\t Train_Loss: 150.5232 Val_Loss: 418.6195  BEST VAL Loss: 418.6195\n",
            "\n",
            "Epoch 452: Validation loss decreased (418.619537 --> 417.236145).\n",
            "\t Train_Loss: 150.2507 Val_Loss: 417.2361  BEST VAL Loss: 417.2361\n",
            "\n",
            "Epoch 453: Validation loss decreased (417.236145 --> 415.869720).\n",
            "\t Train_Loss: 149.9838 Val_Loss: 415.8697  BEST VAL Loss: 415.8697\n",
            "\n",
            "Epoch 454: Validation loss decreased (415.869720 --> 414.519623).\n",
            "\t Train_Loss: 149.7225 Val_Loss: 414.5196  BEST VAL Loss: 414.5196\n",
            "\n",
            "Epoch 455: Validation loss decreased (414.519623 --> 413.185791).\n",
            "\t Train_Loss: 149.4669 Val_Loss: 413.1858  BEST VAL Loss: 413.1858\n",
            "\n",
            "Epoch 456: Validation loss decreased (413.185791 --> 411.868469).\n",
            "\t Train_Loss: 149.2166 Val_Loss: 411.8685  BEST VAL Loss: 411.8685\n",
            "\n",
            "Epoch 457: Validation loss decreased (411.868469 --> 410.566071).\n",
            "\t Train_Loss: 148.9717 Val_Loss: 410.5661  BEST VAL Loss: 410.5661\n",
            "\n",
            "Epoch 458: Validation loss decreased (410.566071 --> 409.279999).\n",
            "\t Train_Loss: 148.7319 Val_Loss: 409.2800  BEST VAL Loss: 409.2800\n",
            "\n",
            "Epoch 459: Validation loss decreased (409.279999 --> 408.009674).\n",
            "\t Train_Loss: 148.4973 Val_Loss: 408.0097  BEST VAL Loss: 408.0097\n",
            "\n",
            "Epoch 460: Validation loss decreased (408.009674 --> 406.754211).\n",
            "\t Train_Loss: 148.2676 Val_Loss: 406.7542  BEST VAL Loss: 406.7542\n",
            "\n",
            "Epoch 461: Validation loss decreased (406.754211 --> 405.514069).\n",
            "\t Train_Loss: 148.0429 Val_Loss: 405.5141  BEST VAL Loss: 405.5141\n",
            "\n",
            "Epoch 462: Validation loss decreased (405.514069 --> 404.288727).\n",
            "\t Train_Loss: 147.8228 Val_Loss: 404.2887  BEST VAL Loss: 404.2887\n",
            "\n",
            "Epoch 463: Validation loss decreased (404.288727 --> 403.078400).\n",
            "\t Train_Loss: 147.6077 Val_Loss: 403.0784  BEST VAL Loss: 403.0784\n",
            "\n",
            "Epoch 464: Validation loss decreased (403.078400 --> 401.882965).\n",
            "\t Train_Loss: 147.3971 Val_Loss: 401.8830  BEST VAL Loss: 401.8830\n",
            "\n",
            "Epoch 465: Validation loss decreased (401.882965 --> 400.701263).\n",
            "\t Train_Loss: 147.1911 Val_Loss: 400.7013  BEST VAL Loss: 400.7013\n",
            "\n",
            "Epoch 466: Validation loss decreased (400.701263 --> 399.533997).\n",
            "\t Train_Loss: 146.9894 Val_Loss: 399.5340  BEST VAL Loss: 399.5340\n",
            "\n",
            "Epoch 467: Validation loss decreased (399.533997 --> 398.381012).\n",
            "\t Train_Loss: 146.7919 Val_Loss: 398.3810  BEST VAL Loss: 398.3810\n",
            "\n",
            "Epoch 468: Validation loss decreased (398.381012 --> 397.241547).\n",
            "\t Train_Loss: 146.5990 Val_Loss: 397.2415  BEST VAL Loss: 397.2415\n",
            "\n",
            "Epoch 469: Validation loss decreased (397.241547 --> 396.116608).\n",
            "\t Train_Loss: 146.4101 Val_Loss: 396.1166  BEST VAL Loss: 396.1166\n",
            "\n",
            "Epoch 470: Validation loss decreased (396.116608 --> 395.004730).\n",
            "\t Train_Loss: 146.2253 Val_Loss: 395.0047  BEST VAL Loss: 395.0047\n",
            "\n",
            "Epoch 471: Validation loss decreased (395.004730 --> 393.906708).\n",
            "\t Train_Loss: 146.0446 Val_Loss: 393.9067  BEST VAL Loss: 393.9067\n",
            "\n",
            "Epoch 472: Validation loss decreased (393.906708 --> 392.820801).\n",
            "\t Train_Loss: 145.8677 Val_Loss: 392.8208  BEST VAL Loss: 392.8208\n",
            "\n",
            "Epoch 473: Validation loss decreased (392.820801 --> 391.748901).\n",
            "\t Train_Loss: 145.6946 Val_Loss: 391.7489  BEST VAL Loss: 391.7489\n",
            "\n",
            "Epoch 474: Validation loss decreased (391.748901 --> 390.690094).\n",
            "\t Train_Loss: 145.5254 Val_Loss: 390.6901  BEST VAL Loss: 390.6901\n",
            "\n",
            "Epoch 475: Validation loss decreased (390.690094 --> 389.643585).\n",
            "\t Train_Loss: 145.3600 Val_Loss: 389.6436  BEST VAL Loss: 389.6436\n",
            "\n",
            "Epoch 476: Validation loss decreased (389.643585 --> 388.609802).\n",
            "\t Train_Loss: 145.1980 Val_Loss: 388.6098  BEST VAL Loss: 388.6098\n",
            "\n",
            "Epoch 477: Validation loss decreased (388.609802 --> 387.588928).\n",
            "\t Train_Loss: 145.0396 Val_Loss: 387.5889  BEST VAL Loss: 387.5889\n",
            "\n",
            "Epoch 478: Validation loss decreased (387.588928 --> 386.580170).\n",
            "\t Train_Loss: 144.8847 Val_Loss: 386.5802  BEST VAL Loss: 386.5802\n",
            "\n",
            "Epoch 479: Validation loss decreased (386.580170 --> 385.583984).\n",
            "\t Train_Loss: 144.7332 Val_Loss: 385.5840  BEST VAL Loss: 385.5840\n",
            "\n",
            "Epoch 480: Validation loss decreased (385.583984 --> 384.599274).\n",
            "\t Train_Loss: 144.5850 Val_Loss: 384.5993  BEST VAL Loss: 384.5993\n",
            "\n",
            "Epoch 481: Validation loss decreased (384.599274 --> 383.626465).\n",
            "\t Train_Loss: 144.4401 Val_Loss: 383.6265  BEST VAL Loss: 383.6265\n",
            "\n",
            "Epoch 482: Validation loss decreased (383.626465 --> 382.665588).\n",
            "\t Train_Loss: 144.2984 Val_Loss: 382.6656  BEST VAL Loss: 382.6656\n",
            "\n",
            "Epoch 483: Validation loss decreased (382.665588 --> 381.716522).\n",
            "\t Train_Loss: 144.1598 Val_Loss: 381.7165  BEST VAL Loss: 381.7165\n",
            "\n",
            "Epoch 484: Validation loss decreased (381.716522 --> 380.778900).\n",
            "\t Train_Loss: 144.0244 Val_Loss: 380.7789  BEST VAL Loss: 380.7789\n",
            "\n",
            "Epoch 485: Validation loss decreased (380.778900 --> 379.852142).\n",
            "\t Train_Loss: 143.8919 Val_Loss: 379.8521  BEST VAL Loss: 379.8521\n",
            "\n",
            "Epoch 486: Validation loss decreased (379.852142 --> 378.937317).\n",
            "\t Train_Loss: 143.7623 Val_Loss: 378.9373  BEST VAL Loss: 378.9373\n",
            "\n",
            "Epoch 487: Validation loss decreased (378.937317 --> 378.033020).\n",
            "\t Train_Loss: 143.6357 Val_Loss: 378.0330  BEST VAL Loss: 378.0330\n",
            "\n",
            "Epoch 488: Validation loss decreased (378.033020 --> 377.139832).\n",
            "\t Train_Loss: 143.5117 Val_Loss: 377.1398  BEST VAL Loss: 377.1398\n",
            "\n",
            "Epoch 489: Validation loss decreased (377.139832 --> 376.257416).\n",
            "\t Train_Loss: 143.3907 Val_Loss: 376.2574  BEST VAL Loss: 376.2574\n",
            "\n",
            "Epoch 490: Validation loss decreased (376.257416 --> 375.385773).\n",
            "\t Train_Loss: 143.2722 Val_Loss: 375.3858  BEST VAL Loss: 375.3858\n",
            "\n",
            "Epoch 491: Validation loss decreased (375.385773 --> 374.525269).\n",
            "\t Train_Loss: 143.1565 Val_Loss: 374.5253  BEST VAL Loss: 374.5253\n",
            "\n",
            "Epoch 492: Validation loss decreased (374.525269 --> 373.674805).\n",
            "\t Train_Loss: 143.0434 Val_Loss: 373.6748  BEST VAL Loss: 373.6748\n",
            "\n",
            "Epoch 493: Validation loss decreased (373.674805 --> 372.834717).\n",
            "\t Train_Loss: 142.9328 Val_Loss: 372.8347  BEST VAL Loss: 372.8347\n",
            "\n",
            "Epoch 494: Validation loss decreased (372.834717 --> 372.004547).\n",
            "\t Train_Loss: 142.8247 Val_Loss: 372.0045  BEST VAL Loss: 372.0045\n",
            "\n",
            "Epoch 495: Validation loss decreased (372.004547 --> 371.184418).\n",
            "\t Train_Loss: 142.7191 Val_Loss: 371.1844  BEST VAL Loss: 371.1844\n",
            "\n",
            "Epoch 496: Validation loss decreased (371.184418 --> 370.374390).\n",
            "\t Train_Loss: 142.6158 Val_Loss: 370.3744  BEST VAL Loss: 370.3744\n",
            "\n",
            "Epoch 497: Validation loss decreased (370.374390 --> 369.574371).\n",
            "\t Train_Loss: 142.5148 Val_Loss: 369.5744  BEST VAL Loss: 369.5744\n",
            "\n",
            "Epoch 498: Validation loss decreased (369.574371 --> 368.784149).\n",
            "\t Train_Loss: 142.4161 Val_Loss: 368.7841  BEST VAL Loss: 368.7841\n",
            "\n",
            "Epoch 499: Validation loss decreased (368.784149 --> 368.003448).\n",
            "\t Train_Loss: 142.3197 Val_Loss: 368.0034  BEST VAL Loss: 368.0034\n",
            "\n",
            "Epoch 500: Validation loss decreased (368.003448 --> 367.231842).\n",
            "\t Train_Loss: 142.2254 Val_Loss: 367.2318  BEST VAL Loss: 367.2318\n",
            "\n",
            "Epoch 501: Validation loss decreased (367.231842 --> 366.470276).\n",
            "\t Train_Loss: 142.1332 Val_Loss: 366.4703  BEST VAL Loss: 366.4703\n",
            "\n",
            "Epoch 502: Validation loss decreased (366.470276 --> 365.717743).\n",
            "\t Train_Loss: 142.0432 Val_Loss: 365.7177  BEST VAL Loss: 365.7177\n",
            "\n",
            "Epoch 503: Validation loss decreased (365.717743 --> 364.974274).\n",
            "\t Train_Loss: 141.9553 Val_Loss: 364.9743  BEST VAL Loss: 364.9743\n",
            "\n",
            "Epoch 504: Validation loss decreased (364.974274 --> 364.239868).\n",
            "\t Train_Loss: 141.8693 Val_Loss: 364.2399  BEST VAL Loss: 364.2399\n",
            "\n",
            "Epoch 505: Validation loss decreased (364.239868 --> 363.514832).\n",
            "\t Train_Loss: 141.7854 Val_Loss: 363.5148  BEST VAL Loss: 363.5148\n",
            "\n",
            "Epoch 506: Validation loss decreased (363.514832 --> 362.797943).\n",
            "\t Train_Loss: 141.7032 Val_Loss: 362.7979  BEST VAL Loss: 362.7979\n",
            "\n",
            "Epoch 507: Validation loss decreased (362.797943 --> 362.089966).\n",
            "\t Train_Loss: 141.6230 Val_Loss: 362.0900  BEST VAL Loss: 362.0900\n",
            "\n",
            "Epoch 508: Validation loss decreased (362.089966 --> 361.391205).\n",
            "\t Train_Loss: 141.5446 Val_Loss: 361.3912  BEST VAL Loss: 361.3912\n",
            "\n",
            "Epoch 509: Validation loss decreased (361.391205 --> 360.700348).\n",
            "\t Train_Loss: 141.4680 Val_Loss: 360.7003  BEST VAL Loss: 360.7003\n",
            "\n",
            "Epoch 510: Validation loss decreased (360.700348 --> 360.018616).\n",
            "\t Train_Loss: 141.3931 Val_Loss: 360.0186  BEST VAL Loss: 360.0186\n",
            "\n",
            "Epoch 511: Validation loss decreased (360.018616 --> 359.344574).\n",
            "\t Train_Loss: 141.3198 Val_Loss: 359.3446  BEST VAL Loss: 359.3446\n",
            "\n",
            "Epoch 512: Validation loss decreased (359.344574 --> 358.679443).\n",
            "\t Train_Loss: 141.2478 Val_Loss: 358.6794  BEST VAL Loss: 358.6794\n",
            "\n",
            "Epoch 513: Validation loss decreased (358.679443 --> 358.022095).\n",
            "\t Train_Loss: 141.1768 Val_Loss: 358.0221  BEST VAL Loss: 358.0221\n",
            "\n",
            "Epoch 514: Validation loss decreased (358.022095 --> 357.373688).\n",
            "\t Train_Loss: 141.1053 Val_Loss: 357.3737  BEST VAL Loss: 357.3737\n",
            "\n",
            "Epoch 515: Validation loss decreased (357.373688 --> 356.740723).\n",
            "\t Train_Loss: 141.0269 Val_Loss: 356.7407  BEST VAL Loss: 356.7407\n",
            "\n",
            "Epoch 516: Validation loss decreased (356.740723 --> 356.222504).\n",
            "\t Train_Loss: 140.8896 Val_Loss: 356.2225  BEST VAL Loss: 356.2225\n",
            "\n",
            "Epoch 517: Validation loss did not decrease\n",
            "\t Train_Loss: 140.2165 Val_Loss: 356.9765  BEST VAL Loss: 356.2225\n",
            "\n",
            "Epoch 518: Validation loss did not decrease\n",
            "\t Train_Loss: 137.3295 Val_Loss: 372.3692  BEST VAL Loss: 356.2225\n",
            "\n",
            "Epoch 519: Validation loss did not decrease\n",
            "\t Train_Loss: 137.1731 Val_Loss: 364.0582  BEST VAL Loss: 356.2225\n",
            "\n",
            "Epoch 520: Validation loss did not decrease\n",
            "\t Train_Loss: 135.3949 Val_Loss: 356.6903  BEST VAL Loss: 356.2225\n",
            "\n",
            "Epoch 521: Validation loss decreased (356.222504 --> 355.136963).\n",
            "\t Train_Loss: 135.5890 Val_Loss: 355.1370  BEST VAL Loss: 355.1370\n",
            "\n",
            "Epoch 522: Validation loss did not decrease\n",
            "\t Train_Loss: 135.8399 Val_Loss: 355.9416  BEST VAL Loss: 355.1370\n",
            "\n",
            "Epoch 523: Validation loss did not decrease\n",
            "\t Train_Loss: 134.8021 Val_Loss: 360.5523  BEST VAL Loss: 355.1370\n",
            "\n",
            "Epoch 524: Validation loss did not decrease\n",
            "\t Train_Loss: 134.3683 Val_Loss: 363.0887  BEST VAL Loss: 355.1370\n",
            "\n",
            "Epoch 525: Validation loss did not decrease\n",
            "\t Train_Loss: 134.7285 Val_Loss: 357.7021  BEST VAL Loss: 355.1370\n",
            "\n",
            "Epoch 526: Validation loss decreased (355.136963 --> 353.181885).\n",
            "\t Train_Loss: 133.6544 Val_Loss: 353.1819  BEST VAL Loss: 353.1819\n",
            "\n",
            "Epoch 527: Validation loss decreased (353.181885 --> 351.671082).\n",
            "\t Train_Loss: 133.4936 Val_Loss: 351.6711  BEST VAL Loss: 351.6711\n",
            "\n",
            "Epoch 528: Validation loss did not decrease\n",
            "\t Train_Loss: 133.1322 Val_Loss: 353.2169  BEST VAL Loss: 351.6711\n",
            "\n",
            "Epoch 529: Validation loss did not decrease\n",
            "\t Train_Loss: 130.2533 Val_Loss: 371.4202  BEST VAL Loss: 351.6711\n",
            "\n",
            "Epoch 530: Validation loss did not decrease\n",
            "\t Train_Loss: 130.9931 Val_Loss: 360.4364  BEST VAL Loss: 351.6711\n",
            "\n",
            "Epoch 531: Validation loss did not decrease\n",
            "\t Train_Loss: 128.4664 Val_Loss: 352.8127  BEST VAL Loss: 351.6711\n",
            "\n",
            "Epoch 532: Validation loss decreased (351.671082 --> 350.008789).\n",
            "\t Train_Loss: 128.6370 Val_Loss: 350.0088  BEST VAL Loss: 350.0088\n",
            "\n",
            "Epoch 533: Validation loss decreased (350.008789 --> 349.408478).\n",
            "\t Train_Loss: 128.9479 Val_Loss: 349.4085  BEST VAL Loss: 349.4085\n",
            "\n",
            "Epoch 534: Validation loss did not decrease\n",
            "\t Train_Loss: 128.1831 Val_Loss: 351.4741  BEST VAL Loss: 349.4085\n",
            "\n",
            "Epoch 535: Validation loss did not decrease\n",
            "\t Train_Loss: 126.7096 Val_Loss: 357.6451  BEST VAL Loss: 349.4085\n",
            "\n",
            "Epoch 536: Validation loss did not decrease\n",
            "\t Train_Loss: 126.8409 Val_Loss: 357.5345  BEST VAL Loss: 349.4085\n",
            "\n",
            "Epoch 537: Validation loss did not decrease\n",
            "\t Train_Loss: 126.6784 Val_Loss: 350.3560  BEST VAL Loss: 349.4085\n",
            "\n",
            "Epoch 538: Validation loss decreased (349.408478 --> 345.842987).\n",
            "\t Train_Loss: 125.3603 Val_Loss: 345.8430  BEST VAL Loss: 345.8430\n",
            "\n",
            "Epoch 539: Validation loss decreased (345.842987 --> 344.461761).\n",
            "\t Train_Loss: 125.3544 Val_Loss: 344.4618  BEST VAL Loss: 344.4618\n",
            "\n",
            "Epoch 540: Validation loss did not decrease\n",
            "\t Train_Loss: 125.2049 Val_Loss: 344.9939  BEST VAL Loss: 344.4618\n",
            "\n",
            "Epoch 541: Validation loss did not decrease\n",
            "\t Train_Loss: 124.4174 Val_Loss: 346.9799  BEST VAL Loss: 344.4618\n",
            "\n",
            "Epoch 542: Validation loss did not decrease\n",
            "\t Train_Loss: 123.7829 Val_Loss: 348.5641  BEST VAL Loss: 344.4618\n",
            "\n",
            "Epoch 543: Validation loss did not decrease\n",
            "\t Train_Loss: 123.6691 Val_Loss: 346.6322  BEST VAL Loss: 344.4618\n",
            "\n",
            "Epoch 544: Validation loss decreased (344.461761 --> 342.681488).\n",
            "\t Train_Loss: 123.1408 Val_Loss: 342.6815  BEST VAL Loss: 342.6815\n",
            "\n",
            "Epoch 545: Validation loss decreased (342.681488 --> 339.937714).\n",
            "\t Train_Loss: 122.4623 Val_Loss: 339.9377  BEST VAL Loss: 339.9377\n",
            "\n",
            "Epoch 546: Validation loss decreased (339.937714 --> 339.168457).\n",
            "\t Train_Loss: 122.0980 Val_Loss: 339.1685  BEST VAL Loss: 339.1685\n",
            "\n",
            "Epoch 547: Validation loss did not decrease\n",
            "\t Train_Loss: 121.4549 Val_Loss: 340.4144  BEST VAL Loss: 339.1685\n",
            "\n",
            "Epoch 548: Validation loss did not decrease\n",
            "\t Train_Loss: 120.3950 Val_Loss: 344.2938  BEST VAL Loss: 339.1685\n",
            "\n",
            "Epoch 549: Validation loss did not decrease\n",
            "\t Train_Loss: 119.5822 Val_Loss: 350.1994  BEST VAL Loss: 339.1685\n",
            "\n",
            "Epoch 550: Validation loss did not decrease\n",
            "\t Train_Loss: 120.0256 Val_Loss: 345.9877  BEST VAL Loss: 339.1685\n",
            "\n",
            "Epoch 551: Validation loss decreased (339.168457 --> 338.654114).\n",
            "\t Train_Loss: 119.0341 Val_Loss: 338.6541  BEST VAL Loss: 338.6541\n",
            "\n",
            "Epoch 552: Validation loss decreased (338.654114 --> 334.930817).\n",
            "\t Train_Loss: 118.1918 Val_Loss: 334.9308  BEST VAL Loss: 334.9308\n",
            "\n",
            "Epoch 553: Validation loss decreased (334.930817 --> 333.715607).\n",
            "\t Train_Loss: 118.2205 Val_Loss: 333.7156  BEST VAL Loss: 333.7156\n",
            "\n",
            "Epoch 554: Validation loss did not decrease\n",
            "\t Train_Loss: 117.8986 Val_Loss: 334.1173  BEST VAL Loss: 333.7156\n",
            "\n",
            "Epoch 555: Validation loss did not decrease\n",
            "\t Train_Loss: 117.1959 Val_Loss: 335.6590  BEST VAL Loss: 333.7156\n",
            "\n",
            "Epoch 556: Validation loss did not decrease\n",
            "\t Train_Loss: 116.5131 Val_Loss: 337.4990  BEST VAL Loss: 333.7156\n",
            "\n",
            "Epoch 557: Validation loss did not decrease\n",
            "\t Train_Loss: 116.1351 Val_Loss: 337.3534  BEST VAL Loss: 333.7156\n",
            "\n",
            "Epoch 558: Validation loss decreased (333.715607 --> 333.657562).\n",
            "\t Train_Loss: 115.8442 Val_Loss: 333.6576  BEST VAL Loss: 333.6576\n",
            "\n",
            "Epoch 559: Validation loss decreased (333.657562 --> 329.688629).\n",
            "\t Train_Loss: 115.1688 Val_Loss: 329.6886  BEST VAL Loss: 329.6886\n",
            "\n",
            "Epoch 560: Validation loss decreased (329.688629 --> 327.619507).\n",
            "\t Train_Loss: 114.7288 Val_Loss: 327.6195  BEST VAL Loss: 327.6195\n",
            "\n",
            "Epoch 561: Validation loss decreased (327.619507 --> 327.265533).\n",
            "\t Train_Loss: 114.4243 Val_Loss: 327.2655  BEST VAL Loss: 327.2655\n",
            "\n",
            "Epoch 562: Validation loss did not decrease\n",
            "\t Train_Loss: 113.9156 Val_Loss: 327.9799  BEST VAL Loss: 327.2655\n",
            "\n",
            "Epoch 563: Validation loss did not decrease\n",
            "\t Train_Loss: 113.4148 Val_Loss: 328.6573  BEST VAL Loss: 327.2655\n",
            "\n",
            "Epoch 564: Validation loss did not decrease\n",
            "\t Train_Loss: 113.0652 Val_Loss: 327.9716  BEST VAL Loss: 327.2655\n",
            "\n",
            "Epoch 565: Validation loss decreased (327.265533 --> 325.685852).\n",
            "\t Train_Loss: 112.6733 Val_Loss: 325.6859  BEST VAL Loss: 325.6859\n",
            "\n",
            "Epoch 566: Validation loss decreased (325.685852 --> 323.023926).\n",
            "\t Train_Loss: 112.1790 Val_Loss: 323.0239  BEST VAL Loss: 323.0239\n",
            "\n",
            "Epoch 567: Validation loss decreased (323.023926 --> 321.125427).\n",
            "\t Train_Loss: 111.7983 Val_Loss: 321.1254  BEST VAL Loss: 321.1254\n",
            "\n",
            "Epoch 568: Validation loss decreased (321.125427 --> 320.285645).\n",
            "\t Train_Loss: 111.4761 Val_Loss: 320.2856  BEST VAL Loss: 320.2856\n",
            "\n",
            "Epoch 569: Validation loss did not decrease\n",
            "\t Train_Loss: 111.0229 Val_Loss: 320.3048  BEST VAL Loss: 320.2856\n",
            "\n",
            "Epoch 570: Validation loss did not decrease\n",
            "\t Train_Loss: 110.5361 Val_Loss: 320.5616  BEST VAL Loss: 320.2856\n",
            "\n",
            "Epoch 571: Validation loss decreased (320.285645 --> 320.153595).\n",
            "\t Train_Loss: 110.1529 Val_Loss: 320.1536  BEST VAL Loss: 320.1536\n",
            "\n",
            "Epoch 572: Validation loss decreased (320.153595 --> 318.778748).\n",
            "\t Train_Loss: 109.7253 Val_Loss: 318.7787  BEST VAL Loss: 318.7787\n",
            "\n",
            "Epoch 573: Validation loss decreased (318.778748 --> 317.117340).\n",
            "\t Train_Loss: 109.1241 Val_Loss: 317.1173  BEST VAL Loss: 317.1173\n",
            "\n",
            "Epoch 574: Validation loss decreased (317.117340 --> 316.073090).\n",
            "\t Train_Loss: 108.4598 Val_Loss: 316.0731  BEST VAL Loss: 316.0731\n",
            "\n",
            "Epoch 575: Validation loss did not decrease\n",
            "\t Train_Loss: 107.7714 Val_Loss: 316.3067  BEST VAL Loss: 316.0731\n",
            "\n",
            "Epoch 576: Validation loss did not decrease\n",
            "\t Train_Loss: 107.0329 Val_Loss: 318.2506  BEST VAL Loss: 316.0731\n",
            "\n",
            "Epoch 577: Validation loss did not decrease\n",
            "\t Train_Loss: 106.5478 Val_Loss: 320.8261  BEST VAL Loss: 316.0731\n",
            "\n",
            "Epoch 578: Validation loss did not decrease\n",
            "\t Train_Loss: 106.5198 Val_Loss: 320.5089  BEST VAL Loss: 316.0731\n",
            "\n",
            "Epoch 579: Validation loss did not decrease\n",
            "\t Train_Loss: 106.0728 Val_Loss: 317.5030  BEST VAL Loss: 316.0731\n",
            "\n",
            "Epoch 580: Validation loss decreased (316.073090 --> 314.776978).\n",
            "\t Train_Loss: 105.0845 Val_Loss: 314.7770  BEST VAL Loss: 314.7770\n",
            "\n",
            "Epoch 581: Validation loss decreased (314.776978 --> 314.063446).\n",
            "\t Train_Loss: 104.2563 Val_Loss: 314.0634  BEST VAL Loss: 314.0634\n",
            "\n",
            "Epoch 582: Validation loss did not decrease\n",
            "\t Train_Loss: 103.8531 Val_Loss: 314.9698  BEST VAL Loss: 314.0634\n",
            "\n",
            "Epoch 583: Validation loss decreased (314.063446 --> 313.807343).\n",
            "\t Train_Loss: 103.8640 Val_Loss: 313.8073  BEST VAL Loss: 313.8073\n",
            "\n",
            "Epoch 584: Validation loss decreased (313.807343 --> 310.802338).\n",
            "\t Train_Loss: 103.5619 Val_Loss: 310.8023  BEST VAL Loss: 310.8023\n",
            "\n",
            "Epoch 585: Validation loss decreased (310.802338 --> 308.482483).\n",
            "\t Train_Loss: 102.9280 Val_Loss: 308.4825  BEST VAL Loss: 308.4825\n",
            "\n",
            "Epoch 586: Validation loss decreased (308.482483 --> 307.220428).\n",
            "\t Train_Loss: 102.4674 Val_Loss: 307.2204  BEST VAL Loss: 307.2204\n",
            "\n",
            "Epoch 587: Validation loss decreased (307.220428 --> 306.630829).\n",
            "\t Train_Loss: 102.0427 Val_Loss: 306.6308  BEST VAL Loss: 306.6308\n",
            "\n",
            "Epoch 588: Validation loss decreased (306.630829 --> 306.557434).\n",
            "\t Train_Loss: 101.5164 Val_Loss: 306.5574  BEST VAL Loss: 306.5574\n",
            "\n",
            "Epoch 589: Validation loss did not decrease\n",
            "\t Train_Loss: 100.9717 Val_Loss: 306.8864  BEST VAL Loss: 306.5574\n",
            "\n",
            "Epoch 590: Validation loss did not decrease\n",
            "\t Train_Loss: 100.5512 Val_Loss: 307.1192  BEST VAL Loss: 306.5574\n",
            "\n",
            "Epoch 591: Validation loss decreased (306.557434 --> 306.503967).\n",
            "\t Train_Loss: 100.2419 Val_Loss: 306.5040  BEST VAL Loss: 306.5040\n",
            "\n",
            "Epoch 592: Validation loss decreased (306.503967 --> 304.774719).\n",
            "\t Train_Loss: 99.8767 Val_Loss: 304.7747  BEST VAL Loss: 304.7747\n",
            "\n",
            "Epoch 593: Validation loss decreased (304.774719 --> 302.485443).\n",
            "\t Train_Loss: 99.3735 Val_Loss: 302.4854  BEST VAL Loss: 302.4854\n",
            "\n",
            "Epoch 594: Validation loss decreased (302.485443 --> 300.435211).\n",
            "\t Train_Loss: 98.8411 Val_Loss: 300.4352  BEST VAL Loss: 300.4352\n",
            "\n",
            "Epoch 595: Validation loss decreased (300.435211 --> 298.976410).\n",
            "\t Train_Loss: 98.4119 Val_Loss: 298.9764  BEST VAL Loss: 298.9764\n",
            "\n",
            "Epoch 596: Validation loss decreased (298.976410 --> 297.984406).\n",
            "\t Train_Loss: 98.0824 Val_Loss: 297.9844  BEST VAL Loss: 297.9844\n",
            "\n",
            "Epoch 597: Validation loss decreased (297.984406 --> 297.182343).\n",
            "\t Train_Loss: 97.7625 Val_Loss: 297.1823  BEST VAL Loss: 297.1823\n",
            "\n",
            "Epoch 598: Validation loss decreased (297.182343 --> 296.405426).\n",
            "\t Train_Loss: 97.3773 Val_Loss: 296.4054  BEST VAL Loss: 296.4054\n",
            "\n",
            "Epoch 599: Validation loss decreased (296.405426 --> 295.649872).\n",
            "\t Train_Loss: 96.9293 Val_Loss: 295.6499  BEST VAL Loss: 295.6499\n",
            "\n",
            "Epoch 600: Validation loss decreased (295.649872 --> 294.930511).\n",
            "\t Train_Loss: 96.4892 Val_Loss: 294.9305  BEST VAL Loss: 294.9305\n",
            "\n",
            "Epoch 601: Validation loss decreased (294.930511 --> 294.174316).\n",
            "\t Train_Loss: 96.1119 Val_Loss: 294.1743  BEST VAL Loss: 294.1743\n",
            "\n",
            "Epoch 602: Validation loss decreased (294.174316 --> 293.285400).\n",
            "\t Train_Loss: 95.7830 Val_Loss: 293.2854  BEST VAL Loss: 293.2854\n",
            "\n",
            "Epoch 603: Validation loss decreased (293.285400 --> 292.243927).\n",
            "\t Train_Loss: 95.4545 Val_Loss: 292.2439  BEST VAL Loss: 292.2439\n",
            "\n",
            "Epoch 604: Validation loss decreased (292.243927 --> 291.110504).\n",
            "\t Train_Loss: 95.1027 Val_Loss: 291.1105  BEST VAL Loss: 291.1105\n",
            "\n",
            "Epoch 605: Validation loss decreased (291.110504 --> 289.957062).\n",
            "\t Train_Loss: 94.7397 Val_Loss: 289.9571  BEST VAL Loss: 289.9571\n",
            "\n",
            "Epoch 606: Validation loss decreased (289.957062 --> 288.819397).\n",
            "\t Train_Loss: 94.3861 Val_Loss: 288.8194  BEST VAL Loss: 288.8194\n",
            "\n",
            "Epoch 607: Validation loss decreased (288.819397 --> 287.710846).\n",
            "\t Train_Loss: 94.0487 Val_Loss: 287.7108  BEST VAL Loss: 287.7108\n",
            "\n",
            "Epoch 608: Validation loss decreased (287.710846 --> 286.650238).\n",
            "\t Train_Loss: 93.7232 Val_Loss: 286.6502  BEST VAL Loss: 286.6502\n",
            "\n",
            "Epoch 609: Validation loss decreased (286.650238 --> 285.660858).\n",
            "\t Train_Loss: 93.4072 Val_Loss: 285.6609  BEST VAL Loss: 285.6609\n",
            "\n",
            "Epoch 610: Validation loss decreased (285.660858 --> 284.745758).\n",
            "\t Train_Loss: 93.1002 Val_Loss: 284.7458  BEST VAL Loss: 284.7458\n",
            "\n",
            "Epoch 611: Validation loss decreased (284.745758 --> 283.888641).\n",
            "\t Train_Loss: 92.7963 Val_Loss: 283.8886  BEST VAL Loss: 283.8886\n",
            "\n",
            "Epoch 612: Validation loss decreased (283.888641 --> 283.070770).\n",
            "\t Train_Loss: 92.4875 Val_Loss: 283.0708  BEST VAL Loss: 283.0708\n",
            "\n",
            "Epoch 613: Validation loss decreased (283.070770 --> 282.281494).\n",
            "\t Train_Loss: 92.1743 Val_Loss: 282.2815  BEST VAL Loss: 282.2815\n",
            "\n",
            "Epoch 614: Validation loss decreased (282.281494 --> 281.498138).\n",
            "\t Train_Loss: 91.8674 Val_Loss: 281.4981  BEST VAL Loss: 281.4981\n",
            "\n",
            "Epoch 615: Validation loss decreased (281.498138 --> 280.684113).\n",
            "\t Train_Loss: 91.5751 Val_Loss: 280.6841  BEST VAL Loss: 280.6841\n",
            "\n",
            "Epoch 616: Validation loss decreased (280.684113 --> 279.809418).\n",
            "\t Train_Loss: 91.2930 Val_Loss: 279.8094  BEST VAL Loss: 279.8094\n",
            "\n",
            "Epoch 617: Validation loss decreased (279.809418 --> 278.879700).\n",
            "\t Train_Loss: 91.0094 Val_Loss: 278.8797  BEST VAL Loss: 278.8797\n",
            "\n",
            "Epoch 618: Validation loss decreased (278.879700 --> 277.934998).\n",
            "\t Train_Loss: 90.7165 Val_Loss: 277.9350  BEST VAL Loss: 277.9350\n",
            "\n",
            "Epoch 619: Validation loss decreased (277.934998 --> 277.023529).\n",
            "\t Train_Loss: 90.4153 Val_Loss: 277.0235  BEST VAL Loss: 277.0235\n",
            "\n",
            "Epoch 620: Validation loss decreased (277.023529 --> 276.172424).\n",
            "\t Train_Loss: 90.1099 Val_Loss: 276.1724  BEST VAL Loss: 276.1724\n",
            "\n",
            "Epoch 621: Validation loss decreased (276.172424 --> 275.376617).\n",
            "\t Train_Loss: 89.8005 Val_Loss: 275.3766  BEST VAL Loss: 275.3766\n",
            "\n",
            "Epoch 622: Validation loss decreased (275.376617 --> 274.614899).\n",
            "\t Train_Loss: 89.4786 Val_Loss: 274.6149  BEST VAL Loss: 274.6149\n",
            "\n",
            "Epoch 623: Validation loss decreased (274.614899 --> 273.878143).\n",
            "\t Train_Loss: 89.1288 Val_Loss: 273.8781  BEST VAL Loss: 273.8781\n",
            "\n",
            "Epoch 624: Validation loss decreased (273.878143 --> 273.192444).\n",
            "\t Train_Loss: 88.7316 Val_Loss: 273.1924  BEST VAL Loss: 273.1924\n",
            "\n",
            "Epoch 625: Validation loss decreased (273.192444 --> 272.638092).\n",
            "\t Train_Loss: 88.2637 Val_Loss: 272.6381  BEST VAL Loss: 272.6381\n",
            "\n",
            "Epoch 626: Validation loss decreased (272.638092 --> 272.426483).\n",
            "\t Train_Loss: 87.6899 Val_Loss: 272.4265  BEST VAL Loss: 272.4265\n",
            "\n",
            "Epoch 627: Validation loss did not decrease\n",
            "\t Train_Loss: 86.9268 Val_Loss: 273.6133  BEST VAL Loss: 272.4265\n",
            "\n",
            "Epoch 628: Validation loss did not decrease\n",
            "\t Train_Loss: 85.7740 Val_Loss: 288.8925  BEST VAL Loss: 272.4265\n",
            "\n",
            "Epoch 629: Validation loss did not decrease\n",
            "\t Train_Loss: 88.7711 Val_Loss: 274.6227  BEST VAL Loss: 272.4265\n",
            "\n",
            "Epoch 630: Validation loss decreased (272.426483 --> 270.092621).\n",
            "\t Train_Loss: 85.1452 Val_Loss: 270.0926  BEST VAL Loss: 270.0926\n",
            "\n",
            "Epoch 631: Validation loss decreased (270.092621 --> 268.179535).\n",
            "\t Train_Loss: 85.0844 Val_Loss: 268.1795  BEST VAL Loss: 268.1795\n",
            "\n",
            "Epoch 632: Validation loss decreased (268.179535 --> 267.674072).\n",
            "\t Train_Loss: 84.8559 Val_Loss: 267.6741  BEST VAL Loss: 267.6741\n",
            "\n",
            "Epoch 633: Validation loss did not decrease\n",
            "\t Train_Loss: 84.2368 Val_Loss: 268.4124  BEST VAL Loss: 267.6741\n",
            "\n",
            "Epoch 634: Validation loss did not decrease\n",
            "\t Train_Loss: 83.4519 Val_Loss: 270.6553  BEST VAL Loss: 267.6741\n",
            "\n",
            "Epoch 635: Validation loss did not decrease\n",
            "\t Train_Loss: 82.9558 Val_Loss: 273.5664  BEST VAL Loss: 267.6741\n",
            "\n",
            "Epoch 636: Validation loss did not decrease\n",
            "\t Train_Loss: 83.2940 Val_Loss: 268.4505  BEST VAL Loss: 267.6741\n",
            "\n",
            "Epoch 637: Validation loss decreased (267.674072 --> 263.940125).\n",
            "\t Train_Loss: 82.1755 Val_Loss: 263.9401  BEST VAL Loss: 263.9401\n",
            "\n",
            "Epoch 638: Validation loss decreased (263.940125 --> 261.792908).\n",
            "\t Train_Loss: 81.9248 Val_Loss: 261.7929  BEST VAL Loss: 261.7929\n",
            "\n",
            "Epoch 639: Validation loss decreased (261.792908 --> 261.350739).\n",
            "\t Train_Loss: 81.8592 Val_Loss: 261.3507  BEST VAL Loss: 261.3507\n",
            "\n",
            "Epoch 640: Validation loss did not decrease\n",
            "\t Train_Loss: 81.2392 Val_Loss: 262.4051  BEST VAL Loss: 261.3507\n",
            "\n",
            "Epoch 641: Validation loss did not decrease\n",
            "\t Train_Loss: 80.3782 Val_Loss: 264.5842  BEST VAL Loss: 261.3507\n",
            "\n",
            "Epoch 642: Validation loss did not decrease\n",
            "\t Train_Loss: 79.9976 Val_Loss: 265.7540  BEST VAL Loss: 261.3507\n",
            "\n",
            "Epoch 643: Validation loss did not decrease\n",
            "\t Train_Loss: 79.9252 Val_Loss: 263.4957  BEST VAL Loss: 261.3507\n",
            "\n",
            "Epoch 644: Validation loss decreased (261.350739 --> 259.965881).\n",
            "\t Train_Loss: 79.2399 Val_Loss: 259.9659  BEST VAL Loss: 259.9659\n",
            "\n",
            "Epoch 645: Validation loss decreased (259.965881 --> 257.508667).\n",
            "\t Train_Loss: 78.6144 Val_Loss: 257.5087  BEST VAL Loss: 257.5087\n",
            "\n",
            "Epoch 646: Validation loss did not decrease\n",
            "\t Train_Loss: 78.1766 Val_Loss: 258.7379  BEST VAL Loss: 257.5087\n",
            "\n",
            "Epoch 647: Validation loss did not decrease\n",
            "\t Train_Loss: 77.2715 Val_Loss: 260.7965  BEST VAL Loss: 257.5087\n",
            "\n",
            "Epoch 648: Validation loss decreased (257.508667 --> 255.638580).\n",
            "\t Train_Loss: 77.1125 Val_Loss: 255.6386  BEST VAL Loss: 255.6386\n",
            "\n",
            "Epoch 649: Validation loss did not decrease\n",
            "\t Train_Loss: 76.4758 Val_Loss: 255.8290  BEST VAL Loss: 255.6386\n",
            "\n",
            "Epoch 650: Validation loss did not decrease\n",
            "\t Train_Loss: 76.3025 Val_Loss: 256.1464  BEST VAL Loss: 255.6386\n",
            "\n",
            "Epoch 651: Validation loss decreased (255.638580 --> 255.309891).\n",
            "\t Train_Loss: 76.0219 Val_Loss: 255.3099  BEST VAL Loss: 255.3099\n",
            "\n",
            "Epoch 652: Validation loss decreased (255.309891 --> 253.878815).\n",
            "\t Train_Loss: 75.6275 Val_Loss: 253.8788  BEST VAL Loss: 253.8788\n",
            "\n",
            "Epoch 653: Validation loss decreased (253.878815 --> 252.466537).\n",
            "\t Train_Loss: 75.2344 Val_Loss: 252.4665  BEST VAL Loss: 252.4665\n",
            "\n",
            "Epoch 654: Validation loss decreased (252.466537 --> 251.376785).\n",
            "\t Train_Loss: 74.8955 Val_Loss: 251.3768  BEST VAL Loss: 251.3768\n",
            "\n",
            "Epoch 655: Validation loss decreased (251.376785 --> 250.629425).\n",
            "\t Train_Loss: 74.5447 Val_Loss: 250.6294  BEST VAL Loss: 250.6294\n",
            "\n",
            "Epoch 656: Validation loss decreased (250.629425 --> 250.103958).\n",
            "\t Train_Loss: 74.1165 Val_Loss: 250.1040  BEST VAL Loss: 250.1040\n",
            "\n",
            "Epoch 657: Validation loss decreased (250.103958 --> 249.708740).\n",
            "\t Train_Loss: 73.6171 Val_Loss: 249.7087  BEST VAL Loss: 249.7087\n",
            "\n",
            "Epoch 658: Validation loss decreased (249.708740 --> 249.364777).\n",
            "\t Train_Loss: 73.0895 Val_Loss: 249.3648  BEST VAL Loss: 249.3648\n",
            "\n",
            "Epoch 659: Validation loss decreased (249.364777 --> 249.195312).\n",
            "\t Train_Loss: 72.5331 Val_Loss: 249.1953  BEST VAL Loss: 249.1953\n",
            "\n",
            "Epoch 660: Validation loss did not decrease\n",
            "\t Train_Loss: 71.9177 Val_Loss: 250.0710  BEST VAL Loss: 249.1953\n",
            "\n",
            "Epoch 661: Validation loss decreased (249.195312 --> 247.494736).\n",
            "\t Train_Loss: 71.6355 Val_Loss: 247.4947  BEST VAL Loss: 247.4947\n",
            "\n",
            "Epoch 662: Validation loss decreased (247.494736 --> 244.675735).\n",
            "\t Train_Loss: 71.0808 Val_Loss: 244.6757  BEST VAL Loss: 244.6757\n",
            "\n",
            "Epoch 663: Validation loss did not decrease\n",
            "\t Train_Loss: 70.6063 Val_Loss: 250.5595  BEST VAL Loss: 244.6757\n",
            "\n",
            "Epoch 664: Validation loss decreased (244.675735 --> 242.078903).\n",
            "\t Train_Loss: 71.6819 Val_Loss: 242.0789  BEST VAL Loss: 242.0789\n",
            "\n",
            "Epoch 665: Validation loss decreased (242.078903 --> 241.582016).\n",
            "\t Train_Loss: 70.0764 Val_Loss: 241.5820  BEST VAL Loss: 241.5820\n",
            "\n",
            "Epoch 666: Validation loss did not decrease\n",
            "\t Train_Loss: 69.6478 Val_Loss: 244.4108  BEST VAL Loss: 241.5820\n",
            "\n",
            "Epoch 667: Validation loss did not decrease\n",
            "\t Train_Loss: 69.2054 Val_Loss: 245.9500  BEST VAL Loss: 241.5820\n",
            "\n",
            "Epoch 668: Validation loss did not decrease\n",
            "\t Train_Loss: 69.1230 Val_Loss: 243.1688  BEST VAL Loss: 241.5820\n",
            "\n",
            "Epoch 669: Validation loss decreased (241.582016 --> 240.544754).\n",
            "\t Train_Loss: 68.4612 Val_Loss: 240.5448  BEST VAL Loss: 240.5448\n",
            "\n",
            "Epoch 670: Validation loss decreased (240.544754 --> 239.032272).\n",
            "\t Train_Loss: 68.1422 Val_Loss: 239.0323  BEST VAL Loss: 239.0323\n",
            "\n",
            "Epoch 671: Validation loss decreased (239.032272 --> 238.620163).\n",
            "\t Train_Loss: 67.8546 Val_Loss: 238.6202  BEST VAL Loss: 238.6202\n",
            "\n",
            "Epoch 672: Validation loss did not decrease\n",
            "\t Train_Loss: 67.3804 Val_Loss: 239.1884  BEST VAL Loss: 238.6202\n",
            "\n",
            "Epoch 673: Validation loss did not decrease\n",
            "\t Train_Loss: 66.9164 Val_Loss: 239.7612  BEST VAL Loss: 238.6202\n",
            "\n",
            "Epoch 674: Validation loss decreased (238.620163 --> 238.143143).\n",
            "\t Train_Loss: 66.6796 Val_Loss: 238.1431  BEST VAL Loss: 238.1431\n",
            "\n",
            "Epoch 675: Validation loss decreased (238.143143 --> 235.428833).\n",
            "\t Train_Loss: 66.2430 Val_Loss: 235.4288  BEST VAL Loss: 235.4288\n",
            "\n",
            "Epoch 676: Validation loss decreased (235.428833 --> 233.863022).\n",
            "\t Train_Loss: 65.8336 Val_Loss: 233.8630  BEST VAL Loss: 233.8630\n",
            "\n",
            "Epoch 677: Validation loss decreased (233.863022 --> 233.758499).\n",
            "\t Train_Loss: 65.5828 Val_Loss: 233.7585  BEST VAL Loss: 233.7585\n",
            "\n",
            "Epoch 678: Validation loss did not decrease\n",
            "\t Train_Loss: 65.2025 Val_Loss: 234.5560  BEST VAL Loss: 233.7585\n",
            "\n",
            "Epoch 679: Validation loss did not decrease\n",
            "\t Train_Loss: 64.8265 Val_Loss: 234.9823  BEST VAL Loss: 233.7585\n",
            "\n",
            "Epoch 680: Validation loss did not decrease\n",
            "\t Train_Loss: 64.5461 Val_Loss: 233.9114  BEST VAL Loss: 233.7585\n",
            "\n",
            "Epoch 681: Validation loss decreased (233.758499 --> 232.016068).\n",
            "\t Train_Loss: 64.1752 Val_Loss: 232.0161  BEST VAL Loss: 232.0161\n",
            "\n",
            "Epoch 682: Validation loss decreased (232.016068 --> 230.494003).\n",
            "\t Train_Loss: 63.8212 Val_Loss: 230.4940  BEST VAL Loss: 230.4940\n",
            "\n",
            "Epoch 683: Validation loss decreased (230.494003 --> 229.753220).\n",
            "\t Train_Loss: 63.5467 Val_Loss: 229.7532  BEST VAL Loss: 229.7532\n",
            "\n",
            "Epoch 684: Validation loss decreased (229.753220 --> 229.635544).\n",
            "\t Train_Loss: 63.2216 Val_Loss: 229.6355  BEST VAL Loss: 229.6355\n",
            "\n",
            "Epoch 685: Validation loss decreased (229.635544 --> 229.506989).\n",
            "\t Train_Loss: 62.8777 Val_Loss: 229.5070  BEST VAL Loss: 229.5070\n",
            "\n",
            "Epoch 686: Validation loss decreased (229.506989 --> 228.570145).\n",
            "\t Train_Loss: 62.5926 Val_Loss: 228.5701  BEST VAL Loss: 228.5701\n",
            "\n",
            "Epoch 687: Validation loss decreased (228.570145 --> 227.056320).\n",
            "\t Train_Loss: 62.2843 Val_Loss: 227.0563  BEST VAL Loss: 227.0563\n",
            "\n",
            "Epoch 688: Validation loss decreased (227.056320 --> 225.731323).\n",
            "\t Train_Loss: 61.9703 Val_Loss: 225.7313  BEST VAL Loss: 225.7313\n",
            "\n",
            "Epoch 689: Validation loss decreased (225.731323 --> 225.009628).\n",
            "\t Train_Loss: 61.6951 Val_Loss: 225.0096  BEST VAL Loss: 225.0096\n",
            "\n",
            "Epoch 690: Validation loss decreased (225.009628 --> 224.836578).\n",
            "\t Train_Loss: 61.3992 Val_Loss: 224.8366  BEST VAL Loss: 224.8366\n",
            "\n",
            "Epoch 691: Validation loss decreased (224.836578 --> 224.764877).\n",
            "\t Train_Loss: 61.0875 Val_Loss: 224.7649  BEST VAL Loss: 224.7649\n",
            "\n",
            "Epoch 692: Validation loss decreased (224.764877 --> 224.222763).\n",
            "\t Train_Loss: 60.8110 Val_Loss: 224.2228  BEST VAL Loss: 224.2228\n",
            "\n",
            "Epoch 693: Validation loss decreased (224.222763 --> 223.147125).\n",
            "\t Train_Loss: 60.5373 Val_Loss: 223.1471  BEST VAL Loss: 223.1471\n",
            "\n",
            "Epoch 694: Validation loss decreased (223.147125 --> 222.001709).\n",
            "\t Train_Loss: 60.2510 Val_Loss: 222.0017  BEST VAL Loss: 222.0017\n",
            "\n",
            "Epoch 695: Validation loss decreased (222.001709 --> 221.174469).\n",
            "\t Train_Loss: 59.9844 Val_Loss: 221.1745  BEST VAL Loss: 221.1745\n",
            "\n",
            "Epoch 696: Validation loss decreased (221.174469 --> 220.720078).\n",
            "\t Train_Loss: 59.7179 Val_Loss: 220.7201  BEST VAL Loss: 220.7201\n",
            "\n",
            "Epoch 697: Validation loss decreased (220.720078 --> 220.404694).\n",
            "\t Train_Loss: 59.4396 Val_Loss: 220.4047  BEST VAL Loss: 220.4047\n",
            "\n",
            "Epoch 698: Validation loss decreased (220.404694 --> 219.864502).\n",
            "\t Train_Loss: 59.1752 Val_Loss: 219.8645  BEST VAL Loss: 219.8645\n",
            "\n",
            "Epoch 699: Validation loss decreased (219.864502 --> 218.945419).\n",
            "\t Train_Loss: 58.9218 Val_Loss: 218.9454  BEST VAL Loss: 218.9454\n",
            "\n",
            "Epoch 700: Validation loss decreased (218.945419 --> 217.868683).\n",
            "\t Train_Loss: 58.6617 Val_Loss: 217.8687  BEST VAL Loss: 217.8687\n",
            "\n",
            "Epoch 701: Validation loss decreased (217.868683 --> 216.962753).\n",
            "\t Train_Loss: 58.4079 Val_Loss: 216.9628  BEST VAL Loss: 216.9628\n",
            "\n",
            "Epoch 702: Validation loss decreased (216.962753 --> 216.368759).\n",
            "\t Train_Loss: 58.1642 Val_Loss: 216.3688  BEST VAL Loss: 216.3688\n",
            "\n",
            "Epoch 703: Validation loss decreased (216.368759 --> 215.981567).\n",
            "\t Train_Loss: 57.9187 Val_Loss: 215.9816  BEST VAL Loss: 215.9816\n",
            "\n",
            "Epoch 704: Validation loss decreased (215.981567 --> 215.541061).\n",
            "\t Train_Loss: 57.6774 Val_Loss: 215.5411  BEST VAL Loss: 215.5411\n",
            "\n",
            "Epoch 705: Validation loss decreased (215.541061 --> 214.842682).\n",
            "\t Train_Loss: 57.4448 Val_Loss: 214.8427  BEST VAL Loss: 214.8427\n",
            "\n",
            "Epoch 706: Validation loss decreased (214.842682 --> 213.925491).\n",
            "\t Train_Loss: 57.2095 Val_Loss: 213.9255  BEST VAL Loss: 213.9255\n",
            "\n",
            "Epoch 707: Validation loss decreased (213.925491 --> 213.001572).\n",
            "\t Train_Loss: 56.9727 Val_Loss: 213.0016  BEST VAL Loss: 213.0016\n",
            "\n",
            "Epoch 708: Validation loss decreased (213.001572 --> 212.246048).\n",
            "\t Train_Loss: 56.7424 Val_Loss: 212.2460  BEST VAL Loss: 212.2460\n",
            "\n",
            "Epoch 709: Validation loss decreased (212.246048 --> 211.677490).\n",
            "\t Train_Loss: 56.5146 Val_Loss: 211.6775  BEST VAL Loss: 211.6775\n",
            "\n",
            "Epoch 710: Validation loss decreased (211.677490 --> 211.170700).\n",
            "\t Train_Loss: 56.2866 Val_Loss: 211.1707  BEST VAL Loss: 211.1707\n",
            "\n",
            "Epoch 711: Validation loss decreased (211.170700 --> 210.558029).\n",
            "\t Train_Loss: 56.0629 Val_Loss: 210.5580  BEST VAL Loss: 210.5580\n",
            "\n",
            "Epoch 712: Validation loss decreased (210.558029 --> 209.769531).\n",
            "\t Train_Loss: 55.8423 Val_Loss: 209.7695  BEST VAL Loss: 209.7695\n",
            "\n",
            "Epoch 713: Validation loss decreased (209.769531 --> 208.882599).\n",
            "\t Train_Loss: 55.6211 Val_Loss: 208.8826  BEST VAL Loss: 208.8826\n",
            "\n",
            "Epoch 714: Validation loss decreased (208.882599 --> 208.040329).\n",
            "\t Train_Loss: 55.4027 Val_Loss: 208.0403  BEST VAL Loss: 208.0403\n",
            "\n",
            "Epoch 715: Validation loss decreased (208.040329 --> 207.332352).\n",
            "\t Train_Loss: 55.1889 Val_Loss: 207.3324  BEST VAL Loss: 207.3324\n",
            "\n",
            "Epoch 716: Validation loss decreased (207.332352 --> 206.742966).\n",
            "\t Train_Loss: 54.9769 Val_Loss: 206.7430  BEST VAL Loss: 206.7430\n",
            "\n",
            "Epoch 717: Validation loss decreased (206.742966 --> 206.176025).\n",
            "\t Train_Loss: 54.7666 Val_Loss: 206.1760  BEST VAL Loss: 206.1760\n",
            "\n",
            "Epoch 718: Validation loss decreased (206.176025 --> 205.534958).\n",
            "\t Train_Loss: 54.5595 Val_Loss: 205.5350  BEST VAL Loss: 205.5350\n",
            "\n",
            "Epoch 719: Validation loss decreased (205.534958 --> 204.802841).\n",
            "\t Train_Loss: 54.3540 Val_Loss: 204.8028  BEST VAL Loss: 204.8028\n",
            "\n",
            "Epoch 720: Validation loss decreased (204.802841 --> 204.049545).\n",
            "\t Train_Loss: 54.1495 Val_Loss: 204.0495  BEST VAL Loss: 204.0495\n",
            "\n",
            "Epoch 721: Validation loss decreased (204.049545 --> 203.360519).\n",
            "\t Train_Loss: 53.9480 Val_Loss: 203.3605  BEST VAL Loss: 203.3605\n",
            "\n",
            "Epoch 722: Validation loss decreased (203.360519 --> 202.767517).\n",
            "\t Train_Loss: 53.7495 Val_Loss: 202.7675  BEST VAL Loss: 202.7675\n",
            "\n",
            "Epoch 723: Validation loss decreased (202.767517 --> 202.231583).\n",
            "\t Train_Loss: 53.5524 Val_Loss: 202.2316  BEST VAL Loss: 202.2316\n",
            "\n",
            "Epoch 724: Validation loss decreased (202.231583 --> 201.676834).\n",
            "\t Train_Loss: 53.3571 Val_Loss: 201.6768  BEST VAL Loss: 201.6768\n",
            "\n",
            "Epoch 725: Validation loss decreased (201.676834 --> 201.051071).\n",
            "\t Train_Loss: 53.1639 Val_Loss: 201.0511  BEST VAL Loss: 201.0511\n",
            "\n",
            "Epoch 726: Validation loss decreased (201.051071 --> 200.364365).\n",
            "\t Train_Loss: 52.9720 Val_Loss: 200.3644  BEST VAL Loss: 200.3644\n",
            "\n",
            "Epoch 727: Validation loss decreased (200.364365 --> 199.675766).\n",
            "\t Train_Loss: 52.7816 Val_Loss: 199.6758  BEST VAL Loss: 199.6758\n",
            "\n",
            "Epoch 728: Validation loss decreased (199.675766 --> 199.043015).\n",
            "\t Train_Loss: 52.5935 Val_Loss: 199.0430  BEST VAL Loss: 199.0430\n",
            "\n",
            "Epoch 729: Validation loss decreased (199.043015 --> 198.480072).\n",
            "\t Train_Loss: 52.4072 Val_Loss: 198.4801  BEST VAL Loss: 198.4801\n",
            "\n",
            "Epoch 730: Validation loss decreased (198.480072 --> 197.954041).\n",
            "\t Train_Loss: 52.2224 Val_Loss: 197.9540  BEST VAL Loss: 197.9540\n",
            "\n",
            "Epoch 731: Validation loss decreased (197.954041 --> 197.412842).\n",
            "\t Train_Loss: 52.0393 Val_Loss: 197.4128  BEST VAL Loss: 197.4128\n",
            "\n",
            "Epoch 732: Validation loss decreased (197.412842 --> 196.827332).\n",
            "\t Train_Loss: 51.8581 Val_Loss: 196.8273  BEST VAL Loss: 196.8273\n",
            "\n",
            "Epoch 733: Validation loss decreased (196.827332 --> 196.208969).\n",
            "\t Train_Loss: 51.6782 Val_Loss: 196.2090  BEST VAL Loss: 196.2090\n",
            "\n",
            "Epoch 734: Validation loss decreased (196.208969 --> 195.596420).\n",
            "\t Train_Loss: 51.5000 Val_Loss: 195.5964  BEST VAL Loss: 195.5964\n",
            "\n",
            "Epoch 735: Validation loss decreased (195.596420 --> 195.021164).\n",
            "\t Train_Loss: 51.3236 Val_Loss: 195.0212  BEST VAL Loss: 195.0212\n",
            "\n",
            "Epoch 736: Validation loss decreased (195.021164 --> 194.485153).\n",
            "\t Train_Loss: 51.1487 Val_Loss: 194.4852  BEST VAL Loss: 194.4852\n",
            "\n",
            "Epoch 737: Validation loss decreased (194.485153 --> 193.962097).\n",
            "\t Train_Loss: 50.9753 Val_Loss: 193.9621  BEST VAL Loss: 193.9621\n",
            "\n",
            "Epoch 738: Validation loss decreased (193.962097 --> 193.419357).\n",
            "\t Train_Loss: 50.8035 Val_Loss: 193.4194  BEST VAL Loss: 193.4194\n",
            "\n",
            "Epoch 739: Validation loss decreased (193.419357 --> 192.844223).\n",
            "\t Train_Loss: 50.6332 Val_Loss: 192.8442  BEST VAL Loss: 192.8442\n",
            "\n",
            "Epoch 740: Validation loss decreased (192.844223 --> 192.251511).\n",
            "\t Train_Loss: 50.4644 Val_Loss: 192.2515  BEST VAL Loss: 192.2515\n",
            "\n",
            "Epoch 741: Validation loss decreased (192.251511 --> 191.670425).\n",
            "\t Train_Loss: 50.2970 Val_Loss: 191.6704  BEST VAL Loss: 191.6704\n",
            "\n",
            "Epoch 742: Validation loss decreased (191.670425 --> 191.121933).\n",
            "\t Train_Loss: 50.1312 Val_Loss: 191.1219  BEST VAL Loss: 191.1219\n",
            "\n",
            "Epoch 743: Validation loss decreased (191.121933 --> 190.604431).\n",
            "\t Train_Loss: 49.9666 Val_Loss: 190.6044  BEST VAL Loss: 190.6044\n",
            "\n",
            "Epoch 744: Validation loss decreased (190.604431 --> 190.097092).\n",
            "\t Train_Loss: 49.8034 Val_Loss: 190.0971  BEST VAL Loss: 190.0971\n",
            "\n",
            "Epoch 745: Validation loss decreased (190.097092 --> 189.576691).\n",
            "\t Train_Loss: 49.6417 Val_Loss: 189.5767  BEST VAL Loss: 189.5767\n",
            "\n",
            "Epoch 746: Validation loss decreased (189.576691 --> 189.034271).\n",
            "\t Train_Loss: 49.4814 Val_Loss: 189.0343  BEST VAL Loss: 189.0343\n",
            "\n",
            "Epoch 747: Validation loss decreased (189.034271 --> 188.479431).\n",
            "\t Train_Loss: 49.3225 Val_Loss: 188.4794  BEST VAL Loss: 188.4794\n",
            "\n",
            "Epoch 748: Validation loss decreased (188.479431 --> 187.931396).\n",
            "\t Train_Loss: 49.1648 Val_Loss: 187.9314  BEST VAL Loss: 187.9314\n",
            "\n",
            "Epoch 749: Validation loss decreased (187.931396 --> 187.404053).\n",
            "\t Train_Loss: 49.0086 Val_Loss: 187.4041  BEST VAL Loss: 187.4041\n",
            "\n",
            "Epoch 750: Validation loss decreased (187.404053 --> 186.896439).\n",
            "\t Train_Loss: 48.8536 Val_Loss: 186.8964  BEST VAL Loss: 186.8964\n",
            "\n",
            "Epoch 751: Validation loss decreased (186.896439 --> 186.396484).\n",
            "\t Train_Loss: 48.6999 Val_Loss: 186.3965  BEST VAL Loss: 186.3965\n",
            "\n",
            "Epoch 752: Validation loss decreased (186.396484 --> 185.890945).\n",
            "\t Train_Loss: 48.5474 Val_Loss: 185.8909  BEST VAL Loss: 185.8909\n",
            "\n",
            "Epoch 753: Validation loss decreased (185.890945 --> 185.379318).\n",
            "\t Train_Loss: 48.3959 Val_Loss: 185.3793  BEST VAL Loss: 185.3793\n",
            "\n",
            "Epoch 754: Validation loss decreased (185.379318 --> 184.880981).\n",
            "\t Train_Loss: 48.2447 Val_Loss: 184.8810  BEST VAL Loss: 184.8810\n",
            "\n",
            "Epoch 755: Validation loss decreased (184.880981 --> 184.379868).\n",
            "\t Train_Loss: 48.0992 Val_Loss: 184.3799  BEST VAL Loss: 184.3799\n",
            "\n",
            "Epoch 756: Validation loss decreased (184.379868 --> 183.891220).\n",
            "\t Train_Loss: 47.9483 Val_Loss: 183.8912  BEST VAL Loss: 183.8912\n",
            "\n",
            "Epoch 757: Validation loss decreased (183.891220 --> 183.393066).\n",
            "\t Train_Loss: 47.8039 Val_Loss: 183.3931  BEST VAL Loss: 183.3931\n",
            "\n",
            "Epoch 758: Validation loss decreased (183.393066 --> 182.889069).\n",
            "\t Train_Loss: 47.6598 Val_Loss: 182.8891  BEST VAL Loss: 182.8891\n",
            "\n",
            "Epoch 759: Validation loss decreased (182.889069 --> 182.387634).\n",
            "\t Train_Loss: 47.5165 Val_Loss: 182.3876  BEST VAL Loss: 182.3876\n",
            "\n",
            "Epoch 760: Validation loss decreased (182.387634 --> 181.895676).\n",
            "\t Train_Loss: 47.3742 Val_Loss: 181.8957  BEST VAL Loss: 181.8957\n",
            "\n",
            "Epoch 761: Validation loss decreased (181.895676 --> 181.414062).\n",
            "\t Train_Loss: 47.2331 Val_Loss: 181.4141  BEST VAL Loss: 181.4141\n",
            "\n",
            "Epoch 762: Validation loss decreased (181.414062 --> 180.937775).\n",
            "\t Train_Loss: 47.0931 Val_Loss: 180.9378  BEST VAL Loss: 180.9378\n",
            "\n",
            "Epoch 763: Validation loss decreased (180.937775 --> 180.462387).\n",
            "\t Train_Loss: 46.9543 Val_Loss: 180.4624  BEST VAL Loss: 180.4624\n",
            "\n",
            "Epoch 764: Validation loss decreased (180.462387 --> 179.986328).\n",
            "\t Train_Loss: 46.8166 Val_Loss: 179.9863  BEST VAL Loss: 179.9863\n",
            "\n",
            "Epoch 765: Validation loss decreased (179.986328 --> 179.512985).\n",
            "\t Train_Loss: 46.6801 Val_Loss: 179.5130  BEST VAL Loss: 179.5130\n",
            "\n",
            "Epoch 766: Validation loss decreased (179.512985 --> 179.046799).\n",
            "\t Train_Loss: 46.5446 Val_Loss: 179.0468  BEST VAL Loss: 179.0468\n",
            "\n",
            "Epoch 767: Validation loss decreased (179.046799 --> 178.589645).\n",
            "\t Train_Loss: 46.4102 Val_Loss: 178.5896  BEST VAL Loss: 178.5896\n",
            "\n",
            "Epoch 768: Validation loss decreased (178.589645 --> 178.139526).\n",
            "\t Train_Loss: 46.2770 Val_Loss: 178.1395  BEST VAL Loss: 178.1395\n",
            "\n",
            "Epoch 769: Validation loss decreased (178.139526 --> 177.690536).\n",
            "\t Train_Loss: 46.1448 Val_Loss: 177.6905  BEST VAL Loss: 177.6905\n",
            "\n",
            "Epoch 770: Validation loss decreased (177.690536 --> 177.238205).\n",
            "\t Train_Loss: 46.0137 Val_Loss: 177.2382  BEST VAL Loss: 177.2382\n",
            "\n",
            "Epoch 771: Validation loss decreased (177.238205 --> 176.781189).\n",
            "\t Train_Loss: 45.8836 Val_Loss: 176.7812  BEST VAL Loss: 176.7812\n",
            "\n",
            "Epoch 772: Validation loss decreased (176.781189 --> 176.321945).\n",
            "\t Train_Loss: 45.7545 Val_Loss: 176.3219  BEST VAL Loss: 176.3219\n",
            "\n",
            "Epoch 773: Validation loss decreased (176.321945 --> 175.865891).\n",
            "\t Train_Loss: 45.6265 Val_Loss: 175.8659  BEST VAL Loss: 175.8659\n",
            "\n",
            "Epoch 774: Validation loss decreased (175.865891 --> 175.417007).\n",
            "\t Train_Loss: 45.4995 Val_Loss: 175.4170  BEST VAL Loss: 175.4170\n",
            "\n",
            "Epoch 775: Validation loss decreased (175.417007 --> 174.976303).\n",
            "\t Train_Loss: 45.3734 Val_Loss: 174.9763  BEST VAL Loss: 174.9763\n",
            "\n",
            "Epoch 776: Validation loss decreased (174.976303 --> 174.541702).\n",
            "\t Train_Loss: 45.2484 Val_Loss: 174.5417  BEST VAL Loss: 174.5417\n",
            "\n",
            "Epoch 777: Validation loss decreased (174.541702 --> 174.109787).\n",
            "\t Train_Loss: 45.1243 Val_Loss: 174.1098  BEST VAL Loss: 174.1098\n",
            "\n",
            "Epoch 778: Validation loss decreased (174.109787 --> 173.678375).\n",
            "\t Train_Loss: 45.0011 Val_Loss: 173.6784  BEST VAL Loss: 173.6784\n",
            "\n",
            "Epoch 779: Validation loss decreased (173.678375 --> 173.247360).\n",
            "\t Train_Loss: 44.8789 Val_Loss: 173.2474  BEST VAL Loss: 173.2474\n",
            "\n",
            "Epoch 780: Validation loss decreased (173.247360 --> 172.818802).\n",
            "\t Train_Loss: 44.7576 Val_Loss: 172.8188  BEST VAL Loss: 172.8188\n",
            "\n",
            "Epoch 781: Validation loss decreased (172.818802 --> 172.394073).\n",
            "\t Train_Loss: 44.6373 Val_Loss: 172.3941  BEST VAL Loss: 172.3941\n",
            "\n",
            "Epoch 782: Validation loss decreased (172.394073 --> 171.973404).\n",
            "\t Train_Loss: 44.5178 Val_Loss: 171.9734  BEST VAL Loss: 171.9734\n",
            "\n",
            "Epoch 783: Validation loss decreased (171.973404 --> 171.555588).\n",
            "\t Train_Loss: 44.3992 Val_Loss: 171.5556  BEST VAL Loss: 171.5556\n",
            "\n",
            "Epoch 784: Validation loss decreased (171.555588 --> 171.138779).\n",
            "\t Train_Loss: 44.2815 Val_Loss: 171.1388  BEST VAL Loss: 171.1388\n",
            "\n",
            "Epoch 785: Validation loss decreased (171.138779 --> 170.722153).\n",
            "\t Train_Loss: 44.1646 Val_Loss: 170.7222  BEST VAL Loss: 170.7222\n",
            "\n",
            "Epoch 786: Validation loss decreased (170.722153 --> 170.306442).\n",
            "\t Train_Loss: 44.0486 Val_Loss: 170.3064  BEST VAL Loss: 170.3064\n",
            "\n",
            "Epoch 787: Validation loss decreased (170.306442 --> 169.893707).\n",
            "\t Train_Loss: 43.9334 Val_Loss: 169.8937  BEST VAL Loss: 169.8937\n",
            "\n",
            "Epoch 788: Validation loss decreased (169.893707 --> 169.485458).\n",
            "\t Train_Loss: 43.8191 Val_Loss: 169.4855  BEST VAL Loss: 169.4855\n",
            "\n",
            "Epoch 789: Validation loss decreased (169.485458 --> 169.082367).\n",
            "\t Train_Loss: 43.7055 Val_Loss: 169.0824  BEST VAL Loss: 169.0824\n",
            "\n",
            "Epoch 790: Validation loss decreased (169.082367 --> 168.683716).\n",
            "\t Train_Loss: 43.5928 Val_Loss: 168.6837  BEST VAL Loss: 168.6837\n",
            "\n",
            "Epoch 791: Validation loss decreased (168.683716 --> 168.287277).\n",
            "\t Train_Loss: 43.4809 Val_Loss: 168.2873  BEST VAL Loss: 168.2873\n",
            "\n",
            "Epoch 792: Validation loss decreased (168.287277 --> 167.891479).\n",
            "\t Train_Loss: 43.3697 Val_Loss: 167.8915  BEST VAL Loss: 167.8915\n",
            "\n",
            "Epoch 793: Validation loss decreased (167.891479 --> 167.496048).\n",
            "\t Train_Loss: 43.2593 Val_Loss: 167.4960  BEST VAL Loss: 167.4960\n",
            "\n",
            "Epoch 794: Validation loss decreased (167.496048 --> 167.101181).\n",
            "\t Train_Loss: 43.1496 Val_Loss: 167.1012  BEST VAL Loss: 167.1012\n",
            "\n",
            "Epoch 795: Validation loss decreased (167.101181 --> 166.708847).\n",
            "\t Train_Loss: 43.0407 Val_Loss: 166.7088  BEST VAL Loss: 166.7088\n",
            "\n",
            "Epoch 796: Validation loss decreased (166.708847 --> 166.319336).\n",
            "\t Train_Loss: 42.9326 Val_Loss: 166.3193  BEST VAL Loss: 166.3193\n",
            "\n",
            "Epoch 797: Validation loss decreased (166.319336 --> 165.932999).\n",
            "\t Train_Loss: 42.8252 Val_Loss: 165.9330  BEST VAL Loss: 165.9330\n",
            "\n",
            "Epoch 798: Validation loss decreased (165.932999 --> 165.549271).\n",
            "\t Train_Loss: 42.7184 Val_Loss: 165.5493  BEST VAL Loss: 165.5493\n",
            "\n",
            "Epoch 799: Validation loss decreased (165.549271 --> 165.167801).\n",
            "\t Train_Loss: 42.6124 Val_Loss: 165.1678  BEST VAL Loss: 165.1678\n",
            "\n",
            "Epoch 800: Validation loss decreased (165.167801 --> 164.787949).\n",
            "\t Train_Loss: 42.5071 Val_Loss: 164.7879  BEST VAL Loss: 164.7879\n",
            "\n",
            "Epoch 801: Validation loss decreased (164.787949 --> 164.410110).\n",
            "\t Train_Loss: 42.4025 Val_Loss: 164.4101  BEST VAL Loss: 164.4101\n",
            "\n",
            "Epoch 802: Validation loss decreased (164.410110 --> 164.034775).\n",
            "\t Train_Loss: 42.2985 Val_Loss: 164.0348  BEST VAL Loss: 164.0348\n",
            "\n",
            "Epoch 803: Validation loss decreased (164.034775 --> 163.662155).\n",
            "\t Train_Loss: 42.1952 Val_Loss: 163.6622  BEST VAL Loss: 163.6622\n",
            "\n",
            "Epoch 804: Validation loss decreased (163.662155 --> 163.292282).\n",
            "\t Train_Loss: 42.0926 Val_Loss: 163.2923  BEST VAL Loss: 163.2923\n",
            "\n",
            "Epoch 805: Validation loss decreased (163.292282 --> 162.923996).\n",
            "\t Train_Loss: 41.9906 Val_Loss: 162.9240  BEST VAL Loss: 162.9240\n",
            "\n",
            "Epoch 806: Validation loss decreased (162.923996 --> 162.557419).\n",
            "\t Train_Loss: 41.8892 Val_Loss: 162.5574  BEST VAL Loss: 162.5574\n",
            "\n",
            "Epoch 807: Validation loss decreased (162.557419 --> 162.192154).\n",
            "\t Train_Loss: 41.7885 Val_Loss: 162.1922  BEST VAL Loss: 162.1922\n",
            "\n",
            "Epoch 808: Validation loss decreased (162.192154 --> 161.828110).\n",
            "\t Train_Loss: 41.6884 Val_Loss: 161.8281  BEST VAL Loss: 161.8281\n",
            "\n",
            "Epoch 809: Validation loss decreased (161.828110 --> 161.466354).\n",
            "\t Train_Loss: 41.5889 Val_Loss: 161.4664  BEST VAL Loss: 161.4664\n",
            "\n",
            "Epoch 810: Validation loss decreased (161.466354 --> 161.107162).\n",
            "\t Train_Loss: 41.4900 Val_Loss: 161.1072  BEST VAL Loss: 161.1072\n",
            "\n",
            "Epoch 811: Validation loss decreased (161.107162 --> 160.750534).\n",
            "\t Train_Loss: 41.3917 Val_Loss: 160.7505  BEST VAL Loss: 160.7505\n",
            "\n",
            "Epoch 812: Validation loss decreased (160.750534 --> 160.396072).\n",
            "\t Train_Loss: 41.2940 Val_Loss: 160.3961  BEST VAL Loss: 160.3961\n",
            "\n",
            "Epoch 813: Validation loss decreased (160.396072 --> 160.043808).\n",
            "\t Train_Loss: 41.1969 Val_Loss: 160.0438  BEST VAL Loss: 160.0438\n",
            "\n",
            "Epoch 814: Validation loss decreased (160.043808 --> 159.693130).\n",
            "\t Train_Loss: 41.1003 Val_Loss: 159.6931  BEST VAL Loss: 159.6931\n",
            "\n",
            "Epoch 815: Validation loss decreased (159.693130 --> 159.344254).\n",
            "\t Train_Loss: 41.0044 Val_Loss: 159.3443  BEST VAL Loss: 159.3443\n",
            "\n",
            "Epoch 816: Validation loss decreased (159.344254 --> 158.997116).\n",
            "\t Train_Loss: 40.9089 Val_Loss: 158.9971  BEST VAL Loss: 158.9971\n",
            "\n",
            "Epoch 817: Validation loss decreased (158.997116 --> 158.651566).\n",
            "\t Train_Loss: 40.8141 Val_Loss: 158.6516  BEST VAL Loss: 158.6516\n",
            "\n",
            "Epoch 818: Validation loss decreased (158.651566 --> 158.308121).\n",
            "\t Train_Loss: 40.7197 Val_Loss: 158.3081  BEST VAL Loss: 158.3081\n",
            "\n",
            "Epoch 819: Validation loss decreased (158.308121 --> 157.966293).\n",
            "\t Train_Loss: 40.6259 Val_Loss: 157.9663  BEST VAL Loss: 157.9663\n",
            "\n",
            "Epoch 820: Validation loss decreased (157.966293 --> 157.626221).\n",
            "\t Train_Loss: 40.5326 Val_Loss: 157.6262  BEST VAL Loss: 157.6262\n",
            "\n",
            "Epoch 821: Validation loss decreased (157.626221 --> 157.287628).\n",
            "\t Train_Loss: 40.4398 Val_Loss: 157.2876  BEST VAL Loss: 157.2876\n",
            "\n",
            "Epoch 822: Validation loss decreased (157.287628 --> 156.950882).\n",
            "\t Train_Loss: 40.3475 Val_Loss: 156.9509  BEST VAL Loss: 156.9509\n",
            "\n",
            "Epoch 823: Validation loss decreased (156.950882 --> 156.615921).\n",
            "\t Train_Loss: 40.2557 Val_Loss: 156.6159  BEST VAL Loss: 156.6159\n",
            "\n",
            "Epoch 824: Validation loss decreased (156.615921 --> 156.283188).\n",
            "\t Train_Loss: 40.1644 Val_Loss: 156.2832  BEST VAL Loss: 156.2832\n",
            "\n",
            "Epoch 825: Validation loss decreased (156.283188 --> 155.952393).\n",
            "\t Train_Loss: 40.0735 Val_Loss: 155.9524  BEST VAL Loss: 155.9524\n",
            "\n",
            "Epoch 826: Validation loss decreased (155.952393 --> 155.623734).\n",
            "\t Train_Loss: 39.9827 Val_Loss: 155.6237  BEST VAL Loss: 155.6237\n",
            "\n",
            "Epoch 827: Validation loss decreased (155.623734 --> 155.296295).\n",
            "\t Train_Loss: 39.8913 Val_Loss: 155.2963  BEST VAL Loss: 155.2963\n",
            "\n",
            "Epoch 828: Validation loss decreased (155.296295 --> 154.950378).\n",
            "\t Train_Loss: 39.7923 Val_Loss: 154.9504  BEST VAL Loss: 154.9504\n",
            "\n",
            "Epoch 829: Validation loss decreased (154.950378 --> 154.172852).\n",
            "\t Train_Loss: 39.6537 Val_Loss: 154.1729  BEST VAL Loss: 154.1729\n",
            "\n",
            "Epoch 830: Validation loss did not decrease\n",
            "\t Train_Loss: 39.7605 Val_Loss: 155.0635  BEST VAL Loss: 154.1729\n",
            "\n",
            "Epoch 831: Validation loss did not decrease\n",
            "\t Train_Loss: 39.4614 Val_Loss: 155.1881  BEST VAL Loss: 154.1729\n",
            "\n",
            "Epoch 832: Validation loss did not decrease\n",
            "\t Train_Loss: 39.4557 Val_Loss: 154.5715  BEST VAL Loss: 154.1729\n",
            "\n",
            "Epoch 833: Validation loss decreased (154.172852 --> 153.546173).\n",
            "\t Train_Loss: 39.3743 Val_Loss: 153.5462  BEST VAL Loss: 153.5462\n",
            "\n",
            "Epoch 834: Validation loss decreased (153.546173 --> 152.608963).\n",
            "\t Train_Loss: 39.2641 Val_Loss: 152.6090  BEST VAL Loss: 152.6090\n",
            "\n",
            "Epoch 835: Validation loss decreased (152.608963 --> 152.082626).\n",
            "\t Train_Loss: 39.1982 Val_Loss: 152.0826  BEST VAL Loss: 152.0826\n",
            "\n",
            "Epoch 836: Validation loss decreased (152.082626 --> 151.998642).\n",
            "\t Train_Loss: 39.1389 Val_Loss: 151.9986  BEST VAL Loss: 151.9986\n",
            "\n",
            "Epoch 837: Validation loss did not decrease\n",
            "\t Train_Loss: 39.0425 Val_Loss: 152.1673  BEST VAL Loss: 151.9986\n",
            "\n",
            "Epoch 838: Validation loss did not decrease\n",
            "\t Train_Loss: 38.9327 Val_Loss: 152.3100  BEST VAL Loss: 151.9986\n",
            "\n",
            "Epoch 839: Validation loss did not decrease\n",
            "\t Train_Loss: 38.8467 Val_Loss: 152.1887  BEST VAL Loss: 151.9986\n",
            "\n",
            "Epoch 840: Validation loss decreased (151.998642 --> 151.719437).\n",
            "\t Train_Loss: 38.7755 Val_Loss: 151.7194  BEST VAL Loss: 151.7194\n",
            "\n",
            "Epoch 841: Validation loss decreased (151.719437 --> 151.023193).\n",
            "\t Train_Loss: 38.6852 Val_Loss: 151.0232  BEST VAL Loss: 151.0232\n",
            "\n",
            "Epoch 842: Validation loss decreased (151.023193 --> 150.335739).\n",
            "\t Train_Loss: 38.5733 Val_Loss: 150.3357  BEST VAL Loss: 150.3357\n",
            "\n",
            "Epoch 843: Validation loss decreased (150.335739 --> 149.840683).\n",
            "\t Train_Loss: 38.4560 Val_Loss: 149.8407  BEST VAL Loss: 149.8407\n",
            "\n",
            "Epoch 844: Validation loss decreased (149.840683 --> 149.575729).\n",
            "\t Train_Loss: 38.3198 Val_Loss: 149.5757  BEST VAL Loss: 149.5757\n",
            "\n",
            "Epoch 845: Validation loss decreased (149.575729 --> 149.507584).\n",
            "\t Train_Loss: 38.1655 Val_Loss: 149.5076  BEST VAL Loss: 149.5076\n",
            "\n",
            "Epoch 846: Validation loss did not decrease\n",
            "\t Train_Loss: 38.1114 Val_Loss: 149.5775  BEST VAL Loss: 149.5076\n",
            "\n",
            "Epoch 847: Validation loss did not decrease\n",
            "\t Train_Loss: 38.0027 Val_Loss: 149.5888  BEST VAL Loss: 149.5076\n",
            "\n",
            "Epoch 848: Validation loss decreased (149.507584 --> 149.420456).\n",
            "\t Train_Loss: 37.8489 Val_Loss: 149.4205  BEST VAL Loss: 149.4205\n",
            "\n",
            "Epoch 849: Validation loss decreased (149.420456 --> 149.017014).\n",
            "\t Train_Loss: 37.7720 Val_Loss: 149.0170  BEST VAL Loss: 149.0170\n",
            "\n",
            "Epoch 850: Validation loss decreased (149.017014 --> 148.511856).\n",
            "\t Train_Loss: 37.6649 Val_Loss: 148.5119  BEST VAL Loss: 148.5119\n",
            "\n",
            "Epoch 851: Validation loss decreased (148.511856 --> 148.042786).\n",
            "\t Train_Loss: 37.5137 Val_Loss: 148.0428  BEST VAL Loss: 148.0428\n",
            "\n",
            "Epoch 852: Validation loss decreased (148.042786 --> 147.675201).\n",
            "\t Train_Loss: 37.3808 Val_Loss: 147.6752  BEST VAL Loss: 147.6752\n",
            "\n",
            "Epoch 853: Validation loss decreased (147.675201 --> 147.320908).\n",
            "\t Train_Loss: 37.2988 Val_Loss: 147.3209  BEST VAL Loss: 147.3209\n",
            "\n",
            "Epoch 854: Validation loss decreased (147.320908 --> 147.115250).\n",
            "\t Train_Loss: 37.1351 Val_Loss: 147.1152  BEST VAL Loss: 147.1152\n",
            "\n",
            "Epoch 855: Validation loss decreased (147.115250 --> 147.107880).\n",
            "\t Train_Loss: 36.9876 Val_Loss: 147.1079  BEST VAL Loss: 147.1079\n",
            "\n",
            "Epoch 856: Validation loss did not decrease\n",
            "\t Train_Loss: 36.8789 Val_Loss: 147.1199  BEST VAL Loss: 147.1079\n",
            "\n",
            "Epoch 857: Validation loss decreased (147.107880 --> 146.969482).\n",
            "\t Train_Loss: 36.7155 Val_Loss: 146.9695  BEST VAL Loss: 146.9695\n",
            "\n",
            "Epoch 858: Validation loss decreased (146.969482 --> 146.334183).\n",
            "\t Train_Loss: 36.5831 Val_Loss: 146.3342  BEST VAL Loss: 146.3342\n",
            "\n",
            "Epoch 859: Validation loss decreased (146.334183 --> 145.425217).\n",
            "\t Train_Loss: 36.4205 Val_Loss: 145.4252  BEST VAL Loss: 145.4252\n",
            "\n",
            "Epoch 860: Validation loss decreased (145.425217 --> 144.829941).\n",
            "\t Train_Loss: 36.2615 Val_Loss: 144.8299  BEST VAL Loss: 144.8299\n",
            "\n",
            "Epoch 861: Validation loss decreased (144.829941 --> 144.733017).\n",
            "\t Train_Loss: 36.1249 Val_Loss: 144.7330  BEST VAL Loss: 144.7330\n",
            "\n",
            "Epoch 862: Validation loss did not decrease\n",
            "\t Train_Loss: 35.9502 Val_Loss: 144.8090  BEST VAL Loss: 144.7330\n",
            "\n",
            "Epoch 863: Validation loss decreased (144.733017 --> 144.616333).\n",
            "\t Train_Loss: 35.7996 Val_Loss: 144.6163  BEST VAL Loss: 144.6163\n",
            "\n",
            "Epoch 864: Validation loss decreased (144.616333 --> 144.438354).\n",
            "\t Train_Loss: 35.6170 Val_Loss: 144.4384  BEST VAL Loss: 144.4384\n",
            "\n",
            "Epoch 865: Validation loss decreased (144.438354 --> 144.306564).\n",
            "\t Train_Loss: 35.4759 Val_Loss: 144.3066  BEST VAL Loss: 144.3066\n",
            "\n",
            "Epoch 866: Validation loss decreased (144.306564 --> 143.923370).\n",
            "\t Train_Loss: 35.2953 Val_Loss: 143.9234  BEST VAL Loss: 143.9234\n",
            "\n",
            "Epoch 867: Validation loss decreased (143.923370 --> 143.091782).\n",
            "\t Train_Loss: 35.1372 Val_Loss: 143.0918  BEST VAL Loss: 143.0918\n",
            "\n",
            "Epoch 868: Validation loss decreased (143.091782 --> 142.507507).\n",
            "\t Train_Loss: 34.9570 Val_Loss: 142.5075  BEST VAL Loss: 142.5075\n",
            "\n",
            "Epoch 869: Validation loss decreased (142.507507 --> 142.419876).\n",
            "\t Train_Loss: 34.8040 Val_Loss: 142.4199  BEST VAL Loss: 142.4199\n",
            "\n",
            "Epoch 870: Validation loss decreased (142.419876 --> 142.284210).\n",
            "\t Train_Loss: 34.6300 Val_Loss: 142.2842  BEST VAL Loss: 142.2842\n",
            "\n",
            "Epoch 871: Validation loss decreased (142.284210 --> 141.851181).\n",
            "\t Train_Loss: 34.4655 Val_Loss: 141.8512  BEST VAL Loss: 141.8512\n",
            "\n",
            "Epoch 872: Validation loss decreased (141.851181 --> 141.660202).\n",
            "\t Train_Loss: 34.2951 Val_Loss: 141.6602  BEST VAL Loss: 141.6602\n",
            "\n",
            "Epoch 873: Validation loss decreased (141.660202 --> 141.628860).\n",
            "\t Train_Loss: 34.1310 Val_Loss: 141.6289  BEST VAL Loss: 141.6289\n",
            "\n",
            "Epoch 874: Validation loss decreased (141.628860 --> 141.141479).\n",
            "\t Train_Loss: 33.9701 Val_Loss: 141.1415  BEST VAL Loss: 141.1415\n",
            "\n",
            "Epoch 875: Validation loss decreased (141.141479 --> 140.460373).\n",
            "\t Train_Loss: 33.8007 Val_Loss: 140.4604  BEST VAL Loss: 140.4604\n",
            "\n",
            "Epoch 876: Validation loss decreased (140.460373 --> 140.166153).\n",
            "\t Train_Loss: 33.6430 Val_Loss: 140.1662  BEST VAL Loss: 140.1662\n",
            "\n",
            "Epoch 877: Validation loss decreased (140.166153 --> 140.000732).\n",
            "\t Train_Loss: 33.4754 Val_Loss: 140.0007  BEST VAL Loss: 140.0007\n",
            "\n",
            "Epoch 878: Validation loss decreased (140.000732 --> 139.418320).\n",
            "\t Train_Loss: 33.3234 Val_Loss: 139.4183  BEST VAL Loss: 139.4183\n",
            "\n",
            "Epoch 879: Validation loss decreased (139.418320 --> 139.048447).\n",
            "\t Train_Loss: 33.1598 Val_Loss: 139.0484  BEST VAL Loss: 139.0484\n",
            "\n",
            "Epoch 880: Validation loss decreased (139.048447 --> 139.040268).\n",
            "\t Train_Loss: 33.0051 Val_Loss: 139.0403  BEST VAL Loss: 139.0403\n",
            "\n",
            "Epoch 881: Validation loss decreased (139.040268 --> 138.705307).\n",
            "\t Train_Loss: 32.8476 Val_Loss: 138.7053  BEST VAL Loss: 138.7053\n",
            "\n",
            "Epoch 882: Validation loss decreased (138.705307 --> 138.167374).\n",
            "\t Train_Loss: 32.6919 Val_Loss: 138.1674  BEST VAL Loss: 138.1674\n",
            "\n",
            "Epoch 883: Validation loss decreased (138.167374 --> 138.006943).\n",
            "\t Train_Loss: 32.5414 Val_Loss: 138.0069  BEST VAL Loss: 138.0069\n",
            "\n",
            "Epoch 884: Validation loss decreased (138.006943 --> 137.839508).\n",
            "\t Train_Loss: 32.3841 Val_Loss: 137.8395  BEST VAL Loss: 137.8395\n",
            "\n",
            "Epoch 885: Validation loss decreased (137.839508 --> 137.213135).\n",
            "\t Train_Loss: 32.2344 Val_Loss: 137.2131  BEST VAL Loss: 137.2131\n",
            "\n",
            "Epoch 886: Validation loss decreased (137.213135 --> 136.957214).\n",
            "\t Train_Loss: 32.0818 Val_Loss: 136.9572  BEST VAL Loss: 136.9572\n",
            "\n",
            "Epoch 887: Validation loss decreased (136.957214 --> 136.888351).\n",
            "\t Train_Loss: 31.9299 Val_Loss: 136.8884  BEST VAL Loss: 136.8884\n",
            "\n",
            "Epoch 888: Validation loss decreased (136.888351 --> 136.226791).\n",
            "\t Train_Loss: 31.7830 Val_Loss: 136.2268  BEST VAL Loss: 136.2268\n",
            "\n",
            "Epoch 889: Validation loss decreased (136.226791 --> 135.996246).\n",
            "\t Train_Loss: 31.6315 Val_Loss: 135.9962  BEST VAL Loss: 135.9962\n",
            "\n",
            "Epoch 890: Validation loss decreased (135.996246 --> 135.913422).\n",
            "\t Train_Loss: 31.4812 Val_Loss: 135.9134  BEST VAL Loss: 135.9134\n",
            "\n",
            "Epoch 891: Validation loss decreased (135.913422 --> 135.222977).\n",
            "\t Train_Loss: 31.3362 Val_Loss: 135.2230  BEST VAL Loss: 135.2230\n",
            "\n",
            "Epoch 892: Validation loss decreased (135.222977 --> 135.167618).\n",
            "\t Train_Loss: 31.1893 Val_Loss: 135.1676  BEST VAL Loss: 135.1676\n",
            "\n",
            "Epoch 893: Validation loss decreased (135.167618 --> 134.916504).\n",
            "\t Train_Loss: 31.0407 Val_Loss: 134.9165  BEST VAL Loss: 134.9165\n",
            "\n",
            "Epoch 894: Validation loss decreased (134.916504 --> 134.285614).\n",
            "\t Train_Loss: 30.8967 Val_Loss: 134.2856  BEST VAL Loss: 134.2856\n",
            "\n",
            "Epoch 895: Validation loss did not decrease\n",
            "\t Train_Loss: 30.7572 Val_Loss: 134.6096  BEST VAL Loss: 134.2856\n",
            "\n",
            "Epoch 896: Validation loss decreased (134.285614 --> 133.498032).\n",
            "\t Train_Loss: 30.6206 Val_Loss: 133.4980  BEST VAL Loss: 133.4980\n",
            "\n",
            "Epoch 897: Validation loss did not decrease\n",
            "\t Train_Loss: 30.4902 Val_Loss: 134.5133  BEST VAL Loss: 133.4980\n",
            "\n",
            "Epoch 898: Validation loss decreased (133.498032 --> 132.375290).\n",
            "\t Train_Loss: 30.3665 Val_Loss: 132.3753  BEST VAL Loss: 132.3753\n",
            "\n",
            "Epoch 899: Validation loss did not decrease\n",
            "\t Train_Loss: 30.2841 Val_Loss: 134.6155  BEST VAL Loss: 132.3753\n",
            "\n",
            "Epoch 900: Validation loss decreased (132.375290 --> 131.684448).\n",
            "\t Train_Loss: 30.1504 Val_Loss: 131.6844  BEST VAL Loss: 131.6844\n",
            "\n",
            "Epoch 901: Validation loss did not decrease\n",
            "\t Train_Loss: 30.0413 Val_Loss: 132.6150  BEST VAL Loss: 131.6844\n",
            "\n",
            "Epoch 902: Validation loss did not decrease\n",
            "\t Train_Loss: 29.8176 Val_Loss: 134.0162  BEST VAL Loss: 131.6844\n",
            "\n",
            "Epoch 903: Validation loss decreased (131.684448 --> 130.112717).\n",
            "\t Train_Loss: 29.7943 Val_Loss: 130.1127  BEST VAL Loss: 130.1127\n",
            "\n",
            "Epoch 904: Validation loss did not decrease\n",
            "\t Train_Loss: 29.8177 Val_Loss: 130.7992  BEST VAL Loss: 130.1127\n",
            "\n",
            "Epoch 905: Validation loss did not decrease\n",
            "\t Train_Loss: 29.4776 Val_Loss: 134.9726  BEST VAL Loss: 130.1127\n",
            "\n",
            "Epoch 906: Validation loss decreased (130.112717 --> 128.884476).\n",
            "\t Train_Loss: 29.7656 Val_Loss: 128.8845  BEST VAL Loss: 128.8845\n",
            "\n",
            "Epoch 907: Validation loss decreased (128.884476 --> 128.400742).\n",
            "\t Train_Loss: 29.6257 Val_Loss: 128.4007  BEST VAL Loss: 128.4007\n",
            "\n",
            "Epoch 908: Validation loss did not decrease\n",
            "\t Train_Loss: 29.6768 Val_Loss: 129.9143  BEST VAL Loss: 128.4007\n",
            "\n",
            "Epoch 909: Validation loss did not decrease\n",
            "\t Train_Loss: 29.1262 Val_Loss: 133.3956  BEST VAL Loss: 128.4007\n",
            "\n",
            "Epoch 910: Validation loss decreased (128.400742 --> 128.328781).\n",
            "\t Train_Loss: 29.7384 Val_Loss: 128.3288  BEST VAL Loss: 128.3288\n",
            "\n",
            "Epoch 911: Validation loss decreased (128.328781 --> 126.809891).\n",
            "\t Train_Loss: 28.9756 Val_Loss: 126.8099  BEST VAL Loss: 126.8099\n",
            "\n",
            "Epoch 912: Validation loss did not decrease\n",
            "\t Train_Loss: 29.5333 Val_Loss: 127.0004  BEST VAL Loss: 126.8099\n",
            "\n",
            "Epoch 913: Validation loss did not decrease\n",
            "\t Train_Loss: 29.0039 Val_Loss: 128.8074  BEST VAL Loss: 126.8099\n",
            "\n",
            "Epoch 914: Validation loss did not decrease\n",
            "\t Train_Loss: 28.6634 Val_Loss: 132.4355  BEST VAL Loss: 126.8099\n",
            "\n",
            "Epoch 915: Validation loss did not decrease\n",
            "\t Train_Loss: 29.2966 Val_Loss: 127.0778  BEST VAL Loss: 126.8099\n",
            "\n",
            "Epoch 916: Validation loss decreased (126.809891 --> 125.482376).\n",
            "\t Train_Loss: 28.4581 Val_Loss: 125.4824  BEST VAL Loss: 125.4824\n",
            "\n",
            "Epoch 917: Validation loss did not decrease\n",
            "\t Train_Loss: 29.1098 Val_Loss: 125.7844  BEST VAL Loss: 125.4824\n",
            "\n",
            "Epoch 918: Validation loss did not decrease\n",
            "\t Train_Loss: 28.5398 Val_Loss: 128.1757  BEST VAL Loss: 125.4824\n",
            "\n",
            "Epoch 919: Validation loss did not decrease\n",
            "\t Train_Loss: 28.0671 Val_Loss: 132.5338  BEST VAL Loss: 125.4824\n",
            "\n",
            "Epoch 920: Validation loss did not decrease\n",
            "\t Train_Loss: 29.1957 Val_Loss: 125.5777  BEST VAL Loss: 125.4824\n",
            "\n",
            "Epoch 921: Validation loss decreased (125.482376 --> 124.243042).\n",
            "\t Train_Loss: 28.1131 Val_Loss: 124.2430  BEST VAL Loss: 124.2430\n",
            "\n",
            "Epoch 922: Validation loss did not decrease\n",
            "\t Train_Loss: 29.2840 Val_Loss: 124.4981  BEST VAL Loss: 124.2430\n",
            "\n",
            "Epoch 923: Validation loss did not decrease\n",
            "\t Train_Loss: 28.5334 Val_Loss: 125.8153  BEST VAL Loss: 124.2430\n",
            "\n",
            "Epoch 924: Validation loss did not decrease\n",
            "\t Train_Loss: 27.9359 Val_Loss: 127.8866  BEST VAL Loss: 124.2430\n",
            "\n",
            "Epoch 925: Validation loss did not decrease\n",
            "\t Train_Loss: 28.6434 Val_Loss: 126.3771  BEST VAL Loss: 124.2430\n",
            "\n",
            "Epoch 926: Validation loss decreased (124.243042 --> 123.643089).\n",
            "\t Train_Loss: 27.9611 Val_Loss: 123.6431  BEST VAL Loss: 123.6431\n",
            "\n",
            "Epoch 927: Validation loss decreased (123.643089 --> 122.320496).\n",
            "\t Train_Loss: 27.6286 Val_Loss: 122.3205  BEST VAL Loss: 122.3205\n",
            "\n",
            "Epoch 928: Validation loss did not decrease\n",
            "\t Train_Loss: 27.9891 Val_Loss: 122.4604  BEST VAL Loss: 122.3205\n",
            "\n",
            "Epoch 929: Validation loss did not decrease\n",
            "\t Train_Loss: 27.6797 Val_Loss: 125.4237  BEST VAL Loss: 122.3205\n",
            "\n",
            "Epoch 930: Validation loss did not decrease\n",
            "\t Train_Loss: 26.9895 Val_Loss: 130.3847  BEST VAL Loss: 122.3205\n",
            "\n",
            "Epoch 931: Validation loss did not decrease\n",
            "\t Train_Loss: 28.2056 Val_Loss: 123.4584  BEST VAL Loss: 122.3205\n",
            "\n",
            "Epoch 932: Validation loss decreased (122.320496 --> 122.145058).\n",
            "\t Train_Loss: 27.2143 Val_Loss: 122.1451  BEST VAL Loss: 122.1451\n",
            "\n",
            "Epoch 933: Validation loss did not decrease\n",
            "\t Train_Loss: 28.0671 Val_Loss: 122.7075  BEST VAL Loss: 122.1451\n",
            "\n",
            "Epoch 934: Validation loss did not decrease\n",
            "\t Train_Loss: 27.2783 Val_Loss: 124.5977  BEST VAL Loss: 122.1451\n",
            "\n",
            "Epoch 935: Validation loss did not decrease\n",
            "\t Train_Loss: 26.8228 Val_Loss: 127.5857  BEST VAL Loss: 122.1451\n",
            "\n",
            "Epoch 936: Validation loss did not decrease\n",
            "\t Train_Loss: 27.8304 Val_Loss: 123.3029  BEST VAL Loss: 122.1451\n",
            "\n",
            "Epoch 937: Validation loss decreased (122.145058 --> 120.744194).\n",
            "\t Train_Loss: 26.6088 Val_Loss: 120.7442  BEST VAL Loss: 120.7442\n",
            "\n",
            "Epoch 938: Validation loss decreased (120.744194 --> 119.713539).\n",
            "\t Train_Loss: 26.8232 Val_Loss: 119.7135  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 939: Validation loss did not decrease\n",
            "\t Train_Loss: 27.0345 Val_Loss: 120.0037  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 940: Validation loss did not decrease\n",
            "\t Train_Loss: 26.5941 Val_Loss: 122.7626  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 941: Validation loss did not decrease\n",
            "\t Train_Loss: 26.1865 Val_Loss: 124.5472  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 942: Validation loss did not decrease\n",
            "\t Train_Loss: 26.5197 Val_Loss: 120.6295  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 943: Validation loss did not decrease\n",
            "\t Train_Loss: 25.9665 Val_Loss: 119.8732  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 944: Validation loss did not decrease\n",
            "\t Train_Loss: 26.2529 Val_Loss: 121.1783  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 945: Validation loss did not decrease\n",
            "\t Train_Loss: 25.7105 Val_Loss: 124.7826  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 946: Validation loss did not decrease\n",
            "\t Train_Loss: 26.2001 Val_Loss: 121.5339  BEST VAL Loss: 119.7135\n",
            "\n",
            "Epoch 947: Validation loss decreased (119.713539 --> 119.599487).\n",
            "\t Train_Loss: 25.4348 Val_Loss: 119.5995  BEST VAL Loss: 119.5995\n",
            "\n",
            "Epoch 948: Validation loss did not decrease\n",
            "\t Train_Loss: 25.6852 Val_Loss: 119.9510  BEST VAL Loss: 119.5995\n",
            "\n",
            "Epoch 949: Validation loss did not decrease\n",
            "\t Train_Loss: 25.2577 Val_Loss: 122.1911  BEST VAL Loss: 119.5995\n",
            "\n",
            "Epoch 950: Validation loss decreased (119.599487 --> 119.182510).\n",
            "\t Train_Loss: 25.5696 Val_Loss: 119.1825  BEST VAL Loss: 119.1825\n",
            "\n",
            "Epoch 951: Validation loss decreased (119.182510 --> 117.650513).\n",
            "\t Train_Loss: 25.0004 Val_Loss: 117.6505  BEST VAL Loss: 117.6505\n",
            "\n",
            "Epoch 952: Validation loss did not decrease\n",
            "\t Train_Loss: 25.2064 Val_Loss: 118.7888  BEST VAL Loss: 117.6505\n",
            "\n",
            "Epoch 953: Validation loss did not decrease\n",
            "\t Train_Loss: 24.8125 Val_Loss: 119.9212  BEST VAL Loss: 117.6505\n",
            "\n",
            "Epoch 954: Validation loss did not decrease\n",
            "\t Train_Loss: 24.9559 Val_Loss: 117.7684  BEST VAL Loss: 117.6505\n",
            "\n",
            "Epoch 955: Validation loss decreased (117.650513 --> 117.524170).\n",
            "\t Train_Loss: 24.6788 Val_Loss: 117.5242  BEST VAL Loss: 117.5242\n",
            "\n",
            "Epoch 956: Validation loss did not decrease\n",
            "\t Train_Loss: 24.6255 Val_Loss: 119.1082  BEST VAL Loss: 117.5242\n",
            "\n",
            "Epoch 957: Validation loss did not decrease\n",
            "\t Train_Loss: 24.5126 Val_Loss: 118.3957  BEST VAL Loss: 117.5242\n",
            "\n",
            "Epoch 958: Validation loss decreased (117.524170 --> 117.081841).\n",
            "\t Train_Loss: 24.3126 Val_Loss: 117.0818  BEST VAL Loss: 117.0818\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3431 Val_Loss: 117.6761  BEST VAL Loss: 117.0818\n",
            "\n",
            "Epoch 960: Validation loss did not decrease\n",
            "\t Train_Loss: 24.1030 Val_Loss: 117.9507  BEST VAL Loss: 117.0818\n",
            "\n",
            "Epoch 961: Validation loss decreased (117.081841 --> 116.286156).\n",
            "\t Train_Loss: 24.1062 Val_Loss: 116.2862  BEST VAL Loss: 116.2862\n",
            "\n",
            "Epoch 962: Validation loss did not decrease\n",
            "\t Train_Loss: 24.0951 Val_Loss: 116.4696  BEST VAL Loss: 116.2862\n",
            "\n",
            "Epoch 963: Validation loss did not decrease\n",
            "\t Train_Loss: 23.7966 Val_Loss: 116.6872  BEST VAL Loss: 116.2862\n",
            "\n",
            "Epoch 964: Validation loss decreased (116.286156 --> 115.256737).\n",
            "\t Train_Loss: 23.7792 Val_Loss: 115.2567  BEST VAL Loss: 115.2567\n",
            "\n",
            "Epoch 965: Validation loss did not decrease\n",
            "\t Train_Loss: 23.6639 Val_Loss: 115.3604  BEST VAL Loss: 115.2567\n",
            "\n",
            "Epoch 966: Validation loss did not decrease\n",
            "\t Train_Loss: 23.4991 Val_Loss: 115.8557  BEST VAL Loss: 115.2567\n",
            "\n",
            "Epoch 967: Validation loss decreased (115.256737 --> 114.617287).\n",
            "\t Train_Loss: 23.4875 Val_Loss: 114.6173  BEST VAL Loss: 114.6173\n",
            "\n",
            "Epoch 968: Validation loss decreased (114.617287 --> 114.447975).\n",
            "\t Train_Loss: 23.3311 Val_Loss: 114.4480  BEST VAL Loss: 114.4480\n",
            "\n",
            "Epoch 969: Validation loss did not decrease\n",
            "\t Train_Loss: 23.2010 Val_Loss: 114.8101  BEST VAL Loss: 114.4480\n",
            "\n",
            "Epoch 970: Validation loss decreased (114.447975 --> 113.680534).\n",
            "\t Train_Loss: 23.1650 Val_Loss: 113.6805  BEST VAL Loss: 113.6805\n",
            "\n",
            "Epoch 971: Validation loss decreased (113.680534 --> 113.347412).\n",
            "\t Train_Loss: 23.0000 Val_Loss: 113.3474  BEST VAL Loss: 113.3474\n",
            "\n",
            "Epoch 972: Validation loss did not decrease\n",
            "\t Train_Loss: 22.8865 Val_Loss: 113.5623  BEST VAL Loss: 113.3474\n",
            "\n",
            "Epoch 973: Validation loss decreased (113.347412 --> 112.517532).\n",
            "\t Train_Loss: 22.8382 Val_Loss: 112.5175  BEST VAL Loss: 112.5175\n",
            "\n",
            "Epoch 974: Validation loss decreased (112.517532 --> 112.210472).\n",
            "\t Train_Loss: 22.6849 Val_Loss: 112.2105  BEST VAL Loss: 112.2105\n",
            "\n",
            "Epoch 975: Validation loss did not decrease\n",
            "\t Train_Loss: 22.5621 Val_Loss: 112.3174  BEST VAL Loss: 112.2105\n",
            "\n",
            "Epoch 976: Validation loss decreased (112.210472 --> 111.382889).\n",
            "\t Train_Loss: 22.5037 Val_Loss: 111.3829  BEST VAL Loss: 111.3829\n",
            "\n",
            "Epoch 977: Validation loss decreased (111.382889 --> 111.108826).\n",
            "\t Train_Loss: 22.3626 Val_Loss: 111.1088  BEST VAL Loss: 111.1088\n",
            "\n",
            "Epoch 978: Validation loss did not decrease\n",
            "\t Train_Loss: 22.2365 Val_Loss: 111.1152  BEST VAL Loss: 111.1088\n",
            "\n",
            "Epoch 979: Validation loss decreased (111.108826 --> 110.219215).\n",
            "\t Train_Loss: 22.1730 Val_Loss: 110.2192  BEST VAL Loss: 110.2192\n",
            "\n",
            "Epoch 980: Validation loss decreased (110.219215 --> 109.907646).\n",
            "\t Train_Loss: 22.0405 Val_Loss: 109.9076  BEST VAL Loss: 109.9076\n",
            "\n",
            "Epoch 981: Validation loss decreased (109.907646 --> 109.849625).\n",
            "\t Train_Loss: 21.9102 Val_Loss: 109.8496  BEST VAL Loss: 109.8496\n",
            "\n",
            "Epoch 982: Validation loss decreased (109.849625 --> 108.958206).\n",
            "\t Train_Loss: 21.8402 Val_Loss: 108.9582  BEST VAL Loss: 108.9582\n",
            "\n",
            "Epoch 983: Validation loss decreased (108.958206 --> 108.645912).\n",
            "\t Train_Loss: 21.7107 Val_Loss: 108.6459  BEST VAL Loss: 108.6459\n",
            "\n",
            "Epoch 984: Validation loss decreased (108.645912 --> 108.574097).\n",
            "\t Train_Loss: 21.5814 Val_Loss: 108.5741  BEST VAL Loss: 108.5741\n",
            "\n",
            "Epoch 985: Validation loss decreased (108.574097 --> 107.718559).\n",
            "\t Train_Loss: 21.5046 Val_Loss: 107.7186  BEST VAL Loss: 107.7186\n",
            "\n",
            "Epoch 986: Validation loss decreased (107.718559 --> 107.441711).\n",
            "\t Train_Loss: 21.3870 Val_Loss: 107.4417  BEST VAL Loss: 107.4417\n",
            "\n",
            "Epoch 987: Validation loss decreased (107.441711 --> 107.344055).\n",
            "\t Train_Loss: 21.2575 Val_Loss: 107.3441  BEST VAL Loss: 107.3441\n",
            "\n",
            "Epoch 988: Validation loss decreased (107.344055 --> 106.533348).\n",
            "\t Train_Loss: 21.1857 Val_Loss: 106.5333  BEST VAL Loss: 106.5333\n",
            "\n",
            "Epoch 989: Validation loss decreased (106.533348 --> 106.254761).\n",
            "\t Train_Loss: 21.0652 Val_Loss: 106.2548  BEST VAL Loss: 106.2548\n",
            "\n",
            "Epoch 990: Validation loss decreased (106.254761 --> 106.090088).\n",
            "\t Train_Loss: 20.9330 Val_Loss: 106.0901  BEST VAL Loss: 106.0901\n",
            "\n",
            "Epoch 991: Validation loss decreased (106.090088 --> 105.241272).\n",
            "\t Train_Loss: 20.8460 Val_Loss: 105.2413  BEST VAL Loss: 105.2413\n",
            "\n",
            "Epoch 992: Validation loss decreased (105.241272 --> 104.893791).\n",
            "\t Train_Loss: 20.7138 Val_Loss: 104.8938  BEST VAL Loss: 104.8938\n",
            "\n",
            "Epoch 993: Validation loss decreased (104.893791 --> 104.712036).\n",
            "\t Train_Loss: 20.5658 Val_Loss: 104.7120  BEST VAL Loss: 104.7120\n",
            "\n",
            "Epoch 994: Validation loss decreased (104.712036 --> 103.898582).\n",
            "\t Train_Loss: 20.4618 Val_Loss: 103.8986  BEST VAL Loss: 103.8986\n",
            "\n",
            "Epoch 995: Validation loss decreased (103.898582 --> 103.660072).\n",
            "\t Train_Loss: 20.3267 Val_Loss: 103.6601  BEST VAL Loss: 103.6601\n",
            "\n",
            "Epoch 996: Validation loss decreased (103.660072 --> 103.320801).\n",
            "\t Train_Loss: 20.2106 Val_Loss: 103.3208  BEST VAL Loss: 103.3208\n",
            "\n",
            "Epoch 997: Validation loss decreased (103.320801 --> 102.514793).\n",
            "\t Train_Loss: 20.0848 Val_Loss: 102.5148  BEST VAL Loss: 102.5148\n",
            "\n",
            "Epoch 998: Validation loss decreased (102.514793 --> 102.398773).\n",
            "\t Train_Loss: 19.9613 Val_Loss: 102.3988  BEST VAL Loss: 102.3988\n",
            "\n",
            "Epoch 999: Validation loss decreased (102.398773 --> 101.450974).\n",
            "\t Train_Loss: 19.8541 Val_Loss: 101.4510  BEST VAL Loss: 101.4510\n",
            "\n",
            "Epoch 1000: Validation loss did not decrease\n",
            "\t Train_Loss: 19.8137 Val_Loss: 101.6531  BEST VAL Loss: 101.4510\n",
            "\n",
            "Epoch 1001: Validation loss decreased (101.450974 --> 100.748039).\n",
            "\t Train_Loss: 19.7083 Val_Loss: 100.7480  BEST VAL Loss: 100.7480\n",
            "\n",
            "Epoch 1002: Validation loss decreased (100.748039 --> 100.347641).\n",
            "\t Train_Loss: 19.4990 Val_Loss: 100.3476  BEST VAL Loss: 100.3476\n",
            "\n",
            "Epoch 1003: Validation loss decreased (100.347641 --> 100.292000).\n",
            "\t Train_Loss: 19.3638 Val_Loss: 100.2920  BEST VAL Loss: 100.2920\n",
            "\n",
            "Epoch 1004: Validation loss decreased (100.292000 --> 99.182632).\n",
            "\t Train_Loss: 19.3419 Val_Loss: 99.1826  BEST VAL Loss: 99.1826\n",
            "\n",
            "Epoch 1005: Validation loss decreased (99.182632 --> 99.068695).\n",
            "\t Train_Loss: 19.2176 Val_Loss: 99.0687  BEST VAL Loss: 99.0687\n",
            "\n",
            "Epoch 1006: Validation loss decreased (99.068695 --> 98.828247).\n",
            "\t Train_Loss: 19.0180 Val_Loss: 98.8282  BEST VAL Loss: 98.8282\n",
            "\n",
            "Epoch 1007: Validation loss decreased (98.828247 --> 97.645767).\n",
            "\t Train_Loss: 18.9591 Val_Loss: 97.6458  BEST VAL Loss: 97.6458\n",
            "\n",
            "Epoch 1008: Validation loss did not decrease\n",
            "\t Train_Loss: 18.8937 Val_Loss: 97.7268  BEST VAL Loss: 97.6458\n",
            "\n",
            "Epoch 1009: Validation loss decreased (97.645767 --> 97.332146).\n",
            "\t Train_Loss: 18.6702 Val_Loss: 97.3321  BEST VAL Loss: 97.3321\n",
            "\n",
            "Epoch 1010: Validation loss decreased (97.332146 --> 96.200821).\n",
            "\t Train_Loss: 18.5765 Val_Loss: 96.2008  BEST VAL Loss: 96.2008\n",
            "\n",
            "Epoch 1011: Validation loss did not decrease\n",
            "\t Train_Loss: 18.5511 Val_Loss: 96.3313  BEST VAL Loss: 96.2008\n",
            "\n",
            "Epoch 1012: Validation loss decreased (96.200821 --> 95.667259).\n",
            "\t Train_Loss: 18.3451 Val_Loss: 95.6673  BEST VAL Loss: 95.6673\n",
            "\n",
            "Epoch 1013: Validation loss decreased (95.667259 --> 94.906242).\n",
            "\t Train_Loss: 18.2089 Val_Loss: 94.9062  BEST VAL Loss: 94.9062\n",
            "\n",
            "Epoch 1014: Validation loss did not decrease\n",
            "\t Train_Loss: 18.1310 Val_Loss: 95.1650  BEST VAL Loss: 94.9062\n",
            "\n",
            "Epoch 1015: Validation loss decreased (94.906242 --> 94.000587).\n",
            "\t Train_Loss: 18.0536 Val_Loss: 94.0006  BEST VAL Loss: 94.0006\n",
            "\n",
            "Epoch 1016: Validation loss did not decrease\n",
            "\t Train_Loss: 17.9530 Val_Loss: 94.1632  BEST VAL Loss: 94.0006\n",
            "\n",
            "Epoch 1017: Validation loss decreased (94.000587 --> 93.634956).\n",
            "\t Train_Loss: 17.7678 Val_Loss: 93.6350  BEST VAL Loss: 93.6350\n",
            "\n",
            "Epoch 1018: Validation loss decreased (93.634956 --> 92.672310).\n",
            "\t Train_Loss: 17.6378 Val_Loss: 92.6723  BEST VAL Loss: 92.6723\n",
            "\n",
            "Epoch 1019: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6051 Val_Loss: 92.8962  BEST VAL Loss: 92.6723\n",
            "\n",
            "Epoch 1020: Validation loss decreased (92.672310 --> 92.030571).\n",
            "\t Train_Loss: 17.4754 Val_Loss: 92.0306  BEST VAL Loss: 92.0306\n",
            "\n",
            "Epoch 1021: Validation loss decreased (92.030571 --> 91.744629).\n",
            "\t Train_Loss: 17.3369 Val_Loss: 91.7446  BEST VAL Loss: 91.7446\n",
            "\n",
            "Epoch 1022: Validation loss decreased (91.744629 --> 91.622139).\n",
            "\t Train_Loss: 17.2220 Val_Loss: 91.6221  BEST VAL Loss: 91.6221\n",
            "\n",
            "Epoch 1023: Validation loss decreased (91.622139 --> 90.733887).\n",
            "\t Train_Loss: 17.1188 Val_Loss: 90.7339  BEST VAL Loss: 90.7339\n",
            "\n",
            "Epoch 1024: Validation loss decreased (90.733887 --> 90.552429).\n",
            "\t Train_Loss: 17.0294 Val_Loss: 90.5524  BEST VAL Loss: 90.5524\n",
            "\n",
            "Epoch 1025: Validation loss decreased (90.552429 --> 90.123367).\n",
            "\t Train_Loss: 16.8836 Val_Loss: 90.1234  BEST VAL Loss: 90.1234\n",
            "\n",
            "Epoch 1026: Validation loss decreased (90.123367 --> 89.478569).\n",
            "\t Train_Loss: 16.7916 Val_Loss: 89.4786  BEST VAL Loss: 89.4786\n",
            "\n",
            "Epoch 1027: Validation loss decreased (89.478569 --> 89.392387).\n",
            "\t Train_Loss: 16.6958 Val_Loss: 89.3924  BEST VAL Loss: 89.3924\n",
            "\n",
            "Epoch 1028: Validation loss decreased (89.392387 --> 88.825615).\n",
            "\t Train_Loss: 16.5856 Val_Loss: 88.8256  BEST VAL Loss: 88.8256\n",
            "\n",
            "Epoch 1029: Validation loss decreased (88.825615 --> 88.487267).\n",
            "\t Train_Loss: 16.4802 Val_Loss: 88.4873  BEST VAL Loss: 88.4873\n",
            "\n",
            "Epoch 1030: Validation loss decreased (88.487267 --> 88.267296).\n",
            "\t Train_Loss: 16.3712 Val_Loss: 88.2673  BEST VAL Loss: 88.2673\n",
            "\n",
            "Epoch 1031: Validation loss decreased (88.267296 --> 87.636345).\n",
            "\t Train_Loss: 16.2844 Val_Loss: 87.6363  BEST VAL Loss: 87.6363\n",
            "\n",
            "Epoch 1032: Validation loss decreased (87.636345 --> 87.320030).\n",
            "\t Train_Loss: 16.1776 Val_Loss: 87.3200  BEST VAL Loss: 87.3200\n",
            "\n",
            "Epoch 1033: Validation loss decreased (87.320030 --> 87.006950).\n",
            "\t Train_Loss: 16.0805 Val_Loss: 87.0070  BEST VAL Loss: 87.0070\n",
            "\n",
            "Epoch 1034: Validation loss decreased (87.006950 --> 86.519028).\n",
            "\t Train_Loss: 15.9844 Val_Loss: 86.5190  BEST VAL Loss: 86.5190\n",
            "\n",
            "Epoch 1035: Validation loss decreased (86.519028 --> 86.329338).\n",
            "\t Train_Loss: 15.8907 Val_Loss: 86.3293  BEST VAL Loss: 86.3293\n",
            "\n",
            "Epoch 1036: Validation loss decreased (86.329338 --> 85.867775).\n",
            "\t Train_Loss: 15.7965 Val_Loss: 85.8678  BEST VAL Loss: 85.8678\n",
            "\n",
            "Epoch 1037: Validation loss decreased (85.867775 --> 85.339340).\n",
            "\t Train_Loss: 15.6967 Val_Loss: 85.3393  BEST VAL Loss: 85.3393\n",
            "\n",
            "Epoch 1038: Validation loss decreased (85.339340 --> 85.115601).\n",
            "\t Train_Loss: 15.6107 Val_Loss: 85.1156  BEST VAL Loss: 85.1156\n",
            "\n",
            "Epoch 1039: Validation loss decreased (85.115601 --> 84.590149).\n",
            "\t Train_Loss: 15.5186 Val_Loss: 84.5901  BEST VAL Loss: 84.5901\n",
            "\n",
            "Epoch 1040: Validation loss decreased (84.590149 --> 84.310440).\n",
            "\t Train_Loss: 15.4269 Val_Loss: 84.3104  BEST VAL Loss: 84.3104\n",
            "\n",
            "Epoch 1041: Validation loss decreased (84.310440 --> 84.018768).\n",
            "\t Train_Loss: 15.3346 Val_Loss: 84.0188  BEST VAL Loss: 84.0188\n",
            "\n",
            "Epoch 1042: Validation loss decreased (84.018768 --> 83.501625).\n",
            "\t Train_Loss: 15.2473 Val_Loss: 83.5016  BEST VAL Loss: 83.5016\n",
            "\n",
            "Epoch 1043: Validation loss decreased (83.501625 --> 83.297699).\n",
            "\t Train_Loss: 15.1634 Val_Loss: 83.2977  BEST VAL Loss: 83.2977\n",
            "\n",
            "Epoch 1044: Validation loss decreased (83.297699 --> 82.885498).\n",
            "\t Train_Loss: 15.0725 Val_Loss: 82.8855  BEST VAL Loss: 82.8855\n",
            "\n",
            "Epoch 1045: Validation loss decreased (82.885498 --> 82.500786).\n",
            "\t Train_Loss: 14.9869 Val_Loss: 82.5008  BEST VAL Loss: 82.5008\n",
            "\n",
            "Epoch 1046: Validation loss decreased (82.500786 --> 82.286072).\n",
            "\t Train_Loss: 14.9028 Val_Loss: 82.2861  BEST VAL Loss: 82.2861\n",
            "\n",
            "Epoch 1047: Validation loss decreased (82.286072 --> 81.830742).\n",
            "\t Train_Loss: 14.8204 Val_Loss: 81.8307  BEST VAL Loss: 81.8307\n",
            "\n",
            "Epoch 1048: Validation loss decreased (81.830742 --> 81.570976).\n",
            "\t Train_Loss: 14.7380 Val_Loss: 81.5710  BEST VAL Loss: 81.5710\n",
            "\n",
            "Epoch 1049: Validation loss decreased (81.570976 --> 81.254822).\n",
            "\t Train_Loss: 14.6542 Val_Loss: 81.2548  BEST VAL Loss: 81.2548\n",
            "\n",
            "Epoch 1050: Validation loss decreased (81.254822 --> 80.817696).\n",
            "\t Train_Loss: 14.5752 Val_Loss: 80.8177  BEST VAL Loss: 80.8177\n",
            "\n",
            "Epoch 1051: Validation loss decreased (80.817696 --> 80.561661).\n",
            "\t Train_Loss: 14.4962 Val_Loss: 80.5617  BEST VAL Loss: 80.5617\n",
            "\n",
            "Epoch 1052: Validation loss decreased (80.561661 --> 80.167175).\n",
            "\t Train_Loss: 14.4173 Val_Loss: 80.1672  BEST VAL Loss: 80.1672\n",
            "\n",
            "Epoch 1053: Validation loss decreased (80.167175 --> 79.829422).\n",
            "\t Train_Loss: 14.3395 Val_Loss: 79.8294  BEST VAL Loss: 79.8294\n",
            "\n",
            "Epoch 1054: Validation loss decreased (79.829422 --> 79.565094).\n",
            "\t Train_Loss: 14.2625 Val_Loss: 79.5651  BEST VAL Loss: 79.5651\n",
            "\n",
            "Epoch 1055: Validation loss decreased (79.565094 --> 79.155365).\n",
            "\t Train_Loss: 14.1881 Val_Loss: 79.1554  BEST VAL Loss: 79.1554\n",
            "\n",
            "Epoch 1056: Validation loss decreased (79.155365 --> 78.851112).\n",
            "\t Train_Loss: 14.1130 Val_Loss: 78.8511  BEST VAL Loss: 78.8511\n",
            "\n",
            "Epoch 1057: Validation loss decreased (78.851112 --> 78.532921).\n",
            "\t Train_Loss: 14.0385 Val_Loss: 78.5329  BEST VAL Loss: 78.5329\n",
            "\n",
            "Epoch 1058: Validation loss decreased (78.532921 --> 78.165756).\n",
            "\t Train_Loss: 13.9661 Val_Loss: 78.1658  BEST VAL Loss: 78.1658\n",
            "\n",
            "Epoch 1059: Validation loss decreased (78.165756 --> 77.927147).\n",
            "\t Train_Loss: 13.8938 Val_Loss: 77.9271  BEST VAL Loss: 77.9271\n",
            "\n",
            "Epoch 1060: Validation loss decreased (77.927147 --> 77.585045).\n",
            "\t Train_Loss: 13.8223 Val_Loss: 77.5850  BEST VAL Loss: 77.5850\n",
            "\n",
            "Epoch 1061: Validation loss decreased (77.585045 --> 77.262115).\n",
            "\t Train_Loss: 13.7513 Val_Loss: 77.2621  BEST VAL Loss: 77.2621\n",
            "\n",
            "Epoch 1062: Validation loss decreased (77.262115 --> 76.990005).\n",
            "\t Train_Loss: 13.6816 Val_Loss: 76.9900  BEST VAL Loss: 76.9900\n",
            "\n",
            "Epoch 1063: Validation loss decreased (76.990005 --> 76.629791).\n",
            "\t Train_Loss: 13.6137 Val_Loss: 76.6298  BEST VAL Loss: 76.6298\n",
            "\n",
            "Epoch 1064: Validation loss decreased (76.629791 --> 76.358696).\n",
            "\t Train_Loss: 13.5456 Val_Loss: 76.3587  BEST VAL Loss: 76.3587\n",
            "\n",
            "Epoch 1065: Validation loss decreased (76.358696 --> 76.058098).\n",
            "\t Train_Loss: 13.4781 Val_Loss: 76.0581  BEST VAL Loss: 76.0581\n",
            "\n",
            "Epoch 1066: Validation loss decreased (76.058098 --> 75.724495).\n",
            "\t Train_Loss: 13.4112 Val_Loss: 75.7245  BEST VAL Loss: 75.7245\n",
            "\n",
            "Epoch 1067: Validation loss decreased (75.724495 --> 75.461037).\n",
            "\t Train_Loss: 13.3458 Val_Loss: 75.4610  BEST VAL Loss: 75.4610\n",
            "\n",
            "Epoch 1068: Validation loss decreased (75.461037 --> 75.140945).\n",
            "\t Train_Loss: 13.2811 Val_Loss: 75.1409  BEST VAL Loss: 75.1409\n",
            "\n",
            "Epoch 1069: Validation loss decreased (75.140945 --> 74.844864).\n",
            "\t Train_Loss: 13.2161 Val_Loss: 74.8449  BEST VAL Loss: 74.8449\n",
            "\n",
            "Epoch 1070: Validation loss decreased (74.844864 --> 74.585609).\n",
            "\t Train_Loss: 13.1525 Val_Loss: 74.5856  BEST VAL Loss: 74.5856\n",
            "\n",
            "Epoch 1071: Validation loss decreased (74.585609 --> 74.271492).\n",
            "\t Train_Loss: 13.0898 Val_Loss: 74.2715  BEST VAL Loss: 74.2715\n",
            "\n",
            "Epoch 1072: Validation loss decreased (74.271492 --> 74.000847).\n",
            "\t Train_Loss: 13.0277 Val_Loss: 74.0008  BEST VAL Loss: 74.0008\n",
            "\n",
            "Epoch 1073: Validation loss decreased (74.000847 --> 73.737968).\n",
            "\t Train_Loss: 12.9656 Val_Loss: 73.7380  BEST VAL Loss: 73.7380\n",
            "\n",
            "Epoch 1074: Validation loss decreased (73.737968 --> 73.449043).\n",
            "\t Train_Loss: 12.9048 Val_Loss: 73.4490  BEST VAL Loss: 73.4490\n",
            "\n",
            "Epoch 1075: Validation loss decreased (73.449043 --> 73.206566).\n",
            "\t Train_Loss: 12.8445 Val_Loss: 73.2066  BEST VAL Loss: 73.2066\n",
            "\n",
            "Epoch 1076: Validation loss decreased (73.206566 --> 72.957764).\n",
            "\t Train_Loss: 12.7840 Val_Loss: 72.9578  BEST VAL Loss: 72.9578\n",
            "\n",
            "Epoch 1077: Validation loss decreased (72.957764 --> 72.699272).\n",
            "\t Train_Loss: 12.7241 Val_Loss: 72.6993  BEST VAL Loss: 72.6993\n",
            "\n",
            "Epoch 1078: Validation loss decreased (72.699272 --> 72.475922).\n",
            "\t Train_Loss: 12.6645 Val_Loss: 72.4759  BEST VAL Loss: 72.4759\n",
            "\n",
            "Epoch 1079: Validation loss decreased (72.475922 --> 72.229881).\n",
            "\t Train_Loss: 12.6048 Val_Loss: 72.2299  BEST VAL Loss: 72.2299\n",
            "\n",
            "Epoch 1080: Validation loss decreased (72.229881 --> 71.986183).\n",
            "\t Train_Loss: 12.5448 Val_Loss: 71.9862  BEST VAL Loss: 71.9862\n",
            "\n",
            "Epoch 1081: Validation loss decreased (71.986183 --> 71.772408).\n",
            "\t Train_Loss: 12.4851 Val_Loss: 71.7724  BEST VAL Loss: 71.7724\n",
            "\n",
            "Epoch 1082: Validation loss decreased (71.772408 --> 71.524086).\n",
            "\t Train_Loss: 12.4256 Val_Loss: 71.5241  BEST VAL Loss: 71.5241\n",
            "\n",
            "Epoch 1083: Validation loss decreased (71.524086 --> 71.275261).\n",
            "\t Train_Loss: 12.3662 Val_Loss: 71.2753  BEST VAL Loss: 71.2753\n",
            "\n",
            "Epoch 1084: Validation loss decreased (71.275261 --> 71.025063).\n",
            "\t Train_Loss: 12.3074 Val_Loss: 71.0251  BEST VAL Loss: 71.0251\n",
            "\n",
            "Epoch 1085: Validation loss decreased (71.025063 --> 70.741692).\n",
            "\t Train_Loss: 12.2495 Val_Loss: 70.7417  BEST VAL Loss: 70.7417\n",
            "\n",
            "Epoch 1086: Validation loss decreased (70.741692 --> 70.479736).\n",
            "\t Train_Loss: 12.1919 Val_Loss: 70.4797  BEST VAL Loss: 70.4797\n",
            "\n",
            "Epoch 1087: Validation loss decreased (70.479736 --> 70.189171).\n",
            "\t Train_Loss: 12.1350 Val_Loss: 70.1892  BEST VAL Loss: 70.1892\n",
            "\n",
            "Epoch 1088: Validation loss decreased (70.189171 --> 69.902039).\n",
            "\t Train_Loss: 12.0787 Val_Loss: 69.9020  BEST VAL Loss: 69.9020\n",
            "\n",
            "Epoch 1089: Validation loss decreased (69.902039 --> 69.633057).\n",
            "\t Train_Loss: 12.0232 Val_Loss: 69.6331  BEST VAL Loss: 69.6331\n",
            "\n",
            "Epoch 1090: Validation loss decreased (69.633057 --> 69.351357).\n",
            "\t Train_Loss: 11.9684 Val_Loss: 69.3514  BEST VAL Loss: 69.3514\n",
            "\n",
            "Epoch 1091: Validation loss decreased (69.351357 --> 69.104607).\n",
            "\t Train_Loss: 11.9144 Val_Loss: 69.1046  BEST VAL Loss: 69.1046\n",
            "\n",
            "Epoch 1092: Validation loss decreased (69.104607 --> 68.835411).\n",
            "\t Train_Loss: 11.8612 Val_Loss: 68.8354  BEST VAL Loss: 68.8354\n",
            "\n",
            "Epoch 1093: Validation loss decreased (68.835411 --> 68.579712).\n",
            "\t Train_Loss: 11.8086 Val_Loss: 68.5797  BEST VAL Loss: 68.5797\n",
            "\n",
            "Epoch 1094: Validation loss decreased (68.579712 --> 68.318108).\n",
            "\t Train_Loss: 11.7569 Val_Loss: 68.3181  BEST VAL Loss: 68.3181\n",
            "\n",
            "Epoch 1095: Validation loss decreased (68.318108 --> 68.050102).\n",
            "\t Train_Loss: 11.7059 Val_Loss: 68.0501  BEST VAL Loss: 68.0501\n",
            "\n",
            "Epoch 1096: Validation loss decreased (68.050102 --> 67.795662).\n",
            "\t Train_Loss: 11.6556 Val_Loss: 67.7957  BEST VAL Loss: 67.7957\n",
            "\n",
            "Epoch 1097: Validation loss decreased (67.795662 --> 67.523705).\n",
            "\t Train_Loss: 11.6061 Val_Loss: 67.5237  BEST VAL Loss: 67.5237\n",
            "\n",
            "Epoch 1098: Validation loss decreased (67.523705 --> 67.276817).\n",
            "\t Train_Loss: 11.5571 Val_Loss: 67.2768  BEST VAL Loss: 67.2768\n",
            "\n",
            "Epoch 1099: Validation loss decreased (67.276817 --> 67.024109).\n",
            "\t Train_Loss: 11.5087 Val_Loss: 67.0241  BEST VAL Loss: 67.0241\n",
            "\n",
            "Epoch 1100: Validation loss decreased (67.024109 --> 66.797112).\n",
            "\t Train_Loss: 11.4601 Val_Loss: 66.7971  BEST VAL Loss: 66.7971\n",
            "\n",
            "Epoch 1101: Validation loss decreased (66.797112 --> 66.485558).\n",
            "\t Train_Loss: 11.4112 Val_Loss: 66.4856  BEST VAL Loss: 66.4856\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3718 Val_Loss: 66.5999  BEST VAL Loss: 66.4856\n",
            "\n",
            "Epoch 1103: Validation loss decreased (66.485558 --> 65.777550).\n",
            "\t Train_Loss: 11.3507 Val_Loss: 65.7775  BEST VAL Loss: 65.7775\n",
            "\n",
            "Epoch 1104: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3986 Val_Loss: 66.5034  BEST VAL Loss: 65.7775\n",
            "\n",
            "Epoch 1105: Validation loss decreased (65.777550 --> 65.400108).\n",
            "\t Train_Loss: 11.3528 Val_Loss: 65.4001  BEST VAL Loss: 65.4001\n",
            "\n",
            "Epoch 1106: Validation loss decreased (65.400108 --> 65.337029).\n",
            "\t Train_Loss: 11.2422 Val_Loss: 65.3370  BEST VAL Loss: 65.3370\n",
            "\n",
            "Epoch 1107: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1532 Val_Loss: 65.5123  BEST VAL Loss: 65.3370\n",
            "\n",
            "Epoch 1108: Validation loss decreased (65.337029 --> 64.611504).\n",
            "\t Train_Loss: 11.1708 Val_Loss: 64.6115  BEST VAL Loss: 64.6115\n",
            "\n",
            "Epoch 1109: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1480 Val_Loss: 64.8642  BEST VAL Loss: 64.6115\n",
            "\n",
            "Epoch 1110: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0315 Val_Loss: 64.9142  BEST VAL Loss: 64.6115\n",
            "\n",
            "Epoch 1111: Validation loss decreased (64.611504 --> 64.185173).\n",
            "\t Train_Loss: 11.0229 Val_Loss: 64.1852  BEST VAL Loss: 64.1852\n",
            "\n",
            "Epoch 1112: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0268 Val_Loss: 64.5178  BEST VAL Loss: 64.1852\n",
            "\n",
            "Epoch 1113: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9199 Val_Loss: 64.3394  BEST VAL Loss: 64.1852\n",
            "\n",
            "Epoch 1114: Validation loss decreased (64.185173 --> 63.632141).\n",
            "\t Train_Loss: 10.8835 Val_Loss: 63.6321  BEST VAL Loss: 63.6321\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8879 Val_Loss: 63.8859  BEST VAL Loss: 63.6321\n",
            "\n",
            "Epoch 1116: Validation loss decreased (63.632141 --> 63.528175).\n",
            "\t Train_Loss: 10.8114 Val_Loss: 63.5282  BEST VAL Loss: 63.5282\n",
            "\n",
            "Epoch 1117: Validation loss decreased (63.528175 --> 63.008099).\n",
            "\t Train_Loss: 10.7593 Val_Loss: 63.0081  BEST VAL Loss: 63.0081\n",
            "\n",
            "Epoch 1118: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7543 Val_Loss: 63.2823  BEST VAL Loss: 63.0081\n",
            "\n",
            "Epoch 1119: Validation loss decreased (63.008099 --> 62.936413).\n",
            "\t Train_Loss: 10.7016 Val_Loss: 62.9364  BEST VAL Loss: 62.9364\n",
            "\n",
            "Epoch 1120: Validation loss decreased (62.936413 --> 62.580200).\n",
            "\t Train_Loss: 10.6426 Val_Loss: 62.5802  BEST VAL Loss: 62.5802\n",
            "\n",
            "Epoch 1121: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6173 Val_Loss: 62.8096  BEST VAL Loss: 62.5802\n",
            "\n",
            "Epoch 1122: Validation loss decreased (62.580200 --> 62.058899).\n",
            "\t Train_Loss: 10.6495 Val_Loss: 62.0589  BEST VAL Loss: 62.0589\n",
            "\n",
            "Epoch 1123: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7272 Val_Loss: 62.7257  BEST VAL Loss: 62.0589\n",
            "\n",
            "Epoch 1124: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5967 Val_Loss: 62.3375  BEST VAL Loss: 62.0589\n",
            "\n",
            "Epoch 1125: Validation loss decreased (62.058899 --> 61.544643).\n",
            "\t Train_Loss: 10.5309 Val_Loss: 61.5446  BEST VAL Loss: 61.5446\n",
            "\n",
            "Epoch 1126: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6121 Val_Loss: 61.8644  BEST VAL Loss: 61.5446\n",
            "\n",
            "Epoch 1127: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4910 Val_Loss: 61.5922  BEST VAL Loss: 61.5446\n",
            "\n",
            "Epoch 1128: Validation loss decreased (61.544643 --> 61.022522).\n",
            "\t Train_Loss: 10.4210 Val_Loss: 61.0225  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1129: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4455 Val_Loss: 150.9813  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 40.1100 Val_Loss: 128.8225  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1131: Validation loss did not decrease\n",
            "\t Train_Loss: 34.5944 Val_Loss: 121.4629  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1132: Validation loss did not decrease\n",
            "\t Train_Loss: 40.0031 Val_Loss: 110.3968  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1133: Validation loss did not decrease\n",
            "\t Train_Loss: 33.3237 Val_Loss: 105.7362  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1134: Validation loss did not decrease\n",
            "\t Train_Loss: 24.2520 Val_Loss: 118.7620  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1135: Validation loss did not decrease\n",
            "\t Train_Loss: 24.8465 Val_Loss: 129.2549  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1136: Validation loss did not decrease\n",
            "\t Train_Loss: 31.2758 Val_Loss: 113.9755  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1137: Validation loss did not decrease\n",
            "\t Train_Loss: 24.5691 Val_Loss: 94.8701  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1138: Validation loss did not decrease\n",
            "\t Train_Loss: 19.9621 Val_Loss: 88.7362  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1139: Validation loss did not decrease\n",
            "\t Train_Loss: 22.4190 Val_Loss: 89.9902  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1140: Validation loss did not decrease\n",
            "\t Train_Loss: 25.2251 Val_Loss: 89.7429  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1141: Validation loss did not decrease\n",
            "\t Train_Loss: 23.0919 Val_Loss: 88.0569  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1142: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9668 Val_Loss: 89.1524  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 20.9367 Val_Loss: 91.9188  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1144: Validation loss did not decrease\n",
            "\t Train_Loss: 22.2814 Val_Loss: 91.1856  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 21.3816 Val_Loss: 88.2870  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 19.2605 Val_Loss: 85.8981  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1147: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4399 Val_Loss: 84.7226  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1148: Validation loss did not decrease\n",
            "\t Train_Loss: 18.9968 Val_Loss: 84.3873  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1149: Validation loss did not decrease\n",
            "\t Train_Loss: 19.3414 Val_Loss: 84.6399  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1150: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4766 Val_Loss: 85.8492  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1151: Validation loss did not decrease\n",
            "\t Train_Loss: 17.6206 Val_Loss: 87.5985  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1152: Validation loss did not decrease\n",
            "\t Train_Loss: 17.7830 Val_Loss: 88.0178  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1153: Validation loss did not decrease\n",
            "\t Train_Loss: 18.3738 Val_Loss: 85.0672  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1154: Validation loss did not decrease\n",
            "\t Train_Loss: 17.5410 Val_Loss: 81.5345  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1155: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5703 Val_Loss: 79.9406  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 16.4187 Val_Loss: 80.2557  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1157: Validation loss did not decrease\n",
            "\t Train_Loss: 16.1942 Val_Loss: 81.3875  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1158: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0860 Val_Loss: 82.1181  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1159: Validation loss did not decrease\n",
            "\t Train_Loss: 16.2017 Val_Loss: 81.2516  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1160: Validation loss did not decrease\n",
            "\t Train_Loss: 15.9072 Val_Loss: 80.0095  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3320 Val_Loss: 79.6973  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1162: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3790 Val_Loss: 79.4009  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1163: Validation loss did not decrease\n",
            "\t Train_Loss: 15.3652 Val_Loss: 78.7182  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1164: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8260 Val_Loss: 78.5913  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1165: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6157 Val_Loss: 78.9173  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6148 Val_Loss: 78.6088  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1167: Validation loss did not decrease\n",
            "\t Train_Loss: 14.4489 Val_Loss: 77.3962  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 14.1951 Val_Loss: 76.3116  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1169: Validation loss did not decrease\n",
            "\t Train_Loss: 14.0964 Val_Loss: 76.1457  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1170: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9302 Val_Loss: 76.6120  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1171: Validation loss did not decrease\n",
            "\t Train_Loss: 13.8537 Val_Loss: 77.0537  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1172: Validation loss did not decrease\n",
            "\t Train_Loss: 13.9304 Val_Loss: 76.9454  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1173: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7370 Val_Loss: 76.5802  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1174: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5094 Val_Loss: 77.0788  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1175: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6868 Val_Loss: 75.8134  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1176: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7731 Val_Loss: 75.9338  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1177: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7419 Val_Loss: 76.6796  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5055 Val_Loss: 76.7925  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5798 Val_Loss: 75.4264  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1180: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2893 Val_Loss: 73.8868  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1181: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0654 Val_Loss: 73.2898  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1182: Validation loss did not decrease\n",
            "\t Train_Loss: 13.2579 Val_Loss: 73.1657  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0354 Val_Loss: 73.2604  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1184: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9536 Val_Loss: 73.1550  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9441 Val_Loss: 72.7375  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1186: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7879 Val_Loss: 72.2316  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7896 Val_Loss: 71.9479  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1188: Validation loss did not decrease\n",
            "\t Train_Loss: 12.7324 Val_Loss: 71.7999  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1189: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5558 Val_Loss: 71.6957  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1190: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5896 Val_Loss: 71.3992  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1191: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5102 Val_Loss: 71.0079  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1192: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4131 Val_Loss: 70.6653  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1193: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4256 Val_Loss: 70.4005  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1194: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3191 Val_Loss: 70.2810  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1195: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2581 Val_Loss: 71.6600  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1196: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3930 Val_Loss: 69.7729  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1197: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1389 Val_Loss: 69.3924  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1198: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0909 Val_Loss: 69.1406  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1199: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0667 Val_Loss: 69.0102  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1200: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9791 Val_Loss: 68.9379  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1201: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9534 Val_Loss: 68.7553  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1202: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8934 Val_Loss: 68.4329  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1203: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8379 Val_Loss: 68.0948  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1204: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8142 Val_Loss: 67.8066  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1205: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7480 Val_Loss: 67.5581  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1206: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7004 Val_Loss: 67.2720  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1207: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6692 Val_Loss: 66.9450  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1208: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6076 Val_Loss: 66.6832  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1209: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5748 Val_Loss: 66.5226  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1210: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5226 Val_Loss: 66.4221  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1211: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4772 Val_Loss: 66.2728  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1212: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4437 Val_Loss: 66.0377  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1213: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3911 Val_Loss: 65.8018  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1214: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3576 Val_Loss: 65.5959  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1215: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3123 Val_Loss: 65.4046  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1216: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2707 Val_Loss: 65.1970  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1217: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2357 Val_Loss: 64.9645  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1218: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1888 Val_Loss: 64.7554  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1219: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1544 Val_Loss: 64.6003  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1220: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1130 Val_Loss: 64.4798  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1221: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0747 Val_Loss: 64.3363  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1222: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0388 Val_Loss: 64.1500  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1223: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9979 Val_Loss: 63.9573  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1224: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9646 Val_Loss: 63.7787  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1225: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9265 Val_Loss: 63.6081  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1226: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8901 Val_Loss: 63.4273  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1227: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8556 Val_Loss: 63.2357  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1228: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8177 Val_Loss: 63.0591  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1229: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7818 Val_Loss: 62.9232  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1230: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7492 Val_Loss: 62.8093  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1231: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7329 Val_Loss: 62.7324  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1232: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6971 Val_Loss: 62.6329  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1233: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6527 Val_Loss: 62.4746  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1234: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6281 Val_Loss: 62.2450  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1235: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5855 Val_Loss: 61.9830  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1236: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5491 Val_Loss: 61.7832  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1237: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4959 Val_Loss: 61.6651  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1238: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4851 Val_Loss: 61.4643  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1239: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5108 Val_Loss: 61.3605  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1240: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4206 Val_Loss: 61.3049  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1241: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4211 Val_Loss: 61.1106  BEST VAL Loss: 61.0225\n",
            "\n",
            "Epoch 1242: Validation loss decreased (61.022522 --> 60.868511).\n",
            "\t Train_Loss: 10.3452 Val_Loss: 60.8685  BEST VAL Loss: 60.8685\n",
            "\n",
            "Epoch 1243: Validation loss decreased (60.868511 --> 60.641579).\n",
            "\t Train_Loss: 10.3339 Val_Loss: 60.6416  BEST VAL Loss: 60.6416\n",
            "\n",
            "Epoch 1244: Validation loss decreased (60.641579 --> 60.478466).\n",
            "\t Train_Loss: 10.2878 Val_Loss: 60.4785  BEST VAL Loss: 60.4785\n",
            "\n",
            "Epoch 1245: Validation loss decreased (60.478466 --> 60.308968).\n",
            "\t Train_Loss: 10.2576 Val_Loss: 60.3090  BEST VAL Loss: 60.3090\n",
            "\n",
            "Epoch 1246: Validation loss decreased (60.308968 --> 60.139202).\n",
            "\t Train_Loss: 10.2185 Val_Loss: 60.1392  BEST VAL Loss: 60.1392\n",
            "\n",
            "Epoch 1247: Validation loss decreased (60.139202 --> 60.029999).\n",
            "\t Train_Loss: 10.1888 Val_Loss: 60.0300  BEST VAL Loss: 60.0300\n",
            "\n",
            "Epoch 1248: Validation loss decreased (60.029999 --> 59.948750).\n",
            "\t Train_Loss: 10.1511 Val_Loss: 59.9487  BEST VAL Loss: 59.9487\n",
            "\n",
            "Epoch 1249: Validation loss decreased (59.948750 --> 59.727913).\n",
            "\t Train_Loss: 10.1413 Val_Loss: 59.7279  BEST VAL Loss: 59.7279\n",
            "\n",
            "Epoch 1250: Validation loss decreased (59.727913 --> 59.550953).\n",
            "\t Train_Loss: 10.0814 Val_Loss: 59.5510  BEST VAL Loss: 59.5510\n",
            "\n",
            "Epoch 1251: Validation loss decreased (59.550953 --> 59.425476).\n",
            "\t Train_Loss: 10.0643 Val_Loss: 59.4255  BEST VAL Loss: 59.4255\n",
            "\n",
            "Epoch 1252: Validation loss decreased (59.425476 --> 59.270336).\n",
            "\t Train_Loss: 10.0206 Val_Loss: 59.2703  BEST VAL Loss: 59.2703\n",
            "\n",
            "Epoch 1253: Validation loss decreased (59.270336 --> 59.094234).\n",
            "\t Train_Loss: 9.9840 Val_Loss: 59.0942  BEST VAL Loss: 59.0942\n",
            "\n",
            "Epoch 1254: Validation loss decreased (59.094234 --> 58.942463).\n",
            "\t Train_Loss: 9.9693 Val_Loss: 58.9425  BEST VAL Loss: 58.9425\n",
            "\n",
            "Epoch 1255: Validation loss decreased (58.942463 --> 58.817139).\n",
            "\t Train_Loss: 9.9172 Val_Loss: 58.8171  BEST VAL Loss: 58.8171\n",
            "\n",
            "Epoch 1256: Validation loss decreased (58.817139 --> 58.626263).\n",
            "\t Train_Loss: 9.9058 Val_Loss: 58.6263  BEST VAL Loss: 58.6263\n",
            "\n",
            "Epoch 1257: Validation loss decreased (58.626263 --> 58.463123).\n",
            "\t Train_Loss: 9.8664 Val_Loss: 58.4631  BEST VAL Loss: 58.4631\n",
            "\n",
            "Epoch 1258: Validation loss decreased (58.463123 --> 58.347626).\n",
            "\t Train_Loss: 9.8378 Val_Loss: 58.3476  BEST VAL Loss: 58.3476\n",
            "\n",
            "Epoch 1259: Validation loss decreased (58.347626 --> 58.197010).\n",
            "\t Train_Loss: 9.8099 Val_Loss: 58.1970  BEST VAL Loss: 58.1970\n",
            "\n",
            "Epoch 1260: Validation loss decreased (58.197010 --> 58.047100).\n",
            "\t Train_Loss: 9.7733 Val_Loss: 58.0471  BEST VAL Loss: 58.0471\n",
            "\n",
            "Epoch 1261: Validation loss decreased (58.047100 --> 57.946430).\n",
            "\t Train_Loss: 9.7517 Val_Loss: 57.9464  BEST VAL Loss: 57.9464\n",
            "\n",
            "Epoch 1262: Validation loss decreased (57.946430 --> 57.846729).\n",
            "\t Train_Loss: 9.7141 Val_Loss: 57.8467  BEST VAL Loss: 57.8467\n",
            "\n",
            "Epoch 1263: Validation loss decreased (57.846729 --> 57.698254).\n",
            "\t Train_Loss: 9.6922 Val_Loss: 57.6983  BEST VAL Loss: 57.6983\n",
            "\n",
            "Epoch 1264: Validation loss decreased (57.698254 --> 57.584179).\n",
            "\t Train_Loss: 9.6616 Val_Loss: 57.5842  BEST VAL Loss: 57.5842\n",
            "\n",
            "Epoch 1265: Validation loss decreased (57.584179 --> 57.472179).\n",
            "\t Train_Loss: 9.6284 Val_Loss: 57.4722  BEST VAL Loss: 57.4722\n",
            "\n",
            "Epoch 1266: Validation loss decreased (57.472179 --> 57.264843).\n",
            "\t Train_Loss: 9.6084 Val_Loss: 57.2648  BEST VAL Loss: 57.2648\n",
            "\n",
            "Epoch 1267: Validation loss decreased (57.264843 --> 57.109192).\n",
            "\t Train_Loss: 9.5735 Val_Loss: 57.1092  BEST VAL Loss: 57.1092\n",
            "\n",
            "Epoch 1268: Validation loss decreased (57.109192 --> 57.025784).\n",
            "\t Train_Loss: 9.5464 Val_Loss: 57.0258  BEST VAL Loss: 57.0258\n",
            "\n",
            "Epoch 1269: Validation loss decreased (57.025784 --> 56.892437).\n",
            "\t Train_Loss: 9.5225 Val_Loss: 56.8924  BEST VAL Loss: 56.8924\n",
            "\n",
            "Epoch 1270: Validation loss decreased (56.892437 --> 56.799255).\n",
            "\t Train_Loss: 9.4882 Val_Loss: 56.7993  BEST VAL Loss: 56.7993\n",
            "\n",
            "Epoch 1271: Validation loss decreased (56.799255 --> 56.750404).\n",
            "\t Train_Loss: 9.4636 Val_Loss: 56.7504  BEST VAL Loss: 56.7504\n",
            "\n",
            "Epoch 1272: Validation loss decreased (56.750404 --> 56.625027).\n",
            "\t Train_Loss: 9.4367 Val_Loss: 56.6250  BEST VAL Loss: 56.6250\n",
            "\n",
            "Epoch 1273: Validation loss decreased (56.625027 --> 56.481117).\n",
            "\t Train_Loss: 9.4059 Val_Loss: 56.4811  BEST VAL Loss: 56.4811\n",
            "\n",
            "Epoch 1274: Validation loss decreased (56.481117 --> 56.380482).\n",
            "\t Train_Loss: 9.3821 Val_Loss: 56.3805  BEST VAL Loss: 56.3805\n",
            "\n",
            "Epoch 1275: Validation loss decreased (56.380482 --> 56.240211).\n",
            "\t Train_Loss: 9.3539 Val_Loss: 56.2402  BEST VAL Loss: 56.2402\n",
            "\n",
            "Epoch 1276: Validation loss decreased (56.240211 --> 56.083294).\n",
            "\t Train_Loss: 9.3259 Val_Loss: 56.0833  BEST VAL Loss: 56.0833\n",
            "\n",
            "Epoch 1277: Validation loss decreased (56.083294 --> 55.979115).\n",
            "\t Train_Loss: 9.3017 Val_Loss: 55.9791  BEST VAL Loss: 55.9791\n",
            "\n",
            "Epoch 1278: Validation loss decreased (55.979115 --> 55.856262).\n",
            "\t Train_Loss: 9.2738 Val_Loss: 55.8563  BEST VAL Loss: 55.8563\n",
            "\n",
            "Epoch 1279: Validation loss decreased (55.856262 --> 55.715466).\n",
            "\t Train_Loss: 9.2479 Val_Loss: 55.7155  BEST VAL Loss: 55.7155\n",
            "\n",
            "Epoch 1280: Validation loss decreased (55.715466 --> 55.635475).\n",
            "\t Train_Loss: 9.2234 Val_Loss: 55.6355  BEST VAL Loss: 55.6355\n",
            "\n",
            "Epoch 1281: Validation loss decreased (55.635475 --> 55.553944).\n",
            "\t Train_Loss: 9.1964 Val_Loss: 55.5539  BEST VAL Loss: 55.5539\n",
            "\n",
            "Epoch 1282: Validation loss decreased (55.553944 --> 55.437836).\n",
            "\t Train_Loss: 9.1718 Val_Loss: 55.4378  BEST VAL Loss: 55.4378\n",
            "\n",
            "Epoch 1283: Validation loss decreased (55.437836 --> 55.347412).\n",
            "\t Train_Loss: 9.1473 Val_Loss: 55.3474  BEST VAL Loss: 55.3474\n",
            "\n",
            "Epoch 1284: Validation loss decreased (55.347412 --> 55.243618).\n",
            "\t Train_Loss: 9.1216 Val_Loss: 55.2436  BEST VAL Loss: 55.2436\n",
            "\n",
            "Epoch 1285: Validation loss decreased (55.243618 --> 55.103951).\n",
            "\t Train_Loss: 9.0979 Val_Loss: 55.1040  BEST VAL Loss: 55.1040\n",
            "\n",
            "Epoch 1286: Validation loss decreased (55.103951 --> 54.995361).\n",
            "\t Train_Loss: 9.0736 Val_Loss: 54.9954  BEST VAL Loss: 54.9954\n",
            "\n",
            "Epoch 1287: Validation loss decreased (54.995361 --> 54.898220).\n",
            "\t Train_Loss: 9.0491 Val_Loss: 54.8982  BEST VAL Loss: 54.8982\n",
            "\n",
            "Epoch 1288: Validation loss decreased (54.898220 --> 54.770840).\n",
            "\t Train_Loss: 9.0261 Val_Loss: 54.7708  BEST VAL Loss: 54.7708\n",
            "\n",
            "Epoch 1289: Validation loss decreased (54.770840 --> 54.658596).\n",
            "\t Train_Loss: 9.0023 Val_Loss: 54.6586  BEST VAL Loss: 54.6586\n",
            "\n",
            "Epoch 1290: Validation loss decreased (54.658596 --> 54.558498).\n",
            "\t Train_Loss: 8.9790 Val_Loss: 54.5585  BEST VAL Loss: 54.5585\n",
            "\n",
            "Epoch 1291: Validation loss decreased (54.558498 --> 54.439606).\n",
            "\t Train_Loss: 8.9564 Val_Loss: 54.4396  BEST VAL Loss: 54.4396\n",
            "\n",
            "Epoch 1292: Validation loss decreased (54.439606 --> 54.341053).\n",
            "\t Train_Loss: 8.9332 Val_Loss: 54.3411  BEST VAL Loss: 54.3411\n",
            "\n",
            "Epoch 1293: Validation loss decreased (54.341053 --> 54.263027).\n",
            "\t Train_Loss: 8.9108 Val_Loss: 54.2630  BEST VAL Loss: 54.2630\n",
            "\n",
            "Epoch 1294: Validation loss decreased (54.263027 --> 54.161865).\n",
            "\t Train_Loss: 8.8886 Val_Loss: 54.1619  BEST VAL Loss: 54.1619\n",
            "\n",
            "Epoch 1295: Validation loss decreased (54.161865 --> 54.052803).\n",
            "\t Train_Loss: 8.8662 Val_Loss: 54.0528  BEST VAL Loss: 54.0528\n",
            "\n",
            "Epoch 1296: Validation loss decreased (54.052803 --> 53.950977).\n",
            "\t Train_Loss: 8.8445 Val_Loss: 53.9510  BEST VAL Loss: 53.9510\n",
            "\n",
            "Epoch 1297: Validation loss decreased (53.950977 --> 53.835033).\n",
            "\t Train_Loss: 8.8227 Val_Loss: 53.8350  BEST VAL Loss: 53.8350\n",
            "\n",
            "Epoch 1298: Validation loss decreased (53.835033 --> 53.721130).\n",
            "\t Train_Loss: 8.8010 Val_Loss: 53.7211  BEST VAL Loss: 53.7211\n",
            "\n",
            "Epoch 1299: Validation loss decreased (53.721130 --> 53.627930).\n",
            "\t Train_Loss: 8.7799 Val_Loss: 53.6279  BEST VAL Loss: 53.6279\n",
            "\n",
            "Epoch 1300: Validation loss decreased (53.627930 --> 53.529140).\n",
            "\t Train_Loss: 8.7585 Val_Loss: 53.5291  BEST VAL Loss: 53.5291\n",
            "\n",
            "Epoch 1301: Validation loss decreased (53.529140 --> 53.422207).\n",
            "\t Train_Loss: 8.7374 Val_Loss: 53.4222  BEST VAL Loss: 53.4222\n",
            "\n",
            "Epoch 1302: Validation loss decreased (53.422207 --> 53.327057).\n",
            "\t Train_Loss: 8.7166 Val_Loss: 53.3271  BEST VAL Loss: 53.3271\n",
            "\n",
            "Epoch 1303: Validation loss decreased (53.327057 --> 53.231838).\n",
            "\t Train_Loss: 8.6957 Val_Loss: 53.2318  BEST VAL Loss: 53.2318\n",
            "\n",
            "Epoch 1304: Validation loss decreased (53.231838 --> 53.130013).\n",
            "\t Train_Loss: 8.6752 Val_Loss: 53.1300  BEST VAL Loss: 53.1300\n",
            "\n",
            "Epoch 1305: Validation loss decreased (53.130013 --> 53.036232).\n",
            "\t Train_Loss: 8.6547 Val_Loss: 53.0362  BEST VAL Loss: 53.0362\n",
            "\n",
            "Epoch 1306: Validation loss decreased (53.036232 --> 52.940521).\n",
            "\t Train_Loss: 8.6342 Val_Loss: 52.9405  BEST VAL Loss: 52.9405\n",
            "\n",
            "Epoch 1307: Validation loss decreased (52.940521 --> 52.832764).\n",
            "\t Train_Loss: 8.6141 Val_Loss: 52.8328  BEST VAL Loss: 52.8328\n",
            "\n",
            "Epoch 1308: Validation loss decreased (52.832764 --> 52.730255).\n",
            "\t Train_Loss: 8.5938 Val_Loss: 52.7303  BEST VAL Loss: 52.7303\n",
            "\n",
            "Epoch 1309: Validation loss decreased (52.730255 --> 52.635883).\n",
            "\t Train_Loss: 8.5738 Val_Loss: 52.6359  BEST VAL Loss: 52.6359\n",
            "\n",
            "Epoch 1310: Validation loss decreased (52.635883 --> 52.537941).\n",
            "\t Train_Loss: 8.5540 Val_Loss: 52.5379  BEST VAL Loss: 52.5379\n",
            "\n",
            "Epoch 1311: Validation loss decreased (52.537941 --> 52.443336).\n",
            "\t Train_Loss: 8.5341 Val_Loss: 52.4433  BEST VAL Loss: 52.4433\n",
            "\n",
            "Epoch 1312: Validation loss decreased (52.443336 --> 52.353760).\n",
            "\t Train_Loss: 8.5144 Val_Loss: 52.3538  BEST VAL Loss: 52.3538\n",
            "\n",
            "Epoch 1313: Validation loss decreased (52.353760 --> 52.258747).\n",
            "\t Train_Loss: 8.4948 Val_Loss: 52.2587  BEST VAL Loss: 52.2587\n",
            "\n",
            "Epoch 1314: Validation loss decreased (52.258747 --> 52.164246).\n",
            "\t Train_Loss: 8.4753 Val_Loss: 52.1642  BEST VAL Loss: 52.1642\n",
            "\n",
            "Epoch 1315: Validation loss decreased (52.164246 --> 52.076962).\n",
            "\t Train_Loss: 8.4559 Val_Loss: 52.0770  BEST VAL Loss: 52.0770\n",
            "\n",
            "Epoch 1316: Validation loss decreased (52.076962 --> 51.985901).\n",
            "\t Train_Loss: 8.4365 Val_Loss: 51.9859  BEST VAL Loss: 51.9859\n",
            "\n",
            "Epoch 1317: Validation loss decreased (51.985901 --> 51.889370).\n",
            "\t Train_Loss: 8.4173 Val_Loss: 51.8894  BEST VAL Loss: 51.8894\n",
            "\n",
            "Epoch 1318: Validation loss decreased (51.889370 --> 51.794811).\n",
            "\t Train_Loss: 8.3981 Val_Loss: 51.7948  BEST VAL Loss: 51.7948\n",
            "\n",
            "Epoch 1319: Validation loss decreased (51.794811 --> 51.699108).\n",
            "\t Train_Loss: 8.3790 Val_Loss: 51.6991  BEST VAL Loss: 51.6991\n",
            "\n",
            "Epoch 1320: Validation loss decreased (51.699108 --> 51.603161).\n",
            "\t Train_Loss: 8.3600 Val_Loss: 51.6032  BEST VAL Loss: 51.6032\n",
            "\n",
            "Epoch 1321: Validation loss decreased (51.603161 --> 51.514111).\n",
            "\t Train_Loss: 8.3410 Val_Loss: 51.5141  BEST VAL Loss: 51.5141\n",
            "\n",
            "Epoch 1322: Validation loss decreased (51.514111 --> 51.426937).\n",
            "\t Train_Loss: 8.3221 Val_Loss: 51.4269  BEST VAL Loss: 51.4269\n",
            "\n",
            "Epoch 1323: Validation loss decreased (51.426937 --> 51.335152).\n",
            "\t Train_Loss: 8.3033 Val_Loss: 51.3352  BEST VAL Loss: 51.3352\n",
            "\n",
            "Epoch 1324: Validation loss decreased (51.335152 --> 51.244354).\n",
            "\t Train_Loss: 8.2845 Val_Loss: 51.2444  BEST VAL Loss: 51.2444\n",
            "\n",
            "Epoch 1325: Validation loss decreased (51.244354 --> 51.155975).\n",
            "\t Train_Loss: 8.2658 Val_Loss: 51.1560  BEST VAL Loss: 51.1560\n",
            "\n",
            "Epoch 1326: Validation loss decreased (51.155975 --> 51.066662).\n",
            "\t Train_Loss: 8.2471 Val_Loss: 51.0667  BEST VAL Loss: 51.0667\n",
            "\n",
            "Epoch 1327: Validation loss decreased (51.066662 --> 50.979839).\n",
            "\t Train_Loss: 8.2283 Val_Loss: 50.9798  BEST VAL Loss: 50.9798\n",
            "\n",
            "Epoch 1328: Validation loss decreased (50.979839 --> 50.895538).\n",
            "\t Train_Loss: 8.2093 Val_Loss: 50.8955  BEST VAL Loss: 50.8955\n",
            "\n",
            "Epoch 1329: Validation loss decreased (50.895538 --> 50.810318).\n",
            "\t Train_Loss: 8.1899 Val_Loss: 50.8103  BEST VAL Loss: 50.8103\n",
            "\n",
            "Epoch 1330: Validation loss decreased (50.810318 --> 50.728504).\n",
            "\t Train_Loss: 8.1703 Val_Loss: 50.7285  BEST VAL Loss: 50.7285\n",
            "\n",
            "Epoch 1331: Validation loss decreased (50.728504 --> 50.665783).\n",
            "\t Train_Loss: 8.1507 Val_Loss: 50.6658  BEST VAL Loss: 50.6658\n",
            "\n",
            "Epoch 1332: Validation loss decreased (50.665783 --> 50.547848).\n",
            "\t Train_Loss: 8.1339 Val_Loss: 50.5478  BEST VAL Loss: 50.5478\n",
            "\n",
            "Epoch 1333: Validation loss decreased (50.547848 --> 50.465820).\n",
            "\t Train_Loss: 8.1356 Val_Loss: 50.4658  BEST VAL Loss: 50.4658\n",
            "\n",
            "Epoch 1334: Validation loss decreased (50.465820 --> 50.255676).\n",
            "\t Train_Loss: 8.1111 Val_Loss: 50.2557  BEST VAL Loss: 50.2557\n",
            "\n",
            "Epoch 1335: Validation loss decreased (50.255676 --> 50.089550).\n",
            "\t Train_Loss: 8.0799 Val_Loss: 50.0896  BEST VAL Loss: 50.0896\n",
            "\n",
            "Epoch 1336: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0720 Val_Loss: 50.0942  BEST VAL Loss: 50.0896\n",
            "\n",
            "Epoch 1337: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0551 Val_Loss: 50.0922  BEST VAL Loss: 50.0896\n",
            "\n",
            "Epoch 1338: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0444 Val_Loss: 50.1607  BEST VAL Loss: 50.0896\n",
            "\n",
            "Epoch 1339: Validation loss decreased (50.089550 --> 50.063171).\n",
            "\t Train_Loss: 8.0113 Val_Loss: 50.0632  BEST VAL Loss: 50.0632\n",
            "\n",
            "Epoch 1340: Validation loss decreased (50.063171 --> 49.800762).\n",
            "\t Train_Loss: 7.9959 Val_Loss: 49.8008  BEST VAL Loss: 49.8008\n",
            "\n",
            "Epoch 1341: Validation loss decreased (49.800762 --> 49.626873).\n",
            "\t Train_Loss: 7.9753 Val_Loss: 49.6269  BEST VAL Loss: 49.6269\n",
            "\n",
            "Epoch 1342: Validation loss decreased (49.626873 --> 49.582104).\n",
            "\t Train_Loss: 7.9432 Val_Loss: 49.5821  BEST VAL Loss: 49.5821\n",
            "\n",
            "Epoch 1343: Validation loss decreased (49.582104 --> 49.555798).\n",
            "\t Train_Loss: 7.9321 Val_Loss: 49.5558  BEST VAL Loss: 49.5558\n",
            "\n",
            "Epoch 1344: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9425 Val_Loss: 49.5896  BEST VAL Loss: 49.5558\n",
            "\n",
            "Epoch 1345: Validation loss decreased (49.555798 --> 49.439735).\n",
            "\t Train_Loss: 7.8932 Val_Loss: 49.4397  BEST VAL Loss: 49.4397\n",
            "\n",
            "Epoch 1346: Validation loss decreased (49.439735 --> 49.177082).\n",
            "\t Train_Loss: 7.8781 Val_Loss: 49.1771  BEST VAL Loss: 49.1771\n",
            "\n",
            "Epoch 1347: Validation loss decreased (49.177082 --> 49.040257).\n",
            "\t Train_Loss: 7.8721 Val_Loss: 49.0403  BEST VAL Loss: 49.0403\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8243 Val_Loss: 49.0447  BEST VAL Loss: 49.0403\n",
            "\n",
            "Epoch 1349: Validation loss decreased (49.040257 --> 49.028702).\n",
            "\t Train_Loss: 7.8155 Val_Loss: 49.0287  BEST VAL Loss: 49.0287\n",
            "\n",
            "Epoch 1350: Validation loss decreased (49.028702 --> 49.011272).\n",
            "\t Train_Loss: 7.8095 Val_Loss: 49.0113  BEST VAL Loss: 49.0113\n",
            "\n",
            "Epoch 1351: Validation loss decreased (49.011272 --> 48.867458).\n",
            "\t Train_Loss: 7.7660 Val_Loss: 48.8675  BEST VAL Loss: 48.8675\n",
            "\n",
            "Epoch 1352: Validation loss decreased (48.867458 --> 48.622860).\n",
            "\t Train_Loss: 7.7589 Val_Loss: 48.6229  BEST VAL Loss: 48.6229\n",
            "\n",
            "Epoch 1353: Validation loss decreased (48.622860 --> 48.507866).\n",
            "\t Train_Loss: 7.7424 Val_Loss: 48.5079  BEST VAL Loss: 48.5079\n",
            "\n",
            "Epoch 1354: Validation loss decreased (48.507866 --> 48.488140).\n",
            "\t Train_Loss: 7.7083 Val_Loss: 48.4881  BEST VAL Loss: 48.4881\n",
            "\n",
            "Epoch 1355: Validation loss decreased (48.488140 --> 48.434948).\n",
            "\t Train_Loss: 7.7005 Val_Loss: 48.4349  BEST VAL Loss: 48.4349\n",
            "\n",
            "Epoch 1356: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6847 Val_Loss: 48.4571  BEST VAL Loss: 48.4349\n",
            "\n",
            "Epoch 1357: Validation loss decreased (48.434948 --> 48.392418).\n",
            "\t Train_Loss: 7.6509 Val_Loss: 48.3924  BEST VAL Loss: 48.3924\n",
            "\n",
            "Epoch 1358: Validation loss decreased (48.392418 --> 48.213497).\n",
            "\t Train_Loss: 7.6342 Val_Loss: 48.2135  BEST VAL Loss: 48.2135\n",
            "\n",
            "Epoch 1359: Validation loss decreased (48.213497 --> 48.105694).\n",
            "\t Train_Loss: 7.6193 Val_Loss: 48.1057  BEST VAL Loss: 48.1057\n",
            "\n",
            "Epoch 1360: Validation loss decreased (48.105694 --> 48.050030).\n",
            "\t Train_Loss: 7.5928 Val_Loss: 48.0500  BEST VAL Loss: 48.0500\n",
            "\n",
            "Epoch 1361: Validation loss decreased (48.050030 --> 47.993053).\n",
            "\t Train_Loss: 7.5785 Val_Loss: 47.9931  BEST VAL Loss: 47.9931\n",
            "\n",
            "Epoch 1362: Validation loss decreased (47.993053 --> 47.991089).\n",
            "\t Train_Loss: 7.5596 Val_Loss: 47.9911  BEST VAL Loss: 47.9911\n",
            "\n",
            "Epoch 1363: Validation loss decreased (47.991089 --> 47.929745).\n",
            "\t Train_Loss: 7.5317 Val_Loss: 47.9297  BEST VAL Loss: 47.9297\n",
            "\n",
            "Epoch 1364: Validation loss decreased (47.929745 --> 47.699154).\n",
            "\t Train_Loss: 7.5199 Val_Loss: 47.6992  BEST VAL Loss: 47.6992\n",
            "\n",
            "Epoch 1365: Validation loss decreased (47.699154 --> 47.534805).\n",
            "\t Train_Loss: 7.4993 Val_Loss: 47.5348  BEST VAL Loss: 47.5348\n",
            "\n",
            "Epoch 1366: Validation loss decreased (47.534805 --> 47.486427).\n",
            "\t Train_Loss: 7.4749 Val_Loss: 47.4864  BEST VAL Loss: 47.4864\n",
            "\n",
            "Epoch 1367: Validation loss decreased (47.486427 --> 47.469738).\n",
            "\t Train_Loss: 7.4648 Val_Loss: 47.4697  BEST VAL Loss: 47.4697\n",
            "\n",
            "Epoch 1368: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4399 Val_Loss: 47.4989  BEST VAL Loss: 47.4697\n",
            "\n",
            "Epoch 1369: Validation loss decreased (47.469738 --> 47.434155).\n",
            "\t Train_Loss: 7.4193 Val_Loss: 47.4342  BEST VAL Loss: 47.4342\n",
            "\n",
            "Epoch 1370: Validation loss decreased (47.434155 --> 47.206806).\n",
            "\t Train_Loss: 7.4059 Val_Loss: 47.2068  BEST VAL Loss: 47.2068\n",
            "\n",
            "Epoch 1371: Validation loss decreased (47.206806 --> 47.064026).\n",
            "\t Train_Loss: 7.3825 Val_Loss: 47.0640  BEST VAL Loss: 47.0640\n",
            "\n",
            "Epoch 1372: Validation loss decreased (47.064026 --> 47.042484).\n",
            "\t Train_Loss: 7.3641 Val_Loss: 47.0425  BEST VAL Loss: 47.0425\n",
            "\n",
            "Epoch 1373: Validation loss decreased (47.042484 --> 47.011379).\n",
            "\t Train_Loss: 7.3484 Val_Loss: 47.0114  BEST VAL Loss: 47.0114\n",
            "\n",
            "Epoch 1374: Validation loss decreased (47.011379 --> 46.964344).\n",
            "\t Train_Loss: 7.3267 Val_Loss: 46.9643  BEST VAL Loss: 46.9643\n",
            "\n",
            "Epoch 1375: Validation loss decreased (46.964344 --> 46.876881).\n",
            "\t Train_Loss: 7.3089 Val_Loss: 46.8769  BEST VAL Loss: 46.8769\n",
            "\n",
            "Epoch 1376: Validation loss decreased (46.876881 --> 46.712147).\n",
            "\t Train_Loss: 7.2929 Val_Loss: 46.7121  BEST VAL Loss: 46.7121\n",
            "\n",
            "Epoch 1377: Validation loss decreased (46.712147 --> 46.602882).\n",
            "\t Train_Loss: 7.2714 Val_Loss: 46.6029  BEST VAL Loss: 46.6029\n",
            "\n",
            "Epoch 1378: Validation loss decreased (46.602882 --> 46.573425).\n",
            "\t Train_Loss: 7.2548 Val_Loss: 46.5734  BEST VAL Loss: 46.5734\n",
            "\n",
            "Epoch 1379: Validation loss decreased (46.573425 --> 46.512661).\n",
            "\t Train_Loss: 7.2374 Val_Loss: 46.5127  BEST VAL Loss: 46.5127\n",
            "\n",
            "Epoch 1380: Validation loss decreased (46.512661 --> 46.447685).\n",
            "\t Train_Loss: 7.2174 Val_Loss: 46.4477  BEST VAL Loss: 46.4477\n",
            "\n",
            "Epoch 1381: Validation loss decreased (46.447685 --> 46.396626).\n",
            "\t Train_Loss: 7.2016 Val_Loss: 46.3966  BEST VAL Loss: 46.3966\n",
            "\n",
            "Epoch 1382: Validation loss decreased (46.396626 --> 46.282604).\n",
            "\t Train_Loss: 7.1832 Val_Loss: 46.2826  BEST VAL Loss: 46.2826\n",
            "\n",
            "Epoch 1383: Validation loss decreased (46.282604 --> 46.160797).\n",
            "\t Train_Loss: 7.1650 Val_Loss: 46.1608  BEST VAL Loss: 46.1608\n",
            "\n",
            "Epoch 1384: Validation loss decreased (46.160797 --> 46.109280).\n",
            "\t Train_Loss: 7.1484 Val_Loss: 46.1093  BEST VAL Loss: 46.1093\n",
            "\n",
            "Epoch 1385: Validation loss decreased (46.109280 --> 46.062183).\n",
            "\t Train_Loss: 7.1306 Val_Loss: 46.0622  BEST VAL Loss: 46.0622\n",
            "\n",
            "Epoch 1386: Validation loss decreased (46.062183 --> 45.992977).\n",
            "\t Train_Loss: 7.1127 Val_Loss: 45.9930  BEST VAL Loss: 45.9930\n",
            "\n",
            "Epoch 1387: Validation loss decreased (45.992977 --> 45.929268).\n",
            "\t Train_Loss: 7.0965 Val_Loss: 45.9293  BEST VAL Loss: 45.9293\n",
            "\n",
            "Epoch 1388: Validation loss decreased (45.929268 --> 45.831524).\n",
            "\t Train_Loss: 7.0785 Val_Loss: 45.8315  BEST VAL Loss: 45.8315\n",
            "\n",
            "Epoch 1389: Validation loss decreased (45.831524 --> 45.721844).\n",
            "\t Train_Loss: 7.0616 Val_Loss: 45.7218  BEST VAL Loss: 45.7218\n",
            "\n",
            "Epoch 1390: Validation loss decreased (45.721844 --> 45.665447).\n",
            "\t Train_Loss: 7.0450 Val_Loss: 45.6654  BEST VAL Loss: 45.6654\n",
            "\n",
            "Epoch 1391: Validation loss decreased (45.665447 --> 45.610374).\n",
            "\t Train_Loss: 7.0275 Val_Loss: 45.6104  BEST VAL Loss: 45.6104\n",
            "\n",
            "Epoch 1392: Validation loss decreased (45.610374 --> 45.526180).\n",
            "\t Train_Loss: 7.0111 Val_Loss: 45.5262  BEST VAL Loss: 45.5262\n",
            "\n",
            "Epoch 1393: Validation loss decreased (45.526180 --> 45.469128).\n",
            "\t Train_Loss: 6.9943 Val_Loss: 45.4691  BEST VAL Loss: 45.4691\n",
            "\n",
            "Epoch 1394: Validation loss decreased (45.469128 --> 45.405891).\n",
            "\t Train_Loss: 6.9773 Val_Loss: 45.4059  BEST VAL Loss: 45.4059\n",
            "\n",
            "Epoch 1395: Validation loss decreased (45.405891 --> 45.296391).\n",
            "\t Train_Loss: 6.9610 Val_Loss: 45.2964  BEST VAL Loss: 45.2964\n",
            "\n",
            "Epoch 1396: Validation loss decreased (45.296391 --> 45.208565).\n",
            "\t Train_Loss: 6.9445 Val_Loss: 45.2086  BEST VAL Loss: 45.2086\n",
            "\n",
            "Epoch 1397: Validation loss decreased (45.208565 --> 45.150814).\n",
            "\t Train_Loss: 6.9278 Val_Loss: 45.1508  BEST VAL Loss: 45.1508\n",
            "\n",
            "Epoch 1398: Validation loss decreased (45.150814 --> 45.083191).\n",
            "\t Train_Loss: 6.9118 Val_Loss: 45.0832  BEST VAL Loss: 45.0832\n",
            "\n",
            "Epoch 1399: Validation loss decreased (45.083191 --> 45.025669).\n",
            "\t Train_Loss: 6.8952 Val_Loss: 45.0257  BEST VAL Loss: 45.0257\n",
            "\n",
            "Epoch 1400: Validation loss decreased (45.025669 --> 44.958031).\n",
            "\t Train_Loss: 6.8790 Val_Loss: 44.9580  BEST VAL Loss: 44.9580\n",
            "\n",
            "Epoch 1401: Validation loss decreased (44.958031 --> 44.855011).\n",
            "\t Train_Loss: 6.8629 Val_Loss: 44.8550  BEST VAL Loss: 44.8550\n",
            "\n",
            "Epoch 1402: Validation loss decreased (44.855011 --> 44.780758).\n",
            "\t Train_Loss: 6.8467 Val_Loss: 44.7808  BEST VAL Loss: 44.7808\n",
            "\n",
            "Epoch 1403: Validation loss decreased (44.780758 --> 44.739075).\n",
            "\t Train_Loss: 6.8307 Val_Loss: 44.7391  BEST VAL Loss: 44.7391\n",
            "\n",
            "Epoch 1404: Validation loss decreased (44.739075 --> 44.670654).\n",
            "\t Train_Loss: 6.8148 Val_Loss: 44.6707  BEST VAL Loss: 44.6707\n",
            "\n",
            "Epoch 1405: Validation loss decreased (44.670654 --> 44.594727).\n",
            "\t Train_Loss: 6.7987 Val_Loss: 44.5947  BEST VAL Loss: 44.5947\n",
            "\n",
            "Epoch 1406: Validation loss decreased (44.594727 --> 44.523140).\n",
            "\t Train_Loss: 6.7830 Val_Loss: 44.5231  BEST VAL Loss: 44.5231\n",
            "\n",
            "Epoch 1407: Validation loss decreased (44.523140 --> 44.434597).\n",
            "\t Train_Loss: 6.7672 Val_Loss: 44.4346  BEST VAL Loss: 44.4346\n",
            "\n",
            "Epoch 1408: Validation loss decreased (44.434597 --> 44.367786).\n",
            "\t Train_Loss: 6.7514 Val_Loss: 44.3678  BEST VAL Loss: 44.3678\n",
            "\n",
            "Epoch 1409: Validation loss decreased (44.367786 --> 44.323933).\n",
            "\t Train_Loss: 6.7357 Val_Loss: 44.3239  BEST VAL Loss: 44.3239\n",
            "\n",
            "Epoch 1410: Validation loss decreased (44.323933 --> 44.254612).\n",
            "\t Train_Loss: 6.7202 Val_Loss: 44.2546  BEST VAL Loss: 44.2546\n",
            "\n",
            "Epoch 1411: Validation loss decreased (44.254612 --> 44.182720).\n",
            "\t Train_Loss: 6.7044 Val_Loss: 44.1827  BEST VAL Loss: 44.1827\n",
            "\n",
            "Epoch 1412: Validation loss decreased (44.182720 --> 44.121113).\n",
            "\t Train_Loss: 6.6890 Val_Loss: 44.1211  BEST VAL Loss: 44.1211\n",
            "\n",
            "Epoch 1413: Validation loss decreased (44.121113 --> 44.041325).\n",
            "\t Train_Loss: 6.6735 Val_Loss: 44.0413  BEST VAL Loss: 44.0413\n",
            "\n",
            "Epoch 1414: Validation loss decreased (44.041325 --> 43.969978).\n",
            "\t Train_Loss: 6.6580 Val_Loss: 43.9700  BEST VAL Loss: 43.9700\n",
            "\n",
            "Epoch 1415: Validation loss decreased (43.969978 --> 43.914547).\n",
            "\t Train_Loss: 6.6425 Val_Loss: 43.9145  BEST VAL Loss: 43.9145\n",
            "\n",
            "Epoch 1416: Validation loss decreased (43.914547 --> 43.846180).\n",
            "\t Train_Loss: 6.6272 Val_Loss: 43.8462  BEST VAL Loss: 43.8462\n",
            "\n",
            "Epoch 1417: Validation loss decreased (43.846180 --> 43.786655).\n",
            "\t Train_Loss: 6.6118 Val_Loss: 43.7867  BEST VAL Loss: 43.7867\n",
            "\n",
            "Epoch 1418: Validation loss decreased (43.786655 --> 43.732281).\n",
            "\t Train_Loss: 6.5965 Val_Loss: 43.7323  BEST VAL Loss: 43.7323\n",
            "\n",
            "Epoch 1419: Validation loss decreased (43.732281 --> 43.655895).\n",
            "\t Train_Loss: 6.5812 Val_Loss: 43.6559  BEST VAL Loss: 43.6559\n",
            "\n",
            "Epoch 1420: Validation loss decreased (43.655895 --> 43.587563).\n",
            "\t Train_Loss: 6.5659 Val_Loss: 43.5876  BEST VAL Loss: 43.5876\n",
            "\n",
            "Epoch 1421: Validation loss decreased (43.587563 --> 43.530659).\n",
            "\t Train_Loss: 6.5506 Val_Loss: 43.5307  BEST VAL Loss: 43.5307\n",
            "\n",
            "Epoch 1422: Validation loss decreased (43.530659 --> 43.466667).\n",
            "\t Train_Loss: 6.5354 Val_Loss: 43.4667  BEST VAL Loss: 43.4667\n",
            "\n",
            "Epoch 1423: Validation loss decreased (43.466667 --> 43.413948).\n",
            "\t Train_Loss: 6.5202 Val_Loss: 43.4139  BEST VAL Loss: 43.4139\n",
            "\n",
            "Epoch 1424: Validation loss decreased (43.413948 --> 43.356506).\n",
            "\t Train_Loss: 6.5050 Val_Loss: 43.3565  BEST VAL Loss: 43.3565\n",
            "\n",
            "Epoch 1425: Validation loss decreased (43.356506 --> 43.282864).\n",
            "\t Train_Loss: 6.4899 Val_Loss: 43.2829  BEST VAL Loss: 43.2829\n",
            "\n",
            "Epoch 1426: Validation loss decreased (43.282864 --> 43.225891).\n",
            "\t Train_Loss: 6.4748 Val_Loss: 43.2259  BEST VAL Loss: 43.2259\n",
            "\n",
            "Epoch 1427: Validation loss decreased (43.225891 --> 43.174122).\n",
            "\t Train_Loss: 6.4598 Val_Loss: 43.1741  BEST VAL Loss: 43.1741\n",
            "\n",
            "Epoch 1428: Validation loss decreased (43.174122 --> 43.117023).\n",
            "\t Train_Loss: 6.4448 Val_Loss: 43.1170  BEST VAL Loss: 43.1170\n",
            "\n",
            "Epoch 1429: Validation loss decreased (43.117023 --> 43.069042).\n",
            "\t Train_Loss: 6.4300 Val_Loss: 43.0690  BEST VAL Loss: 43.0690\n",
            "\n",
            "Epoch 1430: Validation loss decreased (43.069042 --> 43.010529).\n",
            "\t Train_Loss: 6.4152 Val_Loss: 43.0105  BEST VAL Loss: 43.0105\n",
            "\n",
            "Epoch 1431: Validation loss decreased (43.010529 --> 42.950012).\n",
            "\t Train_Loss: 6.4005 Val_Loss: 42.9500  BEST VAL Loss: 42.9500\n",
            "\n",
            "Epoch 1432: Validation loss decreased (42.950012 --> 42.904652).\n",
            "\t Train_Loss: 6.3860 Val_Loss: 42.9047  BEST VAL Loss: 42.9047\n",
            "\n",
            "Epoch 1433: Validation loss decreased (42.904652 --> 42.852482).\n",
            "\t Train_Loss: 6.3716 Val_Loss: 42.8525  BEST VAL Loss: 42.8525\n",
            "\n",
            "Epoch 1434: Validation loss decreased (42.852482 --> 42.802166).\n",
            "\t Train_Loss: 6.3574 Val_Loss: 42.8022  BEST VAL Loss: 42.8022\n",
            "\n",
            "Epoch 1435: Validation loss decreased (42.802166 --> 42.758144).\n",
            "\t Train_Loss: 6.3432 Val_Loss: 42.7581  BEST VAL Loss: 42.7581\n",
            "\n",
            "Epoch 1436: Validation loss decreased (42.758144 --> 42.707031).\n",
            "\t Train_Loss: 6.3292 Val_Loss: 42.7070  BEST VAL Loss: 42.7070\n",
            "\n",
            "Epoch 1437: Validation loss decreased (42.707031 --> 42.661297).\n",
            "\t Train_Loss: 6.3153 Val_Loss: 42.6613  BEST VAL Loss: 42.6613\n",
            "\n",
            "Epoch 1438: Validation loss decreased (42.661297 --> 42.614758).\n",
            "\t Train_Loss: 6.3014 Val_Loss: 42.6148  BEST VAL Loss: 42.6148\n",
            "\n",
            "Epoch 1439: Validation loss decreased (42.614758 --> 42.565231).\n",
            "\t Train_Loss: 6.2877 Val_Loss: 42.5652  BEST VAL Loss: 42.5652\n",
            "\n",
            "Epoch 1440: Validation loss decreased (42.565231 --> 42.521996).\n",
            "\t Train_Loss: 6.2740 Val_Loss: 42.5220  BEST VAL Loss: 42.5220\n",
            "\n",
            "Epoch 1441: Validation loss decreased (42.521996 --> 42.478424).\n",
            "\t Train_Loss: 6.2604 Val_Loss: 42.4784  BEST VAL Loss: 42.4784\n",
            "\n",
            "Epoch 1442: Validation loss decreased (42.478424 --> 42.437672).\n",
            "\t Train_Loss: 6.2469 Val_Loss: 42.4377  BEST VAL Loss: 42.4377\n",
            "\n",
            "Epoch 1443: Validation loss decreased (42.437672 --> 42.392860).\n",
            "\t Train_Loss: 6.2335 Val_Loss: 42.3929  BEST VAL Loss: 42.3929\n",
            "\n",
            "Epoch 1444: Validation loss decreased (42.392860 --> 42.344540).\n",
            "\t Train_Loss: 6.2201 Val_Loss: 42.3445  BEST VAL Loss: 42.3445\n",
            "\n",
            "Epoch 1445: Validation loss decreased (42.344540 --> 42.302101).\n",
            "\t Train_Loss: 6.2069 Val_Loss: 42.3021  BEST VAL Loss: 42.3021\n",
            "\n",
            "Epoch 1446: Validation loss decreased (42.302101 --> 42.257057).\n",
            "\t Train_Loss: 6.1937 Val_Loss: 42.2571  BEST VAL Loss: 42.2571\n",
            "\n",
            "Epoch 1447: Validation loss decreased (42.257057 --> 42.215542).\n",
            "\t Train_Loss: 6.1806 Val_Loss: 42.2155  BEST VAL Loss: 42.2155\n",
            "\n",
            "Epoch 1448: Validation loss decreased (42.215542 --> 42.173042).\n",
            "\t Train_Loss: 6.1676 Val_Loss: 42.1730  BEST VAL Loss: 42.1730\n",
            "\n",
            "Epoch 1449: Validation loss decreased (42.173042 --> 42.123943).\n",
            "\t Train_Loss: 6.1548 Val_Loss: 42.1239  BEST VAL Loss: 42.1239\n",
            "\n",
            "Epoch 1450: Validation loss decreased (42.123943 --> 42.081852).\n",
            "\t Train_Loss: 6.1420 Val_Loss: 42.0819  BEST VAL Loss: 42.0819\n",
            "\n",
            "Epoch 1451: Validation loss decreased (42.081852 --> 42.037834).\n",
            "\t Train_Loss: 6.1293 Val_Loss: 42.0378  BEST VAL Loss: 42.0378\n",
            "\n",
            "Epoch 1452: Validation loss decreased (42.037834 --> 41.993168).\n",
            "\t Train_Loss: 6.1167 Val_Loss: 41.9932  BEST VAL Loss: 41.9932\n",
            "\n",
            "Epoch 1453: Validation loss decreased (41.993168 --> 41.952827).\n",
            "\t Train_Loss: 6.1042 Val_Loss: 41.9528  BEST VAL Loss: 41.9528\n",
            "\n",
            "Epoch 1454: Validation loss decreased (41.952827 --> 41.906910).\n",
            "\t Train_Loss: 6.0917 Val_Loss: 41.9069  BEST VAL Loss: 41.9069\n",
            "\n",
            "Epoch 1455: Validation loss decreased (41.906910 --> 41.865028).\n",
            "\t Train_Loss: 6.0794 Val_Loss: 41.8650  BEST VAL Loss: 41.8650\n",
            "\n",
            "Epoch 1456: Validation loss decreased (41.865028 --> 41.824574).\n",
            "\t Train_Loss: 6.0671 Val_Loss: 41.8246  BEST VAL Loss: 41.8246\n",
            "\n",
            "Epoch 1457: Validation loss decreased (41.824574 --> 41.780060).\n",
            "\t Train_Loss: 6.0549 Val_Loss: 41.7801  BEST VAL Loss: 41.7801\n",
            "\n",
            "Epoch 1458: Validation loss decreased (41.780060 --> 41.740623).\n",
            "\t Train_Loss: 6.0428 Val_Loss: 41.7406  BEST VAL Loss: 41.7406\n",
            "\n",
            "Epoch 1459: Validation loss decreased (41.740623 --> 41.700016).\n",
            "\t Train_Loss: 6.0308 Val_Loss: 41.7000  BEST VAL Loss: 41.7000\n",
            "\n",
            "Epoch 1460: Validation loss decreased (41.700016 --> 41.660404).\n",
            "\t Train_Loss: 6.0188 Val_Loss: 41.6604  BEST VAL Loss: 41.6604\n",
            "\n",
            "Epoch 1461: Validation loss decreased (41.660404 --> 41.623894).\n",
            "\t Train_Loss: 6.0070 Val_Loss: 41.6239  BEST VAL Loss: 41.6239\n",
            "\n",
            "Epoch 1462: Validation loss decreased (41.623894 --> 41.583595).\n",
            "\t Train_Loss: 5.9952 Val_Loss: 41.5836  BEST VAL Loss: 41.5836\n",
            "\n",
            "Epoch 1463: Validation loss decreased (41.583595 --> 41.546120).\n",
            "\t Train_Loss: 5.9834 Val_Loss: 41.5461  BEST VAL Loss: 41.5461\n",
            "\n",
            "Epoch 1464: Validation loss decreased (41.546120 --> 41.510944).\n",
            "\t Train_Loss: 5.9718 Val_Loss: 41.5109  BEST VAL Loss: 41.5109\n",
            "\n",
            "Epoch 1465: Validation loss decreased (41.510944 --> 41.475246).\n",
            "\t Train_Loss: 5.9602 Val_Loss: 41.4752  BEST VAL Loss: 41.4752\n",
            "\n",
            "Epoch 1466: Validation loss decreased (41.475246 --> 41.442451).\n",
            "\t Train_Loss: 5.9487 Val_Loss: 41.4425  BEST VAL Loss: 41.4425\n",
            "\n",
            "Epoch 1467: Validation loss decreased (41.442451 --> 41.408398).\n",
            "\t Train_Loss: 5.9372 Val_Loss: 41.4084  BEST VAL Loss: 41.4084\n",
            "\n",
            "Epoch 1468: Validation loss decreased (41.408398 --> 41.374786).\n",
            "\t Train_Loss: 5.9258 Val_Loss: 41.3748  BEST VAL Loss: 41.3748\n",
            "\n",
            "Epoch 1469: Validation loss decreased (41.374786 --> 41.344753).\n",
            "\t Train_Loss: 5.9144 Val_Loss: 41.3448  BEST VAL Loss: 41.3448\n",
            "\n",
            "Epoch 1470: Validation loss decreased (41.344753 --> 41.313808).\n",
            "\t Train_Loss: 5.9032 Val_Loss: 41.3138  BEST VAL Loss: 41.3138\n",
            "\n",
            "Epoch 1471: Validation loss decreased (41.313808 --> 41.284901).\n",
            "\t Train_Loss: 5.8919 Val_Loss: 41.2849  BEST VAL Loss: 41.2849\n",
            "\n",
            "Epoch 1472: Validation loss decreased (41.284901 --> 41.257065).\n",
            "\t Train_Loss: 5.8807 Val_Loss: 41.2571  BEST VAL Loss: 41.2571\n",
            "\n",
            "Epoch 1473: Validation loss decreased (41.257065 --> 41.229172).\n",
            "\t Train_Loss: 5.8696 Val_Loss: 41.2292  BEST VAL Loss: 41.2292\n",
            "\n",
            "Epoch 1474: Validation loss decreased (41.229172 --> 41.204777).\n",
            "\t Train_Loss: 5.8585 Val_Loss: 41.2048  BEST VAL Loss: 41.2048\n",
            "\n",
            "Epoch 1475: Validation loss decreased (41.204777 --> 41.180164).\n",
            "\t Train_Loss: 5.8475 Val_Loss: 41.1802  BEST VAL Loss: 41.1802\n",
            "\n",
            "Epoch 1476: Validation loss decreased (41.180164 --> 41.156715).\n",
            "\t Train_Loss: 5.8365 Val_Loss: 41.1567  BEST VAL Loss: 41.1567\n",
            "\n",
            "Epoch 1477: Validation loss decreased (41.156715 --> 41.136089).\n",
            "\t Train_Loss: 5.8256 Val_Loss: 41.1361  BEST VAL Loss: 41.1361\n",
            "\n",
            "Epoch 1478: Validation loss decreased (41.136089 --> 41.115490).\n",
            "\t Train_Loss: 5.8147 Val_Loss: 41.1155  BEST VAL Loss: 41.1155\n",
            "\n",
            "Epoch 1479: Validation loss decreased (41.115490 --> 41.098228).\n",
            "\t Train_Loss: 5.8038 Val_Loss: 41.0982  BEST VAL Loss: 41.0982\n",
            "\n",
            "Epoch 1480: Validation loss decreased (41.098228 --> 41.081268).\n",
            "\t Train_Loss: 5.7930 Val_Loss: 41.0813  BEST VAL Loss: 41.0813\n",
            "\n",
            "Epoch 1481: Validation loss decreased (41.081268 --> 41.066280).\n",
            "\t Train_Loss: 5.7822 Val_Loss: 41.0663  BEST VAL Loss: 41.0663\n",
            "\n",
            "Epoch 1482: Validation loss decreased (41.066280 --> 41.054047).\n",
            "\t Train_Loss: 5.7714 Val_Loss: 41.0540  BEST VAL Loss: 41.0540\n",
            "\n",
            "Epoch 1483: Validation loss decreased (41.054047 --> 41.044598).\n",
            "\t Train_Loss: 5.7606 Val_Loss: 41.0446  BEST VAL Loss: 41.0446\n",
            "\n",
            "Epoch 1484: Validation loss decreased (41.044598 --> 41.035542).\n",
            "\t Train_Loss: 5.7499 Val_Loss: 41.0355  BEST VAL Loss: 41.0355\n",
            "\n",
            "Epoch 1485: Validation loss decreased (41.035542 --> 41.033428).\n",
            "\t Train_Loss: 5.7392 Val_Loss: 41.0334  BEST VAL Loss: 41.0334\n",
            "\n",
            "Epoch 1486: Validation loss decreased (41.033428 --> 41.025341).\n",
            "\t Train_Loss: 5.7286 Val_Loss: 41.0253  BEST VAL Loss: 41.0253\n",
            "\n",
            "Epoch 1487: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7181 Val_Loss: 41.0378  BEST VAL Loss: 41.0253\n",
            "\n",
            "Epoch 1488: Validation loss decreased (41.025341 --> 41.023144).\n",
            "\t Train_Loss: 5.7080 Val_Loss: 41.0231  BEST VAL Loss: 41.0231\n",
            "\n",
            "Epoch 1489: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6990 Val_Loss: 41.0694  BEST VAL Loss: 41.0231\n",
            "\n",
            "Epoch 1490: Validation loss decreased (41.023144 --> 41.015083).\n",
            "\t Train_Loss: 5.6941 Val_Loss: 41.0151  BEST VAL Loss: 41.0151\n",
            "\n",
            "Epoch 1491: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6901 Val_Loss: 41.1371  BEST VAL Loss: 41.0151\n",
            "\n",
            "Epoch 1492: Validation loss decreased (41.015083 --> 40.998367).\n",
            "\t Train_Loss: 5.6943 Val_Loss: 40.9984  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1493: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6610 Val_Loss: 41.1577  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1494: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6524 Val_Loss: 41.0719  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1495: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6518 Val_Loss: 41.1477  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1496: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6271 Val_Loss: 41.2837  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1497: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6201 Val_Loss: 41.0931  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1498: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6186 Val_Loss: 41.3354  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1499: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5980 Val_Loss: 41.2960  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1500: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5870 Val_Loss: 41.3482  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1501: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5846 Val_Loss: 41.4105  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1502: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5650 Val_Loss: 41.4034  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1503: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5573 Val_Loss: 41.5192  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1504: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5555 Val_Loss: 41.4505  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1505: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5360 Val_Loss: 41.6229  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1506: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5292 Val_Loss: 41.5166  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1507: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5211 Val_Loss: 41.6478  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1508: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5039 Val_Loss: 41.6801  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1509: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4978 Val_Loss: 41.6412  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1510: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4905 Val_Loss: 41.7284  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1511: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4746 Val_Loss: 41.7324  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1512: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4689 Val_Loss: 41.7909  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1513: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4626 Val_Loss: 41.7126  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1514: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4472 Val_Loss: 41.8942  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1515: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4416 Val_Loss: 41.7437  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1516: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4328 Val_Loss: 41.9006  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1517: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4189 Val_Loss: 41.7781  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1518: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4128 Val_Loss: 41.9074  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1519: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4067 Val_Loss: 41.8120  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1520: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3931 Val_Loss: 42.1064  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1521: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3881 Val_Loss: 41.8324  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1522: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3812 Val_Loss: 42.1472  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1523: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3719 Val_Loss: 41.8366  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1524: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3695 Val_Loss: 42.2541  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1525: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3729 Val_Loss: 41.8036  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1526: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3652 Val_Loss: 42.4917  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3653 Val_Loss: 41.8621  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3554 Val_Loss: 42.4235  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1529: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3382 Val_Loss: 41.9162  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1530: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3173 Val_Loss: 42.2388  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1531: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2978 Val_Loss: 42.2146  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1532: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2826 Val_Loss: 42.1960  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1533: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2783 Val_Loss: 42.3390  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1534: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2774 Val_Loss: 42.0150  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1535: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2751 Val_Loss: 42.6393  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1536: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2748 Val_Loss: 42.1286  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1537: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2648 Val_Loss: 42.6029  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1538: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2515 Val_Loss: 42.2144  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1539: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2350 Val_Loss: 42.5261  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1540: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2187 Val_Loss: 42.4081  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1541: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2063 Val_Loss: 42.3559  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1542: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1982 Val_Loss: 42.4203  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1543: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1917 Val_Loss: 42.2685  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1544: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1853 Val_Loss: 42.8069  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1545: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1809 Val_Loss: 42.4236  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1546: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1718 Val_Loss: 43.0429  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1547: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1687 Val_Loss: 42.6239  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1548: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1718 Val_Loss: 43.5890  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1549: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2252 Val_Loss: 42.6500  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1550: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3679 Val_Loss: 44.7582  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1551: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5566 Val_Loss: 42.3245  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1552: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2743 Val_Loss: 43.4195  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1553: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1102 Val_Loss: 44.3447  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1554: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2843 Val_Loss: 41.3575  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1555: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2620 Val_Loss: 41.8015  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1556: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1529 Val_Loss: 42.1154  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1557: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0778 Val_Loss: 42.1364  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1558: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2059 Val_Loss: 42.9600  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1559: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2418 Val_Loss: 42.4704  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1231 Val_Loss: 43.3989  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1561: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1318 Val_Loss: 44.7333  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1562: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2900 Val_Loss: 41.1196  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1563: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3491 Val_Loss: 41.3570  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1564: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2556 Val_Loss: 41.4301  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1565: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0400 Val_Loss: 42.7410  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1566: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1937 Val_Loss: 45.4129  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1567: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1290 Val_Loss: 43.4103  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1568: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0753 Val_Loss: 42.8082  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1569: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0408 Val_Loss: 44.1132  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1570: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0349 Val_Loss: 43.4440  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1571: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0785 Val_Loss: 42.6161  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0501 Val_Loss: 41.8429  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1573: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1389 Val_Loss: 43.2584  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0378 Val_Loss: 43.8072  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1575: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9859 Val_Loss: 43.0498  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1576: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0609 Val_Loss: 41.0723  BEST VAL Loss: 40.9984\n",
            "\n",
            "Epoch 1577: Validation loss decreased (40.998367 --> 40.322445).\n",
            "\t Train_Loss: 5.0099 Val_Loss: 40.3224  BEST VAL Loss: 40.3224\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1477 Val_Loss: 40.4749  BEST VAL Loss: 40.3224\n",
            "\n",
            "Epoch 1579: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9916 Val_Loss: 41.6606  BEST VAL Loss: 40.3224\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9344 Val_Loss: 43.4297  BEST VAL Loss: 40.3224\n",
            "\n",
            "Epoch 1581: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1266 Val_Loss: 41.5478  BEST VAL Loss: 40.3224\n",
            "\n",
            "Epoch 1582: Validation loss decreased (40.322445 --> 39.801247).\n",
            "\t Train_Loss: 4.8825 Val_Loss: 39.8012  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1583: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0114 Val_Loss: 40.2029  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0549 Val_Loss: 40.5317  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1585: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9388 Val_Loss: 41.4374  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1586: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9090 Val_Loss: 41.0840  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1587: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9625 Val_Loss: 40.4729  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1588: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9493 Val_Loss: 41.5572  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1589: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0289 Val_Loss: 40.5272  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8544 Val_Loss: 40.4312  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1591: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8833 Val_Loss: 42.4571  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1592: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9488 Val_Loss: 41.9198  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1593: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8759 Val_Loss: 41.0929  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1594: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7888 Val_Loss: 41.1738  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1595: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9531 Val_Loss: 41.8317  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1596: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9705 Val_Loss: 45.3212  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1597: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0496 Val_Loss: 43.0171  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7699 Val_Loss: 41.3360  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1599: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0216 Val_Loss: 44.1932  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8979 Val_Loss: 44.6776  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1601: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8633 Val_Loss: 42.4006  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1602: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9193 Val_Loss: 41.9655  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8716 Val_Loss: 41.3073  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1604: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7463 Val_Loss: 41.5087  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8501 Val_Loss: 43.6217  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1606: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8190 Val_Loss: 43.5263  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1607: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7051 Val_Loss: 42.9105  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1608: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8641 Val_Loss: 45.8632  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7502 Val_Loss: 44.8769  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1610: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7143 Val_Loss: 42.4972  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7835 Val_Loss: 43.3242  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7629 Val_Loss: 43.0110  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6637 Val_Loss: 43.0445  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6902 Val_Loss: 44.5457  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6948 Val_Loss: 44.2969  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6357 Val_Loss: 44.2378  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1617: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6782 Val_Loss: 45.4518  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1618: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6829 Val_Loss: 43.5569  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6128 Val_Loss: 42.5719  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1620: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6465 Val_Loss: 44.1102  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6952 Val_Loss: 44.2695  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6145 Val_Loss: 45.2475  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1623: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5886 Val_Loss: 45.4478  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6247 Val_Loss: 43.7906  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1625: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6165 Val_Loss: 44.7236  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5610 Val_Loss: 44.4591  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5614 Val_Loss: 43.8545  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1628: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5774 Val_Loss: 43.5882  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5542 Val_Loss: 44.1386  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5484 Val_Loss: 44.1125  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1631: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5469 Val_Loss: 44.9198  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5545 Val_Loss: 43.4047  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5200 Val_Loss: 43.2082  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1634: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5196 Val_Loss: 45.1903  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5386 Val_Loss: 43.7746  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1636: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5878 Val_Loss: 46.1277  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6342 Val_Loss: 44.0341  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5318 Val_Loss: 44.3126  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4772 Val_Loss: 44.5083  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5016 Val_Loss: 43.6641  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5040 Val_Loss: 44.9273  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4940 Val_Loss: 44.0978  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4530 Val_Loss: 43.6118  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1644: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4662 Val_Loss: 44.5332  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4838 Val_Loss: 43.9745  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1646: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4936 Val_Loss: 45.3185  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1647: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5225 Val_Loss: 44.0378  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4807 Val_Loss: 43.8293  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4529 Val_Loss: 45.8203  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5097 Val_Loss: 43.5328  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4647 Val_Loss: 42.9378  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4712 Val_Loss: 42.5820  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4322 Val_Loss: 43.7716  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1654: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4235 Val_Loss: 44.5185  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1655: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4233 Val_Loss: 44.2782  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1656: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4112 Val_Loss: 44.8849  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1657: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4153 Val_Loss: 43.9042  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1658: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4065 Val_Loss: 44.8071  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1659: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4467 Val_Loss: 43.7545  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1660: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4336 Val_Loss: 43.6775  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1661: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3835 Val_Loss: 43.6824  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1662: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4818 Val_Loss: 45.9480  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1663: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8305 Val_Loss: 42.2287  BEST VAL Loss: 39.8012\n",
            "\n",
            "Epoch 1664: Validation loss decreased (39.801247 --> 38.845894).\n",
            "\t Train_Loss: 5.1202 Val_Loss: 38.8459  BEST VAL Loss: 38.8459\n",
            "\n",
            "Epoch 1665: Validation loss decreased (38.845894 --> 35.816998).\n",
            "\t Train_Loss: 5.1200 Val_Loss: 35.8170  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1666: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7948 Val_Loss: 37.7853  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1667: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6513 Val_Loss: 42.4644  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1668: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9925 Val_Loss: 37.4226  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1669: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8140 Val_Loss: 200.6806  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1670: Validation loss did not decrease\n",
            "\t Train_Loss: 50.4180 Val_Loss: 47.8116  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1671: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3501 Val_Loss: 43.2171  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1672: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8976 Val_Loss: 61.1994  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1673: Validation loss did not decrease\n",
            "\t Train_Loss: 20.6247 Val_Loss: 102.4303  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1674: Validation loss did not decrease\n",
            "\t Train_Loss: 26.5222 Val_Loss: 101.8650  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1675: Validation loss did not decrease\n",
            "\t Train_Loss: 32.7405 Val_Loss: 99.2372  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1676: Validation loss did not decrease\n",
            "\t Train_Loss: 36.6013 Val_Loss: 96.4298  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1677: Validation loss did not decrease\n",
            "\t Train_Loss: 37.0931 Val_Loss: 93.8104  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1678: Validation loss did not decrease\n",
            "\t Train_Loss: 34.0431 Val_Loss: 113.0639  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1679: Validation loss did not decrease\n",
            "\t Train_Loss: 37.1411 Val_Loss: 111.7395  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1680: Validation loss did not decrease\n",
            "\t Train_Loss: 33.7839 Val_Loss: 99.4556  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1681: Validation loss did not decrease\n",
            "\t Train_Loss: 31.7347 Val_Loss: 111.7555  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1682: Validation loss did not decrease\n",
            "\t Train_Loss: 34.0064 Val_Loss: 80.0218  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1683: Validation loss did not decrease\n",
            "\t Train_Loss: 29.3433 Val_Loss: 77.1507  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1684: Validation loss did not decrease\n",
            "\t Train_Loss: 25.0477 Val_Loss: 89.1802  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1685: Validation loss did not decrease\n",
            "\t Train_Loss: 27.2448 Val_Loss: 85.9434  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1686: Validation loss did not decrease\n",
            "\t Train_Loss: 25.3463 Val_Loss: 74.7164  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1687: Validation loss did not decrease\n",
            "\t Train_Loss: 23.8294 Val_Loss: 71.3755  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1688: Validation loss did not decrease\n",
            "\t Train_Loss: 26.2672 Val_Loss: 81.0722  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1689: Validation loss did not decrease\n",
            "\t Train_Loss: 23.4547 Val_Loss: 89.6992  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1690: Validation loss did not decrease\n",
            "\t Train_Loss: 25.4231 Val_Loss: 80.1687  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1691: Validation loss did not decrease\n",
            "\t Train_Loss: 23.1141 Val_Loss: 76.3997  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1692: Validation loss did not decrease\n",
            "\t Train_Loss: 22.5680 Val_Loss: 76.8178  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1693: Validation loss did not decrease\n",
            "\t Train_Loss: 20.8345 Val_Loss: 78.1616  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1694: Validation loss did not decrease\n",
            "\t Train_Loss: 18.3061 Val_Loss: 77.5755  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1695: Validation loss did not decrease\n",
            "\t Train_Loss: 18.4307 Val_Loss: 75.3380  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1696: Validation loss did not decrease\n",
            "\t Train_Loss: 16.7118 Val_Loss: 75.8866  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1697: Validation loss did not decrease\n",
            "\t Train_Loss: 16.6247 Val_Loss: 77.6479  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1698: Validation loss did not decrease\n",
            "\t Train_Loss: 15.8645 Val_Loss: 74.3619  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1699: Validation loss did not decrease\n",
            "\t Train_Loss: 15.6820 Val_Loss: 68.8817  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1700: Validation loss did not decrease\n",
            "\t Train_Loss: 16.6097 Val_Loss: 67.7821  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1701: Validation loss did not decrease\n",
            "\t Train_Loss: 15.8504 Val_Loss: 65.1324  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1702: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8181 Val_Loss: 61.5136  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1703: Validation loss did not decrease\n",
            "\t Train_Loss: 13.3725 Val_Loss: 63.1240  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1704: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0325 Val_Loss: 69.1294  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1705: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5208 Val_Loss: 66.3037  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1706: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6073 Val_Loss: 63.3211  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1707: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6074 Val_Loss: 62.8836  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1708: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6157 Val_Loss: 62.5469  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1709: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6481 Val_Loss: 61.7444  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1710: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2367 Val_Loss: 62.7700  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1711: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9823 Val_Loss: 63.8522  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1712: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8195 Val_Loss: 61.8790  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1713: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5664 Val_Loss: 59.2004  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1714: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4023 Val_Loss: 59.1183  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1715: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0366 Val_Loss: 58.6062  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1716: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8122 Val_Loss: 56.9527  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1717: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6497 Val_Loss: 56.7843  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1718: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5774 Val_Loss: 56.9744  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1719: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5443 Val_Loss: 55.5022  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1720: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3492 Val_Loss: 54.2529  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1721: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1883 Val_Loss: 54.4099  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1722: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9902 Val_Loss: 55.3158  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1723: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8355 Val_Loss: 56.5086  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1724: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7347 Val_Loss: 57.4356  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1725: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6784 Val_Loss: 57.5128  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1726: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5415 Val_Loss: 56.8325  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1727: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3946 Val_Loss: 55.8758  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1728: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3185 Val_Loss: 55.0475  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1729: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1987 Val_Loss: 54.8269  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1730: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1100 Val_Loss: 55.2610  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1731: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0211 Val_Loss: 55.4845  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1732: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9181 Val_Loss: 54.6793  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1733: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8302 Val_Loss: 53.5685  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1734: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7393 Val_Loss: 53.2157  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1735: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6497 Val_Loss: 53.3926  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1736: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5879 Val_Loss: 53.5366  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1737: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4976 Val_Loss: 53.3031  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1738: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4037 Val_Loss: 53.1537  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1739: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3453 Val_Loss: 53.1614  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1740: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2910 Val_Loss: 52.6969  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1741: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2324 Val_Loss: 52.3091  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1742: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1746 Val_Loss: 52.5397  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1743: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1093 Val_Loss: 52.6743  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1744: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0634 Val_Loss: 52.2275  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1745: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0199 Val_Loss: 51.8229  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1746: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9675 Val_Loss: 51.6040  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1747: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9177 Val_Loss: 51.2248  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1748: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8761 Val_Loss: 51.0072  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1749: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8354 Val_Loss: 51.1716  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1750: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7890 Val_Loss: 51.1029  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1751: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7500 Val_Loss: 50.9534  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1752: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7139 Val_Loss: 51.0062  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1753: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6762 Val_Loss: 50.8114  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1754: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6441 Val_Loss: 50.5242  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1755: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6050 Val_Loss: 50.4784  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1756: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5659 Val_Loss: 50.3091  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1757: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5370 Val_Loss: 50.0453  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1758: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5074 Val_Loss: 49.9371  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1759: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4744 Val_Loss: 49.7319  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1760: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4395 Val_Loss: 49.5464  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1761: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4017 Val_Loss: 49.6135  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1762: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3607 Val_Loss: 49.5566  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1763: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3168 Val_Loss: 49.2904  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1764: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2706 Val_Loss: 49.0163  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1765: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2273 Val_Loss: 49.1063  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1766: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1988 Val_Loss: 48.4591  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1767: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2299 Val_Loss: 50.0767  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1768: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2644 Val_Loss: 47.9037  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1769: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2304 Val_Loss: 49.8287  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1770: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1275 Val_Loss: 48.9247  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1771: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0278 Val_Loss: 49.0329  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1772: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9923 Val_Loss: 49.6855  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1773: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0046 Val_Loss: 47.9403  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1774: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0345 Val_Loss: 49.7998  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1775: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0409 Val_Loss: 47.8156  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1776: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9818 Val_Loss: 49.2544  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1777: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8966 Val_Loss: 48.0704  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1778: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8167 Val_Loss: 47.7449  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1779: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7931 Val_Loss: 48.4685  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1780: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8042 Val_Loss: 47.0444  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1781: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8266 Val_Loss: 48.9556  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1782: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8303 Val_Loss: 46.8065  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1783: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7687 Val_Loss: 48.0099  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1784: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6924 Val_Loss: 46.9495  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1785: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6186 Val_Loss: 47.2693  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1786: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5726 Val_Loss: 47.3175  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1787: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5508 Val_Loss: 46.5536  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1788: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5478 Val_Loss: 47.6405  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1789: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6036 Val_Loss: 45.9195  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1790: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8103 Val_Loss: 49.4960  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1791: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0187 Val_Loss: 45.6765  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1792: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9249 Val_Loss: 47.8161  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1793: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5087 Val_Loss: 47.0574  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1794: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4176 Val_Loss: 45.3111  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1795: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6641 Val_Loss: 48.1347  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1796: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6968 Val_Loss: 45.6287  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1797: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3803 Val_Loss: 45.1599  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1798: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3598 Val_Loss: 46.8639  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1799: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5670 Val_Loss: 44.6583  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1800: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4352 Val_Loss: 46.0016  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1801: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2526 Val_Loss: 45.9067  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1802: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2359 Val_Loss: 44.4057  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1803: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3620 Val_Loss: 46.9205  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1804: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4569 Val_Loss: 44.9248  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1805: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3204 Val_Loss: 46.4074  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1806: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1542 Val_Loss: 45.6238  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1807: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1001 Val_Loss: 44.3459  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1808: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2110 Val_Loss: 46.6930  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1809: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3397 Val_Loss: 44.5315  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1810: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2634 Val_Loss: 45.7631  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1811: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0730 Val_Loss: 44.7893  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1812: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0194 Val_Loss: 44.0837  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1813: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0576 Val_Loss: 46.0799  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1814: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1481 Val_Loss: 44.1552  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1815: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0671 Val_Loss: 44.4968  BEST VAL Loss: 35.8170\n",
            "\n",
            "Epoch 1816: Validation loss did not decrease\n",
            "Early stopped at epoch : 1816\n"
          ]
        }
      ],
      "source": [
        "LSTM_best_model, train_losses, val_losses = trainer(LSTM_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "rOoQ6lrfFYQo",
        "outputId": "ff8004af-60a5-4f8f-d1b6-3346a3685bd5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVxb1IHaYwmU"
      },
      "source": [
        "---\n",
        "### 6.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gGzInAaoDBWy"
      },
      "outputs": [],
      "source": [
        "val_predict_LSTM = LSTM_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "0plcD3u8DKqa",
        "outputId": "9942311f-336e-481c-a433-ff89a2c08007"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAAHQCAYAAADKyVH+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1hTZxsG8DsJe28ZKqDiABT3nlgVbbWtdjirdlu18+uwQ6sd9uvXXVu7ba122NZaba1774kLEQcqU2RvCMn5/gg5JhAyICEB7t91eUmSk/e8yTk5OXnO+z6PRBAEAURERERERERE1GJJrd0BIiIiIiIiIiKyLgaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIiIiIiIiIiIhaOAaIqMkKCwuDRCKBRCLBrl27rN2dJmP48OHi+/b9999buztkhNdff13cZrNmzbJ2dxrdrl27xNcfFhZm7e4QmWTWrFni/vv666/rXObq1aviMhKJpHE7aKKW/h2iuZ2uXr1q7e4QEZmFNc+1vv/+e3Hdw4cPb9R1U20MEJHFaZ4c1+dfSzwBJSJS0zxp0/ejtGaQoeY/qVQKDw8PhIeH4+6778ann36K/Pz8Rn0tRGSbLHXedfr0abz44osYOnQogoKC4OzsDAcHB/j4+KBr1664++67sWTJEmzevBllZWVaz6157DPXP81jqK5z1BkzZpj8OsePH1+rnf/85z8NffsAaAdldf1zcnJCq1at0K9fP8ydOxe7d+82qX1d78GBAwfq1T99r7mx1kNEDcMAEZGVcSQUtSS8+m49giCgqKgIV69exbp16/Dkk0+iTZs2+PLLL63dNbKClj4ykSwrPT0dEyZMQExMDN59913s3bsXmZmZKC8vh1wuR15eHs6ePYt169Zh0aJFiIuLg4+PD1atWmXtruPPP/9EcXGx0ctnZWVh06ZNFuyRfhUVFcjKysKRI0fw+eefY/jw4RgwYAAuXrxY7zZfffVVM/bQ+ushIuPZWbsD1LJ4e3ujb9++Jj0nJCTEQr0hImq++vTpAx8fH/G2IAjIzc3F2bNnUV5eDgAoLi7G448/jqysLLz22mvW6ioRNSOXL1/GsGHDkJaWpnV/YGAg2rVrB2dnZ+Tn5+PatWvIzs4WHy8vL0dmZqZ428fHB2PGjNG7rtzcXBw9elS8XfO4p4uzs7Pex0tKSvDHH39g5syZepdTW716NaqqqoxatqGCg4PRtWtXrfvKyspw9epVXL9+Xbzv0KFDGDJkCA4ePIjw8HCT17Nz505s374dI0eObHCfbWE9RGQ8BoioUXXr1s2qV1kIHKVETc7w4cMhCIK1u9HkvPvuuzrn8peWlmLZsmV49dVXIZfLAQCLFi3CmDFjTA7gk3mEhYU1mX2c3yGkj1KpxD333KMVHJo1axZeeOEFdOnSpdbyV65cwT///IM1a9Zg3759Wo8Zc864a9cujBgxQrxd13HPGGFhYeLI1pUrVxodIFq5ciUA1QjZtm3b4tq1a/VavzFGjRpV5xTAM2fOYP78+eIUsxs3buCJJ57Av//+W691vfLKK40SuGms9RCRcTjFjIiIqAVxcXHBCy+8gBUrVoj3CYKApUuXWrFXRNQc/Pbbb4iPjxdvv/3221ixYoXO4BAAtGvXDvPnz8fevXtx6tQpDBo0qJF6WluvXr3Efu7atQspKSkGn3PmzBnx9Q4ePNiqhRS6du2KzZs3o1evXuJ9mzZtwvnz541uo0OHDuLfhw8fxoYNG8zax8ZeDxGZjgEiIiKiFmjatGlaPyS2bdsmjigiIqqPdevWiX+3bdsWL774otHP7datGwYMGGCBXhlPnaBaqVQalQ/phx9+EP9+4IEHLNYvYzk6OuKVV17Rum/Hjh1GPz8mJgZ33323eHvhwoUWGd3YWOshItMxQEQtyrVr1/D2229j6NChaN26NRwdHeHr64vu3bvjP//5DxISEkxus6qqCmvWrMHMmTPRuXNn+Pj4wN7eHj4+PujTpw+eeOIJ/PPPP1AoFOJzNKsNaQ5FHjFihM4KFTWHS9dVivL8+fN48cUX0b17d/j7+0Mqlda6mlWfEsX5+fn47LPPMGHCBLRr1w7u7u5wdHREYGAghg8fjldffRXHjh0z9a2rpa5Sz1euXMFLL72Ebt26wdvbG25uboiMjMSzzz5rdBJGXcmRb968iQ8++ACDBw9G69atYW9vrzd58vr16zFz5kxERETAw8MDrq6uCA8Px6RJk7By5UqTcxCoT0Dj4uIQHBwMJycnhIaGYty4cfj111+19hlD6lOetD5JasvKyrBixQrcf//9iIiIgJeXFxwcHODv749Bgwbhueeew65du7RO9DT7pik8PFzn/l6zL/V5bXl5efjwww8xcuRItG7dGk5OTvD19UXXrl3x1FNP4ciRI0a1U9d7dOjQIcyaNQsdO3aEi4sLvL290adPHyxZsgQFBQVGtW0Lxo4dK/5dXFzcoMThdZWp3b9/P2bOnIlOnTrB1dUVvr6+6Nu3L9555x2jqqg15Hin6ciRI3juuefQo0cPBAQEiMewIUOGYOnSpVq5UIxRUVGB5cuXY9iwYQgICICzszPat2+Pe+65x+QpHfUtcx8fH4+XX34Z/fr1Q3BwMBwdHeHm5oaIiAhMmjQJy5cvx82bN7Weo/4OWLx4sXjfDz/8YFTFJ83nm/IdsmvXLjz++OOIjIyEt7c3nJ2dxWPd8uXLUVJSYlQ7uvpVWFiITz75BAMHDkSrVq3g5OSENm3aYPLkySb9MG6IzMxMvPnmm+jduzf8/f3h4uKCiIgIPPbYYzhx4oTe5w4aNEh8TaYEU8rKyuDl5SU+d82aNQ19GWaRlJQk/t23b19IpU3rp8b06dPFPv/44496l1UoFFi9ejUAwMnJCffee6/F+2eMwYMHa91OTk426flvvPGG+B7Ex8fjt99+M1vfrLEefXQViiksLMSnn36KQYMGITAwEI6OjggNDcXDDz+MS5cu1WpDqVTi119/xbhx49CqVSs4ODggMDAQd955J7Zu3Wpyn06cOIFnn30WMTEx8PPzg6OjI1q3bo3Y2Fi89957yMnJMbnNPXv2YMaMGQgPD4eTkxMCAwMxcOBAfPTRRw2qZioIAtatW4fZs2ejc+fO4vG9bdu2mDBhAr777jteeGqKBCILmzlzpgBAACAMGzbMbO2GhoaK7e7cuVPvsnK5XFiwYIHg6OgoPkfXP5lMJjzzzDNCVVWVUX3YsmWL0LFjR71t6nrtycnJRj2nrvdt586d4mOhoaGCIAjC0qVLBTs7u1rPVT+uNmzYMPGxFStWGHyNH330keDl5WVUPxctWmTU+1aXmu+LIAjCjz/+KDg7O9e5TicnJ+HTTz812Lbmc5KTk4WNGzcK/v7+OttMTk7Weu7ly5eFgQMHGnz9nTt3Fg4dOmTUa01LSxMGDRqkt73Y2Fjh5s2bwqJFi8T7Zs6cqbM9XfuEIca0q2n16tVCcHCwUfuCZnuafTP1ufV5batWrRJ8fX0NrmfatGlCcXGxSe9RZWWl8PTTT+ttNzAwUDh9+rTBfhqr5vtXc/9Uq/n5MXRcFARB+PLLL7Wec/DgwXr3c8WKFVrHLLlcbvC9Cg4OFnbt2qW33YYc7wRBELKysoRJkyYZ3B+8vLyEH374wajXmpCQIERFReltb/LkyUJxcbHWd2Bdx0hdxz59srKyhHvuuUeQSCQGX5eDg4OQmJgoPlfzO8CYfzX3N1O+Q27evCnccccdBtcREhIi/PPPPwZfd81+HT16VAgLC9Pb9rx58wSlUmmwbWPV7MPmzZsFHx+fOtcvlUqFBQsW1NmH77//XuvYIZfLjerHypUrxef5+fkJFRUVZntdxpwb1CUiIkJsJy4urkF9MkbN46Mxxz1Nmp/PSZMmCYIgCLGxseJ9R44cqfO5GzduFJe7//77BUHQ/nw899xz9X5dmjTbNOa7Wi6Xa70nDz/8sN7ldb0HU6dOFe/r3Lmz3vNiY19zY63HFDV/S8THxwvt27ev8/Ps7OystY/dvHlTGDp0qN5j0Msvv2xUX0pLS4VZs2YZPK57e3sb/RmVy+XCo48+qre90NBQ4fjx4yafax07dkzo0aOHweN7RESEcOzYMb1t1Tx/IOtikmpq9srLy3HPPffgn3/+Ee+TSqWIjIyEv78/iouLcfr0aVRUVEChUODDDz9ESkoK1qxZo/dK7tdff405c+ZojfJwcXFB586d4eXlhcLCQiQmJoqlUjUj9M7OzmJljt27d4sVheqqvtGtWze9r/F///sfFixYAEA1vDg6Ohru7u5ISUkxaRSKJqVSiYceeqjWFWI/Pz+0b98eLi4uyM7ORmJionh1oCFXIXT5+++/xeHeMpkMXbt2haenJ5KTk8VqHeXl5Zg/fz4UCgWeeuopo9o9cOAAZs6ciaqqKkgkEnTp0gWtWrVCdnZ2rVFkFy5cQGxsLNLT08X71COYHBwccP78efFqTmJiIkaOHIm///5bb5LM3NxcjBo1SmtdDg4O6Nq1K1xdXZGUlITMzEzs2LEDEyZMQGxsrFGvy5IWLlyIN954Q+s+T09PcTRVXl4ezp8/L+7LmvuCZiWazZs3i/cPHTpUZzWZmhVaTPHJJ5/U2g/atGmDdu3aobCwEGfOnBFHeq1evRpXrlzB5s2b4e7ublT7c+bMwbfffgsA8PX1RadOnSCTyXD27Fnk5eUBUI0miIuLw/nz5+Hh4VHv19IYKisrtW47ODiYre0FCxbgo48+AqD6zERFRcHOzg7nz59Hbm4uAFUp7HHjxmHr1q0YOHCgUe2acrxLTk7G6NGjta76Ojs7IyoqCh4eHrhx4wYSEhIgCALy8/Mxc+ZMFBQUYP78+XWuPzk5GSNHjkRGRoZ4n6urK6KiomBvby++vl9++QVKpdJgxSRTXbp0CWPGjMGVK1e07u/YsSOCgoJQVVWF69evi/lTKisrUVZWJi7Xt29fODk54dKlS7h8+TIA3ZWR1Orb/xs3biA2NlbrOKfeXq6urrh48aL4HqalpeHOO+/Ejz/+iMmTJxvVfkJCAiZPnoyioiJIJBJERUXB398fN2/exLlz58RRjMuWLUNoaCj+85//1Ot16HPixAlMmTIFlZWVkEgk4nlFamqquM8plUosXboUZWVl+PDDD2u1cd999+Hpp59Gfn4+MjMz8ffff+Ouu+4yuO5vvvlG/HvGjBlm/ew2hK+vrziy9/jx4yguLoabm5uVe2WamTNniqPPVq5ciT59+uhcTnN6mbEJrRtDzREmxn6/aVq8eDHWrFmDqqoqJCYm4scffzR6pLEtrscYKSkpuPfee5GdnQ2pVIro6Gj4+vri+vXr4rGyrKwM48ePx8mTJxEcHIxRo0aJOajCw8MRGhqKgoICnDp1CkqlEoAqD1d0dDSmTJlS57pLS0sRFxeHvXv3ivfJZDJER0fD29sbV69eFUdN5uXlYfbs2cjOztZ7XBMEAQ888AB+/vlnrfsjIyMREBCAtLQ0XLx4EdeuXcOoUaPw8ccfG/1ebdq0Cffcc4/W6E8/Pz9ERETA0dERycnJ4gyJixcvYsSIEdi8ebPVp5CSkawcoKIWwNojiB577DFxOQcHB2Hx4sVCTk6O1jLFxcXCG2+8IchkMnHZjz76qM42t2/fLkilUq2rnz/++KNQVlamtZxCoRAOHjwoPPHEE0L//v0b9Do0aUb5nZ2dBTs7O8HOzk548803haKiIq1lL126pHXb2Ku/miMnAAj9+vUTdu3aJSgUCq3lysrKhL/++kuYMGGC8PTTTxvV/7rUvIru5+cnABCmTJkiZGRk1HoP2rVrJy5rZ2cnnDp1qs62Ndt1d3cX271+/brWcunp6UJpaakgCIJQWVkpdO/eXWv/+e9//yuUlJSIy8vlcuGHH34QPD09xeVatWol3Lx5s86+TJ8+Xas/8+bNE3Jzc8XHFQqFsHbtWiEgIEDrfYCeq4eWHEGkeWUH1Vf51q9fX+tKd2VlpbB9+3Zh+vTp4tXBmjTbqWskTH1f28GDB7U+wxEREbVGp2RlZQkPPvigVj8efPDBOtvUfI/Uo5Jat24trFu3TuuzIJfLhXfeeUfryt+rr75q1OszxJIjiObNm6f1nJSUlHr3U3M/8fHxESQSiWBnZye8/fbbWp+ZyspK4euvvxZcXV3F5cPCwrSW0VTf4115ebkQExMjPjcoKEj48ccfa420SElJESZPniwuZ29vLxw9elRnX5RKpdbVYplMJixZskRrJJr69bm5udX6/DZ0BFFJSYkQGRkpLieVSoWnnnpKSE1NrbVsamqq8NFHHwnt27cXTp48WetxU0cQqhn7HXL77beLy0kkEuE///mPkJeXJz6uVCqFDRs2aI1KdHZ2Fi5cuFBnm5rvkfrz+NBDDwnp6elay50/f17o2rWruKyrq6tQUFBg9GvUR9d31G233Vbru/bkyZNCz549tZava5SU5ufwjjvuMNiHpKQkrXbPnTtn1tfVkBFETz31lFZb9957b52fbXOwxAii4uJi8fjk5+cnVFZW1npefn6+4OTkJACqkV/qkS+2MIJozZo1Wu/Jt99+q3d5Xe+BIAjCww8/rHWM1vU+1OyfqSOILLEeU2ieg6tHAk6bNq3WMWXbtm1a53kPPPCAMHfuXAGA0Lt371ojZC5duqR1DGrTpk2t82dNTzzxhNY2mzFjhpCZmam1zL59+4TOnTtrHVd3795dZ5vffPONVpvDhw8XkpKStJaJj48Xj1Oa31X6zrUuXrwofr8BEPr27Svs2rWr1ijJw4cPa40wCg0NFfLz83W2yRFEtoUBIrI4awaIduzYIS7j6OhocCrDqlWrxOU9PT1r/fgQBEGoqKgQWrduLS7XsWNHIS0tzWB/dbVl7OuoSdeUnVWrVhn1XGNO7k+fPq0VALv77rvr/MLWVNdrNJauqXczZsyoc/mUlBQhMDBQXDY2NrbOZWu2+8gjjxjsz8cff6z1nJ9//rnOZffv3y84ODiIyz7++OM6lzty5IhWm//5z3/qbDM+Pl7rR7S+k0NLBYiysrK0+jBw4ECjfmjVtS9ovhZzB4hqnojUPLnSpD6xU/+ra2pgzUBpQECAcO3atTrbnT9/vtYJoTlYKkBUUVEhhISEiMuHhIQ0qJ81A4kAhG+++abO5Tdv3qx1nFmyZInO5ep7vFu4cKG4fHh4eK0T/poeeeQRg8eS3377Tasfy5Ytq7O9LVu2aL0+oOEBoueff15cRiqVCr/++qve1yQIquBlzYsXgmDZANFff/2l9XqWLl1aZ3sXLlzQmqKlb1pSzf3gpZdeqnPZ69evCy4uLuKyhn4kG6tmH0aMGFHn92NBQYFWQK9Dhw46p5qdOXNGXEYmkxk8p3jppZfE5QcMGGD219WQANGZM2dqTZHx8/MT5s+fL2zYsEHIysoyS3/VLBEgEgRBmDFjhnj/unXraj3vq6++Eh9/9tlnxfutHSAqKyurdWHrxo0bep9T13tw/fp1rdQMn3/+ucH+1SdAZO71mELzHBzQPx3vhx9+0PqcSqVSoXv37nWe7yQlJWlNhd6xY4fO5eLj47U+M4899lidfcjKyhLCw8PFZbt06aJzubKyMq2Az6BBg4Ty8nKdy9Y8Thk619K8SDJ+/Hi9vw+Ki4u19se6vucZILItDBCRxWl+IZj6T98BypjASlxcnMGDUk1jx44Vn/PFF1/Uevzbb7/V+oI4fvy4Ue3WxRwBottvv93o9Rlzcq95YtS2bVuhsLDQ6PYbouaPJF9fX60rzrpofmEDqHV1RE1zmVatWhnMPaNUKoVOnTqJz7n77rsN9v+FF14Ql3d1ddV5paTmlbK6vrDVFi9erNX3xg4Qvfbaa+Iy7u7utUZcmUrztZgzQHTgwAGttv/66y+9bZaVlWl99qZPn65zuZoBopUrV+pt9/Lly1rLN/T9EgTLBYhqjh6aP39+g/pZM0CkL2CrNnv2bHH5uq6w1ud4V1JSohV02LNnj1HP0cxdpZm3R+22224THx84cKBJrw9oWIAoPz9fHP0IoMEjNi0ZIBo9erS4TK9evQzmAPr888/F5SUSiVHH8Y4dOxrM1/PAAw+Iy+sbKWgKzT7Y29vXGjlU0+7du7Wes2XLFp3LDRgwQFzmrbfeqrM9uVwuBAUFictaIvDVkACRIGgHZ3X9CwsLE+6//37hs88+q3NbG8tSAaKtW7eK90+cOLHW8wYPHiw+Hh8fL95vzQDRqVOnhCFDhmi9HwsWLDDYfl3vgSAIwpNPPik+FhwcrDPY3NAAkbnXYwrN8wBfX1+954YVFRWCh4eH1vtrKO+k5rHwjTfe0LmMZo6gkJAQgyPuNHNfARC2bdtWaxnNC94ymUxISEjQ22bN41Rd51qHDh3Ser8MnaPXfE5wcLDO7wMGiGxL0yotQGSCmzdvivlO7O3tMXfuXKOeN23aNPFvXRVQNOfyjhs3Dj179mxgTxvu0UcfNVtbcrkcf/zxh3j7qaeeqtf8dXOYPn06vLy89C4zZcoUrbxNmiV26zJ16lS4urrqXSYxMREXLlwQbxuT3+jJJ58UK3KUlJRg27ZttZb566+/xL8ffvhhODo66m3z8ccfh0wmM7huS9Hc32fNmoU2bdpYrS/6aG738PBwTJgwQe/yTk5OePzxx8Xb69evF/MF1MXDw8NgfpR27dohODhYvJ2YmKh3+cYkCAJyc3OxadMmjB49GsuWLRMf8/DwwEsvvWTW9enL46M2b9488e+UlBQcP37c4HOMOd5t3LhRzHPUs2dPDBkyxOBzXFxctMou1zz+FxUVad33xBNPGGxT8/U11N9//42ioiIAqu80c28vcykuLtY69s2fP99gZbbZs2fD09MTgGo/Xb9+vcH1PPjgg7Cz059KU7OakyU+i+PGjUP79u31LjN06FCt/E51fUdp7tffffddnSW/N27cKOZucnd3x/33329iry1v8eLF+OSTT+rMPXT16lX8+uuvmDt3Ljp27IhBgwZp5aezBbGxsWjdujUA1WdPfTwBVJVV9+3bB0BVrj0mJqbR+rV161bExcVp/Rs+fDjCwsIQExOjlcNm+vTptXIHmurll1+Gi4sLAFXOuM8++6xB7Vl7PfpMnjxZ77mhg4OD1raOiopCv3799Lap+fj58+d1LqN5THjkkUfE96EuY8eORadOnXQ+X9d9I0aMQJcuXfS2WfM4VRfNyn4zZ840eI4OqN6DDh06AFBtW1s6LyLdGCCiRuXt7Y0xY8YY/W/YsGH1Xte+ffvEE6yYmBidyZ91iY6OFv+uWZ62qqoKBw8eFG9PmjSp3v0zp5olTRvi+PHjKC0tFW9b8zXGxcUZXMbe3h633XabePvo0aMGn2PM+3X48GHxb1dXV6N+YIaEhKBHjx462wBUJ8WaJafViZv1CQgIQK9evQwuZwmZmZlayX1tZX/XRfO9Nma/AYA77rhD/FudVF6fXr16wd7e3mC7ISEh4t/mTtxuihEjRmiVBZdKpfD19cXYsWO1Su86Ozvjjz/+0ApsNZRUKsWoUaMMLtezZ08EBASIt831+dX8kWRKknd9x//jx49rBRGN+fzWfH0NofmaBg0ahFatWpmlXXM7duyY1vs0duxYg89xcnLSOo7XPHbqYkyyU0t/Fo091mi+B3Xt4/fff78YJLt8+TJ2796tczl1knzA8A9aa5o/fz6uXLmCxYsXa/2Y1eXAgQOIi4vDQw89VCtxvrVIpVJMnz4dgCrR+y+//CI+tnLlSvHvBx54oFH7lZ6ejs2bN2v92717t5gQGFB9V61fvx4//vhjgy8wtWrVSivY/84774jFV8ypsdajj6FgDwAEBgaKf/fv39+k5XUdg65evYqsrCzxtjHHS0D7/EXX8VLzOGPMd5Wx67bEdyvZHlYxo0bVrVs3bNq0qVHWdfbsWfHv69evG30ip1ntJTs7W+uxlJQUrYz91vrhrsnLy8vo4JcxNK9w+Pr6IjQ01Gxtm0rzC0WfqKgo8W919RR9DF3xBaAVGImKihJHBhnStWtXcRSEZhu6bmv2W5+oqCgcOXLEqGXNqebVLlvY3+ui+d4aWwWtc+fOsLOzE6uaXbp0CZGRkXUur3mip4/m1T/NYKstGjlyJD799FODVxdNFR4ebvQP16ioKPEE2dDn19jjnebx/++//8aZM2eM6ktaWpr4d83jv+Y+1qpVK/j5+RnVpubrawjNz2NT+SwGBAQYHSDr2rWrOHq15rFSF2M+j5b+LJrzO8rZ2RkzZswQR/Z98803taphZmRkYOPGjeLthx9+2MQeNy5/f38sXLgQCxcuRGpqKvbu3YujR4+K/yoqKrSW/+677wBoB8Gs6YEHHsA777wDQBUUeuKJJyAIgjiKQiaTaY06txXnz5/XChg11AsvvIDly5ejsLAQ2dnZ+Oijj/Dqq6+arf3GXk9djAm6ax5TjDm2GToG1TzWGXv+orlczTbkcrnW9q/PcUoXQRBw7tw58fbbb7+NTz/91Ki2Nb+Da363ku1hgIiaLc0yn1lZWfUavlxQUKB1W3OIMaA6+bE2c0//0nyN1n59vr6+Ji9nzFViY94zzXaM7QcArR+N6rLnum67uLgYXT7alPWbk+a+4OTkZNOliuuzvezs7ODl5SWerNTcXjXVp4x0XdNEGkOfPn20gilSqRRubm7w9fVF9+7dERsbi4iICIus25R91pTPr7HHO83jf2JiYr2GtNc8/mvuH/V9fQ1hS8dmfSxx7NTF1M+jJT6L9fmOKiwshCAIOqfdPfroo2KA6I8//sCyZcu0pnD88MMPYkC7a9eu6Nu3bwN637hat26NKVOmiKW+y8vL8ffff2Pp0qVaIwq+++47zJ4926wjo+urS5cu6N27N44dO4bDhw8jKSkJN27cwJUrVwAAo0ePbvSRfDNnzsT3338v3pbL5UhJScHBgwfx7rvv4vTp0ygtLRVH45hjmquPjw+ee+45LFq0CADw3nvvYe7cufD29m5w29ZYT11MPaaY4xikebx0dnY2+rxQ83hZUFCgdUyp+T1an+OULgUFBVAoFOLtAwcOGNWurnbItnGKGTVbmiN96qvmwbzm1S5D+WMag7EjW4yl+Rqt/fqM/fLV7GfNbaSLMe+ZZjumnARoLluzL5pD501p01rbwZb2BUMssb2aunfffRebNm0S/23cuBFr1qzB8uXL8dhjj1ksOATUf/82tA2MPd6Z4/hfMyeVtT+/TeXz2JI+i/X5jlIqlZDL5TqX69q1qzhtpby8HKtXr9Z6XD3CBrD90UOGODk54Z577sHhw4drTdP68ssvrdSr2mbOnCn+vXLlSvzwww86H7MWe3t7tGvXDtOmTcPRo0cxbtw48bHnnnsOp06dMst6nnnmGTGAUFBQgP/9739madda67EV5jhe1jym1JymWZ/jlC7m+F4Fan+3ku1hgIiaLfVcfgC4/fbbIaiq9pn8T1PNZGzNMQqu+Rqt/frUCVlNWc7Dw8Ms69bcf4ztR81la+4vmn0zZW69Kes3heaVIF00+19UVGTV0TCGWGJ7Uf3VdxtY4vP7v//9r17H/l27dmm1qdm3+r6+hrClY7M+LemzWJ/vKEdHR70/2DSTVWtOtdq9e7c4Pc3R0VHMj9PU2dnZ4bPPPtPa5vv377deh2qYMmWKmHtu5cqV+O233wCo9vM777zTml2rxcHBAatXr0ZQUBAAVaDAmGT6xnB3d8eLL74o3v7kk0/MMnXWWuuxFZrHy/qeFzo7O2sdU2p+j9bnOKWLZl8BVZ6j+ny3vv7660b1h6yHASJqtjSH/Zrry6VmzgNj8t00NZqvMTU1FeXl5VbrS3JyssnLmSshrOYUDmP7AaiSi+pqA9DuW1VVFVJTU41q05j1a54c1HV1uiZD03k09wWlUqn12mxNfbbXzZs3tU6IbHnaTlNz9epVo5e1xOfXEsd/zb6lpqaKU30MMeX4oY/m59GWv3s0P0emvE/6jp22yhLfUZrJqk+ePImTJ08C0A4WTZw40ay5B63Nzc0NgwYNEm+rq7TZAl9fX3FUTkpKCgoLCwEA9957L5ycnKzZNZ28vLywdOlS8faBAwe0qqc2xLx588TgU0lJCd5++22ztGut9dgCzWOdQqHA9evXjXqevuOlu7u71lQ1Y7+PDR3P3NzctHIqNefAXUvHABE1W5rVBU6dOmWWQIePj4/WtIw9e/Y0uE3NKRO2MEJD832rqqqq9xxjczA2MbPmcj179jTLujXbqVlloi4KhQLHjh2rsy9du3bVqihizOsTBEGrzbpo5mbJz883al/STOSrS9euXbVOBsyxv2vm3TDn/q75Xhu732hW/pBIJFoV6KhhCgoKcOHCBYPLFRUVaeUHMtfnV/M4dujQIbO0qbl/VFRU4PTp0wafU/P1NYTma9q7d2+DPz+W+u7R3IaVlZWIj4836nman0dz7QeWZonvKBcXF63RQd988w0KCgrw+++/i/c19ellumh+hxlTLbIx6apU1tjVy0wxffp0rcpxixYtMstn3NnZGa+88op4+4svvjD6QpctrscWdOvWDXZ2t1IC1+f8RdcxpXv37ia3acxylvhuJdvDABE1W/369ROvwlVWVuLnn382S7uapZt/+OEHo0dr1EWz0o9mBTVrCQ4O1qrk9PXXX1utL7/++qvBZZKTk7W+1MyV2LJv377iqBxBEIzqy9atW7UCSUOGDNF63MXFRav60Jo1awy2uXv3bqOuprZp00b8u7S01OBon5s3b+LgwYN6l7G3t9eqomOOfcFS+7vme71161ajqmSsWrVK/Ds6OrrJTGtpKoz5zPzxxx/iMVQmkxlVutwYmmV9Dxw4YFRVLEMiIiK0RiYZ8/nVfH0Npfndc/36dWzZsqVB7VnqsxgREaE12smY797ExESx+iNQ+9hpq3777TeD+TSKi4vxzz//iLeN+Y7SnGb2008/4dtvvxW3Ubt27TBixIh69th2JSUliX8HBwdbsSe13XHHHVojtsLDw20iiXZdZDIZXn75ZfH2qVOnsH79erO0/cgjj4jVbSsqKvDGG2+YpV1rrcfanJ2dtc4LjTle5uXlaR1TdB0vNe9bu3atwZGcNY9TddH8bl29erXBVAXUNDFARM2Wg4MD5s6dK95+9dVXcePGjQa3O3fuXHEUxPXr17FkyZIGtad5Im2OHzHmoFn14tdff8W2bdus0o+dO3caXPerr74qXhnz8fHB+PHjzbJuT09PTJo0Sby9dOlScWi5LlVVVVonZN27d9d5VWfGjBni37/99pveq+uCIOC1114zqr9eXl4IDw/XalufJUuWGJUIVnNfOHToUIPLD1tqf588ebI42qmystLgHPejR49qvUcPPfSQ2fpCKh9++CFu3rxZ5+Pl5eVaJ/1xcXFmqwjUt29fDBw4EIBqZN/cuXMbnBhTIpFojez47LPPkJ6eXufyNV9fQ/Xp00eratVTTz3VoNLtlvzumT17tvj3F198YXDaxAsvvCD+HRAQgDvuuMOs/bGUy5cvayWO1mXp0qVibhE7Ozujcgd169YN/fr1A6AaEao5muLBBx/UWQHNVvz5558m5VIBgIMHD2pVMrO1AJiDgwNycnLEHCpXrlyx6W0AAFOnTkW7du3E22+++aZZ2nVwcMDChQvF2999951WSXVzaaz12IIHH3xQ/PvPP/80OJJn4cKF4qwIBwcHnccUzXPNtLQ0LF++XG+bmscpfR5++GGxou2VK1e0pjNS88EAETVrzz77LEJCQgAA6enpGD58uMFpNYDqh/B9992HrVu31nosMjJSq3LFm2++ibfeektvFD09PR2fffaZzsc0gwgrVqywieSjs2fPRpcuXQCoghQTJ040eGXh+PHj+OOPP8zel6lTp9a5zd5991389NNP4u1nnnnGrNV9XnjhBXHob0ZGBiZOnKgzSFRZWYnZs2eLuSIAVeBKl5kzZ4pz65VKJSZOnKjzpEehUODJJ5/Evn37jO7v3XffLf797rvval2R1fTJJ5/UuT/WNHbsWK2T9Tlz5hj8QXTx4kWtMryaNPf35cuXm61akZeXl1Yyzs8++6zO15iUlISJEyeKAYPg4GCtH7RkHvn5+bjrrrt0liwvLy/H1KlTxXLREolEKzGpObz77rvi53fLli2YOHGiVql4XSorK7F27Vr0799f57TkJ598UgxEFhcX46677kJOTk6t5crLyzFt2jTx9ZnLO++8I04Nu3DhAkaPHq13hKFcLseKFSt05qDQ/CzGx8dj586dZuvnvHnzxBG8paWluOOOO5CZmVlrOUEQsGDBAmzYsEG874UXXjC5fLQ1Pfnkk7USmqv9/PPPeOedd8TbM2fOrJXLsC6ao4jU+6JMJrP5Y9X777+P8PBwLFmyxKgf9AcOHMDEiRPF21KplAF7M7Czs8NLL70k3j527Bj+/fdfs7Q9c+ZMdOzYEYDq4pgpOedscT3WNn36dHG0lCAImDRpUp3nb59//jmWLVsm3n700Ud15jWLjo7G7bffLt5+/vnndf6mAWofp/Tx8fHROr9duHAhFi9ebHCkbH5+Pj755BPcf//9Rq2HrMvO8CJE5nP69GnExcWZ9JyBAwdqXUUwha+vL/744w+MGDECZWVlSExMRExMDO644w7ExcWhXbt2cHV1RWFhIVJSUnDixAls3rxZPKnRjOprWrZsGY4cOYKEhAQAqmDAqlWrMG3aNHTv3h1eXl4oLCzEuXPnsH37dmzfvh1RUVFaI5rUpkyZIlbZiY+PR0hICHr27Alvb2/xClV0dLTZrv4Yw8nJCb/++isGDhyI4uJiFBUV4Y477kBsbCwmTpyIiIgIODs74+bNmzh58iT++ecfnDx5Ek899ZTWqJuGuu+++7BmzRr06dMHDz/8MEaNGgVPT08kJydj5cqVWj9ooqOjta5Cm0P37t3x2muvYdGiRQAgbsfHH38cvXv3hr29Pc6cOYMvv/wS58+fF583ZcqUOt8Hd3d3LFu2THw8OTkZ3bp1w+OPP46hQ4fC1dUViYmJ+Oabb3D8+HE4OjoiLi7OqCSTc+fOxeeff47y8nLk5+ejX79+ePrppzFw4EDY2dkhKSkJq1atwr59++Di4oIxY8bgzz//NNju6tWr0atXL2RkZEAul+Ohhx7C8uXLcf/99yMqKgru7u7Izc3F6dOnsXnzZuzfvx8TJkzArFmzarU1depUcWrOpk2bEBQUhO7du2tV3YiNjcWTTz5psF81LVmyBBs3bhQ/l/PmzcOff/6J6dOnIzw8HIWFhdixYwe++uorceSFVCrFt99+a7bqWaTSs2dPFBQU4MCBA4iOjsacOXPQp08f2NnZ4fTp0/jiiy+0ToAfeeQRs08rGjRoEN5//3089dRTAIC//voLoaGhmDx5MoYNG4bg4GDY2dkhPz8fFy9exLFjx7Bp0ya9ydvbtm2LN954A8899xwA1Ug09evr169frdfn4+ODnj17mm0U5ogRI/Daa69h8eLFAFTVniIiIjB16lTExsYiKCgIVVVVuH79Og4cOIB169YhOztbK3it1qVLF3Tv3h3x8fEQBAGxsbHo1q0b2rRpo5UT46uvvjI5eXhwcDA++eQT8WLKmTNnEBUVhcceewyDBw+Gi4sLkpKS8N1332nl0hg8eDCeeeaZ+rw1VqH+jho5ciRmzJiB8ePHw9/fH2lpafjtt9+0jq9BQUF49913jW578uTJePbZZ7UuGo0dO7ZRpl898sgjePzxx41efujQoVpTHrOzs7Fo0SK8/vrr6N+/PwYNGoTu3bvD398frq6uYo6yjRs3Yvv27Vr5cZ5++mn07t3brK+npZo5cybeeOMNpKSkAADeeOMNjB07tsHtymQyLF68GFOmTGlwW7awHmtzcXHBd999h9GjR0OhUCA1NRXdu3fHww8/jJEjR8LLywvXrl3DqlWrtII8ERERegM7y5Ytw969e1FYWIiKigrExcVh6tSpuPPOOxEQEFDrOHX//fcbNTX8hRdewJEjR7B27VqxKtm3336LKVOmoF+/fvDz80NVVRVyc3Nx9uxZHDx4EDt37oRcLhdHRpKNE4gsbObMmQKAev+78847dbYbGhoqLrNz5069fTh69KgQEhJi8rr//fffOtvMzs4WBg4caHRbMTExdbb1yiuv6H3usGHDtJbfuXOn+FhoaKje117TsGHDxOeuWLFC77LHjx8XAgMDjX6NTz31lEl9qSk5OVmrvby8PCE6OtrgesPDw4XU1FS9bWsun5ycbFK/nnvuOaPfg4kTJwoVFRUG23zvvfcMtiWVSoWvvvpKWLRokXjfzJkz9bb7+eefG2zX0dFR+OOPP0xq98qVK0KnTp0a/LkVBEGYPn263ufW7Isp+3t6erpR+wwAwd7eXvj555/1tmfKe6RmymfMGJqvX9/+W/PzY+i4aG4rVqzQOmYdPXpU8PLyMrgdbr/9dqGysrLOdhtyvFP3y9HR0eh9V/2vrKyszjbnz59v1Ofs77//1voOXLRokc72am47Q9566y1BIpEY/VpOnjypsx1jtlHN/c2U/fvjjz82up+DBg0S8vPz9bZnzOdAU0P3HUN9SEpKEkaMGGHwtfn6+gqnT582eV1PPPGEVjvr1q0zy2vQxdTPh+Y/zXOUoUOH1qsNqVQq/Oc//xGUSqVJ/a55fDT1uKf5+Zw0aZJJz9VF8/Px3HPPNbi9mm0a+z2ktmzZMq33Z9u2bbWWqc97oFQqhW7dutXajvpec2OtxxSm/JYQBMGo47mmmt+L+vz++++Cg4ODUZ+Xzp07CykpKQbXv2fPHsHFxcVgezNmzDDpeCmXy4W5c+ea/Dnv169fg98nsjxOMaMWoXfv3khISMCSJUsMDu/29vbGfffdhw0bNmglBa3J19cXu3fvxhdffKGV+6UmqVSKAQMGaOWnqenNN9/Ejh07xMoTbm5uNjG/vWfPnkhISMALL7ygN4Gvk5MT7r77bqNyK5jCy8sLBw8exIMPPqhz6pidnR1mzZqF48ePi1MJLeG9997Dxo0btapC1BQWFobvv/8ev//+u1HTI5577jls3LgR7du31/l4REQE/vnnHzzyyCMm9XXOnDn46aef6tzPe/bsiX379mkN6TdGeHg4Tp48iaVLl+r9DNnZ2WHUqFE6R8up/fjjj1i7di3uuececRSfufb3oKAgHD58GIsWLYK3t7fOZaRSKcaOHYsTJ05g8uTJZlkv1da7d28cPXpUK9G5Jk9PT7zzzjv466+/LFq1aNasWTh//jweeughrcTMuoSFhWHevHk4evSo3hLWn3zyCX744QeDnzPNIf7m9PLLL+PIkSMYM2aMVmXEmkJCQvDiiy/WeZzp3bs3zp49i1deeQX9+/eHj4+P1uihhnryySdx4MABvaPDWrVqhffffx87d+4Up6U1Ffb29ti8eTOef/55MS+HJolEggkTJiA+Ph5du3Y1uf2YmBjx76CgIIvtT+a0ceNGrFmzBjNmzEDbtm0NLu/i4oLJkyfj8OHD+N///mcT5z7NyUMPPSROawfQ4LyZahKJpFESRzfWemzBpEmTEB8fjzvuuKPO47qnpydeeeUVHDt2DK1btzbY5pAhQ3DixIk6v4d9fX3x7rvvYuXKlSb11c7OThyhNHr0aL3fQ+oqsW+88YbB/JhkGySCYAN1tYka2enTp3Hq1CncvHkTpaWlcHNzQ0hICDp37oyoqCit8r/GSkhIwPHjx5GVlYXy8nJ4enqiffv26NOnD/z8/CzwKhqXQqHAoUOHkJiYKCae9fHxQefOndGnTx84Ozs3eB1Xr17VCrZpHp5yc3Oxc+dOpKSkQC6Xo02bNrjtttsa/b29fPkyDh48iBs3bkChUMDf3x89e/bUOpE3hSAIOHjwIM6cOYPc3Fy0atUKkZGRWqVE60Mul2Pv3r04d+4ciouLERQUhB49etS7nzX7fOLECZw5cwY3b95EVVUVvLy80LFjR/Tp08dmpmtVVVXhwIEDSExMRE5ODlxcXBASEoJhw4bB39/f2t1rdr7//nsxP8qwYcO08rJcunQJhw8fRnp6OhwdHdG+fXuMHDlSbxDGEiorK3H48GEkJSUhJycHCoUCHh4eCA0NRXR0NMLCwkxqT6FQYPfu3Th//jyKiorEz1m3bt0s8wJ0yMvLw549e5Camoq8vDw4OzsjJCQE3bp106pIaW1paWnYu3cvMjIyUFFRAX9/f0RFRaFv3771+s61NSUlJdi+fTuuX7+OkpIS8VijWWHSVCNGjBA/Ry+99FKTTAibmpqK8+fPIzk5Gfn5+aisrISbmxt8fHwQGRmJrl27mjV3IFFzkJOTg127diEtLQ0lJSXw9fVFx44dMWjQoHpfULl48SL279+PzMxMeHp6Ijw8HLGxsWbJ+VZQUIB9+/YhJSUFubm5sLOzg5eXFzp06IBu3bo1i99BLQkDRERkM/QFiIjItukLEBGRaZKSktCpUycAqivwFy9erHMkGBERkbk0/Us2RERERETNiGZC69GjRzM4REREjYIBIiIiIiIiG7F27VqsWLFCvG3uCp1ERER1YZl7IiIiIiIrOXv2LF599VUolUokJyfj7Nmz4mNxcXGIjY21Yu+IiKglYYCIiIiIiMhKsrOz8ddff9W6v02bNvjmm2+s0CMiImqpOMWMiIiIiMgG2NnZISwsDPPmzcOxY8cQEhJi7S4REVELwipmAJRKJdLT0+Hu7g6JRGLt7hARERERERERmYUgCCgqKkJwcDCk0rrHCXGKGYD09HS0adPG2t0gIiIiIiIiIrKIlJQUtG7dus7HGSAC4O7uDkD1Znl4eJi9fblcji1btmD06NGwt7c3e/tkOm4T28LtYZu4XWwTt4tt4fawTdwutofbxLZwe9gmbhfb01y2SWFhIdq0aSPGPurCABEgTivz8PCwWIDIxcUFHh4eTXqnak64TWwLt4dt4naxTdwutoXbwzZxu9gebhPbwu1hm7hdbE9z2yaGUuowSTURERERERERUQvHABERERERERERUQvHABERERERERERUQvHABERERERERERUQvHABERERERERERUQtn1Spme/bswf/+9z8cP34cGRkZ+PPPP3HXXXeJj7/++uv45ZdfkJKSAgcHB/Tq1QtvvfUW+vXrJy6Tm5uL+fPnY8OGDZBKpZg0aRI+/vhjuLm5WazfVVVVqKysNHp5uVwOe3t7lJaWNovM582BLWwTBwcH2NmxkCARERERERFZn1V/nZaUlCAmJgYPPvggJk6cWOvxjh07YtmyZWjXrh3Kysrw4YcfYvTo0bh06RL8/f0BANOmTUNGRga2bt0KuVyO2bNn49FHH8VPP/1k9v4KgoDr168jJycHgiCY9NxWrVrh0qVLZu8T1Z+1t4lEIoGvry/atm1rsNwgERERERERkSVZNUA0duxYjB07ts7Hp06dqnX7gw8+wLfffovTp09j5MiROH/+PDZt2oSjR4+id+/eAIBPP/0U48aNw3vvvYfg4GCz9jcnJwfZ2dkIDg6Gh4cHf9RTvQmCgMLCQqSnp8PV1RV+fn7W7hIRERERERG1YE1mfktlZSW++uoreHp6IiYmBgBw8OBBeHl5icEhALjtttsglUpx+PBh3H333WZbvyAISEtLg4+PD4KCgszWLrVcrq6uKCsrw7Vr16BQKODv7w+plGnBiIiIiIiIqPHZfIDo77//xuTJk1FaWoqgoCBs3bpVHG2RmZmJgIAAreXt7Ozg4+ODzMzMOtusqKhARUWFeLuwsBCAKi+NXC7X+Ry5XI6qqip4e3s39CURiXx8fJCXl4c1a9YgKioKAwcOhEwms3a3Gp36c1fX54+sg9vFNnG72BZuD9vE7WJ7uE1sC7eHbeJ2sT3NZZsY23+bDxCNGDEC8fHxyM7Oxtdff4377rsPhw8frhUYMsXSpUuxePHiWvdv2bIFLi4uOp9jb2+PVq1aMck0mZV6fyoqKsL69etx+fLlBu3bTd3WrVut3QXSgdvFNnG72BZuD9vE7WJ7uE1sC7eHbeJ2sQ1KAbhcKEGhXIKLv29Dew8B0iaaZaa0tNSo5Ww+QOTq6ooOHTqgQ4cO6N+/PyIiIvDtt99iwYIFCAwMRFZWltbyVVVVyM3NRWBgYJ1tLliwAM8++6x4u7CwEG3atMHo0aPh4eGh8zmlpaW4dOkS8w6RWan3p44dOwIAQkJCMGrUKGt2ySrkcjm2bt2KUaNGMQhrQ7hdbBO3i23h9rBN3C62h9vEtnB72CZuF9ux+dwNLN2YiMzCWzOPAj0c8eq4zhgT1cqKPasf9awpQ2w+QFSTUqkUp4cNGDAA+fn5OH78OHr16gUA2LFjB5RKJfr161dnG46OjnB0dKx1v729fZ0fRH5AyZIkEgmcnZ1RUFDQovc1fZ9Bsh5uF9vE7WJbuD1sE7eL7eE2sS3cHraJ28W6Np3NwPxfTqFm3fIbhRWY/8spLJ/eE3HRTSsvsbH7k1UDRMXFxVplxpOTkxEfHw8fHx/4+vrirbfewoQJExAUFITs7Gx89tlnSEtLw7333gsA6NKlC+Li4vDII4/giy++gFwux7x58zB58mSzVzAjsjSJRAKFQmHtbhAREREREbVICqWAxRsSagWHAEAAIAGweEMCRkUGQtZU55vpYdWSSceOHUOPHj3Qo0cPAMCzzz6LHj16YOHChZDJZEhMTMSkSZPQsWNHjB8/Hjk5Odi7dy+ioqLENlavXo3OnTtj5MiRGDduHAYPHoyvvvrKWi+JbMzw4cMRFhZm7W4QERERERGRjTuSnIuMgvI6HxcAZBSU40hybuN1qhFZdQTR8OHDIQi6YnMqa9euNdiGj48PfvrpJ3N2iywgPj4e69atw6xZsxiwISIiIiIiIpuTVVR3cKg+yzU1Vh1BRLcolAIOXs7BX/FpOHg5Bwpl3YGzpig+Ph6LFy/G1atXrd0VIiIiIiIioloC3J3MulxT0+SSVDdHm85mYPGGBK2hbEGeTlg0PrLJJb8iIiIiIiIiaor6hvsgyNMJmQXlOvMQSQAEejqhb7hPY3etUXAEkZVtOpuBOatO1JrnmFlQjjmrTmDT2Qyr9KuoqAivvvoq+vXrBz8/Pzg6OqJDhw546aWXUFpaqrWsIAj4+uuv0a9fP7i5ucHNzQ1du3bFwoULAQCvv/46Zs+eDQAYMWIEJBIJJBIJZs2aJT4ukUh0ji4KCwvD8OHDte779ddfMWHCBLRt2xaOjo7w8/PDXXfdhdOnT5v9fSAiIiIiIqKWQSaVYNH4SJ2PqVNSLxof2SwTVAMcQdRggiCgTF6/ylMKpYBF68/pzZD++voEDOrgV68d0NleBomkfjtuWloavvnmG0yaNAlTp06FnZ0ddu/ejXfffRcnT57E5s2bxWVnzJiB1atXo1+/fnjllVfg5eWFxMRE/P7771iyZAkmTpyIjIwMfPXVV3j55ZfRpUsXAED79u3r1bdly5bB19cXjz76KAIDA3H58mV89dVXGDRoEE6cOIGIiIh6tUtEREREREQtW1x0EJZP74ln1pxCWeWt3/qBLWCWDwNEDVQmVyBy4WbDC9aDACCzsBxdX99Sr+cnLBkDF4f6beJ27dohJSUF9vb24n1z587Fa6+9hjfffBNHjhxB3759sWbNGqxevRrTp0/HDz/8AKn01qA0pVIJAOjWrRsGDBiAr776CqNGjao1IshUmzZtgqurq9Z9DzzwALp3744PP/wQn3/+eYPaJyIiIiIiopYrLjoIH2+7iPOZRRgWqMQj4/piQIeAZjtySI1TzEgnBwcHMThUVVWFvLw8ZGdn47bbbgMAHD58GACwevVqAMB7772nFRwCUOu2uaiDQ4IgoLCwENnZ2fD390enTp3EfhERERERERHVR1mlAklZxQCAEcFK9Av3afbBIYAjiBrM2V6GhCVj6vXcI8m5mLXiqMHlvp/dp15JsJztZfXplujzzz/HF198gXPnzomjgdTy8vIAABcvXkRQUBBatWrVoHWZ4uTJk3jttdewa9culJSUaD0WHh7eaP0gIiIiIiKi5ichowAKpQB/Nwd4OVRZuzuNhgGiBpJIJPWexjUkwt+oDOlDIvwbPVr5wQcf4LnnnsPo0aPx5JNPIjg4GA4ODkhLS8OsWbNqBYwaQl+epKoq7Q/j9evXMXToUHh4eOC1115Dp06d4OrqColEgqeffhrFxcVm6xcRERERERG1PPEpBQCAriGekEhKDSzdfDBAZEXqDOlzVp2ABNAKElk7Q/qPP/6IsLAw/Pvvv1pTxTZt2qS1XMeOHfHXX3/hxo0bekcR6QsC+fioRkfl5uYiLCxMvL+8vBwZGRno0KGDeN+ff/6J4uJirF+/HiNGjNBqJycnB46Ojka9PiIiIiIiIiJdTqXkAwC6tfYESq1TWdwamIPIytQZ0gM9nbTuD/R0wvLpPa2WIV0mU1VAE4RbYauqqiq88847WstNmzYNAPDCCy/UGlWk+Vw3NzcAqiBQTR07dgQAbNu2Tev+Dz/8sFabMpmsVtsA8PXXXyMzM9PwCyMiIiIiIiLS43RqPgCgW2sP63akkXEEkQ2Iiw7CqMhAHEnORVZROQLcndDXykmw7rnnHixYsABjx47FxIkTUVhYiJ9++kmrqhkA3Hvvvbj//vuxcuVKXLx4ERMmTIC3tzeSkpKwefNmnD17FgDQp08fSKVSvPXWW8jLy4OrqyvCw8PRr18/3HbbbejUqRMWLlyInJwchIeHY9++fTh06BD8/Py01jd27Fi4uLhgxowZmDdvHry9vbF//35s3LgR7du3rzUljYiIiIiIiMhY+aWVuJqjmlbWNdgTB5Ks3KFGxACRjZBJJRjQ3tfa3RA9//zzEAQB3377LZ566ikEBgbi/vvvx+zZsxEZGam17E8//YQhQ4bg22+/xZIlSyCTyRAeHo57771XXKZt27b47rvv8N///hdz5syBXC7HzJkz0a9fP8hkMqxfvx5PPvkkPv30Uzg4OGD06NHYvXs3Bg0apLWu9u3b499//8XLL7+Mt99+GzKZDIMGDcLu3bsxb948XL16tTHeHiIiIiIiImqGTqeq8g+F+brAy8XewNLNCwNEpJNMJsOCBQuwYMGCWo/VnN4llUoxd+5czJ07V2+bM2fOxMyZM3U+1rFjx1r5jQDoDPgMHToU+/btq3X/rl27jLqPiIiIiIiISBd1/qGYNl5W7Yc1MAcRERERERERERGAU2L+IS+r9sMaGCAiIiIiIiIiohZPEASxxH33Np5W7k3jY4CIiIiIiIiIiFq8jIJyZBdXQCaVICqYASIiIiIiIiIiohZHnX+oUyt3ONnLrNsZK2CAiIiIiIiIiIhavFPVFcxaYoJqgAEiIiIiIiIiIiJxBFFLzD8EMEBERERERERERC2cUingTJpqBFFLrGAGMEBERERERERERC3clexiFFdUwdlehogAN2t3xyoYICIiIiIiIiKiFk1d3r5riCfsZC0zVNIyXzURERERERERUTV1/qFurVtm/iGAASIiIiIiIiIiauFOp+YDaLkVzAAGiIiIiIiIiIioBauoUiAhoxAA0J0BIiLbdfXqVUgkErz++ut677Mls2bNgkQisXY3iIiIiIiIyIDzGUWQKwR4u9ijtbeztbtjNQwQUYtz9epVvP7664iPj7d2V4iIiIiIiMjKNKeXteQL/XbW7gBVUyqAaweA4huAWysgdCAglVm7VzYrNDQUZWVlsLMzfRe+evUqFi9ejLCwMHTv3t38nSMiIiIiIqImI15MUO1l1X5YGwNEtiBhPbDpRaAw/dZ9HsFA3H+ByAnW61cDFBUVwd3d3WLtSyQSODk5Wax9IiIiIiIiahlOp6pK3Hdv03IrmAGcYmZ9CeuBNQ9oB4cAoDBDdX/Ceqt06/vvv4dEIsG2bdvw+uuvIzQ0FI6OjujWrRt++eUXrWXDwsIwfPhwnDx5EmPGjIGnpye6desmPn7x4kXMmDEDQUFBcHBwQFhYGJ5//nmUlJTUWu++ffswaNAgODs7o1WrVpg3bx6Ki4trLacvB9Eff/yB4cOHw8vLCy4uLujUqROefPJJVFZW4vvvv8eIESMAALNnz4ZEIoFEIsHw4cPF5wuCgOXLl6NXr15wcXGBm5sbRowYgZ07d9ZaV3l5OZ5//nkEBwfD2dkZffv2xZYtW4x9m4mIiIiIiMiKCsvluHxT9ZuTI4ioYQQBkJfW77lKBfDvCwAEXQ0DkKhGFrUbXr/pZvYuQAPnT7744osoKSnBE088AQBYsWIFpkyZgvLycsyaNUtc7vr164iNjcW9996LSZMmiUGd48ePIzY2Fl5eXnjssccQEhKCU6dO4ZNPPsH+/fuxe/du2NvbAwAOHz6M2267De7u7njxxRfh5eWFX375BQ888IDR/X3llVfw9ttvIzIyEs888wyCgoJw+fJl/PHHH1iyZAmGDh2Kl19+GW+//TYeffRRDBkyBADQqlUrsY0ZM2bg559/xj333IPZs2ejoqICq1evxqhRo7B27VpMmHBrVNeUKVOwbt06jB8/HmPGjMHly5cxceJEhIeH1/s9JyIiIiIiosZxNrUAggCEeDnDz83R2t2xKgaIGkpeCrwdbKHGBdXIonfa1O/pL6cDDq4N6kF2djZOnz4NT0/VULvHH38c3bp1w7PPPov7778fzs6qDO/Jycn4+uuv8fDDD2s9/8EHH0RQUBCOHj2qNeVs5MiRmDhxIlavXi0Gmp555hkolUrs378fHTt2BAA88cQTGDx4sFF9PXLkCN5++22MGDECGzdu1JqC9s477wAAvLy8MGrUKLz99tsYMGAApk+frtXGn3/+idWrV+PLL7/Eo48+Kt7/1FNPoX///njqqacwfvx4SCQSbNmyBevWrcPMmTPx/fffi8sOHToUd999t1F9JiIiIiIiIus5JU4v87JuR2wAp5iRXnPmzBGDQwDg6emJxx9/HHl5edi1a5d4v4+PD2bPnq313DNnzuD06dOYOnUqKioqkJ2dLf4bPHgwXF1dxelYWVlZOHjwIO68804xOAQADg4OeOaZZ4zq6+rVqwEAS5curZWfSD2VzJBVq1bB3d0dd911l1Z/8/PzMX78eFy9ehUXL14EAKxbtw4A8Pzzz2u1cdddd6FTp05G9ZmIiIiIiIis51R1guqYFp5/COAIooazd1GN1KmPaweA1fcYXm7a76qqZqaydzH9OTV06dKl1n2RkZEAgCtXroj3tW/fHjKZ9jS48+fPAwAWLVqERYsW6Wz/xo0bWm117ty5zvUZcvHiRUgkEsTExBi1vC7nz59HUVGR1pSzmm7cuIGOHTviypUrkEqlWgEttS5duuDChQv17gcRERERERFZ3qnqEvctPf8QwABRw0kk9Z/G1T5WVa2sMAO68xBJVI+3j7X5kvcuLrWDUYKgek3PPfcc4uLidD7P29vbrP0wdqRQXQRBgL+/P3766ac6l4mOjq53+0RERERERGQbsgrLkVFQDqkE6BrCEUQMEFmTVKYqZb/mAQASaAeJqoMcce9YNTh0/vx53HnnnVr3JSQkAADatWun97kREREAAJlMhttuu03vsuqkzomJibUeU6/PkI4dO+Lff//FqVOn0Ldv3zqX0xdAioiIQFJSEvr37w83Nze962vXrh2USiWSkpIQFRWl9Zh69BQRERERERHZJnX+oYgAd7g6MjzCHETWFjkBuG8l4BGkfb9HsOr+yAm6n9dIli9fjoKCAvF2QUEBvvjiC3h5eWHYsGF6n9ujRw9ER0fjiy++0JqOplZVVYXc3FwAqipi/fv3x19//YWkpCRxmcrKSnz44YdG9XXq1KkAgJdffhmVlZW1HlePaFIHftTr1vTAAw9AqVRiwYIFOtehnhIHQAyc/e9//9NaZt26dZxeRkREREREZOPU+Ye6teboIYAjiGxD5ASg8+2qnETFNwC3VqqcQzYwrczPzw/9+vUTE1CvWLEC169fxzfffKNzWpkmiUSCH3/8EbGxsejWrRsefPBBREVFobS0FJcuXcLatWuxdOlSsYrZBx98gOHDh2PQoEGYO3euWOa+qqrKqL727dsXL774Iv773/+iZ8+euP/++xEYGIjk5GT8/vvvOHLkCLy8vBAZGQl3d3d8/vnncHFxgZeXFwICAhAbGyuWtl+2bBlOnDiBO+64A35+fkhNTcXBgwdx6dIlMdg1ZswYjB8/Hj/88ANyc3MRFxeHy5cv48svv0R0dDTOnj1b/zeeiIiIiIiILEqdfyiGFcwAMEBkO6QyIHyItXtRy3//+1/s3bsXn332mZicefXq1eJoHUO6d++OkydPYunSpVi/fj2++OILuLu7IywsDLNmzcLIkSPFZQcMGICtW7fipZdewjvvvANPT0/cc889mDNnDrp27WrU+t555x3ExMRg2bJlePfdd6FUKtGmTRuMGzdODGg5Ozvjl19+wauvvoqnn34aFRUVGDZsGGJjYwEA3333HUaMGIGvvvoKS5cuRWVlJQIDA9GzZ08sXbpUa32//vorXn31VaxevRpbt25F165dsXbtWvz0008MEBEREREREdkoQRDEEUQsca/CABHpZWdnh8WLF2Px4sV1LnP16lW9bYSGhuKLL74wan1Dhw7FgQMHat2vnh6mFhYWVus+tSlTpmDKlCl61zNu3DiMGzeuzsdnzJiBGTNmGOyvs7Mz3n//fbz//vta948ePRrff/+9wecTERERERFR47uaU4rC8io42EnRKdDd2t2xCcxBREREREREREQtyunq6WVRwR6wlzE0AjBAREREREREREQtTHz19LKY1l5W7YctYYCIiIiIiIiIiFoUdf6hmDasYKbGABHpNGvWLAiCgOHDh1u7K0RERERERERmI1cocS69EABHEGligIiIiIiIiIiIWowLmUWoqFLCw8kOYb6u1u6OzWCAiIiIiIiIiIhajNOpBQCAbq29IJVKrNwb28EAkYnqKq1OVB/cn4iIiIiIiBoX8w/pxgCRkezt7QEAcrncyj2h5kS9P1VVVVm5J0RERERERC3DqeoS98w/pI0BIiPZ2dnBzs4Oubm51u4KNSO5ublQKBRQKBTW7goREREREVGzV1pZhaQbRQCAmDZe1u2MjbGzdgeaColEgpCQEFy7dg0ZGRnw8PCARMK5ilQ/giCgsLAQeXl5uHnzJgBAoVDAwcHByj0jIiIiIiJqvs6mFUIpAIEeTmjl4WTt7tgUBohM4Ovri+LiYqSlpSE9Pd3a3aEmThAEFBQUoKCgAIIgoKKiAiEhIdbuFhERERERUbOlzj/UrTXzD9XEAJEJJBIJwsLCUFpair179wIAXF1dYWen/21UKpVIS0tDSEgIpFLO6rMF1t4mgiBALpdDoVBALpcjNzcX3t7eaN++faP3hYiIiIiIqKUQ8w9xelktDBDVQ5cuXaBUKnHixAlkZ2cbzB+jVCrFEUcMENkGW9kmEokEdnZ2aNeuHfr374/AwECr9YWIiIiIiKi5UweIujNAVAsDRPUgkUgQHR2NLl26ID8/32AFqqqqKuzcuRMjRowwONqIGoctbRNHR0d4enoypxUREREREZEF5RRXICW3DAAQHcIpZjUxWtEAMpkMvr6+BpeTy+Vwd3dHQEAA7O3tG6FnZAi3CRERERERUctyOq0AANDO3xWezvwdWBPnOxERERERERFRs6dOUN29tZdV+2GrGCAiIiIiIiIiomaPFcz0Y4CIiIiIiIiIiJo1QRBwOlU1xYwVzHRjgIiIiIiIiIiImrXUvDLklFTCXiZBlyAPa3fHJjFARERERERERETNmrq8fedADzjZy6zbGRvFABERERERERERNWu3ppcx/1BdGCAiIiIiIiIiomYtvjpBdQwrmNWJASIiIiIiIiIiarYUSgFn05ig2hAGiIiIiIiIiIio2bqUVYzSSgVcHWRo7+9m7e7YLAaIiIiIiIiIiKjZOlU9vaxra0/IpBLrdsaGMUBERERERERERM2WuoIZ8w/pxwARERERERERETVbYoCI+Yf0YoCIiIiIiIiIiJqlcrkCiRlFABggMoQBIiIiIiIiIiJqlhIyClGlFODn5oBgTydrd8emMUBERERERERERM2SOkF1TGsvSCRMUK0PA0TU4iiUAg4n5+J4tgSHk3OhUArW7hIRERERERFZgDpA1I0Jqg2ys3YHiBrTprMZWLwhARkF5QBkWHnxGII8nbBofCTiooOs3T0iIiIiIiIyo9OpBQCAmDaeVu6J7eMIImoxNp3NwJxVJ6qDQ7dkFpRjzqoT2HQ2w0o9IyIiIiIiInMrKJXjSnYJAJa4N4ZVA0R79uzB+PHjERwcDIlEgnXr1omPyeVyvPjii+jatStcXV0RHByMBx54AOnp6Vpt5ObmYtq0afDw8ICXlxceeughFBcXN/IrIVunUApYvCEBuiaTqe9bvCGB082IiIiIiIiaidNp+QCAtj4u8HZ1sG5nmgCrBohKSkoQExODzz77rNZjpaWlOHHiBF577TWcOHECa9euxYULFzBhwgSt5aZNm4Zz585h69at+Pvvv7Fnzx48+uijjfUSqIk4kpxba+SQJgFARkE5jiTnNl6niIiIiIiIyGJuTS/zsm5Hmgir5iAaO3Ysxo4dq/MxT09PbN26Veu+ZcuWoW/fvrh+/Tratm2L8+fPY9OmTTh69Ch69+4NAPj0008xbtw4vPfeewgODrb4a6CmIauo7uBQfZYjIiIiIiIi2xYvVjBj/iFjNKkk1QUFBZBIJPDy8gIAHDx4EF5eXmJwCABuu+02SKVSHD58GHfffbfOdioqKlBRUSHeLiwsBKCa1iaXy83eb3WblmibjOPrYtyu7utix+1kBfyM2CZuF9vE7WJbuD1sE7eL7eE2sS3cHraJ28X81BXMooLc6vW+NpdtYmz/m0yAqLy8HC+++CKmTJkCDw8PAEBmZiYCAgK0lrOzs4OPjw8yMzPrbGvp0qVYvHhxrfu3bNkCFxcX83ZcQ80RUdR4lALg5SBDfiUASHQsIcDLAbiZcAgbzzdy50jEz4ht4naxTdwutoXbwzZxu9gebhPbwu1hm7hdzCO/AsgqsoMUAlJOH0TWufq31dS3SWlpqVHLNYkAkVwux3333QdBELB8+fIGt7dgwQI8++yz4u3CwkK0adMGo0ePFoNP5iSXy7F161aMGjUK9vb2Zm+fjGMfdgPzfzmlM1E1IMHI6GDccXt0I/eKAH5GbBW3i23idrEt3B62idvF9nCb2BZuD9vE7WJeWxJuACdOoWOgB+4eP6BebTSXbaKeNWWIzQeI1MGha9euYceOHVoBnMDAQGRlZWktX1VVhdzcXAQGBtbZpqOjIxwdHWvdb29vb9GNbun2Sb87ureGnZ0M8346iSqNamUeTnYoLK/CuvgM3Nu7Lfq387ViL1s2fkZsE7eLbeJ2sS3cHraJ28X2cJvYFm4P28TtYh5nM1TVzbu38Wrw+9nUt4mxfbdqFTND1MGhixcvYtu2bfD11f7hPmDAAOTn5+P48ePifTt27IBSqUS/fv0au7vUBMS08RKDQ/eHK7Dqwd448doo3NU9GAqlgHk/nUBGQZmVe0lEREREREQNcTo1HwArmJnCqgGi4uJixMfHIz4+HgCQnJyM+Ph4XL9+HXK5HPfccw+OHTuG1atXQ6FQIDMzE5mZmaisrAQAdOnSBXFxcXjkkUdw5MgR7N+/H/PmzcPkyZNZwYx02pN0E4Aqi/3AQAH9wn1gJ5Ni6cRu6BLkgeziSsxZdQIVVQor95SIiIiIiIjqQ6kUcDqlusR9ay/rdqYJsWqA6NixY+jRowd69OgBAHj22WfRo0cPLFy4EGlpaVi/fj1SU1PRvXt3BAUFif8OHDggtrF69Wp07twZI0eOxLhx4zB48GB89dVX1npJZON2VweIhkZoj0ZzdpDhy+m94Olsj/iUfLy+PsEa3SMiIiIiIqIGSs4pQVFFFZzspejYys3a3WkyrJqDaPjw4RAE3SmDAeh9TM3Hxwc//fSTObtFzVSVQom9F7MBAEMj/JB+Jknr8ba+Lvh4cnfM/v4ofj5yHTGtPTG5b1trdJWIiIiIiIjqSV3ePjrYE3Yym86sY1P4TlGLEZ+Sj6LyKni52KNriKfOZYZ3CsB/RncCACz86xziqw8sRERERERE1DSoA0TMP2QaBoioxdh1QTW9bEiEP2RSSZ3LzRnWHqMjW6FSocScVceRXVzRWF0kIiIiIiKiBjqVqso/1K217oEBpBsDRNRiqPMPDevor3c5qVSC9++LQTt/V2QUlGPu6hOoUigbo4tERERERETUAJVVSiSkFwJQlbgn4zFARC1CdnEFzqSposhDI/wMLu/uZI+vZvSCq4MMh5NzsfTfREt3kYiIiIiIiBooMbMQlQolvFzs0dbHxdrdaVIYIKIWYe9F1eihyCAPBHg4GfWcDgHueP++7gCAb/cl46/4NEt1j4iIiIiIiMzg1vQyL0gkdacWodoYIKIWYXd1/qFhnfRPL6spLjoQTwxvDwB48Y/TOJ9RaPa+ERERERERkXmoE1R3Z/4hkzFARM2eUilgT3V5++EG8g/p8tzoThgS4YdyuRKP/XgcBaVyc3eRiIiIiIiIzEAdIOrW2suq/WiKGCCiZu9segFySyrh5miHnqHeJj9fJpXgk8k90NrbGddzS/HUryehUAoW6CkRERERERHVV3FFFS7dLAYAdGvDEUSmYoCImj319LJBHXxhL6vfLu/t6oAvZ/SCo50Uuy7cxMfbkszZRSIiIiIiImqgM6kFEAQgxMsZAe7G5Z6lWxggomZvl1jePqBB7UQFe+KdSV0BAJ/suIQt5zIb3DciIiIiIiIyj1Op+QCAbsw/VC8MEFGzVlAqx8nreQCAoR0Nl7c35O4erTFrYBgA4Nk1p3C5evgiERERERERWdfp6gBRTBsvq/ajqWKAiJq1fZeyoRSADgFuaO3tYpY2X7m9C/qG+aC4ogqP/XgcxRVVZmmXiIiIiIiI6u9UiqrEfQwTVNcLA0TUrO1OygIADKtH9bK62MukWDatB1p5OOJSVjGe/+0UBIFJq4mIiIiIiKzlZlEF0vLLIJEAXTnFrF4YIKJmSxAE7BbzD5kvQAQAAe5O+HxaL9jLJPj3bCa+2H3FrO0TEemiUAo4nJyL49kSHE7OZUVFIiIiomrq6WUd/N3g5mhn3c40UXzXqNm6cKMINwor4GQvRd9wH7O33yvUG69PiMIrf57F/zYnIjrEA0MizBuIIiJS23Q2A4s3JCCjoByADCsvHkOQpxMWjY9EXHSQtbtHREREZFWnUvIBMP9QQ3AEETVb6vL2A9r5wsleZpF1TO3bFvf3bgOlAMz/+SRSckstsh4iatk2nc3AnFUnqoNDt2QWlGPOqhPYdDbDSj0jIiIisg2nUtX5hzi9rL4YIKJma9cFy0wv0ySRSLD4zijEtPZEfqkcj686jnK5wmLrI6KWR6EUsHhDAnRNJlPft3hDAqebERERUYslCIJY4p4jiOqPASJqloorqnDsWi4AYFinAIuuy8lehuXTe8HX1QHn0gvx8p9nmLSaiMzmSHJurZFDmgQAGQXlOJKc23idIiIiIrIh13NLkV8qh4NMis6BHtbuTpPFABE1Swcv50CuENDWxwVhvuYpb69PsJczPp3aAzKpBGtPpOHHQ9csvk4iahmyiuoODtVnOSIiIqLmRj29rEuwBxzsGOaoL75z1CxplreXSCSNss6B7f2wYGxnAMCSDQk4dCUHBy/n4K/4NBy8nMPpH0RULwHuTmZdjoiIiKi5USeo7s78Qw3CKmbU7AiC0Cj5h3R5aHA4TqUWYMOpdEz9+hA0Y0KsNkRE9dE33AdBnk51TjOTAAj0dLJItUYiIiKipoAVzMyDI4io2UnOLkFqXhkcZFIMaO/bqOuWSCS4rYsq51HNAUOsNkRE9SGTSrBofKTOx9TjIxeNj4RM2jijJYmIiIhsSZVCibPpqilm3Vp7WbczTRwDRNTs7E5SjR7qE+4NV8fGHSSnUAp4599EnY+x2hAR1VdcdJDOEZGtPJywfHpPjkwkIiKiFivpRjHK5Uq4O9qhnZ+rtbvTpDFARM2OOkDU2NPLAFYbIiLLySmpAADMG94OLjJVkPnNu6IYHCIiIqIWTV3evlsbT0g5orpBGCCiZqVcrsDByzkAgGEdLVveXhdWGyIiSyitrML5jCIAwH29W6OHnypAtLM63xoRERFRS3VaHSDi9LIGY4CImpXDybmoqFIi0MMJHVu5Nfr6WW2IiCzhdGoBFEoBgR5OCPJ0QpS3KkC0IzELgsApq0RERNRyxaeo8g/FMEDUYAwQUbOyW6N6WWOVt9ekrjZU15olUFUzY7UhIjLFiet5AICeoV4AgAgPAU72UmQUlCMxs8iKPSMynkIp4ODlHPwVn4aDl3OYj4+IiBqsrFKBpBuqc6GYNixx31Asc0/Nyu6kLADAsE6Nn38IuFVtaM6qE5DgVmJqTaw2RESmOnEtHwDQs603AMBBBgxo54OdF7KxIzELXYI8rNg7IsM2nc3A4g0JWnn6gjydsGh8JPNoERFRvZ1LV42yDnB3RKAHZ2k0FEcQUbORkluKyzdLIJNKMKiDn9X6ERcdhOXTeyLQU/sA5elsz2pDRGQyQRAQn6IaQdSjrZd4//DqRPzbz9+wRreIjLbpbAbmrDpRq4hDZkE55qw6gU1nM6zUMyIiauriU/IBADFtvKwyg6S5YYCImo09F1XTy3q29YKns71V+xIXHYR9L8bi50f6Y0xUIABgRCd/BoeIyGQpuWXILq6EvUyCqOBbQ6dHVI+UPJmSj9ySSmt1j0gvhVLA4g0JOkfUqu9bvCGB082IiKheTqWq8w9xepk5MEBEzYZm/iFbIJNKMKC9L6b3bwsAOHYtz8o9IqKmSJ1/KCrYE072MvH+IE8ndAnygCAAuy5kWat7RHodSc6tNXJIkwAgo6AcR5JzG69TRETUbKgrmMW08bJqP5oLBoioWaisUmL/pWwA1ilvr0+Ptt6QSoDUvDJkFJRZuztE1MSICaqr8w9piu1cPc0skQEisi3lcgW2n7+Bj7YlGbV8VlHdQSRq+hRKAYeTc3E8W4LDybkcMUZEZpFXUolrOaUAgG4hXtbtTDPBJNXULBy/loeSSgV8XR0QFWxbyVrdHO0QGeyBs2mFOHY1D+NjnK3dJSJqQmpWMNMU27kVPtt5GXuSbkKuUMJexus+ZD1F5XLsSMzClnM3sOtCFkoqFUY/N8CdiUWbK+0E5TKsvHiMCcqJyCxOVY8eCvdzhaeLdVOMNBcMEFGzsDtJNb1saEd/SG2wQljvUB+cTSvE8Wt5GB8TbO3uEFETUVpZhfMZqtKtukYQdW/jBR9XB+SWVOLY1TwMaO/b2F2kFu5mUQW2nb+Bzecysf9SNuSKWyNDAj2cMCoyAP+cyUReSaXOPEQSAIGeTugb7tNofabGo05QXnPbqxOUs3gHEdWXQingn9OqIgfBnk5QKAVWijYDBoioWVAHiGwl/1BNvcO88f2Bqzh6lTkWiMh4Z1JVpVtbeTgiyLP2CAuZVILhHf2x9mQadiTeYICI6k2hFHAkORdZReUIcFcFbOo60U7JLcXmc5nYfC4Tx67lQdD49d/O3xVjogIRFxWIbq09IZGoKovOWXUCEkBnkGjR+Eie1DdDhhKUS6BKUD4qMpDbn4hMoj0yEdh/OQeD/7uDIxPNgAEiavJuFJbjfEYhJBJgSIT1ytvr0ztUdWX0fEYhisrlcHfiEEgiMuzE9XwAqtFDdZVuje0SUB0gysIrt0c2Yu+ouah5og1AawqQIAi4cKMIm8+qRgolZBRqPb9ba0+MiQrEmKhW6BDgXqv9uOggLJ/es9Y6XB1keP++GJ7MN1OmJChncJuIjMWRiZbFABE1eXuqRw91DfGEr5ujlXujW6CnE9r4OCMltwwnr+djqI2OdCIi26IvQbXakAh/2EkluHyzBNdyShDq69pY3aNmQN+J9uOrTmBUZCtcvFGEq9VJQAFAKgH6hvtgTFQgRkcFIsTLcG69uOggjIoMxJHkXPx7NgMrD15De39XnsQ3Y8YmHmeCciIyFkcmWh4DRNTkqaeXDbfxoEvvUB+k5Kbh2LU8BoiIyCBBEHBST4JqNU9ne/QO88ahK7nYkZiF2YPCG6mH1NQZOtEGgK0JNwAADnZSDI3ww+ioQNzWpRV8XB1MXp9MKsGA9r4I9XXByoPXcC6jCMUVVXBz5Oloc2Rs4nEmKCciY3FkouWx3Ak1aQqlgL0Xq8vbd7LtoEvvMNUIgGPMQ0RERkjJLUN2cSXsZRJEBXvqXXZk51YAgB0sd08mMHSirfbUyAiceG0UvpnZB/f1blOv4JCmYC9ntPFxhkIp4MS1vAa1RbZJqRRwJi1f7zISqKYyMkE5ERmLIxMtjwEiatLiU/JRUCaHh5MdYlp7Wbs7evUJU50AnbyeD7lCaeXeEJGtU08viwz2hJO9TO+ysV0CAACHruSguKLK4n2j5sHYE+h2/q5mH+Wj/k48ksyLJs3NtZwSTP7qEN7emFjnMuqJH0xQTkSm4MhEy2OAiJo09fSyIRH+sJPZ9u7cwd8Nns72KJMrkJBeaPgJRNSiidPL2noZXLadnytCfV0gVwjYVz2qksgQa55o9wtngKi5USoF/HjwKuI+2osjV3Ph6iDD23d3xfJpPWtVYfR2dWAiWSIyWd9wHwR5OqGusDJHJjacbf+iJjLA1svba5JKJegVWj3NjEPqicgAzQpmhkgkEsR2Vo0i2pF4w5LdomZEfaJdF0ueaPcNV+WGiE/NR7lcYfb2qXGl5ZdhxneH8dpf51AmV6B/Ox9senoopvZri7Fdg7DvxViserA3Qt1UI6hnDwpjcIiITCaTSrBovO6KrRyZaB4MEFGTlVtSidOp+QDQZJI+Mw8RERmjrFKB89WlxHuGGg4QAZp5iG5CqdSVdphImzVPtMN8XeDv7ojKKiVOpxaYvX1qHIIgYM3RFIz5cA/2X8qBk70Ur4+PxE8P90cbHxdxOZlUgn7hPujuqzo2cSQ1EdVXXHQQXrm9S637Az2dODLRDFg2gpqsvRdvQhCAzoHuCNRzBdSWqHMuHL2aB0EQIJEwuk1EtZ1OzUeVUkArD0cEG3l86xvuA1cHGbKLK3A2vQDdbDwvG9mGuOggdAvxwOk07R/sgZ5OWDQ+0mIn2hKJBH3DfPDPmQwcSc7hdIAm6EZhOV764zR2XlCN5u7Z1gvv39cd4X6udT6nTfVDDAoSUUP4uzsCADq1csMTIzogwF012pUjhxqOASJqssTpZTZevUxT1xBPOMikyC6uwLWcUoTpOYkiopZLc3qZsYFkBzsphkT4Y9O5TGw/n8UAERmlokqBSzdLAABLJ0bDxcGu0U60+4arAkSHk3Mxz6JrInMSBAF/xadj0fpzKCiTw8FOiudGdcTDQ9oZ3Gdau6pGEKXllyGvpBLeDayIR0Qt04XMIgBArzAf3Nk9xMq9aV44xYyaJKVSwJ6k6vL2TWR6GQA42cvQtbWqXDXzEBFRXdQVzHoYkaBak7qaGcvdk7GOXc1DaaUC/u6OmNynLe7sHoIB7X0b5SqsetTQiWt5qGJ1zyYhu7gCj686jqd/jUdBmRzdWnvin/mD8diw9kbtM852qumFAHAmjaOIiKh+km4UAwA6tXK3ck+aHwaIqElKyChEdnEFXBxk6B3atIalMw8REekjCAJOmpCgWtPw6hGVZ9IKkFVoXAlzatl2XVAFE4d19G/0ac+dWrnDw8kOJZUKJGQwJ42t23gmA6M/3IPN527AXibBc6M6Yu2cgYgw8QdaVLAHAAaIiKj+km6oRhB1ZIDI7BggoiZJPb1sYHs/ONg1rd24T6g6DxEDRERUW2peGbKLK2AvkyA6xNOk5wa4OyGmepTizgscRUSGWbMaqFQqEUcRsdy97corqcT8n0/iidUnkFtSic6B7vhr7mDMHxkBO5np52DR6gAR8xARUT2UVlbhem4pAKBjKzcr96b5aVq/rImq7b7Q9PIPqalL3V++WYLckkor94aIbI16ellksCec7GUmP39Edbn77ecZICL90vPLkHSjGFIJMCTCzyp9UBdvOMwAkU3alnADoz/agw2n0iGTSjA/tgPWzxuMyOogT310DeEIIiKqv4vV08v83Bzh6+Zo5d40PwwQUZNTWC7H8eofUMMiml6AyNvVAR0CVNHu48xDREQ1nKg+LvQ0Mf+Qmrrc/b5L2aioUpirW9QM7aq+2NK9jRe8XKyTLFg9gujo1VwolYJV+tDSKZQCDl7OwV/xaTh4OQcKpYCCMjmeW3MKD688hptFFegQ4Ia1cwbiudGdGjxyOzJINSUkLb+MF8qIyGQXqqeXdQrk6CFLYBUzanIOXMqGQimgnZ8r2lYnOmxq+oR541JWMY5dzcWoyFbW7g4R2ZAT9cw/pBYV7IEAd0dkFVXg8JVcDG1Cifypce1OUo0yG94pwGp9iA7xhLO9DPmlcly6Wcx8Eo1s09kMLN6QgIyCWznLfFwcoISA/FI5JBLgkSHt8OyojvUa0aiLu5M9wv1ckZxdgjNpBU2q2AgRWV9SJvMPWRJHEFGTo86X0JR/9PRmHiIi0qGsUoHz1cl6Ta1gpiaVSjCiE6uZkX6VVUrsv5QD4FZyc2uwl0nFqdecZta4Np3NwJxVJ7SCQwCQW1qJ/FI5/N0c8NtjA/DyuC5mCw6pqfOrneU0MyIy0QUmqLYoBoioSREEQcw/ZM0T2oZS51w4k1aAcjmngBCRypm0AlQpBQS4OyLEy7ne7ajL3W9PvAFB4LQdqu3E9TwUV1TB19UB0cGmJUM3N/V3IhNVNx6FUsDiDQnQd3SQSaXoUc+RjIZ0qw4QMVE1EZlKnYOIASLLYICImpRLWcVILyiHo50U/dv5Wrs79dbGxxn+7o6QKwScSsm3dneIyEaoE1T3bOvdoJLjgzv4wUEmRUpuGS7fLDZX96gZUecfGtrRH1Jp45a3r+lWJbMcBjQbyZHk3Fojh2rKLCy3WNBOPYKIiaqJyBQFpXJkFqqOXaxgZhkMEFGToj6h7dfO1+zDnRuTRCJBnzDVVbljTFRNRNXEBNWhXg1qx9XRDv3aqX50c5oZ6bLrgmq/sIX8Lz3aesFeJsGNwgqxdDFZVlaR/uCQqcuZKqq6khkTVVND6UqyTs1XUpZqelmIlzPcneyt3JvmiQEialLU+Yds4YS2odR5iI4xDxERQTWFtqEJqjWNZLl7qsONwnIkZhZBIrGNfH5O9jLEtPYCwGlmjSXA3cmsy5nKw8ke7fxcAXAUEdXfprMZGPzfHZjy9SE89Us8pnx9CIP/uwObzmZYu2tkIRfEBNUcPWQpDBBRk1FaWSWeODaHAJE658Kxa3ks7UtESM0rQ3ZxBexlEnH6RUPEVpe7P3YtDwWl8ga3R82HOpdft9Ze8HG1Tnn7mm5NM2OAqDH0DfdBkKcT6ppcKAEQ5OkkbhdLYKJqaoi6kqxnFpRjzqoTDBI1U0nqBNWBjZR/SKmA5No+hOQehOTaPkDZ/HPHMkBETcahKzmoVCgR4uWM9v6u1u5Og3UJcoeLgwxF5VXicEkiarnU+YcigzzMMoW2ra8LOgS4QaEUsOfizQa3R82HLY7G7aMOEHFUbaOQSSVYND5S52PqoNGi8ZGQWTA/VdfqANHp1HyLrYOaJ31J1tX3Ld6QwOlmzZB6BFGnxkhQnbAe+CgadqvuQu9ry2G36i7go2jV/c0YA0TUZKiveA7r5N+g5K22wk4mFctYH7vKPETNAefBU0OcrJ5eZs6qQeppZsxDRGpVCiX2XrS9aqC9Qr0hlQDXckpxo9AyeW9IW1x0EP53T7da9wd6OmH59J6Iiw4y38p0XIW/NYKo0HzroRbBUJJ1AUBGgeWSrJN1CIJwawSRpQNECeuBNQ8Ahena9xdmqO5vxkEiO2t3gMhY6iuewxt6xVPrJMUDaDcUkFon4XXvUB/sv5SDY1dzMb1/qFX6QOax6WwGFm9I0DphCfJ0wqLxkeY9yaZmS6xgFmq+AFFs5wB8uecKdl3IgkIpWHQ0ADUNJ1PyUVheBS8XezHvjy3wcLJHZLAHzqYV4khyLsbHBFu7Sy1CuL8qj4evqz0Wjo9CgLtqWplZjxUJ64FNL8KuMB29AeDacsAjGN1Hvg3AAWn5ZcgproCvm6P51knNmrWTrJN1ZBdXIq9UDokE6BBgwRxESgWw6UWgzjFqEmDTS0Dn2632G9KSOIKImoSr2SW4mlMKO6kEAzv41b8hGxsqqM5DdJQjiJo0zoOnhiqXK5CQrrqK3rN6ZKE59Ar1hoeTHfJK5YhP4XGGbo3GHRLhb3MBw75hvgCYh6gxXblZDADoHOSBO7uHYEB7X/MHh+q4Cu/852zM9DoNgImqyTTWTrJO1qEePRTm62rZatbXDtQ+ZmkRgMI01XLNEANE1CSoRw/1DvOGm2M9B77Z4FDB7m29IJWoyrxmFJQ1+vqp4TgPnszhdGoBqpQCAtwdEeLlbLZ27WRSDOvEamZ0y64k1X7Q4NG4FtA3XDV6jgGixpOcXQIACPezQG5Hg1fhgaervoMUSiaqJpOok6zrY+kk69T4Gq2CWfEN8y7XxDBARE3CrYSaAfVrwIiTFGx6qdEz07s52iEy2AMA8xA1VZwHT+YgTi9r6232HGvMQ0RqN4sqxHwvtlDevib1qNoLN4qQV1Jp5d60DOoAUTs/C/zgMuIqvHdVFvpKEzmCiEyiL8m62pAIP5sbJUkNox5BZPEE1W6tzLtcE8MAEdm8crkCBy/nAGhAxRUbHirYO7S63D0rtzRJnAdP5nDimipA1MOM08vUhnX0h1QCJGYWIS2fIxVbsj3VF1uiQzzg7257+V583RzFvBJH+Z3YKK7crB5BZInqsEZeXQ9APs6kMkBEpomLDkKXoNqBAvVMgzXHUrHmaEpjd4ss6EJjlbgPHQi4BepZQAJ4hKiWa4YYICKbd+xqHsrkCvi7O+r8IjCKDQ8VZB6ips3Y+e2f7byEFfuTWZ2HahEEASdT8gGYN0G1mrerA3pWV0bjKKKWbZcNlrevST0lhAEiy1MqBSTnqEcQWSBAZOTV9Sx4Ib2gHDnFFebvAzVbxRVVuJyl2n/fvy8GH0/ujp8f6Y/4haPw8OBwAMCLa09j/Sl9F4ipqRAEAUmNVeJeIQcc6zomVo9Ki3unWSaoBhggoiZgd3W+hGEdG1De3oaHCvYOU/1wS8wsRFG5vNHXTw2jngdvaM9MulGMxRsS0H/pdtz/5UH8ePAqsnkyTABS88pws6gCdlIJulaXfTa32C6qaWY7GSBqsRRKQaO8fT2nazeCvtUXTTgt1/LSC8pQWaWEvUyC1t4u5l9B6EDAIxio8xtSdRU+26cnACaqJtPsu3gTlQolwnxdMLFHiJhk3U4mxSu3d8HUfm0hCMAzv8Zjy7lMa3eXGigtvwwllQrYyyQIs0RAW00QgL/mAjmXAXsXwK3G96VHMHDfSiByguX6YGUMEJHN222OK55tBwAO+qLN1hsq2MrDCW18nKEUgJPX8xt9/dQw6nnwurJbSar/LZ3YFQvviETPtl4QBOBwci5e++sc+r61DdO+OYSfj1xHLvNttFjq/ENRwR4Wq8oRW52HaP+lbJRVNm6uNbINp1LzkV8qh7uTHXq08bJ2d+qkHkF0Nr0QxRVVVu5N86aeXhbq62qZXC1SGRD3X+jO/1gt7h1Eta7e5gwQkQnUhRdiO7eqdQFZIpHgzTujMbFHCBRKAfN+OikGyKlpUucfaufnBnuZBUMYe94Dzv4OSO2Aqb8Czyaiavo6HAudg6rp64CnzzTr4BDAABHZuPT8MiTdKIZUoko2Vy/qBNWVRXoWEqw6VLAP8xA1aXHRQYgO8ah1f6CnE5ZP74kpfdviwcHhWPvEIOx/KRavjOuCmNaeUArA/ks5WLD2DPq8tQ0PfHcEa46loKC07pFkCqWAg5dz8Fd8Gg5ezmF1tGZAHRju0db808vUOrVyR4iXMyqqlDhwOdti6yHbdau8vR/sLHly3UDBXs5o7e0MhVIQc3ORZVi0gplah5GAY+3vRwBA9CQgcoI4cvI08xCRkZRKATsvqAJEt3XRPSJSKpXg3Xu6YWx0ICoVSjyy8hhHJjZhSTeKAVg4/9C5P4Gdb6r+vv19IHwoIJVBCB2MNJ8BEEIHN9tpZZrqWS+cqHGoE2p2b+MFLxcH0xuoqgD+fEz1gYcE6PkAcGlr7YTV9i5A+JCGd7ieeof5YO3JNOYhaqIKyuRi6c337u0Ge5kUAe6q8qo1r8qGeDnjkaHt8MjQdkjJLcXfpzPw9+l0nEsvxJ6km9iTdBOvyM5gSIQ/7ugWhFGRreDuZA8A2HQ2A4s3JGhVTQvydMKi8ZGIiw5qvBdMZqUeQWSJBNVqEokEsZ0D8OOha9iRmIWRXZpn5Q2qmzr/0PD6VgNtRH3DfZCal4ajV3NtstpacyFWMLNEgmq1I18DFYWAVyiqxn6A+APb0CPEEbIDHwFX9wJVFYiuDhBxBBEZ61RqPrKLK+HuaIfeYXWXsreTSfHx5B4o+/EYdl24iQe/P4rVD/dDjA2PoiTdbuUfslCJ+7QTwJ9zVH/3nwv0mmWZ9TQBtnsJiQjArgsNKG9fUQz8dJ8qOCS1B+75DpjwCfD02VtDBaeuBfw6A/JSYO8HZu698dR5iE6m5EGuUFqtH1Q/28/fgFwhICLADff0aiPOgzc0ZL+NjwvmDG+Pf54cgh3PDcN/RndE50B3yBUCdiRm4dk1p9DrzW14dOUxLNlwDnNWndAKDgFAZkE55qw6gU1nMyz5EslCyuUKJKSryo73tOAIIuDWNLMdiVkQBI48a0lyiitwOjUfgG2Wt6+pX/U0s8O82m9Rl2+qrshbJEE1AFQUAfs/Vv09/CUI7YYhzWcAlENfANyDVYVBzv6BqGAPSCRAekE5c/ORUdQFF4Z29IeDnf6fsw52UnwxvRf6t/NBcUUVHvjuCM5nFDZGN8mMxApmlkhQXZgO/DIVqCoDIkYDo98w/zqaEAaIyGbJFUrsv6SaCjGsk4kntCU5wA/jgSu7AHtXYNoaIHqi6jHNoYLhQ28dBA5/CeRfN98LMEEHfzd4OtujXK4UfyxS07HxjCo4M65r/UfxtPN3w7zYCGx6eii2PTsUT42MQHt/V1RWKbEl4Qa+239VZxYH9X2LNyRwulkTdCatAFVKAf7ujmjt7WzRdQ1o7wsneykyCspxPkPflFtqbvZdyoYgAJ0D3RHoaVzlRWtSV/eMT8lHuZw5syzl1ggiC12RP/wlUJYL+LQHut53636ZA9DvUdXfBz+Du6OdOM2NiarJGLfyDxl3AdnJXoZvZvZBj7ZeKCiTY8a3h8UAKdk+hVLAxSzV9upk7ilmlaXAz5OBogzAvwsw6dsWMY1MHwaIyGadvJ6PoooqeLvYm1bZJz8FWBEHpJ8AnH2AmRuA9rF1Lx8xSjXHVFEB7Hiz4R2vB6lUgt7V5a1Z2rdpKSyXY0+SKpB5ezfzTPPqEOCOZ0Z1xLZnh2HT00NwV/dgvcsLADIKyjm3vglS51jp2dar/lUajeRkL8PgDqpcburcDdQyiKNxTb3YYiXhfq7wc3NEZZWSeWkspFyuQFp+GQAL5SAqLwAOfKr6e/hLgKxGVotes1QX8G6cBa7sEs/zznJ7kwEZBWVIyCiERAKMMDJABABujnb4fnZfRAZ5ILu4EtO+PoyU3FIL9pTM5VpOCSqrlHCyl6KNOSsuKpWqVCQZpwAXX2DqL4BTHTnTWhCjchBNnDjR6AbXrl1b784QaVKXtx8S4W98dY2bF4Af7wYK01RVyWb8Cfh30v8ciQQYtQT4ajhw+leg/xNAcPcG9b0+eof5YHtiFo5dzcPD1kuHRCbafv4GKhVKdAhwM/uwV4lEgs6BHhjROQDr4tMNLp9VVG5wGbIt6vxDlp5epjaicwC2nc/C9vM3MHdEh0ZZJ1mXUimI+fz05h9SKoBrB1TTftxaqap6WukqqkQiQb9wH/xzJgNHknPEymZkPtdySiEIgLuTHXxd65Hj0ZBDXwDl+YBfR1Uy6pqcvYEe04EjXwIHP0PX0HfxV3w6RxCRQerRQz3besPHxH3X09kePz7UF5O/OoSLWcWY+s0h/PbYwCYxsrIlU1cwiwhwh9ScFRd3vQ2cX68a1Xj/asA7zHxtN2FGjSDy9PQ0+p8p9uzZg/HjxyM4OBgSiQTr1q3Tenzt2rUYPXo0fH19IZFIEB8fX6uN8vJyzJ07F76+vnBzc8OkSZNw48YNk/pBtsnk8vapx4Hv4lTBIb+OwENbDAeH1IJ73Br+vPU1wAr5OfpU5yE6di2X+UGakH9OZwJo2PQyQwLcjTtxMXY5sg2CIOBEdQWznqGNEyBSD8c/mZKPHOb6aBHOphcgp6QSbo526FXXfpawHvgoGvjhDuCPh1T/fxStut9K1EGhIyzeYBHJ2dX5h/zdzD96sSwPOPiZ6u/hL9UdaOw/B4AEuLQVfV1VP/qZqJoMUecfMnZ6WU2+bo5Y9XA/hPq6ICW3DNO+OcTcVzbuQmZ1BTNzXog9vQbY8z/V3+M/AUIHmK/tJs6oEUQrVqywyMpLSkoQExODBx98UOcopZKSEgwePBj33XcfHnnkEZ1tPPPMM/jnn3/w22+/wdPTE/PmzcPEiROxf/9+i/SZGsfNogqcTVPl4jEqoeblHcAv0wF5CRDSC5j6G+Dqa9pKY18FEtYByXuAS9tUU88aUXSIJxxkUmQXV+JaTinCLFl2lsyiqFyOPRdVgczbLRgg6hvugyBPJ2QWlOvMQyQBEOjpxKvsTUxqXhluFlXATioxbRptAwR5OiMyyAMJGYXYnXQTE3u2bpT1kvWoy9sPbO+rO5lrwnpgzQNAzaNLYYbq/vtWApETLN/RGtR5iI5fzUWVQgk7GbMimNPlm9X5hyxxrnHwM6CiAAiIBCLvrns5n3Cgyx3A+Q3ocu1HSCTjxETVfm6O5u8XNXlllQoxP+nIOsrbG6OVhxNWP9wP931xEJdvlmD6N4fxy6P961cxmSwuKau6glmgmfKlpRwB/pqn+nvwM0D3KeZpt5mo17dtVVUVtm3bhi+//BJFRaoNlp6ejuJi05J9jR07Fm+++Sbuvlv3l8eMGTOwcOFC3HbbbTofLygowLfffosPPvgAsbGx6NWrF1asWIEDBw7g0KFDpr0osil7q390R4d4wN/dwEnC2bXA6vtUwaF2I4AH1pseHAIA71Cg32Oqv7cuVA23b0RO9jJ0a636kcg8RE3D9vNZqKxSop2/KzpaquwmAJlUgkXjIwGogkG6LBofafxUTLIJ6ullkcEecLJvvKk86quu2xOZh6glEMvbd9LxY0qpADa9iFrBIeDWfZteavTvQ0CViNTDyQ4llQoksOKQ2akTVJs9/1BpLnBouerv4QsAqYGfGgPmAwDsz/6GXj5yAExUTXU7cDkbFVVKhHg5o1MDR5O09nbB6kf6w8/NEYmZRZi54iiKyuVm6imZk7rEvVlGEOVfV1UsU1QAne8AYhc2vM1mxqgRRJquXbuGuLg4XL9+HRUVFRg1ahTc3d3x3//+FxUVFfjiiy8s0U+djh8/DrlcrhVA6ty5M9q2bYuDBw+if//+Op9XUVGBiopbQwkLC1UnHnK5HHK5+Q8M6jYt0XZztTNRNU1wSHtfve+b9Nh3kG5+ERIIUEbeBcX4zwCpI2Dgva5zm/R/CnYnfoQkKwFVx3+E0H1aw16IiXq29cSxa3k4kpyDu2ICG3Xd1tRUPyP/nFblBYqLbIWqqipAqYAk5aCYw0NoM8BsOTxGdvLDp5Nj8ObGRGQWag+FfnlsJ4zs5Gf296+pbpem4nh1IDimtafx77FSAUXyPoTkHoTisisQPtjkfWxohA+W7QT2JN1EaXkF7Dkyo0Fs+XOSXyrHyepA5KB2XrX6KLm2D3aF+vKbCUBhGqqu7IEQOtiCPdWtV6gXdl7IxsFLN9GllWmBDFveLrbgSnUFp1BvJ7O+R9J9H0FWWQyhVVdUdRijdT6mc5sE9YQspDekacfwkON2HEMcTl3Pw+B2jTPttiVrip+RrQmqaf0jOvmpzruABp17tfZ0wA+zemL6d8dwKiUfD35/FN/O6AlnB+tVsWqK28WSKqqUtyou+jo37H2pKILdT/dDUnJTdYwavwxQKFT/9Ggu28TY/pscIHrqqafQu3dvnDp1Cr6+t0Zp3H333XVOA7OUzMxMODg4wMvLS+v+Vq1aITMzs87nLV26FIsXL651/5YtW+DiYsbM6DVs3brVYm03J0oB2JEgAyCBfc5FbNx4sfZCgoBOmevQOfNPAECy30icdrgL2LLdpHXp2ibtfcciOu1nyLe8ju2pLlBIG2+Ys5AnASDD7nOp2OhwrdHWayua0mekXAHsTFTtp255STj502p0TV0NZ/mt0V9l9j4403oaMrz6mG29L3ZRojIrCdKKfOzP98L60kgcPX0eAfnnzLaOmprSdmlKdp1R7T+SnGRs3HjF4PJB+UfFfaw3AFxbXq99TCkAbnYyFJVX4fM1mxHhyZxn5mCLn5OT2RIoBRkCnQXEH9iJ+BqPh+QeVO1LBlzY8TMuBRSoijo0Ivcy1Xfi34cTEViQUK82bHG72IIL6arjT2riCWxMMU+bDvJCjEpQXSg+4joSmf9u0rlczW0SbN8ffXAMQ/L+hCNisf1EEsJLE83TKTKoqXxGBAH491T1eVfBVWzcmKz1vahWn+/Fh9oDyxJkOHo1D/d9shWPdFZC14zcxtRUtoulpZcAVUo7OMsEnNi3o/5fQ4ISfa98jKDCBJTbeWK334Mo37bHpCaa+jYpLTWuap/JAaK9e/fiwIEDcHDQnqMZFhaGtLQ0U5uzigULFuDZZ58VbxcWFqJNmzYYPXo0PDzMX9pOLpdj69atGDVqFOzt7c3efnOiUAr45VgqSqrOw8leikcmjak99UJQQrrlZciqg0OKIS+g9ZDn0dqEI4bebVI1EsIX++FccB1jvZKhHPys7kYsYGCpHF8v3Ymscgn6DbvNMpVFbFBT/IxsOJ2BqiNnEO7rgkei82C3dhlqTtNwkuehT/IyKCatgND5jgavU5L4N2RbXoakSHXF/14ALzj6YFnRwxg79nmzJxptitulqSiXK/Dc4R0ABMwePxytvZ31Li9J/BuyP8y3j+0pP4M/4zNQ5t0O4+KMTOZPOtny52T32rMA0nF7zzCd21lyzQO4ttxgO1HpvyKy4jiUnSdA2WUCEBDVKMGi4JR8rP/qCFIrHBEXN9yk6jW2vF2sLa+0EiUHdwEApt85Gi4OJv8c0Em6fRFkygooA2PQc/IrtfaROreJcjSEz9fDreA6Jsr2YqdiHMaNG2aWPlHdmtpn5Fx6IQoOHYKLgwzz7hsJp8v/mvV7sfe1PMz+4TgSC4B/C1rhk8kxVhlh29S2i6VtOJ0BnD6DyBBv3H5733q3I93+OmSFJyHYOcFu+m+IDelp9HObyzZRz5oyxORvBKVSCYWOYVipqalwdzdviWdDAgMDUVlZifz8fK1RRDdu3EBgYN3TcxwdHeHoWHtUiL29vUU3uqXbb+o2nc3A4g0JyChQleoulysx+uP9WDQ+EnHR1QmAqyqBv+YAZ/8AIAHG/Q+yvo+gvgNBdW4Te3vgtkXAHw9BdvBTyPo8CLgZWUmtgfw97RER4IaLWcU4nVaE0VEtZ5oZ0LQ+I5sTVPlb7ujaCvbbnoCuHB4SCAAksNv6ChA1oWHTzRLWA3/MrrWeQOTizYp3kXwwHO2HTa1/+3o0pe3SVMSnFaFKKcDf3RFh/u76g3tKBbD1ZZhzH7stMgh/xmdgV1I2XhsfXXt9NlLuvCmxtc+JIAjYeykHADCic6DuvrUbCngEA/qmmdk5qaZw5FyCbP8HkO3/APBpB0TeBUTeCQTFWCxY1D3UF872MuSVynEtv6Je+SdsbbvYgtQC1fSyIE8neLrqD04bregGcOw7AIA09lVIHeq+wFV7m9irKpptXoCHZP/il8IRyC9XGs5BSWbRVD4jey6pRgkN7uAHNyd7s38v9u8QgG9m9sHs749iW+JNLFiXgA/u6261/I5NZbtY2uVs1aiXjoEe9X8/Tq4CDi0DAEju/Ax2Yf3q1UxT3ybG9t3ksOjo0aPx0UcfibclEgmKi4uxaNEijBs3ztTmGqRXr16wt7fH9u23phVduHAB169fx4ABLFXXlGw6m4E5q06IwSG1zIJyzFl1ApvOZgAVxcDP96uCQ1J7YNI3QF8LTWuMmggEdQcqi4Dd/7XMOurQu7pyy7FrLO1rq0oqqrCrujLQJL8U/T+uqnN44POBwI93q6oCrZsL/PsisONNYN9HwNFvVOU2EzeqquilnwSyLwFFmar9XlFVZyJZ9XmL775FVkkkS/Vzovrz3aONl+GRX9cOGLeP/f0McOoX4Mou4GYSUFFU5zOGdPSDnVSCyzdLcLV6bj8Amyx3TvWTkFGIm0UVcLaXoU94HflcpDIg7p06WpCo/k38GnjhCjDxG1VCT5kjkHsF2PcB8NUw4JPuqsIOacdVc0D0USqA5L3Amd9V/xs4ZtnLpOgZ6gUAOJzM4g3mcuWmBRJU7/sQqCoDWvepXxXYnjMARw90kKZjuPQUy91TLerCCiO7BBj/vXjtgEnrGNTBD8un9YSdVIK/4tPxyp9nIBg6rpFFqUvcd6pvMZir+4ENT6v+HvYS0PUe83SsGTN5BNH777+PMWPGIDIyEuXl5Zg6dSouXrwIPz8//Pzzzya1VVxcjEuXLom3k5OTER8fDx8fH7Rt2xa5ubm4fv060tNVB4ALFy4AUI0cCgwMhKenJx566CE8++yz8PHxgYeHB+bPn48BAwbUmaCabI9CKWDxhoQ6a6hIAHy4/jDG+H8KSdoxwN4FuH8V0GGk5TollQKj3wB+GA8cXwH0exzw62C59WnoE+aNn49cZyUzG7Y9MQsVVUqE+7ki1OGmcU/KTlT9qxcJdFcZUpFKAC95FqqS98Ou/dB6roMak7qCWc9QIxKxFt8wstEfVP80ObgDHkGAe5BqpIi76m8PjyBMDs7B1lQZdiakY/bQCJstd071s0ujvL2jnZ4r6F5tdd/vEawKHqm3ebd7Vf8qioCLW4Bz64CLW4G8q8D+j1X/PNuqlo+8C2jdW3tkUcJ6VaBb80edRzAQ91+9+1XfMF/sv5SDI8m5mNE/1KjXTvolZ6t+cJktQFSYLo4ewoiX6zeizNEd6DUTOPApHpZtxPG0+zGic/3LmFPzklVUjlMp+QCAEZ0CgGtHjHuisd+fGkZ2aYWPJ/fA/J9P4JejKXCyl+HV27vg6NU8ZBWVI8DdCX3DfVg5tpFcrC5x3zGwHjOVcq8Av04HlHIg6m5g2Itm7l3zZHKAqHXr1jh16hR++eUXnD59GsXFxXjooYcwbdo0ODubNkz12LFjGDFihHhbnRdo5syZ+P7777F+/XrMnj1bfHzy5MkAgEWLFuH1118HAHz44YeQSqWYNGkSKioqMGbMGHz++eemviyyoiPJubVGDmkKRA6Wlb8DSVoa4OwNTPtddeJpaeFDgYgxwMXNwPbXVUGpRtA7VDWC6GxaAcrlikYtf03G+fdMBgBgbHQgJO5Gbp8RrwCebYDKYtUPrIqiOv4u1r4fAvQFhzRduHQRUQwQ2TxBEHDiej4AoGdbIwJEbkb+SGofqxqRUZRRPfqsUDUKMrsIyE6qtfibAN50ApQ7pMCRAKA0B3WXO5eoyp13vp3TzZqI3WJ5ewNTpE9Wf7dFTQJ6zzY8tdDRHYiepPpXWaIKEiWsA5I2AwXXgYPLVP88WlcHi+5UTT/6bSbqE3zsG676TjyanAtBEMyea60lEisC+dfzinxNez9QlYxuOwBoN8Lw8nXp9ziUBz/HINk5bL9yHBgZYZ7+UZO3K1F1POvW2hMBHk6qY5QxjF2uhtu7BaFMHoP//HYK3x+4it+Op6Ck4taIxyBPJ+0UGGQRpZVVuJ6rmmLWydQpxuUFwE+TgbJcILgncNdy1QAAMqheWens7Owwffr0Bq98+PDheoftzZo1C7NmzdLbhpOTEz777DN89tlnDe4PWUdW0a3gkBRK9JUmIgD5yIIXcgR3fO/wLkIkOShzDoTz7L+AgM6N17lRi4FLW4HzG4Drh4G29Zuzaoo2Ps4IcHdEVlEFTqXko187X8NPokZTWlmFnRdUw5zHdQ0CgiIM5PCQqB4f8pzpP6wFAZCXApe2VY/s0G9HqgRRpq2BrCAtvww3iypgJ5WgW2tP/QsrlarpOHpV72PTftfexyqKVIGiwnRV0KjG/1X5aUBxFuwkSqC47sqfKhrD9cOHGPMyyYoKy+U4Xj2NcVhHPQFGeRlw5jfV3z1nmL5tHVyBqLtU/ypLgcvbVSOLkjYBhanAoc9V/yRS1Df42KOtF+xlEmQWliMltwxtfS1XbbalUE8xa2eOEUT5KbdGLtZ39JCaZ2vkhd0O3+T16JPxE4DJDe8fNQvbE1UjgWLVo8pCBxrOn6YOdNfTPb1a49jVXPxyVDs4BNxKgbF8ek8GiSzoUlYxBAHwc3OAr5sJOckUVcBvs4HsC4B7MDDlZ8DeTPnWWoB6BYguXLiATz/9FOfPnwcAdOnSBfPmzUPnzo34w52ajQB3JwDAGOkRLLJfiWDJralVCkECmUTAZWUQ8seuQa/GDA4BQEAXoMd04MRKYMurwENbLF65RSKRoE+YD/45k4Fj1/IYILIxOxKzUC5XItTXBVHBHqr9ofN44MiXOpau3lfi3qnfqAuJRPUDrPMd1SdCGdD1I0uABBmCD764GogHK6rg6mieijRkGerRQ5HBHvpHCCoVwPr5QPxqjTtrTjfUs485uqv++em+Cm8HIPbdbSjOzcSqnknomPCx4c7XY7g+Nb4Dl7KhUApo5+eqP6CS+I/qKqtnGyC8gVWjHFyALuNV/+TlwOUdqpFF5zeoAt110h98dLKXoVtrLxy/lofDyTkMEDWQUimII4jMMsVs7/uAohIIG6Iaed1ALsOeBJLX4zbFPuSkX4VvcFjD+0hNWkWVAnsvZgMAbutSPSJIKgNGvwX8PrvuJ5YXABf+BbrUr4qsQimIIzFrUqfAWLwhAaMiAzndzEIuZFZPLzN19NDml1UXLOxdgKm/AO4tq+hPQ5k8zuqPP/5AdHQ0jh8/jpiYGMTExODEiRPo2rUr/vjjD0v0kZq5vuE+mOwWj+X2HyEQ2nl3ZBIBggD85DAJ3aO7WqeDw19WHWBSj6hOdBtB7zDVtBPmIbI9G6unl43rGqSa6iAvU/3IAgBHD+2FPYLNk7dFKlPl6QAgBgRqWO70MErkArYm8Ae8rVMnqNY7vUxRBax9VBUckshUCYLv+1GVT0hTA/ex4V2CkQVv7ChtZ9wT6jlcnxqXOv/QMIPTy35U/d99qnmH3ts7AZ3HARO/Am7/wLjn6Ak+qqeZHWGi6gbLKCxHRZUS9jIJWns38Ip63rVb+9CIlxveOQDOYX1wShYFe4kCxXs5O4CAQ1dyUVqpQCsPR9WFOTV5mep/SY1jl3sQ4N8ZqCoHfp0G7HhLNRrXRIZSYAgAMgrKeVyyoKQb9QgQHf3m1kXbu79UVdokk5h8mfmFF17AggULsGTJEq37Fy1ahBdeeAGTJk0yW+eoZZBBiUX2KwH5rYpMmgQA/7H/HTIsAepd0L4BPIKAAfOAPe8C214HOo0FZJYtcajOQ3T8Wh6USgFSXpmwCaWVVdhRXUXj9q7VP9QPf6maSuHRGph7WFWBzBLlwSMnqAIBNZO8Su0guWcFfNI6A9svYl18Gu7qEWKedZJFnKxOUN2jrZfuBaoqVRXEzq8HpHbAPd+p8rgAQOfbUXVlD+L3bkb3IWNg125og/axkV0C8N3+ZHyXEoTHPIIhqWOUmjiNrQHD9alxCIJwK0DUUU+AKP86cGW36u/uUy3XIc/Wxi2nJ/jYN9wHy3dd5kUTM7hyU5Wguq2PC+xkDQwK7nkXUFap8g6Z8dhwNGgaYlJfRquLPwMViwBHM+VKoiZpx/lb08vEHGRKpSoxPgDEvqaqnqd57iUogS2vAYeXq/bTjFOqgLWzl9Hr1UyBYY7lyHQXbqiOV0YHiC7vBDa+oPp75EIW1qgnk78ZMjIy8MADtXNhTJ8+HRkZGWbpFLUw1w7AuSxTZ3AIUAWNnMsyTS5VaVaDngRc/YHcy8Dx7y2+ui5B7nBxkKGovApJWXWXqqbGtTPxJsrlSrT1qZ5eVpqrSs4JALGvqk5iw4eoSmiGDzF/Mt/ICcDTZ4GZfwPjPwYgVZ2cB3XDnd2DAQB7L2Yju7jCvOslsymX/5+9846P6yzT9nVmRr13yZItWZKbLPdektiOnTghPZQAIWFhs0tYYAO7H7C7LNmwwBKWXQIsPZSEhIRAIDjNieOSxL3Ktix3FcvqvZcp5/vjPWfUZqSRNH3e6/eDOT7njPQoGs2c87zPfd9WztV2Ak4miMz98NInRHPIGC7M8fXmEIDBiJq7kZrkdai5G6f9GluVl0xshInGHgtVq76u7XXyZjxVqaTEq1xq6Ka+s58Ik4G140mUS14AVCELSsrzXEG6V4iz1xUKxGeP22BYkZuEQYHKll4aOuXN2HRwm0F1y1XtNYTbpod0lHnbKbdlEmnpGiWxlYQaqqra4+23zB/WRL60U/jLRCTAqr8de+1lDIPbviMmSEyRInDml1ug0fU0Wd0Cw13nSSbPJU1iNi/TyfuVzQoV7wuvxpIXhF+naoXFD8DGL3mx0uBi0g2iTZs28f7774/Zv3//fm64QRpXSqaAq54WvvS+iIiDTV8V2/u+A/2dHv12JqPBfvN4rLLNo99L4jq6vOy2RZliFeu9/4aBDshYBIs/7J0iDEZxAbTik0M3VJfeJj8tlsU5CVhtKq+fkc16f6W0pgOLTSU1NmKsvGOwF178qLjwNUUKU8V5t3m0nnCTgRvmpALwl/4VYkpttIzNFCkj7gOIfZqJ/rqCFOceVzYblGjpZcs+4dmCXJDITtR8jI8MY0GWkJZIOcf0cJtB9Xv/LW7ECrfBzNVuqGyIRTlJ/Nqqvfcd/om4CZSEJJcbu7ne1keEycDGwtShAweeEo+rPgWR8Q6fC8CSB+BTbwmftdar8PTNULbDpe+9enYyWQmR47W2yUqItEtgJe6lo89MvbYgMMfRBFHZDniqGJ65Q0xdv/IZkd6aMgfu+qHHPWODGZcaRDt27LD/76677uIrX/kKn/vc53juued47rnn+NznPsdXv/pV7r33Xk/XKwlGPBxV6TaWPyzedHqbh8ZaPYjuQ3RcjtT7BX2D1pHystYKOPpLcXDbE76ZrJh7q3i8tBOAu5cKadkrJTXer0XiEiev6f5DiSPjuge64fcfFsa+YdHw8T9C4Vav1KSnwuy50DhySm3zv4kTDGEeb1RJ3IduqjquvKzyfSExi0gQptKeRpfIjm4+gmgOudB8lD5E7qHcHQbVzZfhzB/E9uZ/cUNVI1k4I56XbTfSpsZCW+WQz58k5HhHk5etL0ghKly7zqo6BNVHwBgBax6d+IvMWAp/t08YqQ92iynd3f85YePRaFB4/M4iwPn84+N3FkmDag9xWfMfmpEQSXzkKGuPsh1iWshRil3LFbj0lhcqDF5cahDdc8899v999rOfpbm5mZ/85Cc89NBDPPTQQ/zkJz+hqamJf/iHf/B0vZJgJHc9TUoKNke2F4Ar4+dewRgGW/9DbB/68fjRmm5A9yE6LieI/IJ9FxvpM1vJSYpiUXYC7PlPsJmhYAsU3uybouZuF4+V78NAF3cuycKgwKlr7VxrGS81SOIrTla1A7A8d5i8rL8Tnrtf/B7D4+DBP7slDchVNs0TDaKzNR00dvYPTand8E8QlQyDXVBzwmv1SKZO94DF7tOj/14dckqbHlp0v/eif4c3H+//FcxcI/a3Vbj09DWyQeQWKpqFp8e0JGbvPik8XubeBtkr3FTZEDERJrLTUnjOqjXJD0mz6lBlz3lNXrZg2CLx/u+Lx6UfhTgXF49jUuETr8Ba7V71/e/B7z8CfeNfY28vzuKnDy4nM2GkjMxoUPjJx2XEvSe5qBtUZ46aHrJZhR+nQ79EjZ1flZOH08ClBpHNZnPpf1ar/EVIJk91+wBfG3A24j7NmHB3M/8DMGsdWPpg77c8+q2WzkrEaFCoae+jtr3Po99LMjGva/KyDyzKQqk9CaUvAwps+8b4T/QkqXMgabaIGC7fR3pcJBu0Eey/yikiv0NV1WETRFqDqK8NfncPVB+GyAR46K+Qu86rdaXFRbBkZiIAezV5EiDec/M3ie0ru71ak2RqHLzSjNmqMis5mjxncfB97cLjCmDZg16rDRhqPi76INyoGYmWvACDPRM+dVWeaBBdbOiivXfQk1UGLf1mK9fbxPXElCeIGi8Ivw/wyPSQzqLsBJ613IJFCRPvj9ePe+x7SfyT1p5B+2emPulKQ5nwE0KB9V+Y3Bc0mmD7t+G+X4IpCq7sgl9sFl9zHLYXZ7H/K1t44ZG1fPeDizEawGpTxzYuJG7F7j80Wl5WdXCCRXoVOmt8610b4Lgx01QimRq7yhp4y7aal+McpKi4KybcXSgKbPtPsX3qeWg457FvFRthokjzXDheJaeIfEm/eUhedntxJrytmfku+ShkLvJdYYoyJP1xIDNT1XFWVyRep6a9j8auAUwGRUyh9bTAM3eK6ZyoZHhoB+S4fzXeFW7WLr53n28ceUCfjru6x8sVSaaCLi/bNC9tpIRxOKUvi/jn9CKYsdyL1Y2iYIswxx7o0Bru45MSG0FBmmhqSG++qXGttRdVhbgIE6mx4VP7Iu9+B1CFNNGD8dGLshNoIpEjMVvEjkP/57HvJfFP3r3UiE2FBVnxZCdqk466xUPRXZBSMLUvvPjD8Om3IGGWmGB8eiuce2XcpxgNCusKUvjwypmsLxALcbvP+9AbNQS46CziPhC8awOcKTWIenp6eOONN/jZz37GD3/4wxH/k0gmy64y8Qecm6GlreTdIMbPH34NHjvrP80hnZmroOgeQIVdj3v0W0kfIv9g38VGegetZCdGsbj3EFTtF9r3Lf/m69KG+RC9DTYbty7MIMJk4GpTjz0tS+IfnLrWDoiL3ajBFvjtB6D+rEhI/ORrwifBR+irs/uvNDNgGTYNnL9ZPNaeFKl9Er9leLz9pnnj+A/p8rKlH/etiafBACv+Rmwf+5VLT1k9W1wnHK1o8VRVQY3doDotxnkDcTzqS+HcX8T2Js9NDwEsykkA4Cf9mpS67K/QVuXR7ynxL/QFC30Bg/ZqKNWm1zY8Nr0vnrVE+BLNvgnMPfDHh+Gd/3BJluR0QUXiNlRV5WK9kwZRoHjXBjCTbhCdOnWKwsJCPvrRj/K5z32Ob37zmzz22GP867/+K0899ZQHSpQEM+29gxzVmh/FlIudc27xXEy4u7j568K49couKN/nsW+j+xDJ1VLf8vrZegA+UJyK8s5/iJ1rH4WEHN8VpTNrvfCt6WmEulPERYaxVdPqv3JKysz8CX1U/qZMM/zmdmg6D7GZ8Mk3IGOhT2tbOCOejPgIegetHCkf1ghKyIa0BcJvpOJd3xUomZCrTT3UtPcRbhwn3r6hTDT7DCZY/BHvFuiIZZ8Qzfa6Epd8rqQP0fQo1/yHpiwv2/df4nHhvR5/zyrKikdR4EB3BoO5N4n3oCM/8+j3lPgPZqvNPhG5ZYHWIDr0Y7BZRFMn2w3TjzEpwvNv/efFv/d/H57/0ISLIVvmi2us41VtdPSap1+HZAzN3YO09ZpRFChMH+WXlrteKEzGy5fzB+/aAGbSDaIvfvGL3HnnnbS1tREVFcXhw4epqqpixYoVfO973/NEjZIgZu/FRqw2lXkZcUQ3nxU7ZyzzbVGukFIAqz4ttt/+dxEZ7AH0CaKL9Z109ssPIV/Qb7ayRxsjfjBiPzRfFHKgG77k48o0TOFQqI3ga6kNdy+dAcCO07VYnbu/S7zMyWvtZNPEoxWfh5bLEJ8Df/MGpM31dWkoisLmecPSzIZToL2+pA+RX6PH26/JTyY63OT4pJLnxePc7RA7zpSRt4hJgYX3iO1jv57wdD3JrLS2k54BiwcLC04qmvQEsykYVNeWwIXXAAVu+qpb63JETISJAs1IuyzvIbHz5LPQ3+Hx7y3xPccqW+nqt5ASE86SnETRtDn5jDi48TH3fSOjCW75plAumKLg6m745WYxLeeEWSnRzEmPxWpT2XdJThF5Aj3BLDc5eii9TsdghO1P4tik2s+8awOUSTeISkpK+Kd/+icMBgNGo5GBgQFmzpzJd7/7Xf71X//VEzVKghhdXnbXnHDouCZ2elDT7lZu/DJExEP9GTj7R498i4z4SGYlR2NTh+QpEu/y7qUmegatFCbAzNNPiZ03fVkYCvsLepqZ5kO0aV46CVFhNHYNcLhcSjH8gX6zle7aS/wh4j+J6a2GxFzRHJqqh4IH0GVmuy80jPSv0huQV/eC9LXyWyaMt7cMwukXxfYyZ8EQPmCltthS+vKEiUIzEqPISYrCahsyfJe4TkXzkMRs0uz7jnhc9EFIn+/GqpyzOFt8zr5rWSwmGQe74cQzXvneEt+ip5dtnp8uYuSP/gLMvZC5eEj67E4WfRD+dpf4bG6rhF9tG9cb7WZtUnvMgorELTj1H9Ipuguylo7d72/etQHKpBtEYWFhGAziaenp6Vy7Jm7qExISqK6udm91kqBmwGLlXc0v4bYUIeEhZQ5ExvuwqkkQkwIbvyi29/wnmPs98m2kD5FveUNLL/v3lD0o3Q3CVFW/ofEXCrcBCtSdhs5awk0Gbl8koldlmpl/cLnsJM+ZvkGO0oyaXAB/8yYk5fq6rBFsKEwl3GSgurWPq03dQwdmrRcyoM7r0HzJdwVKnNI7aLFLA536D11+C3qbhS9D4VYvVjcBM1dDRrFIBy15YcLTV+dJmdlUKW/WJ4gm2SCqOQGX3gTFADd9xQOVOaZYaxCdre2AdVo8+ZGfg1VOVAc7euPl5vnpIuXwyM/FgY2Pec47LXOR8CXK3yyaUX/6lKYSsIr/VbwvEvwq3ufmeULGu+9iExarZ1QEocwlrUE0z1lSnLl/6Hrkzh/5t3dtADLpBtGyZcs4duwYADfddBNf//rXef7553nssccoLi52e4GS4OXg1RZ6Bq1kxEeQN3BR7AwEedlw1j4qdK4d1XD05x75FkM+RPJi2Nv0m63sPt9IGu1sbPy92Hnz40LW5U/EpkHOSrGtyczu0WRmb56tp988semixIM0nqfg9Y+QpbRSG5aL8jdvCG8fPyMmwmT3rhlhvhkePaTll2lmfsnh8hYGrTayE6Psspwx6ObUSz4qZBX+gqLAyk+J7eO/nnBKTZeZHZENoknR3jtIa88gMIUG0V7Ne2jxA5A6x82VOUc3qj5b0wGLPiQM/TuvC8NqSdBS3tRNeXMPYUaFjXNSxXtXX6tYoFtwt2e/eXQyPPjykAn2wR/Cz2+E7xfBM3fAy5+GZ+5g5Ss3cl/USTr6zJyQScNux6lBtc61Q6KJF5sJyz/h/961AcakG0Tf/va3ycoSK9Pf+ta3SEpK4tFHH6WpqYlf/OIXbi9QErzo8rKtCzIw1JWIne4wnfMmYVGw5Wti+73/8UjKzyptgqikuh2zXKXwKu9daqJ7wMK/RP8Vo6UXslcIc05/xC4zEw2iVXnJzEiIpGvAwl45Au076s7Abz9A9GALZbZc3lr1a4jL9HVVTtHTWaQPUWChT+Pe5CzevrMOLr8ttpc96MXKXGTxhyE8VnhzVbw37ql6g6ikun1k4p5kXHR5WWZ8JDERk2gQVh8VgRyKEW76fx6qzjFFWfEYFGjoHKCxD1j9d+LAof+TctcgRv/8WTM7hbgw4OD/iQPrP++d5rbBCNuegA/+Bozh0FAKXfUjTlE66/gf9XvcajgqZWZuRlVVLjWIKWanE0RX3hGPhVt9m8YZpEy6QbRy5Uo2bxbaz/T0dHbu3ElnZycnTpxgyZIA8Y6R+BybTbU3iLYVZUDtKXEg0CaIQCTBZBTDQAe8536j9oK0WBKjw+g322RsuZd542wdBUoN99i0D6Jt/+m/H0R6g6h8H5j7MBgU7tSmiF6RMjPfUHMCnrkTelsoUwr46OC/UVQ429dVjYvuQ3SsspUXj17j0NUWYXReeLM4oXI/WAZ8WKHEEfs0/6FNzvyHzrwoUqBmrvHqBIjLRMQNpaodHz/yfnZqDKmxEQxabJy5Lg2LXaW8aYrysr3fEo9LPwbJ+W6uanyGG1WfrekQk2amSHHNWHXQq7VIvIc93n5BOpz7i/AojUmDpR/3biFFd0NkopODKqDweNjv2FNW68Wigp/ajn66ByyYDAp5KU7er+wNopu9V1gIMekGkUTiDk5fb6epa4DYCBPr0gehq05o2zMX+bq0yaOvNIAw0WurdO+XNyiszJU+RN6m32zlnfONfMX0IgasMO92yNvg67Kck7FQpGJZ+oROHrhnqZAx7b3QJKNYPc0ofwIqD8Kz90B/OwNZK/lI37/QY4hjcU6irysdl3O1HZgMCjYVvvrns3z0l4fZ+OQedjYmiVFuS58Y7Zb4DZXNPVS19BJmVFhfmDr2BFUdkpf54/SQjp4MeuH1Mav1w1EUhdWzxWei9CFyHX2CaPZkDKorD4hFB4MJbvTu9JDOouxhMrOYVCGRBBF5Lgk6OvrMdkuFLfPSYP9T4sCaz4ipfW9SdRB6nE8HKajMUFpIaT1Jpfb3JZk+lzR5WX5aDOEmB62K9mpouiDuG/M3ebe4EMGlBtGyZctYvny5S/+TSFxBnx66aW4aEY1avH3afAifQrKGP1C4VZja2cyw+xtu//IrpA+R19l/uZkFg6XcYjyBqhhh63/4uqTxURSYe6vY1tLMFmTFMy8jjkGrjTdL63xYXJBTtgOeKh7hT8Bvb4eBTsjdyO4VP6OLaBZkxY+Na/UjdpbW8ehzJ7HYRko36jv6efT5U9SkrBU7pA+RX6HH26/MTSbWkXSo+ii0XIGwaP+VyIJocs9cCzaLiDMfB92oWvoQuY49wWwyE0T7NO+hZZ/wmam+7kNUWqNNi639rHi8+Aa0XPVJTRLP8d6lJiw2lcL0WHJbD0LjOSE/XeWDcJDuBpdOS6ddyszcyKWJEsyualL37JXCM0ridlwSct5zzz0eLkMSaoyUl4mb2YCUlw1n2zeEkV3pyyJtI3uF27607kN0oqoNVVUde0xI3MobZ2r51zBhTK0sfwjS5vm4IheYu13IMy69JaYGFIW7l83guzsv8kpJDQ+snuXrCoOPsh3w0kOIcfPhaP9e8UmOVQlj2GWzEr1Z2aSw2lSeeLVszE8B+iA9/KImjycAruwR73cSv0CXl93kLL3s1O/E48J7hZTLn1n1aag+DCd+Cxu/5NRvZPVsYaZ+orIVi9WGySgH4idCTyZ0OeK+4j2ofF94sNz4zx6sbHxGTBABpM0Vn3WXdoopojv+12e1SdzPiPSyA/8kdq74JEQleb+Y2AyXTmskkd0XGvjURv+WkAcKesT9PGcNouH+QxKP4FKD6PHHH7dvP/zww3zqU5/ipptu8lhRkuCmsrmHy43dGA0Km+elw18C2H9oOFmLYckDcPoFePvr8MnX3OZXsygngXCTgebuQSpbeifvISCZFAMWK8r5v7LMcAWrKRrjpn/xdUmuMfsGMEWJlJeGUshcxF1LRIPoSEUrdR19ZCV4eUQ7mLFZYedXGNsc0lHgnccpCf8ZAMtn+eAC10WOVrRS19Hv9LgKvNo9nycigYaz0NUAca5dPEs8R7/ZyuHyFsBJvP1At/DwAP+Wl+kU3Q07vwqdNXD5LZj/AYenzcuMIz7SRGe/hfN1XfYpE4ljbDaVyhZ9gshJyt1wVBX2fltsr/gkJOR4rrgJKJoxzKi6s5/0+EixCHdpJ5T8XgSFyCmCoMBqU9mrTUTelVoLRw+AIUz8vn1B7nqInyFM/h1+zitYYrM42jwfQ3krXf1m4iLDvF1l0GGfIHJkUG01Q/m7YnuObBB5ikkvuXR0dLBt2zbmzJnDt7/9bWprpTGXZHLo00Nr85NJiDJBzUlxINAbRACb/w2MEVC1354m5Q4iTEaWaBfAUmbmeQ5cqOXzqpgeMqz/fODcCIdFDemxNZlZTlI0q/OSUVV49bR8v3YrVQehc7z/pip01hBdfxTw7wZRY5fz5pBOK/G0JxSJf5Tv9XBFElc4UtFKv9lGZnyk49XWsr/CYLcwF561zvsFThZTxFAj65hzs2qjQWGVXWbW4o3KApr6zn76zTZMBoWcJBcWCa7uEV5jxggxyeVDosNHGVUD5N0AmYuFJ9oEpuaSwOHUtTbae80kRIWx4Kr2e138EdGk8QUGI2x/UvuH4wVf0+1Pkpcah8Wm8v7lZu/VFqRYbSqX9QQzR59p148J+X50CmQFwX2jnzLpBtErr7xCTU0Njz76KH/4wx/Izc3ltttu449//CNmszRBlUyMXV62IAM6rkNvszBAzCj2cWVuIHEmrH1UbL/973B135BprW16cby6D5E0qvY8He//nDxDA92mZJQNX/B1OZPD7kM01KC8S08zOyUbRG7FRX+CFFsbqbHhzEz23+mt9LhIl87ryblRbEgfIr/AHm8/10m8/XBz6kCRJq/4G0ARPhOt5U5PW6XF3Uuj6onRE8xmpURPLMcbPj206tMQn+Xh6iZGnxCzN4gUBdZ9Tmwf/aVMVgwS3tHSyz6S14vh4uuAAr6+Biu6Cz78rIO/AwXufxqK7hJpa8A75127JpA451prLwMWG5FhBmYmR489QZeXFWwBg5QWe4op/ZdNS0vjS1/6EqdPn+bIkSMUFhby0EMPMWPGDL74xS9y+fJld9cpCRJaewY5XiUu5rYOj7dPL4Iw125Q/J6NXxSGei2X4Hd3D5nWPlUs/EqmiO5DdLyqzV2VShww0N3GpobfAtC08p8gwoVxfH9CbxBdPw7d4ubxA4uyMBkUyuo6uayN7krcwCT8CZbNSvJr77DVs5PJSoh0skYq1k6zEiLJXKFJfq7uAZvNW+VJnLDvkrihcigva74C1w6KpBc9+SkQSJ49FF18/DdOT1s9eyi8wWZzJvOUAFQ0a/5DrsjLLu+CmuNCrrzhMc8W5iJ2H6LrHUM7i++DuBmiUX/2Tz6qTOJO9lwQDZaPW18RO+Z/wD/8H4vugsdK4eHX4L6nIToVUIXxP7BlvrgW2HexCat8L5oWF7UEs8L0WIwGB1ck0n/IK0yr9VZXV8euXbvYtWsXRqOR22+/nbNnz1JUVMT3v/99d9UoCSJ2n2/ApkJRVjw5SdFDDaJgkJfpVLwnRvpH01knzGyn2CRaoUXdlzf10NItV8s8Rf3r/0USXVSQTe7Wv/d1OZMnfgZkLQFUuLILgKSYcPsN5CslNT4sLsjQ/QnGaau0mtI4apvv1/IyEJKdx+8U8jFnP83jdxZhnLUWwmKgp0n4XEl8RnVrL+VNPRgNTuLtS54XjwU3+06iMVVWaolFp54Ds2P5Y/GMBKLCjLT1mrnS5OAzV2LnqjZBNKFBtarC3m+J7dV/6zfy6jFG1QDGMFijfUYf+rGoXRKwVLf2cqmhm2xDK7OuvyZ2+kmDEhBys9k3wOIPwaIPiX3nXwVgZV4ScZEmWnsGKamWi7jTYdwEs64GqDsttgu2eLGq0GPSDSKz2czLL7/MHXfcQW5uLn/84x957LHHqK2t5ZlnnuGdd97hpZde4hvfkAknkrGMSC+D4GsQ2U1rHaFdvOz86pTkZonR4czNEKt/corIQ3RcZ8YFsWJ9KP/zGEwBajY4d7t41HyIAO5emg3AX0tqUeWFtHsY4U8wGgUVeJJPYsPg1wlmOtuLs/jpg8vJTBg5zRkbYeKnDy5ne3EWmMLFRTIMRc1KfIKeXrZiVhIJUaPeq2xWEZgAgWFOPZq5t0J8DvS1Ch8lB4SbDCzPTQSkzGwi9Ih7pwEXNquQwr/9NagrAVO0X92c60bVjV3CqNrOiodFw7rxnPRFC3D09LKvJu1FsZkhdwPMXOXjqpyw4E7xePENsJoJMxrYNE/IzHafl3H302HcBDNd2p61BGLTvVhV6DHpBlFWVhaPPPIIubm5HD16lOPHj/OZz3yG+Ph4+zmbN28mMTHRnXVKgoB+s9Vu4LatKEOs9gRbg8hF01qqDk7py0sfIs9i3f0twtRBjtjmU7DhQ74uZ+roMrMre8AiIta3LsggJtzI9bY+TsgGo/sougu2f2fs/vgZtN7xNH/oXobRoLA4QFKWthdnsf8rW3jhkbU8sGomAPMzY0VzSEdfuZM+RD7lXS3tx2G8/dU90FUHUckw7zYvV+YGDEaRngXjmhDrRtWyQTQ+eoMo31GDqGyHkMA/cwcc+j+xz2CY8nWKJ4gON1GYPsqoGkT0+fJPiO1DP/ZBZRJ3sftCI/F0c2v/m2KHHzUoxzBrrZCZ9bdD5X4Abp4vGhZ6o0syNS6Pl2Am5WVeY9INou9///vU1tby4x//mKVLlzo8JzExkYqKiunWJgky9l9ups9sJTsxioUz4qGtUry5GsOFB1Ew4KJprcvnjUL3ITpWKW/w3U79WQxnxIr7T8I+ycrZKT4uaBpkLYOYdBjsgqoDAESFG7m1OBMQU0QSNxKtvVZS58P9vxI+BY+d5VD4egAWZMURHW7yYYGTw2hQWFeQwqObCgA4fb2DvsFhU48Fmj/MtcMw2OODCiUDFisHr4r0rpvmOmgQnfqdeFz8EZEMFogsf0gEWFQfgfqzDk9ZPcyoWk5GOmbAYuV6Wy8As0dLzMp2COn76IWtwZ5pSeI9QbEmMzsz3IcIYM1nhM/WlXeg8bwPKpNMl+4BC4evtvCg8R3Crb2QvhDmbPN1Wc4xGIU/EthlZpvmpWFQ4EJ9l/3vTTI5Bi02u6H+mAkim3VoUarQj18bQcKkG0Sf+MQniIwMEjNhiVfR5WVbF6QLs9ZaLd4+o1jIFoIBF01rXT5vFPpq6bnaUTdskumz63EUVF6zriV38Q2OzfECBYMB5t4itoelmd2jycxeP1uH2SoNht1GvaaJn70RFn1QSLAMRk5WtQP+HW8/HrOSo8lOjMJsVe3hAgCkFEDCLLAOQuUB3xUYwhyvbKN30EpaXIRYcBlOTwtceENsL/u494tzF3EZMP8Ose0k8n7ZzCTCjAr1nf1Ut/Z5sbjA4VpLLzZVSEXTYoc1C+2SeEeNtelJ4j2B7kNUWjOqQZQ8e+h1ok9ASQKK/ZebUaz9/G2Ydr2y8TH/T11ccJd4vPAa2GwkRoezUpvyl1NEU6OiuQeLTSUuwkTWKKk7tSVCchyRADl+Kj0MImQ+nMQrWG0quy/o/kNiisEuL8te7qOqPIALprXEZ4vzpkBOUhQZ8RGYrSqnr7dPtUrJaK7ugau7MWPku5aPcFux72N9p43dh+hNu3nn+oIUUmPDae0Z5P3LTT4sLsioOyMeMxeP2H3ympj0C9QGkaIorM0X01H6tIp2AAp1mZn0IfIF+zR52Y1zHMTbn30JbGbIWgqZi7xfnDtZpZlVn3kJ+jvHHI4KN7I4JxGAIxUtY45LoLx5yKB6xGvFw5J4d7N4dNT9cPTI+zMvQbe8OQ809lxo4EPGd0mmQyw+LLzP1yVNzOwbISJeKAKuHwNgiz3uXr4Gp8LFYfKyMZ9rurws/yYwBs5EdqAiG0QSr1BS3UZz9yBxkSbW5IsOO7Ul4jFY/IdglGmtoyaRKvxKDMYpfXlFUewrFNKHyE3YbPD21wH4nWUbvTGz7LKFgCZ/s5BvtlVC82UATEYDdywWaUavnJIyM7egqkOpGllL7Lv7zVbO1YobmUBtEAGsKxANokNXR918Sx8in/KuZlA9Jt5eVeGkJi8LRHPq0eTdAKlzwdwDZ/7g8JThcfeSseiSjTEG1R6WxLuboqwEu1F1Q+eoZLtZa8RUgXUQjv7SNwVKpoTNpvLu+XoeMb4udqz/XGA0AEzhQwtx54UUc6vWIDp8tYWeAYuvKgtYLtXrCWaxYw9K/yGvIhtEEq/wtiYv2zwvnTCjQdyUB2ODCIRp7YefhXgHUygG07T9llZKHyL3cvYlaDhLvyGGH1nuYXtxRmDLy3QiYsXNFYxIM7tnmZCZ7SprkBcw7qCjWnipGUyQvsC++1xtB2arSkpMODOTo3xX3zTRG0RnazroHv56mX2T8P1ovgTt1T6qLjSpbe/jUkM3BgVumDMq3r6uRCQ6GSOE3DHQURRY+SmxffzXDqPMV0uj6nGpaO4GHDSIPCyJdzdR4cYho+rRPkQA6/5BPB57GsxSbhgonKnpYFXf++QaGlGjkgOrsa2nmZ1/FVSVgrRYZiVHM2i1sf9Ks29rC0AuOou4722FmuNiu/BmL1cVmsgGkcQrjIm3b70qDHRNUZA6z4eVeYiiu+CxUmFWe/+v4OFXhbGrzQKvf8nhRa6r6D5EJ6+1YbVJU85pYe6D3f8JwM/Vu2kjntsXBYG8TMcuMxvyIVqSk0BeSjR9Zitvl9X7qLAgQpeXpS0YYQas+w8tm5U0dlQ6gMhOjGJWcjRWm8qx4TfgUYmQvVJsyykir6JPDy2dmUhi9Cj/vlPPiccFd4iEp2BgyUfFtUJjmTBGH8WKvCQUBSpbesdOlkiGEszSRq3Ke1gS7wkWZScCTmRm8++ExFnCp2T3f8LZP0HF+37joSRxzJ6yej5jEkbPypq/h3AHSXv+SuHN4r2pvQrqz6IoClvm63H3/jF5F0hcchZxX74XVJu4zkrI8UFloYdsEEk8ztWmbsqbeggzKkPj8Lr/UNbiwBglnQoGozCrXfRBoVX+wPfAFAkV74oLlykyPzOOmHAjXf0W+5upZIoc+Tl0XmcgOpOf9N1Camw4awI5vWw0ulH1tUPQJybOFEXhbs2sWsrM3IADeRnAqWrNfyg30csFuZ91mg/RofJRMjN9JU/6EHkV3X9o07z0kQfM/XD2j2I7kFbhJyIqERbdL7YdRN7HR4ZRlCWMuuUU0Vh0idmYiPsRkvjRaE2jaUjiPcGibPF7dtggMpqGpmYP/xhe/jQ8cwc8VexXaWySkbSUvk2xoRKLMQpW/52vy5kc4TFDn4NamtnWBWIhfM+FJmxyEddl+gatXGsV6W9jIu6vaNcYcnrIa8gGkcTj6NNDa/NTiIsMEzv1BlGwycvGIzkfbvxnsf3Wv9hv2CeLyWhg2axEAH69v4JDV1vkJNFU6G2F9/8XgL8mf4oBwrl1YWZwyMt0kvLEiotqHfqABe5eKnyI9l9pprl7wEfFBQn12gRR1iiD6gBPMBvOhD5E5fvkKr2XGLTYOHDFSbz9hdegvwMSZgoJYDCxUjOrLvsr9IyVbkgfIsd09Jpp6RkEIG90gwjEtPPGL47dHz9DSOWL7vJwhZNj0XhG1WU7oOT3Y/d31sFLD8kmkR9S39HPbe0vAmBe8iBEB6D/o55mpjWIVs9OJjbCRHP3gOPXqcQhVxq7UVVIiQkndXjaoqpK/yEfIBtEEo+jN4huKRqmY6/RIu5DqUEEsP4LwnCzp8kubZosO0vrKKkWHzp/PHGdj/7yMBuf3MPO0jp3Vhr8vPffMNCBmr6QJ2vEzf0HgklepjP3VvE4zIcoPy2WxTkJWG0qr5+Rr5tp4SDBrLa9j/rOfowGxZ68E8joDaJztR109JqHDsxYDpEJoimhv6dLPMrJa210D1hIjgm3x37b0eVlSz/mV1MfbiF7ubhesA7Cqd+NOSx9iBxT0SKmhzLiI4iNcDKtbdLipPM3a5L41+Cxs37XHIIho+qm0UbVNivs/ArgaLFM27fzq7KR7WecOrKHjcZzWDASdeMXfF3O1Jh7KxjCoOk8NF8m3GTgxrnCG07KzFzHqf9QQ6kwyg+L9iu5a7AjG0QSj9LUNWCPet6qN4islqFV91BrEJki4ANiaoXjv4brxyf19J2ldTz63MmRZrGIVZhHnzspm0Su0lphTzopLf5nWvpsJMeEB0d62Wh0H6LLu8TfnoZdZlZS44uqgoPuJuiqBRTILLbv1t/zFmTFER0e+BLajPhI8tNisKmjosSNpqFJFelD5BX2XRT+QzfOScUwfNqx/ZqY5ALRIApG9Cmi478RQRfDWKW9d1+o76K9d9Dblfkt5U1ODKqHo1+PzdmmSeJv8NsGY1S4kTnp4gZyhFF11UHoHE8yrUJnjThP4jeklfwUgCsZ24V/VCASlSii18E+RbRlvrjf2X1Bxt27it1/aIy8TJsemn3jCJ9HiWeRDSKJR9l9vgFVhcU5CWQlaEk+zZfA3AvhsZAyx7cF+oLZNwjTTVR47bERN+3jYbWpPPFq2XjrYzzxapmUmznDZhWGlWf/BDs+DzYz5G/m983iNXjrwkxMxiB8S8xZJcxq+9vh+lH77juXZGFQ4NS1dqq0VWbJJKnX/IdSCiBi6KLGblA9M/DlZTrSh8g/GIq3H+U/VPICoIqL6KQ8r9flFYrvFxNr7VVjXm+psREUpIkmiEz4HMKpQfVwnPio+SvF2uTcmeHynW4XJzVcPU/icfrrL7G8530Awm96zLfFTJfhaWbApnlpKAqcq+2krkMm6rnCRS3ifs7oiHu7/5CUl3mTILwbkvgT9vSyBcPkZXaD6qVgCNGX4Lb/hMhEqD8LR3/h0lOOVrRS1+E8oUUF6jr65Yi9I8p2CKPKZ+4QxpWV4qLEmr+Ft86JJK+glJeBmPKYo5lVD5OZpcdFsqFQjEH/tUSaVU8JB/IyGJogCgaDap0JfYiuH4e+du8WFUJYbSpvnK3jfF0nAOsLhpnp22xQosnLln3CB9V5ifBoWKJNRx0ba1a9WgsYOFrRMuZYqFLe7MSgWqe3FTqqxXbmIi9VNT10o+rS4Q2i2AwnZ4/C1fMkHqdl1/cwKCoHDCuYvWCVr8uZHvM+AChQexLaq0mNjWDZzEQA9sgpIpdwmGA20CVCVkAaVHuZEL07l3iD3kEL+68IM8ltCx00iGYs9X5R/kJsGmx7Qmzv/RZ0TCzzaexyLb7X1fNChrIdwqDSwfi54Z2vs6pvP8kx4azND0J5mY7dh+itEbuHy8xUVU6eTRq7QfXQyvuAxUpZrbiJDwaDap212gTRhfouWnuGSXgSZ4lJUNUKFe/5qLrgZmdpHRuf3MNnnx/yebr7xweGJMWV7wuJWUTC0Ep2sLLyU+Lx8lvQXj3i0BpNZnZUThDZ0RPMnErM9OmhpNliOisAWJSTCAijavvnVu56YayNs5AJBeKzpYeJv9DVQHr5XwA4n/8pFCXAw0Fi04ZeWxdeA+BmPc3svGwQTURHn9m+AD5neIOo/F2wWUTIT3K+j6oLTWSDSOIx3rvUzIDFxszkqJEd4VBMMHPEsocgZzUMdgvzxAlIj4t06cu6el5IMK5xJYDK42G/Y3tRanDKy3QKbgbFCE0XhP+Sxq0LM4gwGShv6uGc1tSQTAK7NGNogqi0ppNBq42UmHBmJUf7qDD3kxobwVxt9PvIaJmZPkUkfYjcju47N3p6dITvnG5OXXwfhEX5oEovkjZXRJmrNjjx2xGHdB+i0poOegZck24HMzabSuVEEjMnKYz+TFFW/DCjai2F02CE7U9qZzhpNmz/jt96K4Ua6uGfEqYOcsI2h4KV23xdjnsYJTPbMl/IgPdfaaZvUJqjj8eVRjE9lJUQSUJU2LADMr3MVwTxHZHE1wzJyzKHVgesZiGrAtkgMhjgju+LG/fzO8ZMd4xm9exkshIix1sfIyshMjiNlqfKBMaVCjBDaeGBjOveq8kXRCUOrW4Ne53FRYaxVVvleuWUNKueFP2d0FoutjOHJohOafKyZbOSAn9VdBQu+RDJSTS34Yrv3Pd2HEM9r8V3B7O8bDirNLPqk8+CZWiaLTsxiuzEKKw21S7zDGXqO/vpM1sxGRRykpw0DgPMfwhGGVUPl5kV3QUffhbiR8nFw2PFfj9MZQtJ+juwHX0agF+p97CuINXHBbmJ+XeIx6qD0N3I/Mw4shOjGLDYOHi12be1+TkX64WZ/ogEM1WV/kM+RDaIJB7BYrWx54LWIBoeb99YBtYBMQovxwVF8tG6z4rtN/4ZBnudnmo0KDx+ZxHgfIj68TuLMBqC66Z0WrhoSFkUFwImgg7i7gHuXjoDgB2na6XB+WRoKBWP8dkQM+QHE4z+QzpOfYhyN4iY3/ZrQ00zybRxxXdudc8+FEs/pC0QUfChwPw7hJdMT6NdzqFjl5lJLz67QfWs5GjCnE3I2n3UAqdBBLAoR8jhRjSIQDSBHiuFh1+D1X8v9sVny+aQP3H8NxjNXVyyZWMpvIXIsCCZ6kqcqS18q3DxDRRFsU8RyTSz8XGYYNZ8GTqugTEc8jb6qLLQRTaIJB7hRFUbbb1mEqPDWJU3zIdjuP9QkK2uT5mbvgrxOeLm6r3/HvfU7cVZ/PTB5WQmjJSRJUSF8dMHl7O9OEiNlqeKi4aUptErjsGIHndfuV8Y/2lsmpdOQlQYjV0DHB49GSJxjpOV92BMMNNZMzsFRYHLjd0jvc4iYmHWWrF9RaaZuQtX/OQ+bNwnNpY9GDqfqcYwWP6Q2D7+6xGHVssGkR27QXWaE/+hgW5ouSK2A0hiBrBISzI7e7197EGDUaTFbvoqKAZovggdQT4lHChYBuCwiLb/hfUOtizI9HFBbmaUzOzmBaJBtOd8o/R5HAc9wWzEBJEuL8vdAOFO3sMkHkM2iCQeQZeXbZmXPtLbRW8QhcpKpytExMLt3xXbB38IjefHPX17cRb7v7KFFx5Zyy3adNa2BemyOeSICYwrbSr0R2eFhnFlSqGY2rOZ4epe++5wk4HbtQQ3KTObBA4SzGrb+6jv7MdoUFgyMzAMXydDUkw4CzJFgtDh8lE34NKHyO1M5Cc3V6lmqeEqNsUEiz/ipar8hBWfFDf/le9D00X7bt2H6FR1OwMWm4+K8w/Km4Rsw6lBdUMpoEJcFsSme68wN6BH3Z+t6XR+4x2dDDO0a035vuQfnH4RuuupVZP5q3WDfcImaFigTaqVvwt97azNTyEqzEh9Z7/0eRwHfYJo7vCIe+k/5FNkg0jidlRVZdd5B/IykAbVzpj/AZh3u3Drf+1LE/p4GA0K6wpS+OCKHEBcJEkcMMK4ciQ2FVDAdPuToWFcqSgw9zaxPcrv6h5NZraztJ5+szRTdIlR5q5Wm8ofjolUpZlJUUSYgvM15VRmpvsQVb4/whdGMnUm8p37sPFdAJR520WKTiiRkDM0FTlsiig/NYbU2HAGLbax8qMQQ5eYzU51YlAdgP5DOkVZ8RgNCs3dw4yqHaG/L8nJRt9js4pFUOBXlttYkJNCenyQhaqkzoG0+WIh7vLbRIYZ2ThHeCztlmlmDmnuHqClZxBFgcJ07b3K3AdVB8S2bBD5BNkgkridy43dVLX0Em4ycOPcYRet5n5oKBPbskE0ltuehLBouHYQSn7v0lOWzkwE4HJjl0xtcUbRXXDnU2N215PCczP/E1Px3d6vyVfoPkSX3wLb0Or6qrxkZiRE0jVgYY/Uyk+MuX9o0i9riT2G/Ae7LwNQ2dLLxif3DMWQBxG6UfUYOWLGIohOFamM14/6oLLgQ/edc7RcEI6Fe4z7AVBCxZx6NCs1s+qSF2BQNEMURbHLzI6FeNx9xUQSMwdTkIGCMKoWN5PjNgILtAZR+T7RoJB4H5sVKt6Ht78GLVfoUWJ40bol+KaHdOwyMxEesFWXmV1wzRMz1LikyctmJUcTHW4SOysPgKVf2G+kzfNhdaGLbBBJ3I4uL9tQkEJMhGnoQOM50VWPToGEmT6qzo9JnCU08yA+SHsn9lBIj48kMz4SmyqifSVOiNKS3RLzsN33NH9veIKNAz9g1sYHfFuXt5m1DiLioadpaJoPMBgU7lqaDcBfS6TMbEIay0C1QlQyO68ZJo4hDyJW5ydjUMTNZ/3wn9lgGJKZydV6t7G9OIuPrZ41Zv99saWkKp3CZy1UV1gLtkBSHgx0QOnL9t2r82SDaMBipbpVhF7kO5OYBfAEEQyTmTnyIdLJXiFCUfrboeakV+qSDKNsBzxVDM/cAYd/AoBqs7HRcJab57vmERlw6A2iy+/AYC+b54kG0enrHS75yoUaQ/Ky4f5Du8Rj4c2h463nZ8gGkcTtvK3H2xeNMp8bLi+Tf/COWftZSC+CvlbY9XWXnqJ7nZwe7yIp1Kk5Lh7zb+Jo7Bbe6p1DXFQEGwqDJF7VVUzhQzfxl94ccUhPM9t7oYmOXrO3KwssNHmZmrmYJ147P24M+ROvlgVVOlx8ZJj9xuxQ+ajoXulD5BH6LWLy4d5lM/jBA0t54ZG1/Nds7eZ+yQNgNI3z7CDGYICVnxLbx35l3716tphyO1ndjjV4/vQmRXVrLzYVYsKNpMVFjD3BMgBN+hRk4E0QwTCj6vEWx4wmyL9JbF+VjWuvUrYDXnoIOmtH7I6mj5+FP8XCjn2+qcvTZC4WC76WPri6m/T4SBZrqXt75YT2GC42CK+0eY4MqkN18cMPkA0iiVtp6OzndHU7MDRWaadG+g9NiDEM7vi+2D71O6g6NOFTFuckAmJ1QuKE6yfEY85K3jgrJjq2FWU4j/4NZnTfjlFx9wuy4pmXEceg1cabQTb14na0lffa6LkTxpDXdfQHXaKSLjMb40NUsFk81p2GnlHNI8mUOVElJmHuXprN3UuzWZduRtEvoJc+6MPK/IClD4IxAupKoEa8z8/LjCM2wkjPgJXdNQpHKlqDqknrCuVNurwsFsXRglxjmfA8jEoK2Inuoaj7cYyqQfoQ+QKbFXZ+BRwsnxi0l6PhrX8JTtmfogyZVetpZtq0lPQhGot9gkiPuG+tEOmKhmHNXYnXCcG7I4kneUczp146M3Gs+Zw0qHaNWWuHInxf+yJYx5/m0H2I9MacZBQ2q/21V2Ir4C9aUtdtC4MsXtVV5mwDFKg/Cx0j5WR3LxNTRK9Imdn4aN4ddVFzXTo92MbKdaPqg6MbRHGZkFEMqMLzQzJtGrv6qWrpRVFgeW6S2Hn6RSFxnLkG0lx7DQYtMSmw8B6xfUyYVe8qq8esjQ69Xm3kwV8fD1pPMGeU2w2qXfAfCtCJ7uFG1fWd47zH6j5ENcehL3Rlh16l6uCYyaHhKACdNeK8YESXmV3cCZZBe9z9/ivNMghkGKqq2j2I7BNE+qTfzDUQGXxpsIGCbBBJ3MquMifpZYO9Q+PMeuyoxDlbnxBeTU3n4dCPxz1VX0W73tZHS/c4aR6hSuN5MPfQQyT3/amVrn5h5v1vr5SG1A2DnZhUyFklti+PTDO7a4loEB2paKWuo8/blQUGVgs0nAPAOGOpS0+ZKK480FiVl4zJoHC9rc/uc2JH+hC5lROaj868jDjiI8NEwuWp58TBZSE+PaSjm1WXvsw7Jy/w6HMnx0TcB6snmDMqmiZqEAW2/xBAZNgwo+rxJqgTZ0LqXFBtIn5c4nm6XTRkdvW8QCNntfCHG+iAyvdYOCOejPgIegetYwMeQpi6jn66BiyYDMrQe5V+7aBP/kl8gmwQSdxG94CFg1fEG9+tC0c1iOrPig/n2EyIz/JBdQFGdDLc8k2xve870Fbl9NT4yDB7SskZKTMbQ+lRIcU4bc3HNuwtr6EztG4YRqCnmY2Ku89JimZ1XjKqCjtKnK/+hTQtl4W3QHgsi5csJyvBefNHAbISIu2pSsFCTITJ7qlwaPTF7nAfovFkHxKXOK7Jy1bmadND1UfFazAsGhbe68PK/IiZq8XkmqWP0td/FlKeYM6YMMFM81EL5AYRDPkQTRjSoU8RSR8i7xDrogG1q+cFGgYDzP+A2D7/KoqisEWTmcmk2CEuavKy2akxhJsMYBkcauJK/yGfIhtEErfx3qUmBq02ZqfGUJAWO/KglJdNniUfhdyN4mb0zS+Pe7O11O5D1O6d2gIEq02louQ9AE6phSOOhdoNwwjm3SYey/eJ6b5hDMnMZIPIIbo0I6MYo9HI43cWOTxNF208fmcRRkNgSjjGQ5eZHR4tM5u1DkxR0F0vfE4k08LeIJqVIKKid39DHCi6GyLixnlmCKEodrPqO807ceR7AsHrCeaI8mZh/JqfGjv2oM0K9aViO9AbRFqj+sxEDSK7D5FsXHuF3PUQP4OhT8LRKBCfLc4LVnSZ2YXXwWbl5vlCZrb7fOP4nlkhhC4vs/sPVR8Gcw/EpEPGIh9WJpENIonbGC4vG2OKKBtEk0dR4I7/BUOYMBS+8LrTU/XVfOlDNJKjFa3MtVwEoMRWOOZ4KN0wjCC9SBiTWvqh8v0Rh24vzsJkUDhf18llLV1CMgz7yrtI/tlenGX/+xtOZkIkP31wOduLg3Nicl2+SAA8VN4y8mI3LBLyNohtmWY2LfoGrZyr6eBWw1Hu2HOLiIqu2i8OXt4lUoIkgsUfxmKKpsBQxzrD+I3JYPMEG01Hn5nm7kEAZjuaIGrWpiDDYiC5wMvVuZfiYRNE4950524QZuad16H5kpeqC2EMRtj+JDC2XavqTaPt3xHnBSt5N0BkIvQ0QfURNhSmEmEyUNPeZ5+cCXUujU4ws6eX3SymsCQ+Q/7Xl7gFs9VmH5sc4z8EskE0VdLmwYYviO03vwwDjm/Yl2hG1WeuT3CRFGK0tjYzRxGGy44aRDrBfsMwBkUZkpldHBl3nxQTzqZ5aQC8eiYE5XcTMcq7Y8Bi5Uqj+Lv89r3F9hjy/V/ZErTNIYAVuUmEGRXqOoSJ8ggKZGqQOyipbudmjvCz8Kcwdo/6W+xtERHSskkkiIijOV9I7h407hr31GDzBBtNpSYvS4+LIDbCNPYE/T0sc1HA34QNGVUPjm9UHR4NuevEtnxf8g5Fd3Fh0T+PmSFqIJlT634ARXf5pCyvYQyDebeL7bIdRIUb2VAoFlZkmpnAnmCmN4guy3h7fyGwPxkkfsOxilY6+sykxISzfFbSyIMDXUMrNi6aukqGccM/Q2KuSHzY918OT1mQFY/JoNDSM8j1NmkurJM3cAmDonJdTaWJRKfnBfsNg0PscfdvjRm5v3tpNgB/PHGd402hGRPtEFUdmiDKFBNER8pb6R20kh4XwUdXzxIx5AUpQSkrG05UuJFlM8V7vVMfoqqDYJbvR1PlZGUTj4c9CzgSamh/jzu/GpxR0VMgbfNnAbjFcII0xqZVBasn2Gh0eZlTg+pRU5CBjMtG1SB9iLzMztI6njkp/g7PWvP4wuDneGDwa2zo/wH37U0NDf9HXWZ2/lVQVXua2e7zQWrOPQmsNpXLjVqCWWacSL1rPAcokL/Zt8VJZINI4h7e1uRlW+anj70xqjsNqBCfA7Hp3i8u0AmPhtu/J7YP/1QYfo8iMszIgqx4QBpVD2eBVcjLTtscj9GHyg2DQ/JuEEa3XbVjXlM2VQyBN/eY+d2V0IyJdkh7FfR3CNln2nxgyHBy87z0sdLaIGets7j7tHnCX8I6AFUHfFBZcNB16X1mKK1OXTxADe6o6ElizCqmLWU5YYqVjxr3OjwnWD3BhqMnmOWP9oLUCYIEs+HoRtVnXfUhqjwA5hCbGvYyVpvKE6+WUaxUAHBAXcQO23oO24qwareeIeH/WLBZSDk7r0PtKbZoPkSnqttDPnW4urWXfrONCJOBWcnRQ5N92cshJsW3xUlkg0gyfVRVdR5vD8PkZUu9V1SwMfcWYUqqWuG1L4LNNuYUuw+RNKq2Y6g9AcApB/KyYDcRnpCwSMjfJLaHpZntLK3jsRdLxvgGhFpMtEP0G6uMIjCFo6oqey9qDaL5odf8Xq81iA5dHeVDpCjiwhjgquMbdcn42GwqrQ3XXDs5WKOip0DSTY8C8PGwvRgZmqyKizAFtSfYcK7qCWaOJohUdchoPzPwJ4hgyKh6wgZRehHEZQn/pWuyqepJjla0UtfRz0KDaBCdtc0ecTxk/B/DomDONrF9/lWyEqIoyopHVWHvxSbf1uZjdB+mORmx4hrc7j+0zYdVSXRkg0gybc7XdVHT3kdkmIEb5qSNPUFvEGUv925hwcb270B4LFw/BiefGXNY9yGSRtUaqgrXjwNgmrVqzOFgNxF2CXvc/U5gaNVPxkQ7YdSNVXlzD1UtvYQZFTbOSfVhYb5h2axEIkwGmrsHuNo0yh9N+hBNi0uNXVwbjHft5GCNip4KRXdDdAoZtLB79Qm+lLSftYYyijKjQ+a9Xp8gcigxa6uEgQ4whtunIAOdRa4aVSvKkPxVvi95lMaufkxYWKBUA3BWne30vKDHLjPbAarKVk1mtudCaDf27Qlm6XFgtUC5tpgk/Yf8AtkgkkwbfXpoY2EaUeEOEgmkQbV7iJ8BW74mtnd9HcpehbN/EtHHNitLtKj7szUdoXsDP5yOauhpBIOJQ30zAfjClsKQMRF2iTlag6jmBHQ32lf9nBEyq37OsHt3CGnGXk1etmZ2imMz2CAnwmRkRa7mQzRaZpa/CVCg6bzwFpBMiuOVbRy1zafFkEpIR0VPFlMEzFoLQN6Zp/hC3094MfybPFX/EANn/uLj4jyPqqpU6BNEjhLM9Pew9AVgCvdiZZ5jwTCj6vE+v4ChBpFMWPQo6XGRzFWuE6GY6VSjuaY6nrANCf/HObeIhmzLFWi6yJYFoqH/3qVmBi1j1QChgj5BNDczTlyD9neI1Dc5TOAXyAaRZNrsOl8PwC2O5GV9bdBaLrazlnqvqGBl1SPCsHqgE156EF7+tIg+fqqYwpY9RIcb6R20jl3ND0W06SFLWhGn64XW++H1eSFjIuwS8Vna36UKl3e5vJoXEqt+jhjl3WH3HwpBeZnOunxNZjbaqDo6eehCT96MTZoTVW3YMHBg7v9jbFA0ECpR0ZOlbAdceGPM7gxaCf/z3wR96lt9Zz99ZitGg8LM5OixJwSZ/xCMMqqeSGZWsAVQoLFMNq49yOrZyWyMuQ5AqS2P0U3ukPJ/jIwfMl0+/yqLsxNIjY2ge8ASuottDCWYzcuIG5KXFWyRn2d+gmwQSaZFbXsfpTWdKApsWeDgJkm/GEnKEzcMkulx8Q1hlDuazjqMf3yYTyULs+ESKTOzN4iqo4sAEYebEhvhy4r8E3ua2Zsur+aFxKrfaLoaNK8XBTIW0tVvtl/c3RzKDSLNh+hweSu20ZOLcrV+yhyvEq+txOX3Q9E9Y0+InwEffjb4o6Ing80KO7+Co4aaWA9Qgz71TZeXzUqOJszo4BI/yPyHdHQPxtKJGkTRyUPT7PJ9yWMYDQofz20HxsrLQtL/cZjMzGBQ2DJf2HHsDlGZ2aDFRrn2XjU3c1iDSMrL/AbZIJJMi3e0qMYVs5JIdXTzXXNSPEp52fSxX/w6QlwQP9L7SwzYOCONqqFGNIiODoqLk1D0iHEJ3Yfo6l5Wz4whKyFyPEFL6Kz6jUaXZqTOgfAY9l9uxmJTyU+NIc9ZnHQIsDgnkagwI609g1zSImvt2GOl9wb1Tbm7aejsp7q1D4MifJ7skw4b/hHu/xU8/Bo8dlY2h0ZTdXDcqRAFgj71rXw8g2oYNkG01DsFeQndh8ilFNfCqfujWW0qh6628NeSGg5dbZFy/nHI7b8EQOkog+qQ9H+cdzsoBnEd0VbJlvlCcbH7fOP4vllBSmVLDxabSmyEiRmm7iErEv1vU+JzQs80QeJWxk0vA+k/5E4muPgFlYTBBlYbLnC6OslrZfklVrP9QvgvjTMA2FAoG0QOyVoqTG67GzBWH+TxO4t49LmTKDgWtoTUqt9w6krEo5SXjSDcZGBlXhLvX27m4JUW5mcOM1bOWQnhcdDXKv4epbeASxyvbANgXmY8cfQKfwbQJMYzfViZn+NqmlsQp76Vj2dQ3VUvfPkUA2Qs9HJlnqV4lFG1oozzGVVwM7z338IU12Z1WdKys7SOJ14tG+FzlJUQyeN3FoVWs8MVrBbUhlIUoFSdzU8+vhyz1UZ6nFhgCrlriJgUyN0Ale/D+de4YcVnCDcauNbay9WmbgrT43xdoVe5qBtUZ8SilO8FVMhYBHGZvi1MYkdOEEmmTGe/mcOa74TzBlGJeJQNounj4kVtOu1cqO+k3xzCK/YNpWDpxxqRwJGuJMKNBlbnheDUiysYDMJEEeDSW2wvzuKnDy4nM2GkjCzCZAi9Vb/hDJNm2GxD8fZbQrxBBLC+QDRfx/gQGcMg/yaxfVWmBrmKLi9blZcElftBtUJKoWwOTYSraW5BnPpW0Sz8B2c7MqjWp4dS50K4A3+iAEY3qm7pccGoOmclRMQLj0z9GnUCdpbW8ehzJ8d87fqOfh597iQ7S+umWHmQ0nQBxdJPlxpFwoy53L4oS/o/LtAmPs+/SkyEibWaPHv3+UYfFuUb7P5DI+RlcnrIn5ANIsmU2XexCbNVpSAthvy02LEn9DRDxzWxHWTjzD7BxYvavshUzFaV83WdHi7Ij9H8hxpiF6JiYEVukuOEPYlA9yG6+CaoKtuLs9j/lS0896mV3J0rGo02m8qNc9N8WKSPsSeYLeZsTQfN3YPERphYJRuPdh+iI+UOJBcFmjnn1b1eripwOVElJohW5CZB+T6xUzc5lTgnd73wZnIikrWp0B6WHtSpb/YEs1QH12RB6j8Ewqh6boaYwpjQqNoYBrNvFNsuNK6tNpUnXi1zOFGr73vi1TIpNxuO1ow8p+axdWGILiqNZv4HxGP1Eeiqt3sXhmKDSJ8gmpMWMyT1nLPNhxVJRiMbRJIpMyQvczISqK/MpMwRLv6S6THBxa8eeWzNERG/LmnxgxVNknHSlg9I/6EJyd8kYljbq6BZ+AYYDQprZiezOUslJykKs03l4JWW8b9OsNLXDm2VYjtzsV1etrEwlXCT/BgtnhFPbISJzn7L2Ma07kNUfQT6Q7hp7SK9gxbO1Yr/Tivzkocaa/mbfFdUoGAwwvYntX+M/JzUb93/W/lk0KbkDFpsVLf1AU4i7kfJZIONRdniOvOsm32Ijla0jjuVpAJ1Hf0hnUg1Gst1YS9RastjqzOFQaiRkA3ZKwEVLrxunz4+XtVKe++gb2vzMvoE0fLwa9DbLKToOat9XJVkOPLKVjIlBi029mk3SbcslP5DXmGci1+BCtu/w6KZYjX/dCgnmV0/BsAbbTmA9B+akIjYoRXVSztHHFIUuElrsOmyqpCjXqQDkjALopOlvGwUJqPBblx+6OqoJmLybEiaDTaLkEtJxqXkWjtWm0pWQiTZSgu0XBaeMXkbfV1aYFB0l0h3ix89taDwRcvneb5zKddaen1Smqe51tqL1aYSE24kPc5BaMiwKchgRDeqnnCCCIYa19ePQf/45zd2TSBZm+R5oUB3pZjiromaJ2LMJQJ7mtmrzEyOZl5GHDYV3r3U5Nu6vEjfoJWqVvEePKfziNiZfxOYwn1YlWQ0skEkmRJHKlroGrCQFhfB0pxExyfJBpH7cXrxizCdLLqLpTMTATgdqklmfW3QcgWAQ/15xEWa7BeOknGwy8x2jjl001zRINp3sSkkEzeG31g1dvXbp/M2zQ9hyd0o1uWLxvQYHyIYWq2XPkQTclyTl63MS4byd8XO7BUQlei7ogKNorvgsVIsD77C8dzPoMakoaBSmCpuQA5cbfZxgZ5Bl5fNTosZa9Lc2wrtmuQ/CCVmAIu0a1HdqHpcknKFr5dqHfo7c0C/2crh0U1vJ6THRU58UihgsxLdWgZAypzV4xuGhxp6g6jyfehtZcsCscj0TgjJzK42daOqkBwTTnT1PrFT+g/5HbJBJJkSurxs64J0DM4M52plxL1H0C5+efg1EXl8/68BAzScg7rTLM4RzZCrTT109pt9W6sv0ORlHZE5tBHP+lA2RZwMulF19WFxMzGMtbOTCTcZqGnv43Jjtw+K8zH2aOgl7LsoVvoWZSfIG4Jh6D5ERytasVhtIw8WbBGPV/d4uarAw94gyk0SKUsg5WVTwWBEzd1ITfJ6bCsfAeB+2y4A9l8JzgZReZNmUO3If0ifgkzMDdpm4/zMOEyaUXXtREbVMDRF5KBxbbOp/LWkhpv/511eOFY97pdREGlm+hRlqGNtvEi4OkC3GsmyZat8XY5/kVIA6QvFRO2lt9iqNYjevdiIefTnZpCi+w8tTQOl+qjYWSAbRP6GbBBJJo2qqrwzUbx9Zx101YnR+MxFXqwuRDAYYfYNsOiDsOh+KL5P7D/4f6TERpCTFAVAaSj6EF0XDaKzyhxA+MRIXCApF9KLQLWN8WWICjfaJ0T2haLMbJi5614Zb++QBVnxxEea6B6wjJV45N0ABhO0lkNrhW8KDACsNpVTukH1rMRhBtWbfFVSUGBb8jEwmJjRdYa5SjWHrrZgC0JD4SGD6nESzILUfwiEUfUc3ah6Uj5Ee2DYxNGxylbu/ckB/vHFEmra+8hKiOST63NRcO4A+fidRXIhSuPauYMAXFTyWJ0vr7/GYJeZ7WDpzCSSY8Lp7LfYwwmCHd1/6NbIC2KCL3WuuP6U+BU+bRC999573HnnncyYMQNFUXjllVdGHFdVla9//etkZWURFRXF1q1buXz58ohzWltb+fjHP058fDyJiYl8+tOfprs7BFe4vci52k5qO/qJDjfa443HoJshps4T/iYSz7L+8+Kx9GXouM4SbdS6JBRlZjVC+763exYAG+dIGZDLzL1VPF4aKzPbPE/8d9x7IXS08gAM9kLzRbGZVsz7l8X0gfQfGonRoLDGmcwsMn7IgFJOETnlYn0XXQMWYsKNzDdUQ08ThEVL887pEpcJ824H4OHwPbT2DHK+PvgM08ubtAaRI4PqIPcf0lmsyclLXfEhytsowhk6rkHLFSqbe/jM707woZ8d4vT1DmLCjfzzLXPZ80+b+I+7ivnpg8vJTBg5NRoTbuSnDy5ne7FM6tJpuSx8ZbqSFhJmlHMIY9AbRFd2YzT3sEm7ttp9vsGHRXmPi1qDaIVFLOZSuNWH1Uic4dO/3J6eHpYsWcKPf/xjh8e/+93v8sMf/pCf/exnHDlyhJiYGG699Vb6+4dGRz/+8Y9z7tw5du3axWuvvcZ7773H3/3d33nrRwhJ3tamh26ck0ZkmJM0EN1/KHu5l6oKcWYsFSbDqhUO/5QlM8VF0pnqEJsgUlV7xP0JSwHZiVHkpUT7uKgAQvchurILrJYRhzbNEw2RY5WtdIWSdLGxTExVxaRxvCWC7gELqbHh9hsRyRDrNZnZGKNqkDIzFzhRJaSdy3OTMFVqvii5G6R5pztY+TcA3GvYTyQDHAhCmVm57kE07gTRUu8V5AOKcyZhVB0eA7NE6uubf32Obd9/l53n6jEo8LE1s9j3/zbzuS1ziAoX17nbi7PY/5UtvPDIWh5eJyYe8lKjZXNoFBFNpQDE50t5mUMyForgBusAXNnFzfOFEmP3hdCYzr5U3wWozGo9JHbIBpFf4tMG0W233cY3v/lN7r333jHHVFXlqaee4mtf+xp33303ixcv5tlnn6W2ttY+aXT+/Hl27tzJ008/zZo1a9i4cSM/+tGPePHFF6mtrfXyTxP8WG0qh6628KcT1wG4ecE4K+jSoNr7rP9H8XjityxLF3/aIWdU3VoOfa1YlDDK1Fw2FKZIg8TJkLMKopJFqkv1kRGH8lJjmJ0ag8WmBuXNlVP0G6vMxezR/IdumjuO91oIo/sQHa9sY9Ayyk+hUGsQVbwH1hBqME4C3X9oRW6SlJe5m9mbICmPaLWHO42HOHDFNePhQKGz30xz9wDgoEE02APN2vR9kBpU6wxPMpvIqHrAYuWIQVyjRlTuw2xV2TQvjZ2P3ci3711EmoMkOKNBYV1BCo9uKgTgfF1XaHo9OqGysZN8y1UA5iy9wcfV+CmKIrxEAc6/yg1zUzEZFMqbeuwy0WCls99MbUc/c5XrhPfWgylKLIJI/A6/nf2rqKigvr6erVuHOosJCQmsWbOGQ4dE1/HQoUMkJiaycuVK+zlbt27FYDBw5MiRMV9TMnV2ltax8ck9fPSXh6lt7wPge29dZGdp3diTVVU2iHxB4c3CQ2awmyUNf8GgQF1HP42dIRS9qhlUXzHkM0iYjLefLAbjkFm1A5nZplCUmQ2TZuyR8fbjMjc9juSYcPrMVs6Mbk5nLYWoJBjotE/5SUZyvFI0iFblxELVAbGzYLMPKwoiDAZY8UkAPmbcw9GKVgYsVt/W5EYqNHlZWlwEcZFhIw/WlwIqxGZCnBPfyCBBN6puHceoWlVVXj9Tx7b/fY//KMsEYL3xPM99cgm//ZvVzHUhlj0zIZLclGhsKiHjHeMKx08eJUYZoF+JIC57ga/L8V8WaA2iS28Rb7SyJl8YnAe7zOyyJi+7I/qc2JG3EcJk2Ic/YvJ1Ac6or68HICNj5IdZRkaG/Vh9fT3p6SMv1E0mE8nJyfZzHDEwMMDAwID9352dQotuNpsxm92/EqB/TU98bW/w1rkGPv/iaUavxTR2DfDocyf50QNLuHXhsN9TZw1hPU2oBhOW5Lnghz93oP9OnKGs+SymVz9H2LGfMy/1/zjfNMjJypbxp738AHf9PgzXjmIEDg3kAbAmNyHofseeRinYiunMi6gX38S88V+Aod/LDYXJ/OZAJfsuNjI4OBgS01nG2hIMQH1UIeVNPZgMCutm+/Z15c/vX6vzkth5roH9l5tYkj3yRss4+yYMZa9gvbwL24yVTr5C4OGO30ddRz817X0YFFisngdzL2pMGpakOX75GRoIjPm9FH8E055vsYwr5A2Uc6y8mTVBkjx1uUFcx+alRI95HRpqTmEEbBnFWH38WvL0e5cRKEyL4UJDN0+/e4WbF6SzMjfJbiB96lo7/7XzIqc0+X167Bz6jKlEDTSz1nARs9n1BtrK3ESqWno5fKWZjflJnvhxPI67fx915w8D0BE/H6PVBiGSzDVp0hdhistC6arDcnk3N82Zx4ErLew+38DDa2f69Wf8dDhfK/7utpjOgBWs+ZuxBcjPGCy/E1fr99sGkSf5r//6L5544okx+99++22ioz3nV7Jr1y6PfW1PYVPhiZNGrTk08mZQ1f7/a38uwVxpRVdcZLUfYzXQETGDd3ft9Wa5kyYQfyfjodii2RaWRFR3PXdEvs15NvHnd08yUBEYH9LT/X3ceHE3ScApWyHZ0SpH3hsbXysZH5PFzG0YMbRc5uBrz0FEhv33YrZBuMFIQ9cAT//pTbIdWF0EE4pq4QP1YqXr16f6gCTyYq28v8c/3jf88f0rtlcBjLx+7BKzey+MODarO4VlQOfJv/BeT/ClKU3n93GyWfx3mxGtUrvvt8wDrocXcvLNN91WX6gy/PeyMn4Z2e1H+ZhxN797ayYtswLjs3EidlUbAAOm3hbeeOONEceWVr1OLnC5O5oLo475Ck+9d51uUShvMgAKvzl0jd8cukZiuMrWbBtXOxVOtQjhRLhBZcsMG1tm9NJ0fR6zBpqpeOdpyrJdD7mJ6BB/s2+XlFNkuTzh+f6MO34fPWaIbikFEzQb0jnsJ681f2VRxELyu+qoeednGNL/FjBxpKKFl3e8QZR2d+6Pn/HT4e0KA9EMMm9A+FTtrTbS0xRYr5NA/5309va6dJ7fNogyM8XYZ0NDA1lZQwZwDQ0NLF261H5OY+NIUy+LxUJra6v9+Y74l3/5F770pS/Z/93Z2cnMmTO55ZZbiI+Pd+NPITCbzezatYtt27YRFhY28RP8iCMVrbQfHk8OoNA+CGlFa+0rcYa9J6EC4ufdyO233+6dQidJIP9OJsKQUgl7vsHHTHv4b26iNyqN229f4euyxsUtvw/LAKYzfwtAiVrI9mV53L59nhurDCE6n4eq/dyY2cebbYz4vbzRfoo9F5uwps/n9pvyfVyoh2ksw1hiRg2P5XzEQqCND66fz+0b8nxalj+/f81t7OZPPzpIVa+Jm7dtJmJ4kEHnUvjRr0jsreD2zeuE5CwIcMfv4/jrF+DyNTYvymVOk/BQzNrwMW5f4p+foYGAo9+LUhkLz9/HPcYDvKF8httv3+TbIt3E2384A9fruXHZfG7fmDfimOnp/wagYON95M/37evJk+9db51r4DeHxk67tw8q/KlCvA8pCty/LJvHbi4gI15IW5TSXvjrAQqpIm8S16zFrb38/vv7ud5rYPPWrXYz60DCnb+PV0pqKTjzbQDmbbiLufK9a1yUyjh4/h1m9ZXy8L238PvrRyhv7iFq9nK2zU/x28/46fDib46zzrCLMCyoibncdO+nxB9lAODP112TQVdNTYTfNohmz55NZmYmu3fvtjeEOjs7OXLkCI8++igA69ato729nRMnTrBihbgB3rNnDzabjTVr1jj92hEREUREjDWfCwsL8+gv3dNf3xO09FomPkk7z/6z1QtTV0POCgx+/vMG4u9kQlZ9Gvb/L0ndV7jJcIaSmpWYTKaAkANN6/dRXwLWQdqI55qazg1z04Pvd+st5t0GVfsxVeyGxE+P+L1sWZDBnotNvHelhc9vDfIGXFMZALaMRRwpF6PRW4uy/OZ15Y/vX/NnJJIWF0FT1wBn63rsxtUApORC2nyUpguEXdsPxff5rlAPMJ3fx6nqdgDWZ4dhOH0SANOcLeBnv99AZMTvpXAL5sR84trLmV2/k37rprGePQFIZatYFS7MiB/5GrQMQpOY5DPlLPeb15O737usNpVvvXlxTHNoOBEmAy8/up7i0QmUc7cBCkrjOcL6WyDO+QLzcPLT48lKiKSuo5/Sum7WB7DnoTt+H/suNvIdpQoA08wVfvNa81vyb4SoZJS+VsJqj7G1KINfvFfOvsst3L5IvAb98TN+Olxu7OYLBuHrqBRuJSw88BI6A/134mrtPjWp7u7upqSkhJKSEkAYU5eUlHDt2jUUReGxxx7jm9/8Jjt27ODs2bM89NBDzJgxg3vuuQeABQsWsH37dh555BGOHj3KgQMH+NznPscDDzzAjBkzfPeDBRHpca6Zh9nPkwbVvicqEZY/DMDfm16no89MVYtrI4UBTY2YdDtpLSDMaGB1kHhL+AQt7l6p3M+s5ndRqvaDTRi66kbVJ6ra6OgNbC32hGgJZtcj5zBotTErOZqCtCDX1U0TRVFYl6/F3Zc7iru/WTzKuHs73QMWymrFqt4aQxmoNkiZAwk5Pq4sCFEUwlaJyPsHDLs5Ut7q44Kmj6qq9vSj/NHvT03nwWaGyERInOX94rzE0YpW6pyYUusMWGx09TtY9IxJhSxN8jqJ9yVFUezXGUcqAv91NB0GLFYqL50lTunDZoyE1CBfPHIHRhPoE33nX+VmLfxi78VGrLbxE/gCkebuAZq7B9hkKBE7ZLy9X+PTBtHx48dZtmwZy5aJRsKXvvQlli1bxte//nUAvvzlL/P5z3+ev/u7v2PVqlV0d3ezc+dOIiOHmhbPP/888+fP5+abb+b2229n48aN/OIXv/DJzxOMrJ6dTFaC8yaRAmQlRA7djLdVQn87GMMhfaE3SpQ4Yu2joBhZbyhloVIRGnH3WjJSia2A5bOSiA732wFJ/6exDBQjimplWfWvMD13DzxVDGU7yEmKZk56LDYV3rsc5GlmWoLZ0T5xo75lfnpATOL5Gn1q6PBVRw0iLe7+6h6xoCCh5Fo7NhWyE6NIrj8odsr0Ms+x9ONYlDAWGyq4ema/r6uZNg2dA/QOWjEaFGYmjfLR1JrcZC0OGCnHVGjsci2t1el5hVrj+srkfAv1a9+jId4gOlzeSoEWb69kLhLND8nE6Glm519jxawE4iNNtPeaKdEmSoOJSw1d5Cn1zDI0gSEMZt/o65Ik4+DTBtGmTZtQVXXM/377298Cojv/jW98g/r6evr7+3nnnXeYO3fuiK+RnJzM73//e7q6uujo6ODXv/41sbGxPvhpghOjQeHxO4scHtMvNR6/s8ieEGGfHsooBlPgjQ4GDYkz7fKNR0yvc1pL7AhqtAmiErWQjQE86u1zynbASw+BOioCurNO7C/bweZhK11Bi80G9WcBeKVeNDw2y3h7l9AniE5Vt9E3OOp1lLseDOHQWQMHfwQV79un00KV41Xi5nJlXhKUa8EO+Zt8V1CwE5NCY84tAMy8+qKPi5k+5c3CWHlmUhThplGX9XWiyU3mYi9X5V0mPe0+Gn2ysXyveO93Ed178+S1NgYtwWF4PhXeKWug2FABgDIj+AIIPMbsmyA8DrpqMdWXsGmeuMbYczH4Ft8u1XdxkyYvI3cdRMh7dX/Gpw0iSWCwcEaCw/2ZCZH89MHlbC8eMhGX8jI/Yv3nAbjDcJjaqos+LsbD9DSL6TXgtK2AjXNkg2hK2Kyw8yvg0MlB27fzq2yaIy6K37vUhC0IR6EBaKuAgU5sxgiOdKcRFWYMmkhsT5ObEk1WQiRmq8qJqraRB6+8MzTJsOvf4Zk77NNpoYr+3+jG9H5ouQKKAfI2+riq4CZuwyMA3DT4Lk1NgX0zNiQvc3DDZZ8gWuq9gnyAPu3ubEZqzLT7aGauFjfqvS1QV+Ly9y1IiyU5JpwBi42zNe2TrDo4UFWVd843sEgRDaJgf625lbBImCua1Zzfwc0LRIPo9bP1nGhWOFLRGjRys0uN3dxk0N6PpLzM75ENIsmEPHdEmM5tLEzhhUfW8oMHlvLCI2vZ/5UtI5tDIBtE/kTWEnpzNmJSbKxtfAmzNYhXtzR52VVbFmpkAotGm1BKXKPqIHTWjnOCCp01rFIuEhthorl7kNLaIJ1O0+RlTVEFWDCxoTCVyLDAS6nxBcN9iA5ebR46oE+nWQdGPmHYdFqoYbHaOKk1iNYqIvqX7BUQKd/DPEncvE1UG3OIUQa4/v6zvi5nWpQ3iQbR7NRR/kM2KzRor6ms4J4gGj7tPrpJ5HDafcwXGCZ5ueq6zExRFFbnhbYP0bnaTuo7eu0TRMxY6tN6Ao4Fd4rH869i0abQatr7efaykQd/fZyNT+5hZ2mdDwt0DxV1LawziOAP2SDyf2SDSDIu/WYrfzhWDcDD62ezriCFu5dms64gZewHrc0GtSViWzaI/ILIG78IwAeVPVy5dt3H1XgQTV52Sp3DuvwUTEb51jYluhtcOi2sr9Eu49t7IbBX352iSTNOW4Sx6xYpL5sUawtGGVW7OJ0WanKzC/Vd9AxaiYswkdVyWOzMl/5DHkdRuJh9PwDpl14IaD8sfYJoTIOo5QqYeyEsGlIKfVCZd9lenMVPH1xO5ijfTIfT7o4o1PzRrkzOQD/UfYh2lTUwS2kkTukDYwSkzfd1SYFF4Tbx3621nF++/NqYw/Ud/Tz63MmAbhKpqkpc4zGilEHM0RmQ7ti6ROI/yLsoybi8erqW9l4z2YlRE98gtV6FwS4wRcoPCD/BMOdmrplmE6v0M3D4V74ux3MMM6iW8rJpEJvh8nmb54s0s6D1IdKkGe92iURM/eeVuIY+QXTmegfdAxaXp9OoOuidAv0EXV62fFYChvJ9Yqf0H/IKMas/wYAaRnb/ZdSaE74uZ8o4TTCz+w8tAkNoTD9uL85i/1e2TDzt7gjdh+j6UejvdPl76g2i45VtQSMHmgwj5GUZC8U0lsR1ImJRtfCGWw3HxhzWX1FPvFoWsK+v+s5+VlvEe6xhzragNswPFmSDSOIUVVV55lAlAJ9Yl+t8NFdHl5dlLpYJBv6ConAm9yEA8q/+DiwDEzwhALHZ7Bf3p2yFbJAG1VMndz3Ez2DskL6OAvHZkLuem+aKhvHp6+20dAfZ60pV7RKzc7Y8FmTFk5UQ5eOiAouZydHkJEVhtakcq2x1eTrN5fOChONag+iWtFbobYawGMhZ5eOqQoOlc/PZqa4BoOvAL31czdQYtNi41toLQH7qKA8i3UsnyA2qR2M0KONPuzsjeTYk54PNAhXvufz9FmTFExdhonvAwvk61xtLwUBNex/najtZJOVl0+JqqmgQbTcedXhcBeo6+gN2Su3iMINq41wpLwsEZINI4pRT1e2U1nQSbjLw4ZUzJ36C3iDKXu7ZwiSTImzJ/dSpycRbWuDsn3xdjvtpuYIy0Em/GkZH3BzyR4/ZS1zHYITtT2r/cHJRvf07YDCSmRDJgqx41GCMu++qh54mbBi4oM5ki5wemhLrh8fdT2I6LZQ4USku+DegTXvkbZAJoF4iKtzIqbR7AYi+9Ar0B56fWnVbL1abSnS4kYz4iJEHtSY3WTJVymX0KaJJ+BAZDYpIIAQO65LaEGH3edHQXxetWRhIg+opcSlxAxbVwAJDNXmKcylZY1e/F6tyH7WVF5ljqMGGQU7IBgiyQSRxyu8OCXPqu5bMIDnGhQtWaVDtlyzOTec3llsBsB34YUB7LTjkuhjJPavOZt2cTBQ5ujo9iu6CDz8L8Q5G8gu2iOMam+eJxsm+YItk1eRl5WTTT4T0H5oi64b7EE1iOi1UqGnvo7ajH6NBYWa7tnIsL569StrCm7hky8Zk7YczL/m6nEkz3KB6xGefqg5LMAutCaJpUag1iK7sntS10hpNUhuoEx5TZVdZA6Ay11Yudshm5JRISsnkkE348txqOO70vPS4SKfH/Jmwyn0A1Mcvgqgk3xYjcQnZIJI4pKlrgNfPiC72Q+tyJ36CzTp0MSIbRH5FZnwkb0fdRpcahaH5goiZDiZ0g2rbHOk/5C6K7oLHSrE8+ArHcx/FuvU/xf6qA9A95Dm0WWucvHupKWC18Q7RVt7PWHNJig5j6Ux5QTMV1uWLv8fSmg46BmwuT6eFCse16aElWVEYqw+JnbJB5FXWF6bye6toCqjHfx1wCygVzd2AA4Pq9mtiIsoQBmkLfFBZgJJ3g/hv1l4FreUuP033ITpW2YotmD4Lx6Gr38zh8hZmKo1EWjrBGC7Nh6fI6tnJHIoQiyPbjWN9iBQgKyHS/joLNHJaDgDQPXOTbwuRuIxsEEkc8odj1xi02lg6M5HFOYkTP6HpokjLCI8NibSMQEJRFApnZvOCVUvoOPAD3xbkZizXxIdpia2A9QWyQeQ2DEbU3I3UJK/DtvozkL0SLP1w+Kf2U5bNTCQ+0kR7r5mS6nbf1eputGb3OVsuN81Nc93DQjKCzIRIZqfGYFO1lXWn02kK3PfLEdNpoYBuUH1XynXx+RmTLm+wvMyi7AR2hW2iXw1DaSyzT6QGCkMG1aP9h7QFu/QFUrI4GSJiYdZasX3FdZlZ8YwEosKMtPWaudLU7aHi/Iv3LjVjtqpsTdAkUelF8rU2RYwGhVW3PohNVVhmuEImI6WKKvD4nUWBdy1is2K7spdlA2IhN3qB9B8KFGSDSDIGi9XG80euAfDwehemh2BIXpa1JKRWgAOFJTkJ/MayHStGqHx/6PcV6Az2YmgqA6ArdQlpcRETPEEyJRQFbviS2D72tN2rw2Q0cONcXWYWRGlmWvrPOXW2fUpKMjXWatKLQ1e1C15tOo2HX4P7nhZNEdSQ/Nw4XikaRBuUUrEjf5NMd/EyJqOBhfm5vGZbJ3Yc/41vC5okVzWJ2RjvPek/NHW0RKnJ+BCFmwwsz00E4EiIyMx2ldUDsC1JPEqD6umxedUSOlKXAnCLcaTMLDU2nE3zAuxapGwHPFWM4bl7iFTMAGS/9fdiv8TvkQ0iyRjeOd9AXUc/KTHh3L7IhWhQkP5Dfs6SmYnUkcJu40ax4+CPfFuQu6g7jUG10qgmMneOHKP3KHNvg7T5MNAJx35l371Zu2gJmrj73lboEA3y86qYIJJMnRE+RDoGI8y+ARZ/CJZ9XOwrfdkH1fmOrn4zF+pF4lFuh/Qf8iUbClN53qJ5z5z7M/S1+bagSaBPEI2RmNn9h2SDaNLoPkQV74Nl0OWnrc4LHR8is9XGngviM3+hIv2H3EXSivsB+HL6Uf57xn7+dIuZzFgjzd2D/OI91yWPPqdsB7z0EHTWjtitdNWJ/bJJ5PfIBpFkDM8cFObUD6yeSYTJxVVd2SDyaxbnJADw/d7tYse5V6CtyncFuQvNf6jEVsDGOfJG3qMYDLDhMbF9+Kdg7gOwTxCV1nQGbMLGCOrPAnDNlsbc3BwSo+XI/HRYmy88E87XddLW4+Bmq/iD4vHy29DX7r3CfMypa+3YVFiQZCW8oUTslA0in7ChMJVTaiEX1FlCRnv6D74uySW6+s00dQ0AMDttdINIThBNmYxFEJMG5h6oPuzy03R/mKMVLagB5mU1WY5XttHZbyE5Ooz4tnNip0wwmz5hUQDEtp3nQ60/YeV7D7PX9I/cajjKj/deobq118cFuoDNCju/ghDGjUbbt/Or4jyJ3yIbRJIRXG7o4lB5CwYFPr7GRXmZ1Wy/qZINIv8kMTqcvJRozqu5tGZuANU6wksmUOkpFxdvp5kTsOZ9AcWiD0LCLOhphJLnAUiLi7A3IN8NhjQzbeW9VJ3NlvmhFbnuCdLjIpmTLvxRjlQ4iIDOWCgm06yDcOE1L1fnO45r/kMfSq4E1QapcyEh27dFhSgFaTFkxEfynD5FdOI3AWFWrU8PpcZGEB8ZNnSgqwG66wFF/H1JJofBMCQzm4QP0bJZiYQZFRo6B7gWCDfy0+AdLd7+/gIVpa9NGHvL19r0KNsBr//zmN2R/Q38LPwpNtkO883Xy3xQ2CSpOjhmcmgkKnTWiPMkfotsEElG8KwWbb+tKIMZiVGuPanxPFgHICIBkvM9WJ1kOiyZmQjAe6kfFTtOPhtQo/QOuS4miHrTlhETYfJxMSGAMQzWf15sH/ghWC0Adm18MMTdW2p1g+o8GW/vJuwys6sOGkSKIhqPAGf/6MWqfMuJKiFD2WAc5j8k8QmKorChMJW/WjcwaIiEpgtwzfXJEV9hN6h25j+UOhfCRx2TuEaB1iychA9RZJiRJVqoSzD7EKmqqsXbw+2p4lGYoUsPyCkzztSNggooPB72O3adq+PdS35+ndXd4N7zJD5BNogkdrr6zfz55HUAHl6X5/oT7fKypdJg04/R0+he654HGcVifDrADDlH0FVPTH89NlUhfd5aX1cTOix7EKJTRQzwuT8DsHmekJm9d7kJs9Xmy+qmzUC1eD9riJ7L3IzYCc6WuMK6fAc+RMMpFr4LVLwnph+CHIvVxqlr7QDMtvsPbfZdQRI2FqbSRTT7wm4UO074/2djuW5QPUZeViIesxZ7t6BgQp8gqj8L3a776w3JzIK3QXS5sZtrrb2EmwzSf8hdTDB1o6AyQ2lhteECT+w4x4DFf+VZ1hjXFtZcPU/iG2SDSGLnzydr6Bm0Upgea1/xdYnak+JRysv8mqUzhQyo5Hon6rrPiZ1HfgaWAR9WNXWs1SKO+JKaw+r5LsohJdMnPBrWfkZs7/8+2GwszkkkOSacrn4LJ6sCeCptsIeoTnHBmzp3FYpseLuFNVqD6FJDt90zZQTJ+ZC9Qkityl7xbnE+4HxdF72DVuZGthHeUQGKEfI2+LqskGZDYSoAP+66Qew494owrPdjyp0aVEv/oWkTmwaZWoPt6h6XnxYKDSJ9emhDQQrhDdprTSaYTQ8Xp2kKIrspb+7h1/srPVvPNDhqnU+tmozNiUrXpkKtmsJR63zvmp81JwAAXvJJREFUFiaZFLJBJAHEyOizhyoBeGhd7uRujKRBdUBQlJWA0aDQ3D1A3cwPQNwM8aEUoLKOlotCv3xOmcMSzQNH4iVWPQLhcdBYBpffxmhQ7GlfewNYZqbWl2JApVFNZPUimYrnLpJjwpmfGQfAYWdTRIs+JB4D9P1oMhzX5GUfSdFW37NXQKR8D/MlGfGRFKbHctqWT0dikZDNn37B12WNS0VzNzBOglmmnCCaFnqa2SR8iFbmJWNQ4FprL3UdfR4qzLfo/kPbFmQMS8uT1//TItY1v8Nb1oim74/2XPbb11djj5knzA/h6C5Sbxo9Yf4EjT1mr9YlmRyyQSQB4ODVFq429RAbYeK+5TmuP9HcDw2aaZpsEPk1UeFG5mWIm7QzdT2w9lFx4OCPwBZ4siBz1REAulOXYjLKtzKvEpUIqz4ltvf/L6gqmzSZ2b4AjrtvvCSm0s6reazLT/VxNcGFw7j74Sy8FxQDXD8GbZXeK8wH6AbVNxi0cIcCKS/zBzYWpgIK+2I/IHYc91+zalVVqbBLzIZJYfvahPwXpMRsuth9iPa4fI0UG2GiOFs0e4Nxiqixq5+S6nYAts20QG+zmIDMKPJtYYFO7nqInwEO2yqI/fHZ3Lj1LlbmJtE7aOVbr5/3ZoUukx4XySHbQgYZ6wtaTwqPmh/jLdtq0uMifVCdxFXkXZUEgGcOVgJw3/JsYidj9tt4DmxmiEqGxFmeKU7iNpboMrPqDljxSYiIF4acV97xbWGTxWYluUM0JhPnrvdxMSHK2s+CMQKqj0DVQW6ck4aiwIX6Lmrb/XNlayJar4gGUWdiEVHhRh9XE1zoPkSHHRlVA8RlQt5GsV36speq8j6qqnK8shUFG7M7T4id0qDaL1ivNTF/0bYcwmOh5TJUHfBxVY5p7BqgZ9CK0aAwKzl66ICeKJs4C6KSfFNcsDBzjXgd9DYPGX+7wOo8ITMLRqPqPecbUVVYkpNAWqfWoEhfYI9nl0wRgxG2P6n9w0mTaPt3UIwmnrh7IQYFXjtTx8GrzV4r0VVW5SXxwfBDRCgWLthyeGDw3/jC4Od4YPBrbBz4AW/bVpOVECmTh/0c2SCSUNPeZx8ZfWjdJL1chsvLpF+H36MnbJy53g6R8bDiYXHg4A99VtNUGKgrI0rto1uNZOGS1b4uJzSJy4SlHxPb+/+XpJhwlmlJeX6fsuGEiGaRKBU/e4WPKwk+1sxOQVGEb0pDZ7/jk+wysz95rzAvc72tj4bOARYarxM20AJhMZC90tdlSYC1BSkYFDjXotIz916x00+DHHSD6plJUYSbhl3KS/8h92EKhzzNk2oSaWbB7EOk3ytsXZAxzAx9qc/qCSqK7oIPPwvxWaMOKHD/0+I4sHBGAh9fI+7VHv/rOb8LBvn1/nLuR/y9/MG6mcO2heywreewrQhVazs8fmcRRoO8Z/RnZINIwvOHq7CpsKEwhcL0uMk9WW8QZS93f2ESt6MnmZ293oHNpsKaR8Fggsr3oeakb4ubBNfOvAfARUMhBRnxPq4mhNnwBSELuvIO1J1hsxZ3v/dC4MnM2ru6yTZXAjB3mTQMdjcJ0WEUzxATjA7j7gEW3AmGMOFtpUuXg4wTmrzs/sTLYkfeRnEjKvE58ZFh9s/Ig0l3ip3nd0CPk9erD6lwalCt+w/JBpFbsPsQuW5UvUqbILrS2E1zd2CGgDiib9DK+5fFxMq2hcP8h6RBtfsougseK8Xy4Cscz/0ManQqoIoAh2H80y1zSY4J53Jjt10B4g/sudDAa2+9yUJDFWYlnIMxW0ccz0yI5KcPLmd78egmmMTfkA2iEKffbOXFY9UAfGJt3uS/QG2JeJT+QwHB3IxYIsMMdA1YRAJKQjYUf1AcPPgj3xY3CXrKhf9QR/JimTTlS5LzYeF9Ynv/99k8XzSIDlxp9usYVkecOnGYCMVCNzFk5cp0DU9g9yFy1iCKSoI528R2aXBOEekG1TcYxbSalJf5Fxu1NLPXm9LFdY11EEqe93FVYylv0g2qY0ceqJcTRG5Fj7uvPgwDXS49JSkm3O73eLwyeKaI9l9pZsBiIycpinnpsUPX//K15l4MRtTcjdQkr8e28tNi34lnRpySGB3Ol2+dB8BT71ym0dlUrhe51NDFF14o4SOGvQCYiu/mja/ezQuPrOUHDyzlhUfWsv8rW2RzKECQDaIQ5/UzdbT2DDIjIZKtC9In9+TBXmjUNMiyQRQQmIwG+yr+ac1okPVa5H3ZK9BW5ZO6JktiawkA0flrfFuIBDZ+UTyWvUJRRDNpcRH0DFo5XhlYcfd1F44C0BI3T8plPYTuQ3SwfBzfhEVaw/rsn/zWIHg6HK9sIxwzs3u01XfZIPIr9Lj7A1dbUFf8jdh54rd+91rUJ4jy04ZNEA32QvMlsS0Nqt1DSgEk5YHNAhXvu/w0XWYWTD5Eu8rqASEvU7rroadRTBBnFPu4suDFtuRj4r9x1X5ovjLi2IdXzmRJTgLdAxa+8+YFH1UoaOsZ5G+fOY5toJt7ww4BoCx/CKNBYV1BCncvzWZdQYqUlQUQskEU4jx7WDQEPr42d/JJUPVnQbWKeMY42REOFJZoPjFnrreLHZmLxCqZaoPDP/FZXa7S1tbKLMs1AAqXb/JtMRLILIY5t4Bqw3DwB2zS4+4DSGZmtako2sp7eM5S3xYTxKyanYzRoFDd2sf1tl7HJ829TfjytFfB9ePeLdDDdPabudjQxQrDJYzWfvHZmb7A12VJhrE8N5HIMANNXQNcybgVwuOg9SpUvOfr0kZQrjeIhkvMGs6Jz/HYDOERJ3EP9jSz0PUhstpUdp8Xn+nbijKGpofS5kN4tPMnSqZHfDYUajKtU8+OOGQwKHzj7mIUBf58qoZjPppWM1ttPPr8Ca619vKJ+BNEq71iulz375IEJLJBFMKUVLdzurqdcKOBB1bNnPwXkAbVAcniHC3J7HrH0M71nxePJ38Hvf59QXPx5LsYFJVGJZXUrDxflyMB2Pgl8Xj6BW6dJVba9wZQ3H1JdRuFtqsApM+VpueeIjbCxKLsCXyIwqNh/u1iO8hkZier2lBVuD3motiRv0l+dvoZESaj3UPm/co+WPxhceD4r31Y1UjMVhvXWkWDdfbwCSLdNDhTTg+5FbsP0eQbRGV1nXT2mz1RlVcpqW6npWeQuEiT+Nl0/yFpUO15lmthMiW/B8vgiENLZiba79/+/ZVSLD4wrP6PHec4XN5KTLiRx5LE9BDLH5KfbQGObBCFMM8eqgTgjsVZpMRGTP4LDG8QSQKGpdoE0fnaTgYt2odJ/mbIWATmHr+6EHZE+2XxAdScsMjHlUjs5K6DWevAOsgNLS9hNChcbeqhutXJlIifsed8PQsUMZVmzF7q22KCHLsPUfk4xr96mlnpn8Fq8UJV3kE3qL7ReE7skPIyv0SXmR282gwrNZnZhdeg2z+a3tWtvVhtKlFhRjLjI4cOSP8hz5B3gwjzaKuA1nKXnpIRH0leSjSqCicCTG7tCD29bPO8dMKMhmEJZvK15nHm3iqmAnua4NKbYw7/v1vnkxAVxoX6Lp4/cs2rpf3uUCXPH7mGosDTt8UQ1XBS/K0s+ZhX65C4H9kgClFaugd47UwdAJ+YbLS9jmwQBSSzkqNJjA5j0GrjQn2n2KkoQ1NER34OFv9N3ohuKgEgLE9OevgV2hRRRMkz3DjTBMC+AJkiulh2hlilH6shAlLm+LqcoEb3ITp8tQXVma9L/mZhWN3TKBIWg4TjlW3E083M/mETRBK/QzeqPlzeijltIWSvFB40p57zcWUCPeJ+dmrMyJAG+1SHnCByK5HxMFPzO5zEFNGa2dp7XYX/peBNlnfKtHj7ogyxwx5Qs9Qn9YQUxjBYqjVcTj475nByTDj/rBlW/8/bF72WnHfgSjP/8apIG/3yrfNZ1/66ODB3O8RleKUGieeQDaIQ5Q/Hqxm02Fick2CfKJkUA11DZoiyQRRQKIpij/I9PVxmVnyf0Dv3NMKZP/imuAm41tzDfIu4ucoulvpmv2LONmFWOdjNo1EiEnjvxSYfFzUxdR19RDWfBcCWvhCMJh9XFNyszEsizKhQ29Fvl8mMwRQORXeL7SCRmZmtNkqq21lnKMOADVLnQfwMX5clcUBRVjyJ0WF0D1iEV58+RXTyGbB5X8IxGnvE/XB5mWVwKDRETnW4Hz3N7KrrcffB4kNU2dzD5cZuTAaFm+amQVc9dNcL8+RMOcntFZY/JB6v7Ib2sVNCH1s9i6KseDr7LXx3p+cNqyuae/js8yex2lTuXZbNZzbMgDMvioMrPunx7y/xPLJBFIJYbSrPHxZvMA+ty5taTHjdGUCF+ByInWT6mcTnLMkZlWQGYpVi7aNi++D/+cWF8GhOlpaSrrRjxUD0rBW+LkcyHEWxJ5otr3+RKPo5eLWZfrN/x93vvdDEQoMw6w+TBtUeJzrcZH//+em+qxy62oLV5mCSSJeZlb3q1xONrlJW20mf2cqWcLHiKqeH/BeDQWG9JoU8cKUFFt4HEQnQVgnle31bHEMG1QXDDaqbLoB1ECITIHGKU+ES5+g+RBXvjfGBcYbeIDp7vYPewcCVyurysjX5ySREhQ1NqqXOhfCYcZ4pcRvJ+TD7RkB1OMloNCj85z0LAXjp+HVOXfOcrLGz38zfPnOMjj4zS2cm8l/3LUI5/xr0tYl7Qr2ZKgloZIMoBNl9voGa9j6SosO4Y/EU08dqT4pHOV4akCzRJojsSWY6yx+GiHhovgiX3/Z6XRPRfOEAAC0xc2Ryhj9SdA8k5WHqb+ORmAP0m20cHs9rxg/Yc6GRIqVS/EOau3qcnaV1XKjvAuDFY9V89JeH2fjkHnaW1o08cdZ6iJsBAx1weZcPKnUvxzX/oU0m6T8UCOg+RPuvNIvPmiUfEQdO/MaHVQnKm7qBURNEuv9Q5mJpDusJMpdAdCoMdsP1oy49JScpihkJkVhsKqeutXu2Pg+yS5OXbVswSl4mJ9W8i25Wfeo5sI1deFuRm8z9y3MA+PpfzzleeJkmVpvK539/iqtNPWTGR/KLT6wgMswopisBlj0IBqPbv6/E+8gGUQjy7CGxWv6RVbPEH/ZUkP5DAc3imWIF/3JjN90Dw1a2IuOHxkMP/sj7hY2DzaYSVi8ak0rOSh9XI3GI0QQb/hGATxleJQwL+/xYZtZvtnLgShPFhgqxQ3p3eJSdpXU8+txJugdGXtzWd/Tz6HMnRzaJDAYhe4WgkJmdqGolR2kiw1IDihHyNvq6JMk46D5Ep661iemPFbpZ9RtCYuND7BKz1NihnXb/IXnT7hEMBijYLLZd9CFSFMU+RXRksjIzmxUq3oezfxKPDhoC3qCtZ9De3L5ZbxDZDaqX+qSmkGX+HcKbr7PG6Wvwq7fNJy7CxNmaDv5wrNrtJXznzfO8e6mJyDADv3xoJenxkdByVfMKVESDSBIUyAZRiHGlsZv9V5oxKPDxNbOm/oVkgyigSY+LZEZCJKoKpTUdIw+ufVSkEFTth5oTvinQAWV1nSywCd+rpHnrfVyNxClLPgaxGSSaG7nbeMCvjaqPVLSSaG4iWelGVYyQvtDXJQUtVpvKE6+W4WhNU9/3xKtlI1c9F31QPF7cKXzvAhRVVTle2cZ6Q6nYkbNSNOMlfsus5GiyE6MwW1XhIZNRJIyKVSuc+p3P6uoesNDYJSSXs4dLzOpkgpnHKdBkZlcnE3cvpIpHJ2NUXbYDniqGZ+6Alz8tHp8qFvu9zN6LjVhtKvMz45iZrE1t681IqSDwLmGRsOSjYluf2BlFWlwEX9w2F4DvvnWBth7X5JCu8NLxan75vlhM+58PLWWRJhW3G2cXboXEmW77fhLfIhtEIcZzh8X00Jb5GUNv9pOlr20o6lM2iAIWu1H1cB8iEMapuv+HH00RHbxUxyJFfDiZZq7ycTUSp4RFwtrPAvCo6VWqWrrtK97+xt4LjSw0VAKgpM0XtUs8wtGKVuo6+p0eV4G6jv6Rhq5ZSyG5ACx9YnIjQKlu7aOxa4AbjVqDSMrL/B5FUexTRAeuNIudKz8lHk8867OJjgotwSw1Nlz4wYCopV4Y7UuZrAfRvVXqTkO3a5Ox+gTRqWvtDFhceM2U7YCXHoLO2pH7O+vEfi83iXT/oW16ell3k5hgQZEG1b5AN6u++CZ0NTg85aF1uczLiKO918z33r7olm97vLKVf/uLeI/5ws1z+IBuT2I1Q8nvxfaKh93yvST+gWwQhRDdAxb+dOI6AA+vn4aJob56kJgL0cluqEziC5Zo6XVnrneMPahH3p97Bc685PMxZ4BrF44TpQwyYIqVUeT+zspPQWQCBUottxhOsPeC/00RqarKnguNUl7mJRq7nDeHnJ6nKENTRAEsMzte1YqCjRuMuv/QZt8WJHGJ9YXDjKpBJOtFJkLHtUmlWbmT8mbNf2j49FBrOZh7wBQFqfKz0WPEZUB6sdh+9zsuXRMVpMWQEhPOgMXGWUfXWsOxWuCN/wfjzVnu/KrXrsMGLFbe1STiW0fLy1IKISLOK3VIhpG+AHJWi0nGkucdnmIyGnjibjEN/fuj18aqBCbJ9bZePvPcCcxWlduKM3ns5mHvMZd2iuTjmHQRby8JGmSDKIT4y6kaugcs5KfGsKEgdepfSJeXZS93T2ESn6AnCZWMniACyFgIGYsAFf78iM/HnPvNVky1Qu5myVwu/AAk/ktkPKx6BIBHTX9l7wXHK12+5GpTD9daeynWEsykNMOzpMe5Np015rxirUF0dQ/0+LfhuTOOV7WxQLlGgtoJ4bFCYibxe9Zr10lldZ20dA9AWBQs/Zg4eNw3ZtX6NGa+I/+hzGJpEOtJynZAe6XYPva0S9dEw32ITlypgebL4r3s5LOw99vwymfhmbvgh8vhW5kiPt4pqpjeqTrovp9pHA6Xt9IzaCUjPoJF2ZqcSDeolvIy36FP6px81mna8Nr8FO5aMgNVhX//aym2KRpW9wxYeOTZEzR3D1KUFc//fHgJBsMwE/wTmtRt6cdEErIkaJB3WSGCqqo8e7ASgE+syx35Bz5ZpP9QUFCck4CiQE17H83do2Kky3ZAw9mxT/LRmPOJqjaKuQJA9OzVXv3ekimy9lFspkiWGsoxVL7ndzG/+lTT0rBrYoeUZniU1bOTyUqIxNknjwJkJUTab6bspM0VvxubBcpe8XCVnuFEZRsbdP+h3A3yQjpASIuLYH6mmJI4eFVrTuohDpd2jpUBeYFyTWI2IsFMGlR7Hl36Ndg9cr9+TXTuFfF6qD4KpS/DgR/CG1+GFz7GNxs+y8mIv+Pv92+E/1sJv7sXdnwe3n1STIFUvAutV8Fmdq2Wbu8suLyjpZfdvCBj6J5BGlT7noX3QngctFUIr1An/NsHFhATbuTUtXZePnl90t/GZlP50kslnK/rJDU2nF8+vJLocNPQCe3VcOUdsa1L3yRBg2wQhQiHylu43NhNdLiR+1fkTO+L1cgGUTAQHxlGvjamPiLu3maFnV9x8izvjzmDiBpeqogGkZIj/YcCgphUlGWfAOAR5RUOXvGv6Y89FxpJopNUq+YlIf0UPIrRoPD4nUUADptEKvD4nUUYHS1e2GVmL3usPk/R0WvmUmMXNxi0hnuBlJcFEnrc/cGrmg9R2jzR5FOtcNL7ZtVDE0QOGkSyye0Z7NdEzqRfKvzxYfjfBfCrbfCnT8Guf4ejP4eLr5PSdYFkRTSW1PA4SFsAhdtEMt6Wf4d7fwGffB3uf9q1emIz3PWTOUVV1SH/oQXDvp9sRvqe8Jihz8QTjs2qATLiI/mCJgf7zpsX6OhzsQGp8dQ7l3jrXAPhRgM//8QKshOjRp5Q8jygQt4NkFIwqa8t8X9kgyhE+J0WbX/vsmziI6exetnTLPT3ID8gggDdh+h09TCNctXBCVZGvTvmDFByqZJCg1aTlGcEDMqGL2DFyEbjOS6XvOvrcux09ps5VtnKQl1elpwvU6W8wPbiLH764HIyE8bKzcKNBlbkOvG0W6jF3VcdhI7Jr4T6kpPX2ghXB1lt1MxCpUF1QKEbVe/XjaphKPL+pHfNqlVVHWoQ6RNEqgr1MsHMo0x4TaRjgIRZMGs9LPowbPwifOB/sD7wB+7neyzqf5qzD5XCPxyGB/8Edz4FN/4zLPkI5G0U73PxM3DcQkfsj8+GXM+nuJ6r7aSuo5+oMCPrCoQXFz0t0KFFp0vPPt+iy8zO74DeVqen/c2G2RSkxdDSM8j3d11y+cu/erqWH+4Ri7Lfvm/R2M9m27AG+XJpTh2MyAZRCFDb3sfb2qjoQ+vypv6FbFY49ZzYjp8hvBQkAc0SPcls+ASRq+PLXhpzbu8dxFRfAoAlYRbETMM/S+JdEmfRmHcnAAuu/ApVnZoO3t28f6kZi03lhljtol+uvHuN7cVZ7P/KFl54ZC0/eGApLzyyhsU5CQxabfzs3auOn5Q4U9x0oULpn71a73Q5XtXKcsNlIhmE2ExIm+/rkiSTYPXsZEwGherWPq619IqdRXdBVDJ0Xof9T3ktxKGpa4DuAQsGhaEU2o5qkSxrMAkDW4n7cfVa576fwxfPwqfehPt/CVv/A1b9Lcb520nMW0oX0SNTGkdjMML2J7V/OGkSbf+OV3ymdmn3DDfOTSUyTPt+dZp6ILkAIhM8XoNkHLKWiqln6yCc+YPT08JNBv7jLmFY/eyhSs7XdU74pc9cb+ef/ygmxf7uxnw+6Eh1cnWPeP+LSoIFd07pR5D4N7JBFAL8/sg1rDaVtfnJzMucYupA2Q5hxvfO4+LfnbU+MyyWuI/hSWb2m3dXx5e9MOYMwvthqXIZANNM6T8UaCRt+zIAN1qPUHWxxLfFaOzR/Ic26g0iuRrqVYwGhXUFKdy9NJt1Ban80y3zAHjucBUNnU7SzhbdLx4DLM3seGUbG3V5Wf4mkcwmCRhiIkwsm5UIwAFdZmaKgJlrxPaeb3gtxKFcmx6amRxNhEm/ademh9IXiLok7sfVa524LKeHdG+1I+M1iEA0Hz/8LMQ7+FpbHxfHvcBQvH3m0E5dXiYNqn2PogxN7px4RkwSOuGGOWncVpyJTYXH/3pu3IW6hs5+Hnn2OAMWG5vnpfGV7U4WNE5q0rbFD0CYawEUksBCNoiCnAGLlRePCUnYlKeHdHO+0SO2PjIslriPBVlxhBkVWnsGud7WJ3bmrvebMWfQ/IcM2mSBlJcFHJHZCzkRtQGDojKw7398XQ42m8q7l0SDKN8sRqilNMO33DgnlZW5SQxYbPx47xXHJxXdA4pR3KQ0OznHzxi02Dh9vX3IoFrKywKSDaNlZmU74NKbY0/08DWR3aDaof+QfA/zGG64JtIbRMcqWydOlCq6Cx4rhYdfg/t/BbM3if2tFZOtfErUtPdxrrYTgwKb56UNHdATzOTnpX+w+MNgioKm83D92Linfu2OIiLDDBytbGXHacdyyX6zlb/73QkaOgeYkx7LDz+6zLEnYHcjXNTe/6Q5ddAiG0RBzptn62nuHiQzPpJtRVOY+JjQnA+vGxZL3EeEyciCLOG9YpeZTTjmrHptzBngwOUmlhq0G8Js2SAKROoWfQaAgvo3fO4hc6amg+buQdIiLER2VYqd8ubKpyiKwpe2zQXgxaPV1LT3jT0pJhUKtojtAJkiOlfbQbi5k8UG7cYu/ybfFiSZEnqD6NDVFmwWi89CHCqahdHxiAaR9B/yPONeE2n/nuCaqDg7gagwI+29Zi43djs9b8T3nH2DMCO+6f+JfaUvw4ALz50mu7XpoRW5SaTEDptKkwlm/kVkgkg0g3HNqgGyE6P43OZCAL71+nm6+kcaVquqyldfPsPp6nYSo8N4+uGVxDnzqy15XqSK5qyCjKJp/xgS/0Q2iIKcZw5VAvCxNbMIM07h1+2HhsUS97I4R2jJT1e3D+0cb8w5MtFrNzrVrb3Y2ipJUbpQDWEyaSpAKV59MwetRZiwMvj+D31aiy4v+3BOOwqqkAXEpk3wLImnWV+Yytr8ZAatNv5vj5MJIT255ewfxx2p9xdOVLWx3lCGAVV4D8XP8HVJkimwdGYiMeFGWnsGqSp5x2fXREMG1cP8H+2pUlIm61GcXRPFzxD7J5B+hRkNrMhNAuBoxSQTPXM3QEohDHZ7JclR9x/aOjy9rLcV2mVAjd+hT/Cc+zP0j+8v9MiN+eSlRNPYNcAPdl/m0NUW/lpSw6GrLfx43xVeKanFZFD4yceXk5sS4/iLqKow5wdpTh3kyAZREHP2egenrrUTZlR4YPXMqX0RPzMslrifIaPqjpEHRo85P/gXSMqH/nbY8y2v1HbgSjPLFCEvUzIXSa1zgJKXGsOfYz4CgOHUsyINxUfs1RpENyfWiR3yYtdv+NI24UX0x+PVQ4bAw5n/ATBFQsuVoRtjP2aM/5AkIAkzGliTL5KcysudGKmPxgPXRLrEzB5x390IXXWAAhnFbv9+klGMviZ6+DV47KzLvkAu+xCNRlGGGgEnx58UmS5d/WYOl4vP5xGqA/39NikPohI9WoNkEsxaC6nzwNw74WRthMnI43cKw+qn36/go788zD++WMJHf3mY770lEs7+466FrC8YJwimcj+0louQIn16SRKUyAZREPOsNj10+6Is0uOmeGPtZ4bFEvejG1WX1nRgHa2NHz7mXLgF7vhfsf/YL6H2lMdr23+lmWUGYVBNziqPfz+J54gr2soZ22xM1j448jOf1NDY2c/ZGtEIXYAm+5EJZn7D6tnJ3DAnFYtN5Ud7Lo89ISIO5m4X234uM1NVleNVbdJ/KEhYr0V9H20Od+0Jbr4mMlttXGsVTVO7xEw3qE4phAiZKusVhl8Tzb5hUlJ7vUF0tKJ18omeSz4mkupqTkB96eSeOwneu9SM2aqSnxYzalKtRDxKeZl/Mbx5OIHMDIQv7Xikxk7w/qY3KBd9UL7nBDmyQRSktPUM2o3IHlqXO/UvpKo4N+YDbxsWS9xPQVosMeFGegetXJlIG1+wGRZ9CFQbvPZFj3pP2WyqSDCTBtVBweb5GfzUIlZa1aM/h4Eur9ew72ITAEtyEohqOSd2SmmGX6F7Ef35VI1dUjMCXWZW+mew2bxY2eSoauklsuc6sw0NqIoR8jb6uiTJNNg4R6yqP1eXjRo3nmExHrkmut7Wh8WmEhVmJDNeW/Cr1+VlcgoyEFg6M5Fwo4HGrgGqHE1IjkdsGsy7XWzrEh8PsKusHoBtC0Y1OHWDaplg5n8seQAMYaKJN85krdWm8sSrZU6PK8ATr5aNXSjW6W0dMuCX8rKgRzaIgpSXjlczYLGxcEY8y2clTe2LXHobfv8hhgyqp2bOJ/FvjAaF4mwHPkTOuOVbEJEgJoiO/9pjdZXVddLd08NCpVLsyF7hse8l8TyrZyfznnEtV21ZKP0dcOK3Xq9B9x/aOjcRGs+LnfLmyq9YNiuJLfPTsdpUfvDOpbEnFG6DiHjh83LtkPcLdJHh00NKziox/SQJWOZlxJEaG06PWeXy8q9pe500iW74J7dfE5U3icWbvNQYDHqykPQfCigiw4wsmSmutY5OVmYGsEK7KT/zIpgdGPlPE7PVNvQZOTrUxv5aW+r27yuZJjGpsOAOsT1O8/BoRSt1Hf1Oj6tAXUe/89fmmT+AdUB4gc5YNo2CJYGAbBAFIVabyu8OVwHw8Lo8FGW8CSAnlL4ML34ULP1ipP/+X03ZnE/i/yzVZGb2JLPxiMuArV8X27u/AV31HqnpwJVmFihVhCsWiEqG5HyPfB+Jd4gMM7KuMI2fWe8UOw79GCwDXvneVpvK+5ea2HtRXPxuS20TKRyRiZAwRX82icf44lYxRfTX07VcaRw1aRYWCQu0zxw/lpmdqGqV/kNBhKIodm+O18wrHRsWG7TUnxO/dfsNvN2gekTEvUwwCzTWzBZSxUn7EAHkb4GEWdDfMTTJ4UaOV7bR2W8hOSZ85MJyXzu0aZJs+VrzT/SJnjN/hEHH02mNXc6bQxOeN9qceir3lZKAQjaIgpB9Fxu53tZHQlQYdy6ZQmrKid/Cnz4tbqAWfQg+8pwY65+GOZ/Ev1lsN6pud+0JK/5GTPQMdMJb/+qRmvZfaR6Sl2WvkB9IQcCmeem8Yt1IsyFVmKuefsHj33NnaR0bn9zDJ359lAGLkCS9/Pob4mDWYvm68kMW5SRwS1EGqgrff8eBF9Gi+8XjuVfAah573A84UdHCeoMmY5QNoqBgoxZ3v/9Ks2PD4s8dh+gUET3/2hfdlrRntakc0oyDw02KkID0dwzdtEsftYBhyKh6CkENBgMs/4TY9oBZ9TtavP2W+ekYDcM+F/XpocRZEJ3s9u8rcQOzb4LEXBjogLK/OjzFVS9ah+ddPw6NZWCKEveFkqBHNoiCkGcOiemhj6yaSVT4JMecD/wAXv1HQIWVn4J7fwFGbVVsGuZ8Ev9GH3u+UNdFv9kFXyGDEe74PigGMW12Zbdb6+k3WzlW2SoNqoOMTfPSMGPip4O3iR0HfuBRH6udpXU8+tzJMWPVswbE66oirNBj31syPb6oeRG9fqaO83Wj4nvzboSYNOhrhat7fVDd+LT3DhLWXEaK0oUaFiP904KE9YVi+uP09Q66+s1jr4mS8+BDvxWfi6dfgGNPT/t76g3u3efF9ONfTtWy8ck9HDm0T5yQIG/aA4nluUkYDQrX2/qoaZ/ClNnSj4vXV9UBaHbQPJ8iqqo6jrcHaVAdCLjQPFw9O5mshEin7mkKkJUQaW9ijuDkb8Xjwntkil2IIBtEQUZ5UzfvXWpCUeDBNZMwp1ZVIRfapUmHNn4RPvC/4k1HEvRkJ0aREhOOxaZSNvpmzBlZS2DNZ8T26//k1pH6k1Vt9JttrDCWix050n8oGMhJimZOeiwvWDYzGJ4g4lL3PQln/wQV77u1WaQbMjpawy8yVALw2/J454aMEp+yICueDywSEp6nRnsRGU2w8D6x7YcysxNVbWzQ5GXK7BuGFlkkAU1OUjR5KdFYbSpHyp1IhGbfCNu+IbZ3fhWuHZ7y93PW4K7v6Oftd94W/5D+QwFFbISJ4hnxABybiswsIRvm3CK23WhWfaWxh2utvYSbDNwwZ1TMuT5BJA2q/ZulD4rm4bVD0HRxzGGjQeHxO4sAp46yPH5n0cjpMYD+ThEKAUOJaZKgR979BxnPHb4GwOZ56cxKiXbtSTYbvPHP8P7/iH9v/Q/xPym9CBkURbHH3Z9xxahaZ/O/QtwMMeq+//tuq2f/lWaS6GQmmr+RNKgOGjbPT6eXSM5FLhc73nsSXv40PHMHPFXsNm8FZ4aMBmwsUMT75P6e7KmZhUq8wmNb56Ao8Na5BkprOkYe1NPMzr/m1HPBVxyvamOjjLcPSjZoMrMDV5udn7Tuc6KBabPASw9BZ92kv894DW4VWKg1uW1SXhZwDMnMpvjZo9+kl/weLINuqWm3Zk69oSCFmAjTyIN6gpn0H/Jv4rNgzq1i20nzcHtxFj99cDmZCSNlZJkJkfz0weVsL84a+6TSl8HcC6lzYdY6d1ct8VNkgygIsNpUDl1t4aXj1bxwVMjLXI62t5rhL3+vjUIrQja08YueK1bityzO0ZLMrndMcOYwIuLgtu+I7f3fd9vI84ErzSzR/YdSCiFqikl8Er9j07w0bjUcZWnn3rE3P5114oZqmk0ii9XGm6WOb8pmK3VEKwP0qhFUqFkuGzdKvM+cjDju1nz0/nfXqCminFXCE8PcA5d2+qA655ypaGC14YL4R/5m3xYjcSv2BtGVcRpEigJ3/x+kF0F3A/zx4UnfyE+UOKSne15UZk/q60p8z2rNqProVHyIQDQBYjOhtxkuvuGWmnZfbAIcpJf1d0Crdi2WJZOr/B496e70C05DQLYXZ7H/K1t44ZG1/OCBpbzwyFr2f2WL4+YQDEnWlj8kBwdCCNkgCnB0ffpHf3mYL//pDH1mG0aDQu+AC1INc7+4GTv7EhhMcP/TwndIEpIsmUyS2XAW3CWip62D8PqXpm3M2dFr5kxNB8vsBtXSvyOYWDkzgSfCfoeqOgqJ1l47O786JbnZgMXK749cY8v/vMuzmhfbaPQbq/PqLGwYXDZulPiGL9w8B4MCey40cupa29ABRYFibYqo9GXfFOeAAYsVU+1RIhUzlugMSJvn65IkbmRdfgqKApcaumnsHKe5HB4jAj4iEqD6CLz1L5P6PnqsvSMiGaBQqQHgWvicSX1die9ZlScWvK429dDcPYUkT6MJln1cbLvBrLpzcGhhcIz/UL2WxJgwE2JSpv29JB6mcBvEZUFvC1x43elpRoPCuoIU7l6azbqClLGyMp26M1B7SiQ0Lvmoh4qW+COyQRTAONOnW20q//D7k+x0soIOwEAXPP9BsfpgioQHfj80si8JSZZoSWblTT109E0iGUhR4Pb/Fq+jivfg7B+nVceh8mZUFdZHaAkt0uA1qAivOUym0oKz6xFQobMGqg66/DV7Biw8/X45N353L//6l7Nca+0lKTqM2AjTmCaULs0os+U5N2SU+A35abHctzwHcDBFpH9mXX5bRDH7AaU1naxRRfy4sXCTXHENMpJiwv9/e/cdH2WV/v//NTPpkEIgpFBC6FIE6V1EhPhRAbsgCmt3Lcv603X16y7qrqtYdtWPflzLCgKKytrAAiIdBRFIVIoQIPQEJCEFUpk5vz/uJBDTG5mZvJ+PB48MM2fu+2SuZHLmus+5Dj2Lash8t6eKGSAtO8HVbwI2a5Z2wrtVHj/5+Cn+3yc/8/iibRW26W47iMNm+NWEEBLRribdFzcQFuRH96hgoJZ1iAAuKCpIvGclnCj/Ykh1bTthwxjo0zaUyJDfXDDR8jLP4vCxCplD/dSoKj5G98ugWavK24pXUYLIQ1W2Pr3YE4u3l1+ANScd3pkA+9aCXzBM/Qi6jm+wvopnCG/mR7vwQICy9T6qfHIcjHrIur30Ucg9UXn7SqzbfRwbLnqx27pDCSLvcvJovbXLyCngpW+SGD5rBX//YgdHs/KJCgngr5f34Ns/j+H5a636HGd/RO9VNINoq+lQfkFGcTv3j+mCj93G2qTj/LDvrA9UkT0h4jxr9uKOxY3XwbNs3p/O8KL6Q7ZOYxq5N9IQhp+93X1Vuo6H0UWzhz7/o3U1vhyb95/grnmbGfPCKt79/gCFLoOvo/z3puIk9257JwZ11KwOT1TnOkThcUX1zQwkzKvVIZwuw/fJ6aw7av2cjeneumwj7WDmeYp3M9u7Ek7sq/1xCnLgpw+t28VL16TJUILIQ1W1Pt0AKZl5ZQuwZqXA7P+BI1sgMBymLYIOIxq2s+IximcRJdakUHWxYfdDq25w6ldrR7xaWpd0nDhbKoHObGtWUmSvWh9L3FDzyKrbAKz/vzO7p/zGsaw8nv5yB8OfWcG/vtlFRk4hHVoGMevq3qz500XcMiKOID+fcgoympIPVxPj4ytecy9upX3LIK4dUDSL6OsKZhG5yW5m2/fsp7etaPZj3IWN2xlpECPOqkNkqrOketRD0PVScObDBzfBKWvmkctl+HpbKte89h1Xv/YdS7alYoz1QX3B7UN4+YYLsFF2KW6vop+v1l0HKsHtoYoTRHXaJKFf0Yf2hHfBebpGTy0uTzH17U0cOmV9FJy3YX/ZlQfFM4i0g5nnaNHhTO27LbVLHgKw/TPIz7Rq/cWNroeOiSdRgshDVbewaql26ckwOx5+3WGtUf3dV9CmXwP1UDxRcYLop5rWIQLw8YPL/2nd3jQbDm2q8SEOnchlX1oO/RzFRRH7aItobxM7DEJicFXV7shmeH0ULJwOx63ZZAfTc3js058Z8exKXl+zl1MFTrpHBfO/ky9g+f83musHtsfPp/SftbMLMr4xoTVhtlMYuw/Dho5skG9PGsa9Y7rg57Czfm8a3529g1Svq62vyWsgu5qz0xqIMQa/g+uw2wy5YV2sXWXE6wyIDcfPYSclM4/k46eqfoLdDle9DuGdIPMgzoW/Y8H6vYz952rumLeZTftP4OuwcW3/tnz9x1G8PX0gQzu15NLe5e841MfX2oWx0/nDG+Lbk3OgOEG0IzWrZkv6z9b9MutCb/YR2P1NtZ9WUXmKtJMF3D3/rPIU+dmQVjSTW0vMPEvJTnc1Tx6WKF5edsHN1nuYNCmKuIeqbmHVknbHdsDb8dZ0wxZxcMsSaN294TooHqmkUPXBGi4xK9ZhBPSZAhj4fEaN/zCt32tdWb04+KB1hwpUex+7A+JnYQPKroAtul5+6XPQ+zrr9rZPMK8OYsOLU5j8/EfM33CAgtMu+rUP4+3pA/jqDyO5ok9MpVfSiwsyjmthJRBsrc8DH/8G+galIbQJC+SGQVa9lX9+vevMzI3wOOt9wrhg2yeN2EPYl5ZDn4JEAHy7anmZtwr0c9A/1io0XOluZmcLCCVz4hwK7IE49q0m84u/svf4KYIDfLh7dCfWPTyG567tQ9fI4FJP++2OQ+/f2p/z7EV/H6O1xb2nah0cQMdWzTAGNu2r5SwiH3/oO8W6Xc1i1ZWVpyi+r6Q8RerP1r3BMdC8nOVn4r66XwZBLSE7BXYvq/nzf90FB74Dm/1MQXRpUpQg8lCD4sKJDg0oZxcgiw3OFGA9vBlmXwonU61tV29ZYk1BFPmNXm1CsNsgNSuPo5Xt0FKZcX+DgDBrcLHxjRo99ds91kCpZAeztv1r1wdxbz0msOei10ildIFoExID182FwXfA1W+y68qv+DFoKDbjZEjGFyz3/SOvtfov/725Cx/dPYwx3SOx1aQIcIpVPJgoXQ31RL8f3Rk/Hzub9p9gbdJZH8x7X2t9beRlZpsPZJTUH/JR/SGvNqJL8TKzqrcqP5iew+OLtjHkrSPMyLsdgLt8FvOfAYdY/8jFPBzfvWxx4LOcvePQkODj2JwF1u5oLbTFvSern2VmRTNFdi21SkhUoUblKbS8zHP5+J/ZdWxzLXa6K044dhkPITH11y/xGEoQeSiH3cbMK3oAZdenF/9/5hU9cOxfZxWkzj1hXWWd/gUER53TvornCPLzKbmC+WNt6hCBtdPBJUU1iFY+BZmHq/U0l7FmEPlTQOucJOvOtgNr1wdxeztbjGZk/svcUPAY9xfcyw0FjzEi70W+cg5kw940bvrP94xbkM7E9Pu4Kv9xdgX0wd9WyKUnP2bApxdhW/U05GXV7KSpRQkiXXn3SFGhAUwdHAvAC8vOmkXU80rrSuehH6yl1I0kefcOOtiP4rQ5oIOW/3izYZ2s4tBrdh3jk4TDrN+TVmZTkJ8OZXDPe1u48LmVzPluH7mFTvZFjiOp8y0AXLzzCZpnJtXsxMV12aJ6a4c8D1fnQtUAEd2g/VAwTkicX2XzGpWnUIFqz1acPExaCllHqv+80/nw4wLrtopTN1lKEHmwsgVYLVGhAbw2tR/xvokw/2ooOAlxo+DmzyBIWzpL5c5vGwrAj7WpQ1Tsgpug3WDrZ2/Jn6v1lJQcSD9VyAC//djNaWjWGkK1ha83WrI1hXvfS8CJnQ2uHixyDWODqweHswq5+90t3PDGBtYmHcdht3FVvzbMmnEbXR9eDVM/tgarBSdh9Sx46Xz49mUozK3eiUs+XClB5KnuHt2JAF87Px7MYMUvx6w7gyOhQ1FNqa0fNVrf/A+sBSC7ZV/wD668sXi0Ixm52ICcQhd//CCRyW9uYMSsFXz5UworfjnK9a+vZ8Ir3/LFTym4DIzqGsG7tw3mi/tH0GXyc1YB88JT8P6NkJtR/ROXJLk1C9LTFSeIth7O5FR+LevEwJli1Vvmgavy6n41Kk9R/PdSM4g8U0ny0GXVIqqunV9CTppVq7bzJQ3XP3FrShB5uPhe0ax76EK+uAL+O+IwX1wB6x66kHjXOmvg4cyHbpfBlIXg37yxuyseoLgO0U+HalmHCKyCdpf/C2wO2LHImv5chZ2Z1tXQy1sWXeloO0BXSL1QZTUQznbj4PasenA0/7yuL10ig62fhc4Xwx2rrGVorbpaMyOX/QVe7mcVRndWUuzz5K/WenxsEKWd8TxVRLA/04Z2AOCfZ88iKtnNrHESRCcLoVvOZgD8u13cKH2Qc6M4wf3b97CUzDx+/94Wbpmzie+T0/Gx27jqgjZ89YeRzL1lEMM7t7KWxDp84Jq3rQsg6Xvgk7uq/GB/5iRFH9o1C9LjtW0RRJuwQE67DAkHMmp/oB4TrSWHGfsheXWlTQfFhRMWWPHGHyXlKdr4w/GiHSOVjPRcNUgelihektb3Ruu9SpokJYg83fZFOF7uTc9lUxiw6SF6LpuC4/mO8PFt1pTT82+wPkz5Vu+qgUjxTmY/Hsyo3ha+FYnsCUPvsW5/+SAU5JTbzOkyfJ+czg/HrGTQIN+91gNtVH/IG1VVA6HY5efH0C48qOwDNps1IL57PUx81fqQlX3EKor+6iD4+b9lB0Iu55kraCHR4FvOccVj3HlhJ5r5Odh2JIul24p2LjvvCnD4wbHtcHTbOe9TcpZhWFH9oUAliLxWdRLcNuC2kXGsffgi/nl9X86LDinbqFkruH4eOPxh11ew5rmqT+5yFRUORh/avcSZOkRV17KqkF8QnF9Uh62KYtW/pGZxqqD82UqlylMc22rNPGkepbIUnqwGyUPA2sho70rrdr+bGrRr4t6UIPJk2xfBhzeXXVuaVzTzo/MlMOk1ZYClRrpFBePnYycr7zT70spP6lTbhQ9DSFvIOFDuAHjJ1hRGzFrB1Lc3cSTXejsKPJZoPdhWO5h5oxrVQKiMwwcumAr3bYb4WdAsAtL3wke3wusjYecSMMZ6n3yxF3wz03pe1hHr/9sX1fE7kcYS3syP3w23CvT+a9kuXC4DgS3OTIf/+dwXq3ZmHCDcdpI8e5CS216sOgluA1zcPZLo0MDKDxZzgTXTFmDV01XPtE3fay2v9QmAll2q32lxW/VShwjOzBTZ8TmcKn9nvfRTBdwxdzOFTkOP6BCiQiooT9ErWgWqvUUNkocAJBTVseo4WpsZNXFKEHkqlxOWPAyVXcc6tr3yx0XK4euw0zPGuuJZ60LVxfybw/88a93+7mU4tqPkoSVbU7h7/pZSg+0IMojhV1zGxrLMNnU7t7ilGtVAqA4ffxhyF9yfCGMeA/8QOLoVFlwPrwyCD28qm0TPSrGS60oSeazbRsYR7O/DzqPZfLm1aPee3ldbX7d+ZCUHz6Gok9aspfSIQeCoeAmHeLZ6S3AXu+BGGHArYOCj2yFtT8VtU4uWl0X20oU/L1GcIEo4mEH+aWftDxR9vpVwdBWeKTB8ltNOF/ct2MLhjFxiWwax4PYhfPvnMcy/ZQA3d3Ey/5YBrHt4jJUcgrOWMmqmmserRvIQAOfpMwmi4udIk6UEkafa/13VVemzDlvtRGqoZJlZXQpVF+t+GXT7H3Cdhs8fAGMqnKbf174bgCTThr8uOVBmVxjxfIPiwokODSiz+2KxkhoIcTUsqO/fHEY9BH/4EYbPAEcApO2qoHHRz9WSP1vJdvE4YUF+3DrSmkX04jdJ1ntF10vBt5k1nf7QpnPSD6fLsDbpOD0LreVlfl20vb03q/cEN0D8M9B2EORnwgdTIf9k+e1Uf8jrdGzVjFbN/Sk47apb3Uc486F+8ztlEuTPLt3Jt7vTCPJz8MZNAwgN8sVhtzE4Lpz+rQyD48Jx2M/6q6wdzLxH9PlWHF2F8OP7Fbfbvcyq0xjU0hq3S5OmBJGnOnm0ftuJnKVPu6KdzOo6g6jYpbOsui8HvoPE9yqcpl+cIEp0dSYlM4+NdZ12LW7HYbcx84oeAGWSRKVqINgrSiFVISgcLnkCrnq9ioZGSXQPd8uIOEIDfdl97CSLfjxsTacvHtj+vLDBz79kawqjnlnG7PmzGWy3ZkfetyGEJcUzmsTrNEiC28fPqhXZPNKa+b3ovvJnwKVoBzNvY7NZSRqg7uOd3tdYCfK0JDiwvuTuzxIP88Yaq7bjc9f0oVtUFTssFuTAr79Yt7XEzDsUb1e/pWzysERxceo+k62Z2dKkKUHkqZpH1m87kbMUzyDadiSLQmc1dz6oTFh7GF203f3Xj5F+PLXcZn1t1vT6RNMJqME0ffEo8b2ieW1qP6JCK6mBUFeuam4brCS6xwoJ8OWOUR0BeOmbJE47XWd2M9v2iTVlvoEs2ZrCp+/9m4X5dzLPbxa+Nmsm2guFT/Lpe/9WkshLNViCOyQarn0H7D6w7WNY/0rpx405M4MoSjOIvEm91SHyD4ZeV1q3t8wFYPuRLB7+yEos3j26E5edX42/rUe3WQWqm7W2tjoXz9frGusi7fFdcGBD2cezjkBSUQ20fjef276JW1KCyFPFDoOQGMoOUYrZIKSN1U6khjq0bEZwgA/5p13sTM2un4MO+T2uiPMgN53TS/5S5mE7Ls63W1e5El2dgRpO0xePEt8rmnUPj2HB7UN46Ya+LLh9SOkaCHWlJHqTMH1YB8Kb+bEvLYePEw5DpzEQGA6njsG+tQ1yTqfLsOrTt/k/3xeJovSHuijS+T/fF1n16dtaIuulGizBHTsUxj9t3V72V9h71q5DWYchN91KILXuUcueizsqThBt3pduJbnrot906+u2T8lI+5U75m0ir9DFqK4RPDiuW/WOUbK8rI+1a6h4voAQ6HmVdbsoeVhK4rtWUrD9UIio5s+JeDUliDyV3WHt3ANUeB0r/hmrnUgN2e22kllEdV4XjzUT6Plv9vK7X6cAMNEsZ4B9Z6k2nWxHCLblcsr4k2Ta1q4OjXgUh93G0E4tmdi3DUM7taz9srLyKIneJDTz9+GuC61ZRC8vT6LAOKytfQG2NsxuZhv3/Mr9hW8B8Nsf2eL/31/4Hzbu+bVBzi+Nr8ES3INut5Z4GBf893eQcdC6v3j2UER38NWFE2/SLTKYkAAfThU42XYkq24HazvASiCezuXTeS9y6EQu7cODePmGvtX/+6odzLxT8TKzbZ9AbsaZ+10u2DLPuq3i1FJECSJP1mOCtW495DcDkpAY6/4eExqnX+IVzm9b9zpESUezefi/PzHimZW8snI3q/M68bmPtRX1263ew5fTJR/fL7AnAfCz6YgTR93q0Igoid5k3DSkA62a+3PoRC7/3XzozDKz7YvhdH79niwrhRYbnyPGll4mOVTMboMYWxrOfd/W77nFrTRIgttmg8v/ZS0jy0mzdmHMPwU7FluPN49SYX0vY7fbSi6G1bkOkc1W8iF/UPpiAn3tvHFzf8KC/Kp/DBWo9k5tB0LEeXA6t3SNvuRV1sYO/qFnLq5Ik6cEkafrMQFmbIVpn8PV/7G+zvhZySGpsz7twoCa72RmjOG7Pcf53eyNXPKvNXyw6SAFThf92ofx76n9uHTG6xDUkpCsJBYP/Llkmn5fW9EOZr7d6q8OjTRtSqI3CYF+Dn4/2qpb9sqKJPLbDIbgGGtXqKRldTt49lH4+b+w+A+Yl/vDP7vTPemNaj21tS2jbueWpsk3EK6fD4Et4EgCPN/5zNble76BF3vB9kWN20epV/VWhwj4yjGKfONLD/t+3rjYTveokOo/uTAPjlkF9zWDyMvYbGfqC529zKz49vnXWhs9iOABCaLs7GxmzJhBbGwsgYGBDBs2jB9++KHkcWMMf/3rX4mOjiYwMJCxY8eSlJTUiD1uBHYHxI20rprGjdQVcakXxUvMdh3NJqeg6mKvhU4XnyUe5opX1jHlze9ZufNXbDaI7xnFR3cP5ePfDye+VzSO5i1h3N8B6P7Lq6y7szPzbxnAmEArQTTlqquUHJL6oyR6kzBlcHsiQ/w5kpnH+z8chl5F9Ra+f81K8CSvrd7Mi5PHYOvH8Pkf4ZWB8EJX+OhW2DwHW/punMbGHldUtfrUqWOnOnxH0qS1iLWWmwEU5pR+LCsFPrxZSSIvMiiuJQA/7EvHVYfaZduPZPHHRfv50jUIgJHZX9bsAEe3gXFaW52HtKl1P8RN9bkBHH6Q+pOVfD51HHZ8bj2m5WVyFp/G7kBVbrvtNrZu3cq8efOIiYlh/vz5jB07lu3bt9OmTRueffZZXn75Zd555x3i4uL4y1/+wvjx49m+fTsBAVqnLVJbUaEBtA7241h2Aa+v3suQji0ZFBdeZhp9dl4hH/xwkNnf7uNwRi4AAb52ru3fjltHxNGhVbOyB+8zGRLmw/5vcSx5mCEX3Iyj0Kq14GjTr8G/N2liipPo4rUCfB3ce1Fn/vLZNl5duZspI1vhC7BvnfUPrJlj8bNKJwdPHS9qs9b6Wry9cxGDjV/owLrT57HBdR7JQX2YMrIH0esvIyD3aLnLzFwG8oOiCOwwvMG+X/FyLqf1N7JcBrDBkj9D98t0UdAL9IwJIcjPQWZuIbuOZdds1k+RE6cKuHO+VZR6e7tJXPnrt1ZyfNxT4N+8egdJSbC+RvdVgWpvFBQO502w6vOtesba+c5VaMU7WrsjyhlunSDKzc3lo48+4rPPPmPUqFEAPP744yxevJjXXnuNv/3tb7z44os89thjTJxorZucO3cukZGRfPrpp9xwww2N2X0Rj7ZkawqZudbMoZeWJ/HS8iSiQwOYeUUP4ntFk5KZy5xv9/He9wfIzrfatWrux7ShHbhxSCzhzSpZ826zwWX/hNeGwq6v8Nn11ZnH3h5X9kOciEgVrhvYjn+v3kuvrNX4rHipbIPimRfD7rNqE+1bC8e2l2lWGNGTBHsv5qa0Y01BV7JoTmzLIO4c1Ymr+7fB38cBEc9jPrwZF6bUVGwXYLPZCLziOX1wl9rb/5219XSFjLWz2f7vlPz2Ar4OO/1jW7A26Tgbk9NrnCByugz3v5/AwfRc2oUHcs/0afCff0P6Htj2cfW3LleBau/Xqov1ddeSM/edSLZmJGrcLUXcOkF0+vRpnE5nmZlAgYGBrFu3juTkZFJTUxk7dmzJY6GhoQwePJj169dXmCDKz88nP/9M4cqsLGvXgMLCQgoLC+v9+yg+ZkMcW2pHManc0m1Hue/9H/ntROfUzDzumr+FgbFhJBzM5HTRVOiOrZpx6/BYJvaJxt/X+lBU1WtrO7oDh3GVKR9sij7EOa+ejel+eT19R1Ib+j1xT4pL+ezAPaPaM3rpXAymnP3rit7Rvnu59L0R5+GKHcGvLQfyxoFo5v10kkKn1bZ7ZHOeGBVHfM9IfBx2MC4KC13Q5VJsV8/G8fWjkH3mg7wtuA3OcU9hulwKik+j8uTfE1vm4WoN0E9nHsZ40PfnyTFpaP3bh7E26Tgb9hxn8oCaLe96duku1iYdJ9DXzv9N7kszPzvOvjfiWPEkrk1zcPaeXO7zfhsPnyOJ2IDTrXt51M+Vt2mo3xPbL5/jWPWMdfus+01elsbdVfCW967q9t9mjKn9YtdzYNiwYfj5+fHee+8RGRnJggULmDZtGp07d2b27NkMHz6cI0eOEB19pmbJddddh81m44MPPij3mI8//jhPPPFEmfvfe+89goJUoEuaNpeBJ7Y4yCiAircIt3QOcXFRjKFHmKlwR59yGRfjtj1AQGF6uWcwQK5vOMt6/hNsbl8qTUTcRIusHYza83SV7Y6E9ONQ+DDSmndnX0EI3xyxs+W4rSStFBdsuKSNix5hpvKVFsZFy5M7CSjMIM83jLTm3fSeJXXWMnsHI3ZX/XO8rvMjpAWfdw56JA1tTxa8vM2HEF/Dk/2d1V7hteW4jXeSrAtz07o46dfK+ljnX5jJuK0zsONkRfenyA5sV+lx7K5CLvvpDuzGydc9XiDXP6JO34+4GY27BcjJyWHKlClkZmYSElLxTEW3nkEEMG/ePG655RbatGmDw+GgX79+TJ48mc2bN9f6mI888ggPPPBAyf+zsrJo164d48aNq/TFqq3CwkKWLVvGJZdcgq+vb70fX2pOManY98npZGzYVGW7J684j8mDKh9wVMS2fx0+iRXv1mEDggrTuaxXGCZ2RK3OIXWn3xP3pLhUzLYtF/ZU3a71mDs5FHwxH67Zy8qdx0vuH9WlJXeN6sjADi2qfc7CwvGKhxvy6N8T13jMK+9Adgq2MnN5rdpYhMQw+NoZHrWU0aNj0sDyC538e+dKsgpd9BxyIR1allO/8Td+Sc3mz298D7i4bUQHHh7ftXSD01/DL4u5sPk+XOPvLPP8s+Ph9+tW7D86MYEtuGjSzapB1Iga4vdE4+668Zb3ruJVU1Vx+wRRp06dWL16NadOnSIrK4vo6Giuv/56OnbsSFSUtZPI0aNHS80gOnr0KH379q3wmP7+/vj7+5e539fXt0GD3tDHl5pTTMpKy6l6xzKA0Gb+tX/tctOq1cwnNw0Un0an3xP3pLiUI7R6SzOeWpPB7CMbAetz0P/0iubu0Z3o1Sa01qdWPNyTZ8bFFy6dZdXMwgalkkQ2awZA/DP4+nvmZiyeGZOG5evrS992YWxMTmfzgSy6RIVV2j4jp4DfL0gkt9DFyC6teOR/epTZRIQB0+GXxTi2fohj/N/AN7DCc/v8uhUAW3RffP0qqSEp50y9/p5o3F0vPP29q7p995g5ZM2aNSM6OpoTJ06wdOlSJk6cSFxcHFFRUSxfvrykXVZWFt9//z1Dhw5txN6KeK7WwdUbcFa3XbmaR9ZvOxERgNhh5AZGUdFO0S4DR0xL3jnSBl+HjesHtGP5Axfy6o396pQcEql3PSbAdXMhJLr0/SEx1v0qKOt1BseFA7AxueKZHmAVpb5vwZmi1C/fcEHZ5BBAx4sgtB3kZcKOxZWfPOVH62t0n9p0Xdydxt1SA24/g2jp0qUYY+jWrRu7d+/moYceonv37vzud7/DZrMxY8YM/v73v9OlS5eSbe5jYmKYNGlSY3ddxCMNigsnOjSA1My8cia2W9cyo0IDGFQ0kKmV2GHWIDcrBSo6S0iM1U5EpJqc2Hmi8Gb+wbO4DKVqoxUnjZ4ovImbh3Xkzgs7Eh1a/hV1EbfQY4K1lf3+7+DkUevDW+wwj1pWJtVXPK76vooE0XNLd7I26TgBvnZenzqAFhXtGmt3wAU3wap/wOZ34PzrKj6odjDzbhp3Sw24/QyizMxM7rnnHrp3787NN9/MiBEjWLp0ackUqT/96U/cd9993HHHHQwcOJCTJ0+yZMmSMjufiUj1OOw2Zl7RAyhborr4/zOvKGcqc03YHdZW9pWdJf4ZDYJFpEY2Jqfz/sm+3F04g1RKJ7FTacndhTNY6hrE+J5RSg6JZ7A7rK3se19jfdXfRa/Vr30LHHYbhzNyOXQip9w2n/90hH+vtgqtPXtNH3rEVFE79YKpVtHh/evg+O7y2zgL4Nh263Z031r2Xtyaxt1SA26fILruuuvYs2cP+fn5pKSk8MorrxAaemYauM1m48knnyQ1NZW8vDy++eYbunbtWskRRaQq8b2ieW1qP6JCSydao0IDeG1qP+J7RVfwzBrQ9HkRqWfHsvMAWOoaxIj8l7mh4DHuL7iXGwoeY0T+Syx1DSrVTkTEXTTz9ylZ6vrDvrKziH5JzeKhhT8BcMeojkzoE1P1QUPbQOdLrNtb3im/za+/WEmigFBo0aE2XRdPoHG3VJPbLzETkcYR3yuaS3pEsTE5nWPZebQOtpaV1Wnm0G8VTZ8/vXcNiWuX0nfkeHw6jtIVDBGplbNro7mws8HVo8p2IiLuYnBcOD8ezGBjcjpXXtC25P6MnALumLuZ3EInwzu35E/ju1X/oP2nQdJSSHwPxvwFfEovSbOdXX9Iu5d5Ny1blWpQgkhEKuSw2xjaqWXDnsTuwMSO4PC2LPrEjtAfKRGptXNSQ01EpIEM6hDOG2v2lqpD5HQZ7n8/kQPpObRtEcgrk/vh46jBIpAu46F5FJxMhV1fQY+JpR62pRYniPrWw3cgbq942apIBdx+iZmIiIhIdZyTGmoiIg1kYAcreb3311PM37Cf9XvSeHbpL6zZ9atVlPqm/hUXpa6Iwwf6TrFuby67zKxkBpEKVIsIShCJiIiIFzknNdRERBrA+r3H8SlKYD/26VYmv7mB11fvBWDW1efTMya0sqdXrN9N1tc9K+DE/pK7beY0NhWoFpGzaImZiIiIeJVzUkNNRKQeLdmawt3zt5S7PBbA36cO1/XDO0LchZC8GhLmw5j/B0Bw7hFsznzwD4EWcbU/voh4Dc0gEhEREa9TXENtYt82DO3UUskhEXFbTpfhicXbK0wO2YAnFm/H6aqoRTX0n2Z9TZgPztMAhOUmW/dF9wG7PhaKiBJEIiIiIiIijWZjcjopmXkVPm6AlMw8Np5VvLrGul8OgeGQfQR2fwNAWM4+67HoPrU/roh4FSWIREREREREGsmx7IqTQ7VpVy4f/zPFqrfMBSC0JEHUt/bHFRGvogSRiIiIiIhII2kdHFB1oxq0q1C/m62vu5ZA1mFCcg9a/9cOZiJSRAkiERERERGRRjIoLpzo0AAqqpRmA6JDrWL7dRLRDdoNAePEsfhefEwBxicAwjrU7bgi4jWUIBIREREREWkkDruNmVf0ACiTJCr+/8wretRPsf3o8wGw71trHf90Hrx8PmxfVPdji4jHU4JIRERERESkEcX3iua1qf2ICi29jCwqNIDXpvYjvld03U+yfRFsfLPs/Vkp8OHNShKJCD6N3QEREREREZGmLr5XNJf0iGJjcjrHsvNoHWwtK6uXmUMuJyx5GGtPtN8ygA2W/Bm6XwZ2R93PJyIeSQkiERERERERN+Cw2xjaqWX9H3j/d5B1pJIGBrIOW+3iRtb/+UXEI2iJmYiIiIiIiDc7ebR+24mIV1KCSERERERExJs1j6zfdiLilZQgEhERERER8WaxwyAkhrL7pBWzQUgbq52INFlKEImIiIiIiHgzuwPiZxX957dJoqL/xz+jAtUiTZwSRCIiIiIiIt6uxwS4bi6ERJe+PyTGur/HhMbpl4i4De1iJiIiIiIi0hT0mADdL+P03jUkrl1K35Hj8ek4SjOHRATQDCIREREREZGmw+7AxI7gcPhQTOwIJYdEpIQSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZwSRCIiIiIiIiIiTZxPY3fAHRhjAMjKymqQ4xcWFpKTk0NWVha+vr4Ncg6pGcXEvSge7klxcU+Ki3tRPNyT4uJ+FBP3oni4J8XF/XhLTIpzHcW5j4ooQQRkZ2cD0K5du0buiYiIiIiIiIhI/cvOziY0NLTCx22mqhRSE+ByuThy5AjBwcHYbLZ6P35WVhbt2rXj4MGDhISE1PvxpeYUE/eieLgnxcU9KS7uRfFwT4qL+1FM3Ivi4Z4UF/fjLTExxpCdnU1MTAx2e8WVhjSDCLDb7bRt27bBzxMSEuLRP1TeSDFxL4qHe1Jc3JPi4l4UD/ekuLgfxcS9KB7uSXFxP94Qk8pmDhVTkWoRERERERERkSZOCSIRERERERERkSZOCaJzwN/fn5kzZ+Lv79/YXZEiiol7UTzck+LinhQX96J4uCfFxf0oJu5F8XBPiov7aWoxUZFqEREREREREZEmTjOIRERERERERESaOCWIRERERERERESaOCWIRERERERERESaOCWIRERERERERESauCabIHr66acZOHAgwcHBtG7dmkmTJrFz585SbfLy8rjnnnto2bIlzZs35+qrr+bo0aOl2tx///30798ff39/+vbtW+k5d+/eTXBwMGFhYdXq46uvvkqHDh0ICAhg8ODBbNy4sdTje/bs4corryQiIoKQkBCuu+66Mv3zNOcqLvv27cNms5X5t2HDhir7WFVc3njjDUaPHk1ISAg2m42MjIwavw7uwBtiMXr06DLHveuuu2r+YrgRb4iL3rvq9jfFGMPzzz9P165d8ff3p02bNjz11FNV9nHhwoV0796dgIAAevfuzZdfflnq8Y8//phx48bRsmVLbDYbiYmJNXoN3Ik3xGP69Ollfv/i4+Nr9kK4GW+Iy9GjR5k+fToxMTEEBQURHx9PUlJSzV4IN3Ou4vL444+X+3elWbNmVfZRY68z3D0WGnu5Z1w09qrb35SlS5cyZMgQgoODiYiI4Oqrr2bfvn1V9tETx15NNkG0evVq7rnnHjZs2MCyZcsoLCxk3LhxnDp1qqTNH//4RxYvXszChQtZvXo1R44c4aqrripzrFtuuYXrr7++0vMVFhYyefJkRo4cWa3+ffDBBzzwwAPMnDmTLVu20KdPH8aPH8+xY8cAOHXqFOPGjcNms7FixQq+/fZbCgoKuOKKK3C5XDV4JdzLuY7LN998Q0pKSsm//v37V9q+qrgA5OTkEB8fz6OPPlrD7969eEMsAG6//fZSx3322Wdr8Cq4H0+Pi9676h6XP/zhD7z11ls8//zz/PLLLyxatIhBgwZV2r/vvvuOyZMnc+utt5KQkMCkSZOYNGkSW7duLWlz6tQpRowYwaxZs2rxCrgXb4gHQHx8fKnfvwULFtTwlXAvnh4XYwyTJk1i7969fPbZZyQkJBAbG8vYsWNLfQ+e5lzF5cEHHyz185ySkkKPHj249tprK+2fxl6eFQvQ2Mvd4qKxV93ikpyczMSJExkzZgyJiYksXbqU48ePl3ucs3ns2MuIMcaYY8eOGcCsXr3aGGNMRkaG8fX1NQsXLixps2PHDgOY9evXl3n+zJkzTZ8+fSo8/p/+9CczdepUM3v2bBMaGlplfwYNGmTuueeekv87nU4TExNjnn76aWOMMUuXLjV2u91kZmaWtMnIyDA2m80sW7asyuN7ioaKS3JysgFMQkJCjfpTVVzOtnLlSgOYEydO1Ogc7soTY3HhhReaP/zhDzU6rqfxtLjovatucdm+fbvx8fExv/zyS436c91115nLLrus1H2DBw82d955Z5m2tY29O/PEeEybNs1MnDixRsf1NJ4Wl507dxrAbN26teRxp9NpIiIizJtvvlmjc7mzhh4TF0tMTDSAWbNmTaXtNPbyrFho7GVxp7ho7FW3uCxcuND4+PgYp9NZct+iRYuMzWYzBQUFFfbHU8deTXYG0W9lZmYCEB4eDsDmzZspLCxk7NixJW26d+9O+/btWb9+fY2OvWLFChYuXMirr75arfYFBQVs3ry51Lntdjtjx44tOXd+fj42mw1/f/+SNgEBAdjtdtatW1ej/rmzhowLwIQJE2jdujUjRoxg0aJFlbatTly8mafG4t1336VVq1b06tWLRx55hJycnBr3zZ15Wlz03lW3uCxevJiOHTvy+eefExcXR4cOHbjttttIT0+v9Hnr168vdW6A8ePHN4n3LvDceKxatYrWrVvTrVs37r77btLS0qrdN0/gaXHJz88HrPesYna7HX9/f71/1cJbb71F165dK51dr7GXZ8ZCYy/3iovGXnWLS//+/bHb7cyePRun00lmZibz5s1j7Nix+Pr6Vvg8Tx17KUEEuFwuZsyYwfDhw+nVqxcAqamp+Pn5lakXFBkZSWpqarWPnZaWxvTp05kzZw4hISHVes7x48dxOp1ERkZWeO4hQ4bQrFkzHn74YXJycjh16hQPPvggTqeTlJSUavfPnTVkXJo3b84LL7zAwoUL+eKLLxgxYgSTJk2q9ANwdeLirTw1FlOmTGH+/PmsXLmSRx55hHnz5jF16tRq983deWJc9N4VVqptTeOyd+9e9u/fz8KFC5k7dy5z5sxh8+bNXHPNNZU+LzU1tUm+d4HnxiM+Pp65c+eyfPlyZs2axerVq7n00ktxOp3V7p8788S4FH+weOSRRzhx4gQFBQXMmjWLQ4cO6f2rhvLy8nj33Xe59dZbK22nsZfnxUJjrzPcJS4ae4WValvTuMTFxfH111/z6KOP4u/vT1hYGIcOHeLDDz+s9HmeOvZSggi455572Lp1K++//369H/v2229nypQpjBo1qtzH165dS/PmzUv+vfvuu9U6bkREBAsXLmTx4sU0b96c0NBQMjIy6NevH3a7d4S1IePSqlUrHnjgAQYPHszAgQN55plnmDp1Ks899xxQ+7h4K0+NxR133MH48ePp3bs3N954I3PnzuWTTz5hz5499f59NAZPjIveu+rG5XKRn5/P3LlzGTlyJKNHj+Y///kPK1euZOfOnRw4cKBUXP7xj3/Uex88jafG44YbbmDChAn07t2bSZMm8fnnn/PDDz+watWqev8+GoMnxsXX15ePP/6YXbt2ER4eTlBQECtXruTSSy/V+1cNffLJJ2RnZzNt2rSS+zT2Ks1TY6GxV/2oz7ho7FU3qamp3H777UybNo0ffviB1atX4+fnxzXXXIMxxuvGXj6N3YHGdu+99/L555+zZs0a2rZtW3J/VFQUBQUFZGRklMo6Hj16lKioqGoff8WKFSxatIjnn38esAoculwufHx8eOONN5g8eXKpauWRkZH4+/vjcDjKVFj/7bnHjRvHnj17OH78OD4+PoSFhREVFUXHjh1r+Cq4n4aOS3kGDx7MsmXLABgwYECt4+JtvCkWgwcPBqwdBTt16lSnPjY2T46L3rvCSu6vaVyio6Px8fGha9euJfedd955ABw4cICLLrqoVFyKp1lHRUU1ufcu8K54dOzYkVatWrF7924uvvjiavfRHXlyXPr3709iYiKZmZkUFBQQERHB4MGDGTBgQLX7567O5d+Vt956i8svv7zU1XWNvc7wplho7OUecdHYK6zk/prG5dVXXyU0NLRUsfX58+fTrl07vv/++zJx8fSxl3ekDGvBGMO9997LJ598wooVK4iLiyv1eP/+/fH19WX58uUl9xVfdRo6dGi1z7N+/XoSExNL/j355JMEBweTmJjIlVdeSWBgIJ07dy75FxwcjJ+fH/379y91bpfLxfLly8s9d6tWrQgLC2PFihUcO3aMCRMm1OIVcQ/nKi7lSUxMJDo6GqBe4uLpvDEWxW/excf2RN4UF7131Twuw4cP5/Tp06WuxO7atQuA2NhYfHx8SsWleJAydOjQUucGWLZsmVe+d4F3xuPQoUOkpaXp/asazkVcQkNDiYiIICkpiU2bNjFx4sRq98/dnOu/K8nJyaxcubLM0hmNvbwzFhp7uVdcNPaqeVxycnLKzLRyOBwAJRM/vGrs1Silsd3A3XffbUJDQ82qVatMSkpKyb+cnJySNnfddZdp3769WbFihdm0aZMZOnSoGTp0aKnjJCUlmYSEBHPnnXearl27moSEBJOQkGDy8/PLPW91dzF7//33jb+/v5kzZ47Zvn27ueOOO0xYWJhJTU0tafP222+b9evXm927d5t58+aZ8PBw88ADD9TuBXET5youc+bMMe+9957ZsWOH2bFjh3nqqaeM3W43b7/9dqX9q05cUlJSTEJCgnnzzTdLdh5ISEgwaWlp9fhKNTxPj8Xu3bvNk08+aTZt2mSSk5PNZ599Zjp27GhGjRpVz6/UueXpcTFG7111iYvT6TT9+vUzo0aNMlu2bDGbNm0ygwcPNpdcckml/fv222+Nj4+Pef75582OHTvMzJkzja+vr/n5559L2qSlpZmEhATzxRdfGMC8//77JiEhwaSkpNTjK3VueHo8srOzzYMPPmjWr19vkpOTzTfffGP69etnunTpYvLy8ur51Tp3PD0uxhjz4YcfmpUrV5o9e/aYTz/91MTGxpqrrrqqHl+lc+9cj4kfe+wxExMTY06fPl2t/mns5Tmx0NjLPeNijMZedYnL8uXLjc1mM0888YTZtWuX2bx5sxk/fryJjY0tda7f8tSxV5NNEAHl/ps9e3ZJm9zcXPP73//etGjRwgQFBZkrr7yyTLAuvPDCco+TnJxc7nmrmyAyxpj//d//Ne3btzd+fn5m0KBBZsOGDaUef/jhh01kZKTx9fU1Xbp0MS+88IJxuVw1eRnczrmKy5w5c8x5551ngoKCTEhIiBk0aFCpLRArU1VcZs6cWeX34Ak8PRYHDhwwo0aNMuHh4cbf39907tzZPPTQQ6W2+PREnh4XY/TeVde/KYcPHzZXXXWVad68uYmMjDTTp0+v1oegDz/80HTt2tX4+fmZnj17mi+++KLU47Nnzy733DNnzqzLS9MoPD0eOTk5Zty4cSYiIsL4+vqa2NhYc/vtt5ca7HsiT4+LMca89NJLpm3btsbX19e0b9/ePPbYYxVeFPQU5zIuTqfTtG3b1jz66KM16qPGXrNL2rhzLDT2cs+4GKOxV13jsmDBAnPBBReYZs2amYiICDNhwgSzY8eOKvvoiWMvmzHGICIiIiIiIiIiTVaTrUEkIiIiIiIiIiIWJYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJo4JYhERERERERERJq4/x8ySRSM4yZl3wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_LSTM.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by LSTM RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WfqFMJLan26"
      },
      "source": [
        "---\n",
        "---\n",
        "## 7 Comaprison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "R7E2YreFari1",
        "outputId": "c8fd89d0-5548-443c-b48f-24e64ab5b234"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_RNN.detach().cpu(), label=\"predicted_RNN\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_GRU.detach().cpu(), label=\"predicted_GRU\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_LSTM.detach().cpu(), label=\"predicted_LSTM\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McCOcrqqhXkt"
      },
      "source": [
        "\n",
        "<b>1-Rank these architectures based on their performance?\n",
        "\n",
        "2-Why are they ranked in this order?\n",
        "\n",
        "3-Run the notebook again with look_back = 15.\n",
        "write about the difference in the comparison plot and the possible cause for that difference.</b>\n",
        "\n",
        "<font color='#73FF73'><b>Your answer:</b></font>\n",
        "\n",
        "\n",
        "**Q1** :1-LSTM  ... 2-GRU ...  3_RNN\n",
        "\n",
        "**Q2**: Because while RNNs are simpler and can be used effectively on short sequences, they struggle with longer sequences due to the vanishing gradient problem. LSTMs and GRUs, on the other hand, are designed to combat this issue and work well on longer sequences, with LSTMs being more complex and GRUs providing a simpler and faster alternative.\n",
        "\n",
        "**Q3**: If we reduce the dependancy of prediction on past datas the RNN perform better thatn before because its problem about forgeting the long sequences and vanishing gradients become much better."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "r7FFJaRgaqC1",
        "prWan6z7eKEp",
        "u78V0kKvjp8f",
        "nIfT0qVkOTzG",
        "uwmm5DhEBl8d",
        "5Ay_sL8ZBq1R",
        "utvDPP4kXEI-",
        "fkIg9lbrXiKZ",
        "-SlbDcQ-XuYx",
        "xIIm3f8yX9XF",
        "5Jcdsct5Y9a8",
        "mDMYeUTH5Ki7",
        "6WfqFMJLan26"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
